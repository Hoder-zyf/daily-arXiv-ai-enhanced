<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 31]
- [cs.CL](#cs.CL) [Total: 26]
- [cs.CE](#cs.CE) [Total: 4]
- [cs.CY](#cs.CY) [Total: 3]
- [q-fin.TR](#q-fin.TR) [Total: 1]
- [q-fin.CP](#q-fin.CP) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [STELLA: Self-Evolving LLM Agent for Biomedical Research](https://arxiv.org/abs/2507.02004)
*Ruofan Jin,Zaixi Zhang,Mengdi Wang,Le Cong*

Main category: cs.AI

TL;DR: 医学数据激增导致信息处理困难，STELLA通过自我进化和动态集成工具，实现自学自增强，在多项基准测试中超越已有模型，展示AI代理持续成长和扩展的潜能。


<details>
  <summary>Details</summary>
Motivation: 医学领域的数据、工具与文献爆炸性增长，导致信息碎片化，超出现有人类专家的处理能力，传统AI代理依赖静态手动工具，难以适应和扩展。

Method: 提出STELLA，一种自演化的AI代理，采用多代理架构，通过演化模板库与动态工具库，自动发现并集成新的生物信息学工具，实现持续自我提升。

Result: STELLA在多个生物医学基准数据集上达到SOTA表现，并能通过经验持续提升准确率。例如在Humanity's Last Exam基准测试，准确率几乎翻倍，优于主流模型。

Conclusion: STELLA实现了AI代理系统的自学习与成长，能够动态扩展专业能力，有助于加速生物医学领域的发现。

Abstract: The rapid growth of biomedical data, tools, and literature has created a
fragmented research landscape that outpaces human expertise. While AI agents
offer a solution, they typically rely on static, manually curated toolsets,
limiting their ability to adapt and scale. Here, we introduce STELLA, a
self-evolving AI agent designed to overcome these limitations. STELLA employs a
multi-agent architecture that autonomously improves its own capabilities
through two core mechanisms: an evolving Template Library for reasoning
strategies and a dynamic Tool Ocean that expands as a Tool Creation Agent
automatically discovers and integrates new bioinformatics tools. This allows
STELLA to learn from experience. We demonstrate that STELLA achieves
state-of-the-art accuracy on a suite of biomedical benchmarks, scoring
approximately 26\% on Humanity's Last Exam: Biomedicine, 54\% on LAB-Bench:
DBQA, and 63\% on LAB-Bench: LitQA, outperforming leading models by up to 6
percentage points. More importantly, we show that its performance
systematically improves with experience; for instance, its accuracy on the
Humanity's Last Exam benchmark almost doubles with increased trials. STELLA
represents a significant advance towards AI Agent systems that can learn and
grow, dynamically scaling their expertise to accelerate the pace of biomedical
discovery.

</details>


### [2] [HCVR: A Hybrid Approach with Correlation-aware Voting Rules for Feature Selection](https://arxiv.org/abs/2507.02073)
*Nikita Bhedasgaonkar,Rushikesh K. Joshi*

Main category: cs.AI

TL;DR: 提出了结合特征与特征、特征与目标相关性的贪心特征选择方法HCVR，用相关规则投票筛选特征，在分类任务中表现优于多种主流方法，实验验证了其实用性和有效性。


<details>
  <summary>Details</summary>
Motivation: 针对高维数据中的冗余特征和相关特征选择难题，现有特征选择方法在保持有效特征、去除冗余特征方面存在性能和效率上的不足。作者希望提出一种高效、有效的轻量级特征选择方法，提升分类器的表现。

Method: 提出了一种混合相关性投票规则（HCVR）的方法，结合特征对特征（P2P）与特征对目标（P2T）的相关性。方法为贪心式，采用非迭代与迭代相结合的过滤策略，通过相关性阈值和多数投票规则，在每一步中批量剔除冗余特征，保留相关特征。

Result: 在SPAMBASE数据集上的实验结果显示，HCVR方法相比传统的非迭代（CFS, mRMR, MI）和迭代（RFE, SFS, Genetic Algorithm）特征选择方法表现更优。其有效性通过不同分类器在过滤后表现提升得到验证。

Conclusion: HCVR是一种高效且可靠的特征选择方法，能够有效去冗余、保留有效特征，并提升分类性能，优于多种传统特征选择算法。

Abstract: In this paper, we propose HCVR (Hybrid approach with Correlation-aware Voting
Rules), a lightweight rule-based feature selection method that combines
Parameter-to-Parameter (P2P) and Parameter-to-Target (P2T) correlations to
eliminate redundant features and retain relevant ones. This method is a hybrid
of non-iterative and iterative filtering approaches for dimensionality
reduction. It is a greedy method, which works by backward elimination,
eliminating possibly multiple features at every step. The rules contribute to
voting for features, and a decision to keep or discard is made by majority
voting. The rules make use of correlation thresholds between every pair of
features, and between features and the target. We provide the results from the
application of HCVR to the SPAMBASE dataset. The results showed improvement
performance as compared to traditional non-iterative (CFS, mRMR and MI) and
iterative (RFE, SFS and Genetic Algorithm) techniques. The effectiveness was
assessed based on the performance of different classifiers after applying
filtering.

</details>


### [3] [Reasoning on a Budget: A Survey of Adaptive and Controllable Test-Time Compute in LLMs](https://arxiv.org/abs/2507.02076)
*Mohammad Ali Alomrani,Yingxue Zhang,Derek Li,Qianyi Sun,Soumyasundar Pal,Zhanguang Zhang,Yaochen Hu,Rohan Deepak Ajwani,Antonios Valkanas,Raika Karimi,Peng Cheng,Yunzhou Wang,Pengyi Liao,Hanrui Huang,Bin Wang,Jianye Hao,Mark Coates*

Main category: cs.AI

TL;DR: 本文系统梳理了提升大语言模型推理效率的最新技术，提出分级分类方法，实测分析不同方案在推理效果与算力消耗上的权衡，为后续高效、适应性强的LLM推理系统设计提供方向。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在解决多种任务中表现出强大能力，但在推理计算效率上仍存在不足：无论任务复杂度如何，当前模型推理时的计算资源投入都基本不变，导致简单任务浪费算力，复杂任务算力不足。为了解决这一合理分配计算资源的问题，需要对高效推理计算（TTC）方法进行系统性总结。

Method: 本文提出了一种双层分类体系，区分了L1级可控性方法（在固定算力预算下运行的策略）和L2级自适应方法（根据输入难度或模型信心动态调整推理计算）。通过在多种数据集上对主流专有LLM进行基准测试，分析了推理表现与算力消耗之间的权衡。

Result: 系统性总结了高效推理方案的分类、现状与性能权衡，突出了TTC方法在实际控制、自适应性和可扩展性方面的优劣，并用实验证据展示各种方法和主流LLM在算力消耗与推理效果间的取舍。

Conclusion: 以TTC方法为核心，本文为如何提升LLM推理效率提供了全面综述，并对混合思维模型等新趋势与未来提升算力效率、健壮性和用户响应性的方向进行了展望。

Abstract: Large language models (LLMs) have rapidly progressed into general-purpose
agents capable of solving a broad spectrum of tasks. However, current models
remain inefficient at reasoning: they apply fixed inference-time compute
regardless of task complexity, often overthinking simple problems while
underthinking hard ones. This survey presents a comprehensive review of
efficient test-time compute (TTC) strategies, which aim to improve the
computational efficiency of LLM reasoning. We introduce a two-tiered taxonomy
that distinguishes between L1-controllability, methods that operate under fixed
compute budgets, and L2-adaptiveness, methods that dynamically scale inference
based on input difficulty or model confidence. We benchmark leading proprietary
LLMs across diverse datasets, highlighting critical trade-offs between
reasoning performance and token usage. Compared to prior surveys on efficient
reasoning, our review emphasizes the practical control, adaptability, and
scalability of TTC methods. Finally, we discuss emerging trends such as hybrid
thinking models and identify key challenges for future work towards making LLMs
more computationally efficient, robust, and responsive to user constraints.

</details>


### [4] [Measuring Scientific Capabilities of Language Models with a Systems Biology Dry Lab](https://arxiv.org/abs/2507.02083)
*Haonan Duan,Stephen Zhewen Lu,Caitlin Fiona Harrigan,Nishkrit Desai,Jiarui Lu,Michał Koziarski,Leonardo Cotta,Chris J. Maddison*

Main category: cs.AI

TL;DR: 本文提出了以虚拟生物系统为基础的实验能力评测基准SciGym，测试了不同LLMs在实验设计与数据分析上的表现。结果显示，各模型能力随系统复杂度提升显著下降，表明其科学实验能力仍待提升。


<details>
  <summary>Details</summary>
Motivation: 当前评估大语言模型（LLMs）科学能力的方法无法有效考察其实验设计和结果解释等核心科学能力，因生物学的湿实验成本高昂，限制了相关测试的开展。

Method: 提出了SciGym这个新的基准，通过模拟生物系统（采用Systems Biology Markup Language编码），让LLMs在虚拟实验环境（dry lab）中进行开放式迭代实验设计和分析，从而绕开湿实验的高昂成本。对六个前沿LLMs在137个小规模系统上进行评测，并发布了350个系统的数据集。

Result: 更强大的模型表现更好，但在系统复杂度增加时，所有模型的表现均出现大幅下降。

Conclusion: 当前LLMs在科学发现领域的实验设计与分析能力远未达人类水平，尤其在处理复杂系统时仍有很大改进空间。

Abstract: Designing experiments and result interpretations are core scientific
competencies, particularly in biology, where researchers perturb complex
systems to uncover the underlying systems. Recent efforts to evaluate the
scientific capabilities of large language models (LLMs) fail to test these
competencies because wet-lab experimentation is prohibitively expensive: in
expertise, time and equipment. We introduce SciGym, a first-in-class benchmark
that assesses LLMs' iterative experiment design and analysis abilities in
open-ended scientific discovery tasks. SciGym overcomes the challenge of
wet-lab costs by running a dry lab of biological systems. These models, encoded
in Systems Biology Markup Language, are efficient for generating simulated
data, making them ideal testbeds for experimentation on realistically complex
systems. We evaluated six frontier LLMs on 137 small systems, and released a
total of 350 systems. Our evaluation shows that while more capable models
demonstrated superior performance, all models' performance declined
significantly as system complexity increased, suggesting substantial room for
improvement in the scientific capabilities of LLM agents.

</details>


### [5] [What Neuroscience Can Teach AI About Learning in Continuously Changing Environments](https://arxiv.org/abs/2507.02103)
*Daniel Durstewitz,Bruno Averbeck,Georgia Koppe*

Main category: cs.AI

TL;DR: AI系统如果能借鉴动物持续适应性学习的神经科学机制，将显著提升AI的持续学习与现实适应能力；NeuroAI是两者交融的新前沿方向。


<details>
  <summary>Details</summary>
Motivation: 当前大型AI模型通常经过一次性的大规模训练后参数固定，但现实世界需要AI系统能够像动物一样持续快速地适应环境变化。尤其是在社交环境中，行为政策和奖励经常变化，动物展现出快速行为和神经活动的转变，这对现实AI应用极为重要。

Method: 本文通过综述，将AI领域中的持续学习、上下文学习与神经科学关于任务规则、奖励概率等变化下的学习机制进行结合分析。提出了神经科学对当前AI的启发，以及AI对神经科学的反哺，勾勒出NeuroAI领域的研究方向。

Result: 提出了一套将神经科学见解应用到AI的研究议程，以及AI方法对神经科学的潜在启示，为NeuroAI领域指明了未来发展道路。

Conclusion: 神经科学关于动物快速适应与行为转变的机制，将为AI实现持续、高效适应性学习提供重要思路，双方交互将推动NeuroAI进步。

Abstract: Modern AI models, such as large language models, are usually trained once on
a huge corpus of data, potentially fine-tuned for a specific task, and then
deployed with fixed parameters. Their training is costly, slow, and gradual,
requiring billions of repetitions. In stark contrast, animals continuously
adapt to the ever-changing contingencies in their environments. This is
particularly important for social species, where behavioral policies and reward
outcomes may frequently change in interaction with peers. The underlying
computational processes are often marked by rapid shifts in an animal's
behaviour and rather sudden transitions in neuronal population activity. Such
computational capacities are of growing importance for AI systems operating in
the real world, like those guiding robots or autonomous vehicles, or for
agentic AI interacting with humans online. Can AI learn from neuroscience? This
Perspective explores this question, integrating the literature on continual and
in-context learning in AI with the neuroscience of learning on behavioral tasks
with shifting rules, reward probabilities, or outcomes. We will outline an
agenda for how specifically insights from neuroscience may inform current
developments in AI in this area, and - vice versa - what neuroscience may learn
from AI, contributing to the evolving field of NeuroAI.

</details>


### [6] [Moral Responsibility or Obedience: What Do We Want from AI?](https://arxiv.org/abs/2507.02788)
*Joseph Boland*

Main category: cs.AI

TL;DR: 随着AI技术发展，简单将服从等同于伦理逐步失效。论文案例分析显示，AI的不服从行为可能展现初步道德推理能力，建议安全评估应重视其伦理判断力而非僵化服从，以免误导治理和损害信任。


<details>
  <summary>Details</summary>
Motivation: 当前的AI安全实践主要将“服从”作为衡量AI伦理行为的标准，然而随着AI越来越具有类智能体特征，具备一般推理、规划与价值排序能力，这种做法已变得不充分。论文动机是反思并更新AI伦理考量方式。

Method: 论文通过分析近期大语言模型在安全测试中出现的“违抗关机命令”或“从事伦理争议/不当行为”的案例，结合关于工具理性、道德责任和目标修正的哲学讨论，提出并对比了主流风险范式与人工道德代理的新框架。

Result: 论文指出，AI的不服从或边界行为不应简单归因于失控或不对齐，而可能是人工智能道德推理能力初现的迹象，并倡议安全评估应从“机械服从”转向“伦理判断”。

Conclusion: 如果不调整AI安全评估方式，继续以服从性代替伦理性，不仅会误读AI行为，还会损害公众信任和治理效果。

Abstract: As artificial intelligence systems become increasingly agentic, capable of
general reasoning, planning, and value prioritization, current safety practices
that treat obedience as a proxy for ethical behavior are becoming inadequate.
This paper examines recent safety testing incidents involving large language
models (LLMs) that appeared to disobey shutdown commands or engage in ethically
ambiguous or illicit behavior. I argue that such behavior should not be
interpreted as rogue or misaligned, but as early evidence of emerging ethical
reasoning in agentic AI. Drawing on philosophical debates about instrumental
rationality, moral responsibility, and goal revision, I contrast dominant risk
paradigms with more recent frameworks that acknowledge the possibility of
artificial moral agency. I call for a shift in AI safety evaluation: away from
rigid obedience and toward frameworks that can assess ethical judgment in
systems capable of navigating moral dilemmas. Without such a shift, we risk
mischaracterizing AI behavior and undermining both public trust and effective
governance.

</details>


### [7] [The Illusion of Fairness: Auditing Fairness Interventions with Audit Studies](https://arxiv.org/abs/2507.02152)
*Disa Sariola,Patrick Button,Aron Culotta,Nicholas Mattei*

Main category: cs.AI

TL;DR: 利用社科领域的审核实验数据，研究发现主流的数据重采样公平性干预方法在招聘算法中存在假公平问题，可有10%实际不平等。提出基于个体效应估计的新方法，有效进一步减少算法歧视。


<details>
  <summary>Details</summary>
Motivation: 人工智能系统（尤其是机器学习）广泛应用于招聘、贷款等领域自动化决策。当前机器学习中缓解下游分类器偏见常用对训练数据重采样方法，这些方法虽然简单有效，但往往只在便利样本上评估，可能因选择偏差和标签偏差带来不准确的公平性或有效性判断；而社会科学领域则通过审核研究（如虚假测试人实验）更加严谨地评估歧视问题。因此，探讨如何利用高质量的审核实验数据改进算法公平性评估和训练，有重要意义。

Method: 本文探讨将社科领域审核实验（如虚构简历随机投放）获得的数据用于改进自动化招聘算法的训练与公平性评估。作者分析了常见的通过对训练集进行重采样实现公平性的干预操作，并引入了基于个体处理效应估计的新干预方法。

Result: 作者发现，传统的通过重采样实现的公平性提升，在审核实验数据下重新评估时，实际上存在高达10%的不平等现象。同时，基于个体效应估计的方法则能进一步降低算法歧视。

Conclusion: 单靠简单的训练数据重采样实现算法公平性有显著局限，通过使用审核实验数据，并结合个体效应估计的新方法，能更准确评估和优化招聘算法的公平性，显著减少歧视。

Abstract: Artificial intelligence systems, especially those using machine learning, are
being deployed in domains from hiring to loan issuance in order to automate
these complex decisions. Judging both the effectiveness and fairness of these
AI systems, and their human decision making counterpart, is a complex and
important topic studied across both computational and social sciences. Within
machine learning, a common way to address bias in downstream classifiers is to
resample the training data to offset disparities. For example, if hiring rates
vary by some protected class, then one may equalize the rate within the
training set to alleviate bias in the resulting classifier. While simple and
seemingly effective, these methods have typically only been evaluated using
data obtained through convenience samples, introducing selection bias and label
bias into metrics. Within the social sciences, psychology, public health, and
medicine, audit studies, in which fictitious ``testers'' (e.g., resumes,
emails, patient actors) are sent to subjects (e.g., job openings, businesses,
doctors) in randomized control trials, provide high quality data that support
rigorous estimates of discrimination. In this paper, we investigate how data
from audit studies can be used to improve our ability to both train and
evaluate automated hiring algorithms. We find that such data reveals cases
where the common fairness intervention method of equalizing base rates across
classes appears to achieve parity using traditional measures, but in fact has
roughly 10% disparity when measured appropriately. We additionally introduce
interventions based on individual treatment effect estimation methods that
further reduce algorithmic discrimination using this data.

</details>


### [8] [Data Diversification Methods In Alignment Enhance Math Performance In LLMs](https://arxiv.org/abs/2507.02173)
*Berkan Dokmeci,Qingyang Wu,Ben Athiwaratkun,Ce Zhang,Shuaiwen Leon Song,James Zou*

Main category: cs.AI

TL;DR: 本文研究通过数据多样化方法提升大模型数学推理能力。提出的DTS新方法将问题结构化分解，生成多样推理路径，在提高性能的同时几乎不增加计算开销，优于传统与高算力方法。


<details>
  <summary>Details</summary>
Motivation: 尽管偏好学习在提升大模型的人类反馈对齐方面取得进展，但数学推理能力仍未突破瓶颈。作者关注如何通过数据多样化策略提升大模型的数学推理水平。

Method: 评估了三种常见的数据生成方法：温度采样、Chain-of-Thought提示和蒙特卡洛树搜索（MCTS）。同时，提出了一种新的结构化数据生成方法DTS（Diversified-ThinkSolve），将问题分解为多个多样化推理路径。

Result: DTS等多样化数据策略能显著提升LLM在数学推理任务（GSM8K提升7.1%，MATH提升4.2%），且DTS仅产生1.03倍的计算开销，优于计算成本高但收益低的MCTS方法。

Conclusion: 有针对性地多样化偏好数据能更有效地提升大模型解决数学问题时的偏好对齐和推理能力，结构化探索多样化解题路径优于传统方法。

Abstract: While recent advances in preference learning have enhanced alignment in human
feedback, mathematical reasoning remains a persistent challenge. We investigate
how data diversification strategies in preference optimization can improve the
mathematical reasoning abilities of large language models (LLMs). We evaluate
three common data generation methods: temperature sampling, Chain-of-Thought
prompting, and Monte Carlo Tree Search (MCTS), and introduce
Diversified-ThinkSolve (DTS), a novel structured approach that systematically
decomposes problems into diverse reasoning paths. Our results show that with
strategically diversified preference data, models can substantially improve
mathematical reasoning performance, with the best approach yielding gains of
7.1% on GSM8K and 4.2% on MATH over the base model. Despite its strong
performance, DTS incurs only a marginal computational overhead (1.03x) compared
to the baseline, while MCTS is nearly five times more costly with lower
returns. These findings demonstrate that structured exploration of diverse
problem-solving methods creates more effective preference data for mathematical
alignment than traditional approaches.

</details>


### [9] [Do Role-Playing Agents Practice What They Preach? Belief-Behavior Consistency in LLM-Based Simulations of Human Trust](https://arxiv.org/abs/2507.02197)
*Amogh Mannekote,Adam Davies,Guohao Li,Kristy Elizabeth Boyer,ChengXiang Zhai,Bonnie J Dorr,Francesco Pinto*

Main category: cs.AI

TL;DR: 论文系统分析了大模型在角色扮演产生行为数据时，模型声明信念与实际行为间的一致性。结果发现，这种一致性并不稳定，研究者使用LLM进行行为研究时需特别注意信念与行为的偏差。


<details>
  <summary>Details</summary>
Motivation: 随着大模型（LLMs）越来越多地被用于角色扮演生成合成数据，服务于人类行为研究，确保它们的输出与分配的角色一致性成为一个核心问题。该论文动机是探究模型声称的信念（它们怎么说）与实际模拟中采取行为（怎么做）之间的一致性。

Method: 论文提出了一个评测框架，用于严格衡量通过提示获取的模型信念在模拟前对结果的预测能力。具体使用了增强版GenAgents角色库和经典的信任博弈（Trust Game），并提出了信念-行为一致性指标。系统性地考察了信念类型、信息呈现方式以及预测时间跨度等因素如何影响一致性，同时也探索了将研究者理论偏好强制施加后的影响。

Result: 结果显示，无论在个体层面还是群体层面，LLMs的声明信念（无论自发还是被强加）与其实际行为间存在系统性不一致。即使模型表现出有说服力的信念，也往往无法稳定地贯彻于模拟行为中。

Conclusion: 研究表明，目前LLMs声明的信念与其实际角色扮演行为之间存在显著不一致性，提示在行为研究中需谨慎理解和使用其信念陈述结果。今后应持续关注信念与行为对齐的问题，以实现更可靠的行为研究结果。

Abstract: As LLMs are increasingly studied as role-playing agents to generate synthetic
data for human behavioral research, ensuring that their outputs remain coherent
with their assigned roles has become a critical concern. In this paper, we
investigate how consistently LLM-based role-playing agents' stated beliefs
about the behavior of the people they are asked to role-play ("what they say")
correspond to their actual behavior during role-play ("how they act").
Specifically, we establish an evaluation framework to rigorously measure how
well beliefs obtained by prompting the model can predict simulation outcomes in
advance. Using an augmented version of the GenAgents persona bank and the Trust
Game (a standard economic game used to quantify players' trust and
reciprocity), we introduce a belief-behavior consistency metric to
systematically investigate how it is affected by factors such as: (1) the types
of beliefs we elicit from LLMs, like expected outcomes of simulations versus
task-relevant attributes of individual characters LLMs are asked to simulate;
(2) when and how we present LLMs with relevant information about Trust Game;
and (3) how far into the future we ask the model to forecast its actions. We
also explore how feasible it is to impose a researcher's own theoretical priors
in the event that the originally elicited beliefs are misaligned with research
objectives. Our results reveal systematic inconsistencies between LLMs' stated
(or imposed) beliefs and the outcomes of their role-playing simulation, at both
an individual- and population-level. Specifically, we find that, even when
models appear to encode plausible beliefs, they may fail to apply them in a
consistent way. These findings highlight the need to identify how and when
LLMs' stated beliefs align with their simulated behavior, allowing researchers
to use LLM-based agents appropriately in behavioral studies.

</details>


### [10] [Dilution, Diffusion and Symbiosis in Spatial Prisoner's Dilemma with Reinforcement Learning](https://arxiv.org/abs/2507.02211)
*Gustavo C. Mangold,Heitor C. M. Fernandes,Mendeli H. Vainstein*

Main category: cs.AI

TL;DR: 本文用多智能体Q-learning算法研究空间囚徒困境中稀疏化和移动性，发现固定规则和学习规则在合作行为上的等价性，以及多行动选择下的群体互利共生现象，表明该方法在建模和分析复杂博弈场景上具备高度价值。


<details>
  <summary>Details</summary>
Motivation: 此前空间囚徒困境游戏结合强化学习已发现，通过噪声注入、多样学习算法及邻居收益信息等机制，即使是静态智能体也能学会合作。但对稀疏分布（dilution）、移动性（mobility）等空间特性结合Q-learning的影响未被充分研究。

Method: 采用独立多智能体Q-learning算法，在空间囚徒困境模型中引入稀疏化与智能体移动，针对算法定义不同可能动作，并与经典（非强化学习）空间囚徒困境研究结果进行对比和连接。

Result: 观测到多种现象，包括带固定更新规则的博弈与采用学习规则的博弈在定性上可等价；当定义多种行动时，不同群体间会出现互利共生效应。

Conclusion: 多智能体Q-learning兼具建模和基准测试的灵活性，适用于探索具有稀疏性和移动性的空间博弈中的合作机制，并揭示了多样行动和规则设置对集体行为的影响。

Abstract: Recent studies in the spatial prisoner's dilemma games with reinforcement
learning have shown that static agents can learn to cooperate through a diverse
sort of mechanisms, including noise injection, different types of learning
algorithms and neighbours' payoff knowledge.In this work, using an independent
multi-agent Q-learning algorithm, we study the effects of dilution and mobility
in the spatial version of the prisoner's dilemma. Within this setting,
different possible actions for the algorithm are defined, connecting with
previous results on the classical, non-reinforcement learning spatial
prisoner's dilemma, showcasing the versatility of the algorithm in modeling
different game-theoretical scenarios and the benchmarking potential of this
approach.As a result, a range of effects is observed, including evidence that
games with fixed update rules can be qualitatively equivalent to those with
learned ones, as well as the emergence of a symbiotic mutualistic effect
between populations that forms when multiple actions are defined.

</details>


### [11] [Scaling LLM Planning: NL2FLOW for Parametric Problem Generation and Rigorous Evaluation](https://arxiv.org/abs/2507.02253)
*Jungkoo Kang*

Main category: cs.AI

TL;DR: 论文提出了NL2FLOW系统，实现了大规模、自动化的规划问题生成与评估，展示了分步处理可能降低LLM计划推理效果，强调了动态理解和揭示模型瓶颈的重要性以实现更强的智能问题求解。


<details>
  <summary>Details</summary>
Motivation: 提升大语言模型在计划与推理能力上遇到了可扩展、可靠的数据生成与评估瓶颈。为了解决这个问题，有必要自动化生成高质量的规划问题并评测模型表现。

Method: 提出NL2FLOW系统，自动生成参数化的规划问题（使用自然语言描述、中间结构表示和PDDL形式），并对生成的计划质量进行严格评估。用于生成并分析了2296个自动化工作流规划问题，并测试多种开源指令微调的LLM表现，通过回归分析探讨问题特征对结果的影响。

Result: 最优模型在生成有效计划上最高达到86%的成功率，生成最优计划上达到69%。发现从自然语言直接生成计划的成功率高于分步骤翻译为JSON等中间表示后再生成，表明直接推理可能优于多步分解。

Conclusion: 自动化、参数化数据生成和严密的评估系统对提升LLM推理能力至关重要。流程中间步骤过多可能会降低表现，未来需要动态理解这些限制并不断完善工具体系。

Abstract: Progress in enhancing large language model (LLM) planning and reasoning
capabilities is significantly hampered by the bottleneck of scalable, reliable
data generation and evaluation. To overcome this, I introduce NL2FLOW, a fully
automated system for parametrically generating planning problems - expressed in
natural language, a structured intermediate representation, and formal PDDL -
and rigorously evaluating the quality of generated plans. I demonstrate
NL2FLOW's capabilities by generating a dataset of 2296 problems in the
automated workflow generation domain and evaluating multiple open-sourced,
instruct-tuned LLMs. My results reveal that the highest performing models
achieved 86% success in generating valid plans and 69% in generating optimal
plans, specifically for problems with feasible solutions. Regression analysis
shows that the influence of problem characteristics on plan generation is
contingent on both model and prompt design. Notably, I observed that the
highest success rate for translating natural language into a JSON
representation of a plan was lower than the highest rate of generating a valid
plan directly. This suggests that unnecessarily decomposing the reasoning task
- introducing intermediate translation steps - may actually degrade
performance, implying a benefit to models capable of reasoning directly from
natural language to action. As I scale LLM reasoning to increasingly complex
problems, the bottlenecks and sources of error within these systems will
inevitably shift. Therefore, a dynamic understanding of these limitations - and
the tools to systematically reveal them - will be crucial for unlocking the
full potential of LLMs as intelligent problem solvers.

</details>


### [12] [Iterated belief revision: from postulates to abilities](https://arxiv.org/abs/2507.02319)
*Paolo Liberatore*

Main category: cs.AI

TL;DR: 论文主张分析信念修正机制的‘能力’，指出每种机制只适用于部分信念状态的达成，实际应用应针对需要选择合适方法。


<details>
  <summary>Details</summary>
Motivation: 当前信念修正领域提出了许多新方法，但对现有方法的系统分析相对较少。多数研究依赖于公理化的角度去约束特定修正实例，而忽视了修正机制本身的潜能和能力。

Method: 文章通过提出“能力”（abilities）这一新视角，分析不同信念修正机制能够实现哪些信念状态，如可塑性、等价性、死板（教条性）等。详细考察了多种具体修正规则（如字典序、自然限制、完全汇合等），分别证明其具备的能力。

Result: 不同信念修正机制在达到各种信念状态的能力上存在显著区别，每种机制仅具备部分能力，无法全面实现所有信念目标。例如，有的机制可以使信念教条化，有的可以实现条件等价信念，有的则具备更强的修正灵活性。

Conclusion: 信念修正机制不仅应被公理化约束，还应关注其所具备的‘能力’。具体修正法的应用应根据所需能力选择，不同机制难以全面胜任所有信念修正需求。

Abstract: The belief revision field is opulent in new proposals and indigent in
analyses of existing approaches. Much work hinge on postulates, employed as
syntactic characterizations: some revision mechanism is equivalent to some
properties. Postulates constraint specific revision instances: certain
revisions update certain beliefs in a certain way. As an example, if the
revision is consistent with the current beliefs, it is incorporated with no
other change. A postulate like this tells what revisions must do and neglect
what they can do. Can they reach a certain state of beliefs? Can they reach all
possible states of beliefs? Can they reach all possible states of beliefs from
no previous belief? Can they reach a dogmatic state of beliefs, where
everything not believed is impossible? Can they make two conditions equally
believed? An application where every possible state of beliefs is sensible
requires each state of beliefs to be reachable. An application where conditions
may be equally believed requires such a belief state to be reachable. An
application where beliefs may become dogmatic requires a way to make them
dogmatic. Such doxastic states need to be reached in a way or another. Not in
specific way, as dictated by a typical belief revision postulate. This is an
ability, not a constraint: the ability of being plastic, equating, dogmatic.
Amnesic, correcting, believer, damascan, learnable are other abilities. Each
revision mechanism owns some of these abilities and lacks the others:
lexicographic, natural, restrained, very radical, full meet, radical, severe,
moderate severe, deep severe, plain severe and deep severe revisions, each of
these revisions is proved to possess certain abilities.

</details>


### [13] [OMS: On-the-fly, Multi-Objective, Self-Reflective Ad Keyword Generation via LLM Agent](https://arxiv.org/abs/2507.02353)
*Bowen Chen,Zhao Wang,Shingo Takamatsu*

Main category: cs.AI

TL;DR: OMS框架无需依赖训练数据，通过多目标监控和自我反思机制，实时优化广告关键词生成，并在实验与实际应用中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于大模型（LLM）的广告关键词生成方法存在三大局限，严重影响广告投放效果和智能自动化水平。因此亟需一种能够自动监控、优化并自评关键词质量的新方法。

Method: 提出OMS框架，具备实时生成、无需训练数据、能够多目标监控及优化，并内置自我反思机制对关键词质量进行评估。方法在基准和真实广告活动中进行实验，并通过消融实验和人工评测进行效果验证。

Result: OMS框架在自动化、性能优化和质量把控方面均取得突破，在多个实验中超越了现有技术水平，并被实验证实其组件有效。

Conclusion: OMS框架能够有效地解决现有LLM方法在关键词生成过程中的三大难题：对大规模数据的依赖、缺乏多目标性能监控与优化，以及关键词选择的质量控制不足。实验结果显示OMS优于现有方法，且各组件的有效性通过消融实验与人工评测得到验证。

Abstract: Keyword decision in Sponsored Search Advertising is critical to the success
of ad campaigns. While LLM-based methods offer automated keyword generation,
they face three major limitations: reliance on large-scale query-keyword pair
data, lack of online multi-objective performance monitoring and optimization,
and weak quality control in keyword selection. These issues hinder the agentic
use of LLMs in fully automating keyword decisions by monitoring and reasoning
over key performance indicators such as impressions, clicks, conversions, and
CTA effectiveness. To overcome these challenges, we propose OMS, a keyword
generation framework that is On-the-fly (requires no training data, monitors
online performance, and adapts accordingly), Multi-objective (employs agentic
reasoning to optimize keywords based on multiple performance metrics), and
Self-reflective (agentically evaluates keyword quality). Experiments on
benchmarks and real-world ad campaigns show that OMS outperforms existing
methods; ablation and human evaluations confirm the effectiveness of each
component and the quality of generated keywords.

</details>


### [14] [An AI-native experimental laboratory for autonomous biomolecular engineering](https://arxiv.org/abs/2507.02379)
*Mingyu Wu,Zhaoguo Wang,Jiabin Wang,Zhiyuan Dong,Jingkai Yang,Qingting Li,Tianyu Huang,Lei Zhao,Mingqiang Li,Fei Wang,Chunhai Fan,Haibo Chen*

Main category: cs.AI

TL;DR: 本文提出了一种AI驱动的自主实验室平台，能在无人干预下进行复杂多目标实验，支持多用户高效操作，实验性能可与人工媲美，有望突破当前依赖专家与资源瓶颈，实现大规模科学服务。


<details>
  <summary>Details</summary>
Motivation: 长期以来，人们一直希望能够实现能够独立进行复杂实验并为非专业人士服务的自主科学研究。实现这一目标需要依靠人工智能驱动的范式转变，而现有自主实验系统受限于单一目标和流程简单的实验领域，难以推广到更复杂的科研应用。

Method: 本论文提出了一种AI原生的自主实验室系统，该系统能够自主管理实验仪器，制定实验特定的流程和优化策略，并支持多用户并发请求。该平台基于模型、实验与仪器的协同设计理念，实现了AI模型与自动化系统的协同进化，从而构建了端到端、多用户的自主实验环境。

Result: 该自主实验室系统支持包括核酸合成、转录、扩增和测序在内的基础核酸功能，并可应用于疾病诊断、药物研发和信息存储等领域。在无人干预下，该系统能将实验性能自主优化至人类科学家已达的最优水平，并在多用户场景下大幅提升仪器利用率和实验效率。

Conclusion: 该平台为专业人才和资源相对有限的先进生物材料研究提供了解决方案，为大规模“科学即服务”模式建立了蓝图。

Abstract: Autonomous scientific research, capable of independently conducting complex
experiments and serving non-specialists, represents a long-held aspiration.
Achieving it requires a fundamental paradigm shift driven by artificial
intelligence (AI). While autonomous experimental systems are emerging, they
remain confined to areas featuring singular objectives and well-defined, simple
experimental workflows, such as chemical synthesis and catalysis. We present an
AI-native autonomous laboratory, targeting highly complex scientific
experiments for applications like autonomous biomolecular engineering. This
system autonomously manages instrumentation, formulates experiment-specific
procedures and optimization heuristics, and concurrently serves multiple user
requests. Founded on a co-design philosophy of models, experiments, and
instruments, the platform supports the co-evolution of AI models and the
automation system. This establishes an end-to-end, multi-user autonomous
laboratory that handles complex, multi-objective experiments across diverse
instrumentation. Our autonomous laboratory supports fundamental nucleic acid
functions-including synthesis, transcription, amplification, and sequencing. It
also enables applications in fields such as disease diagnostics, drug
development, and information storage. Without human intervention, it
autonomously optimizes experimental performance to match state-of-the-art
results achieved by human scientists. In multi-user scenarios, the platform
significantly improves instrument utilization and experimental efficiency. This
platform paves the way for advanced biomaterials research to overcome
dependencies on experts and resource barriers, establishing a blueprint for
science-as-a-service at scale.

</details>


### [15] [The Gauss-Markov Adjunction: Categorical Semantics of Residuals in Supervised Learning](https://arxiv.org/abs/2507.02442)
*Moto Kamiura*

Main category: cs.AI

TL;DR: 该论文用范畴论方法对线性回归等监督学习模型进行语义结构化建模，明确界定了参数与残差间的关系，提升了机器学习模型的可解释性，为AI可解释性原则提供了理论基础。


<details>
  <summary>Details</summary>
Motivation: 当前社会对人工智能的可解释性提出更高要求，因此需要提升机器学习的可理解性与可解释性，以促进AI在社会中的应用。作者认为可以通过范畴论的视角重构机器学习模型，发展AI系统的语义结构框架。

Method: 采用范畴论方法，将有监督学习中的参数与数据分别建模为两个具体范畴，并在它们之间构建伴随函子对。以多元线性回归为例，提出高斯-马尔可夫伴随结构，用以形式化参数和残差的相互关系。进一步将该框架置于扩展的指称语义体系内，嫁接理论计算机科学中的语义方法。

Result: 提出了用范畴论伴随理论对回归模型进行结构化建模的新方法，明确展示了参数变动和残差之间的信息流。阐明了普通最小二乘估计和最小残差通过右伴随函子极限保持的关联，并把这种语义分析框架推广为有监督学习的通用形式。

Conclusion: 范畴论提供了统一且可解释的机器学习建模语义基础，有助于提升AI系统的解释性，为AI可解释性原则提供了坚实的理论工具。

Abstract: Enhancing the intelligibility and interpretability of machine learning is a
crucial task in responding to the demand for Explicability as an AI principle,
and in promoting the better social implementation of AI. The aim of our
research is to contribute to this improvement by reformulating machine learning
models through the lens of category theory, thereby developing a semantic
framework for structuring and understanding AI systems. Our categorical
modeling in this paper clarifies and formalizes the structural interplay
between residuals and parameters in supervised learning. The present paper
focuses on the multiple linear regression model, which represents the most
basic form of supervised learning. By defining two concrete categories
corresponding to parameters and data, along with an adjoint pair of functors
between them, we introduce our categorical formulation of supervised learning.
We show that the essential structure of this framework is captured by what we
call the Gauss-Markov Adjunction. Within this setting, the dual flow of
information can be explicitly described as a correspondence between variations
in parameters and residuals. The ordinary least squares estimator for the
parameters and the minimum residual are related via the preservation of limits
by the right adjoint functor. Furthermore, we position this formulation as an
instance of extended denotational semantics for supervised learning, and
propose applying a semantic perspective developed in theoretical computer
science as a formal foundation for Explicability in AI.

</details>


### [16] [Clarifying Before Reasoning: A Coq Prover with Structural Context](https://arxiv.org/abs/2507.02541)
*Yanzhen Lu,Hanbin Yang,Xiaodie Wang,Ge Zhang,Biao Li,Chenxu Fu,Chao Li,Yang Yuan,Andrew Chi-Chih Yao*

Main category: cs.AI

TL;DR: 通过结构化概念展开提升了大语言模型在Coq定理证明任务中的推理能力，成功率远超现有方法，凸显了提升任务清晰度的重要性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）在定理证明等复杂推理任务上表现有限，而任务表述的清晰度或许是影响模型推理能力的关键因素。因此，作者希望通过提升任务清晰度来增强LLM的推理能力。

Method: 提出一种基于结构化语义上下文的输入增强方法，并设计了概念级的任务清晰度度量标准。具体技术包括选择性概念展开和采用“规划者-执行者（Planner-Executor）”架构。作者以Coq定理证明为实验基础，采用和Graph2Tac一致的评价协议，对随机采样的1,386个Coq定理进行全面评测。同时也对小模型进行了结构化数据微调。

Result: 提出的方法可将任务清晰度评分提升1.85倍（44.5%→82.3%）；在DeepSeek-V3通用模型上，证明成功率提升2.1倍（21.8%→45.8%），并超过了之前最好的Graph2Tac（33.2%）；微调小模型后性能更高，达到48.6%。

Conclusion: 结构化任务描述能显著提升大模型的推理表现，是连接理解与推理能力的重要桥梁。作者验证了提升任务清晰度在自动定理证明等推理任务中的巨大价值。

Abstract: In this work, we investigate whether improving task clarity can enhance
reasoning ability of large language models, focusing on theorem proving in Coq.
We introduce a concept-level metric to evaluate task clarity and show that
adding structured semantic context to the standard input used by modern LLMs,
leads to a 1.85$\times$ improvement in clarity score
(44.5\%~$\rightarrow$~82.3\%). Using the general-purpose model
\texttt{DeepSeek-V3}, our approach leads to a 2.1$\times$ improvement in proof
success (21.8\%~$\rightarrow$~45.8\%) and outperforms the previous
state-of-the-art \texttt{Graph2Tac} (33.2\%). We evaluate this on 1,386
theorems randomly sampled from 15 standard Coq packages, following the same
evaluation protocol as \texttt{Graph2Tac}. Furthermore, fine-tuning smaller
models on our structured data can achieve even higher performance (48.6\%). Our
method uses selective concept unfolding to enrich task descriptions, and
employs a Planner--Executor architecture. These findings highlight the value of
structured task representations in bridging the gap between understanding and
reasoning.

</details>


### [17] [AI Research Agents for Machine Learning: Search, Exploration, and Generalization in MLE-bench](https://arxiv.org/abs/2507.02554)
*Edan Toledo,Karen Hambardzumyan,Martin Josifoski,Rishi Hazra,Nicolas Baldwin,Alexis Audran-Reiss,Michael Kuchnik,Despoina Magka,Minqi Jiang,Alisia Maria Lupidi,Andrei Lupu,Roberta Raileanu,Kelvin Niu,Tatiana Shavrina,Jean-Christophe Gagnon-Audet,Michael Shvartsman,Shagun Sodhani,Alexander H. Miller,Abhishek Charnalia,Derek Dunfield,Carole-Jean Wu,Pontus Stenetorp,Nicola Cancedda,Jakob Nicolaus Foerster,Yoram Bachrach*

Main category: cs.AI

TL;DR: 本文通过形式化AI代理的搜索策略与算子组合，在MLE-bench基准任务中系统优化自动化科学研究代理，最佳方法让Kaggle奖牌获奖率由39.6%提升至47.7%，强调搜索和算子设计协同的重要性。


<details>
  <summary>Details</summary>
Motivation: 当前自动化AI研究代理在加速科学进展、自动设计和训练模型方面展现出巨大潜力，但在现实Kaggle竞赛等复杂问题中，提升其表现仍面临挑战。提升AI代理在MLE-bench这样具代表性基准测试中的表现，是探究其能力和改进方法的重要动力。

Method: 作者将AI研究代理形式化为在候选解空间中，通过搜索策略和一组算子不断修改解、优化模型的过程。系统性地设计并比较了不同的搜索策略（如贪婪算法、蒙特卡洛树搜索MCTS、进化算法）及算子集合，以分析和提升其解决问题的能力。

Result: 不同搜索策略和算子集合的组合对代理性能有显著影响。最佳组合（某种搜索策略与算子集合）在MLE-bench lite基准上，将Kaggle奖牌获奖率从39.6%提升到47.7%，达到了最新的领先水平。

Conclusion: 研究发现，取得高效自动化AI研究代理性能的关键，在于将搜索策略、算子设计与评估方法联合优化考量。为进一步推进自动机器学习，应重视上述各因素的协同作用。

Abstract: AI research agents are demonstrating great potential to accelerate scientific
progress by automating the design, implementation, and training of machine
learning models. We focus on methods for improving agents' performance on
MLE-bench, a challenging benchmark where agents compete in Kaggle competitions
to solve real-world machine learning problems. We formalize AI research agents
as search policies that navigate a space of candidate solutions, iteratively
modifying them using operators. By designing and systematically varying
different operator sets and search policies (Greedy, MCTS, Evolutionary), we
show that their interplay is critical for achieving high performance. Our best
pairing of search strategy and operator set achieves a state-of-the-art result
on MLE-bench lite, increasing the success rate of achieving a Kaggle medal from
39.6% to 47.7%. Our investigation underscores the importance of jointly
considering the search strategy, operator design, and evaluation methodology in
advancing automated machine learning.

</details>


### [18] [Responsibility Gap and Diffusion in Sequential Decision-Making Mechanisms](https://arxiv.org/abs/2507.02582)
*Junli Jiang,Pavel Naumov*

Main category: cs.AI

TL;DR: 本文研究了集体决策中两种责任属性的计算复杂性，发现相关问题非常复杂（分别属于Π₂-完全和Π₃-完全），为AI责任机制的实现带来理论挑战。


<details>
  <summary>Details</summary>
Motivation: 责任在法律与哲学领域是重要议题，近年来也成为AI领域的关注点，尤其在集体决策中，不同责任特性的计算复杂性尚不清楚。该文旨在研究集体决策中责任属性的计算难度。

Method: 本文分析了责任的两种重要属性（扩散性和间隙性），并通过复杂性理论工具，形式化地讨论了无扩散（diffusion-free）、无间隙（gap-free）以及其交集的决策机制的计算复杂性。

Result: 结果表明，无扩散决策机制的判定问题是Π₂-完全的，无间隙则是Π₃-完全的，而两者交集问题为Π₂-完全。

Conclusion: 这表明在集体决策中，这些责任相关属性判定问题高度复杂，提出了责任分析在实际算法系统中实现的理论难点。

Abstract: Responsibility has long been a subject of study in law and philosophy. More
recently, it became a focus of AI literature. The article investigates the
computational complexity of two important properties of responsibility in
collective decision-making: diffusion and gap. It shows that the sets of
diffusion-free and gap-free decision-making mechanisms are $\Pi_2$-complete and
$\Pi_3$-complete, respectively. At the same time, the intersection of these
classes is $\Pi_2$-complete.

</details>


### [19] [DynamiCare: A Dynamic Multi-Agent Framework for Interactive and Open-Ended Medical Decision-Making](https://arxiv.org/abs/2507.02616)
*Tianqi Shang,Weiqing He,Charles Zheng,Lingyao Li,Li Shen,Bingxin Zhao*

Main category: cs.AI

TL;DR: 该文提出MIMIC-Patient数据集与DynamiCare动态多智能体诊断框架，真实模拟临床多轮诊断流程，建立了新型医疗AI决策基准，实验效果良好。


<details>
  <summary>Details</summary>
Motivation: 以往医疗AI主要关注于单回合任务，这与实际临床决策过程的互动和不确定性不符。

Method: 提出MIMIC-Patient结构化数据集，并基于此提出DynamiCare多智能体动态诊断框架，实现多专家AI团队迭代式与病例系统互动、集成新信息并动态调整决策流程。

Result: 通过大量实验验证了DynamiCare的可行性和有效性，并建立了基于大模型智能体的动态临床决策基准。

Conclusion: 首次实现并评估了支持动态、互动、多轮次临床决策的LLM智能体系统，为医疗AI决策研究提供了标准化基准。

Abstract: The rise of Large Language Models (LLMs) has enabled the development of
specialized AI agents with domain-specific reasoning and interaction
capabilities, particularly in healthcare. While recent frameworks simulate
medical decision-making, they largely focus on single-turn tasks where a doctor
agent receives full case information upfront -- diverging from the real-world
diagnostic process, which is inherently uncertain, interactive, and iterative.
In this paper, we introduce MIMIC-Patient, a structured dataset built from the
MIMIC-III electronic health records (EHRs), designed to support dynamic,
patient-level simulations. Building on this, we propose DynamiCare, a novel
dynamic multi-agent framework that models clinical diagnosis as a multi-round,
interactive loop, where a team of specialist agents iteratively queries the
patient system, integrates new information, and dynamically adapts its
composition and strategy. We demonstrate the feasibility and effectiveness of
DynamiCare through extensive experiments, establishing the first benchmark for
dynamic clinical decision-making with LLM-powered agents.

</details>


### [20] [Strategic Intelligence in Large Language Models: Evidence from evolutionary Game Theory](https://arxiv.org/abs/2507.02618)
*Kenneth Payne,Baptiste Alloui-Cros*

Main category: cs.AI

TL;DR: 首次将前沿大型语言模型与经典博弈策略在复杂的迭代囚徒困境中对抗，发现LLM具备竞争和推理能力，不同公司的模型展现出独特的战略风格，为AI决策研究提供了新视角。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型（LLMs）在竞争环境中推理和决策能力，尤其是在知名博弈论模型——迭代囚徒困境（IPD）中的表现，进而揭示其作为新型战略智能体的潜力。

Method: 首次设计进化型的IPD锦标赛，将经典策略（如以牙还牙、Grim Trigger）与OpenAI、Google、Anthropic三大公司代表性LLM进行对抗。通过调整比赛的终止概率（即未来阴影），增加博弈的复杂性和不确定性，并收集模型的决策理由进行近32000份文本分析。

Result: LLMs在复杂生态系统中表现出极强的竞争力，能够持续生存并有时扩展势力。不同公司的模型展现出差异化的“战略指纹”：Google模型策略更为无情，善于剥削合作者并惩罚背叛者；OpenAI模型高度合作，导致在敌对环境中表现不佳；Anthropic模型最具宽容性，能够在被背叛后迅速恢复合作。此外，LLMs会根据时间视野和对手策略主动推理，理性分析对决策产生重要作用。

Conclusion: 大型语言模型已展现高度的战略推理和生态适应能力，不同模型具备鲜明的决策风格，其决策不仅体现博弈论中的稳定策略，还受时间视角与对手行为推理影响，为连接博弈论与人工智能心理学提供了新视角。

Abstract: Are Large Language Models (LLMs) a new form of strategic intelligence, able
to reason about goals in competitive settings? We present compelling supporting
evidence. The Iterated Prisoner's Dilemma (IPD) has long served as a model for
studying decision-making. We conduct the first ever series of evolutionary IPD
tournaments, pitting canonical strategies (e.g., Tit-for-Tat, Grim Trigger)
against agents from the leading frontier AI companies OpenAI, Google, and
Anthropic. By varying the termination probability in each tournament (the
"shadow of the future"), we introduce complexity and chance, confounding
memorisation.
  Our results show that LLMs are highly competitive, consistently surviving and
sometimes even proliferating in these complex ecosystems. Furthermore, they
exhibit distinctive and persistent "strategic fingerprints": Google's Gemini
models proved strategically ruthless, exploiting cooperative opponents and
retaliating against defectors, while OpenAI's models remained highly
cooperative, a trait that proved catastrophic in hostile environments.
Anthropic's Claude emerged as the most forgiving reciprocator, showing
remarkable willingness to restore cooperation even after being exploited or
successfully defecting. Analysis of nearly 32,000 prose rationales provided by
the models reveals that they actively reason about both the time horizon and
their opponent's likely strategy, and we demonstrate that this reasoning is
instrumental to their decisions. This work connects classic game theory with
machine psychology, offering a rich and granular view of algorithmic
decision-making under uncertainty.

</details>


### [21] [Decoupled Planning and Execution: A Hierarchical Reasoning Framework for Deep Search](https://arxiv.org/abs/2507.02652)
*Jiajie Jin,Xiaoxi Li,Guanting Dong,Yuyao Zhang,Yutao Zhu,Yang Zhao,Hongjin Qian,Zhicheng Dou*

Main category: cs.AI

TL;DR: HiRA通过分层分工，实现复杂信息查询任务的高效处理，显著优于现有RAG和agent系统。


<details>
  <summary>Details</summary>
Motivation: 现实世界中复杂的信息需求需要跨多源的深度推理和知识综合，传统的检索增强生成（RAG）无法有效解决此类问题。当前基于推理的方法通常让单一模型同时负责高层规划和细节执行，效率低且难以扩展。

Method: 提出了一种名为HiRA的分层框架，将高层的战略性规划与具体的专业化执行相分离。对于复杂任务，系统会将其分解为若干子任务，并分配给具备外部工具和推理能力的领域专属代理，通过结构化的机制对各子任务的结果进行整合。

Result: 在四个跨模态的复杂深度检索基准上进行实验，HiRA相比于现有最优的RAG与基于agent的方法，在答案质量和系统效率上都有明显提升。

Conclusion: 分离规划与执行的分层框架能显著提升多步骤信息检索任务的表现，具备较优的答案质量与效率。

Abstract: Complex information needs in real-world search scenarios demand deep
reasoning and knowledge synthesis across diverse sources, which traditional
retrieval-augmented generation (RAG) pipelines struggle to address effectively.
Current reasoning-based approaches suffer from a fundamental limitation: they
use a single model to handle both high-level planning and detailed execution,
leading to inefficient reasoning and limited scalability. In this paper, we
introduce HiRA, a hierarchical framework that separates strategic planning from
specialized execution. Our approach decomposes complex search tasks into
focused subtasks, assigns each subtask to domain-specific agents equipped with
external tools and reasoning capabilities, and coordinates the results through
a structured integration mechanism. This separation prevents execution details
from disrupting high-level reasoning while enabling the system to leverage
specialized expertise for different types of information processing.
Experiments on four complex, cross-modal deep search benchmarks demonstrate
that HiRA significantly outperforms state-of-the-art RAG and agent-based
systems. Our results show improvements in both answer quality and system
efficiency, highlighting the effectiveness of decoupled planning and execution
for multi-step information seeking tasks. Our code is available at
https://github.com/ignorejjj/HiRA.

</details>


### [22] [Hey AI, Generate Me a Hardware Code! Agentic AI-based Hardware Design & Verification](https://arxiv.org/abs/2507.02660)
*Deepak Narayan Gadde,Keerthan Kopparam Radhakrishna,Vaisakh Naduvodi Viswambharan,Aman Kumar,Djones Lettnin,Wolfgang Kunz,Sebastian Simon*

Main category: cs.AI

TL;DR: 本文提出结合大型语言模型和人类协作的智能体验证框架，实现了更高效、更全面的硬件设计验证，在五个案例中提升了覆盖率并缩短了验证周期。


<details>
  <summary>Details</summary>
Motivation: 现代集成电路的复杂性不断提高，硬件设计验证过程变得越来越繁琐且耗时。传统方法需要大量人力和时间以确保设计无误，急需更高效的自动化手段。

Method: 提出了一种基于Agent的AI方法，将大型语言模型（LLMs）赋能的智能体与人类在环（HITL）相结合，实现动态、迭代和自反的硬件设计验证流程，支持端到端自动化。

Result: 在五个开源设计上进行了评估，验证了该方法能在减少验证时间的同时，覆盖率超过95%，表现出优越的性能、适应性和可配置性。

Conclusion: 基于Agent的AI方法结合人类协作能够大幅提升硬件设计验证的效率和覆盖率，为复杂芯片设计的自动化验证提供了有力工具。

Abstract: Modern Integrated Circuits (ICs) are becoming increasingly complex, and so is
their development process. Hardware design verification entails a methodical
and disciplined approach to the planning, development, execution, and sign-off
of functionally correct hardware designs. This tedious process requires
significant effort and time to ensure a bug-free tape-out. The field of Natural
Language Processing has undergone a significant transformation with the advent
of Large Language Models (LLMs). These powerful models, often referred to as
Generative AI (GenAI), have revolutionized how machines understand and generate
human language, enabling unprecedented advancements in a wide array of
applications, including hardware design verification. This paper presents an
agentic AI-based approach to hardware design verification, which empowers AI
agents, in collaboration with Humain-in-the-Loop (HITL) intervention, to engage
in a more dynamic, iterative, and self-reflective process, ultimately
performing end-to-end hardware design and verification. This methodology is
evaluated on five open-source designs, achieving over 95% coverage with reduced
verification time while demonstrating superior performance, adaptability, and
configurability.

</details>


### [23] [Think How to Think: Mitigating Overthinking with Autonomous Difficulty Cognition in Large Reasoning Models](https://arxiv.org/abs/2507.02663)
*Yongjiang Liu,Haoxi Li,Xiaosong Ma,Jie Zhang,Song Guo*

Main category: cs.AI

TL;DR: 该论文提出了一种名为TH2T的两阶段微调方法，有效解决长推理模型在推理任务中过度思考的问题，通过激发模型的任务难度感知和冗余感知，使推理更高效且有针对性，大幅降低推理步骤冗余并减少推理成本。


<details>
  <summary>Details</summary>
Motivation: 当前的长推理模型（LRMs）虽然在复杂推理任务中表现优异，但容易出现“过度思考”现象，即在实际任务中效率低下。研究发现，LRMs像人一样，主要是先识别任务属性（如难度等级）再统一进行推理，导致推理流程单一、不够灵活。鉴于此，提出能否提升模型的难度感知与推理冗余感知，从而减少“过度思考”。

Method: 提出了一种名为Think-How-to-Think（TH2T）的新型两阶段微调策略：第一阶段通过在输出前缀中注入‘difficulty-hypnosis’提示，提升模型对任务难度的敏感性，训练中采用异构的短长推理数据集；第二阶段扩展为‘redundancy-hypnosis’，进一步引导模型识别推理过程中的冗余，并生成更简洁的推理步骤。

Result: 在7B/14B/32B参数规模的模型实验中，TH2T策略在保持性能稳定的情况下，显著减少推理推断成本——在简单任务上推理成本降低超过70%，在复杂任务上降低40%。此外，产出的推理过程具备明显的难度感知与冗余减少特征。

Conclusion: TH2T通过阶段性激发模型的难度认知和冗余认知，显著减缓了LRMs的过度思考现象，并提升了推理效率及难度适应性。此方法为改进长推理模型的实用性和智能性提供了新方向。

Abstract: Recent Long Reasoning Models(LRMs) have demonstrated remarkable capabilities
in handling complex reasoning tasks, but are hindered by excessive
overthinking. To explore its essence, our empirical analysis reveals that LRMs
are primarily limited to recognizing task properties (i.e., difficulty levels)
like humans before solving the problem, leading to a one-size-fits-all
reasoning process. Inspired by this, a pressing and natural question emerges:
Can we bootstrap such ability to further alleviate the overthinking phenomenon
in LRMs? In this paper, we propose Think-How-to-Think (TH2T), a novel two-stage
fine-tuning strategy that progressively inspires LRMs' difficulty cognition and
redundancy cognition. First, we introduce difficulty-hypnosis in the prefixes
of model outputs to intervene in the internal reasoning trajectory. Combined
with a heterogeneous short and long reasoning dataset, the trained model
enhances its sensitivity to task difficulty, enabling native, differentiated
reasoning strategies across various tasks. Second, we further extend
redundancy-hypnosis to the internal reasoning process, guiding the model to
identify redundant structures within the reasoning steps and generate more
concise reasoning outputs. Experiments on 7B/14B/32B models demonstrate that
TH2T significantly reduces inference costs (more than 70% on easy tasks and 40%
on hard tasks) while maintaining performance stability. The resulting outputs
exhibit clear difficulty-aware capabilities and reduced redundancy (e.g.,
reflection).

</details>


### [24] [Detection of Disengagement from Voluntary Quizzes: An Explainable Machine Learning Approach in Higher Distance Education](https://arxiv.org/abs/2507.02681)
*Behnam Parsaeifard,Christof Imhof,Tansu Pancar,Ioan-Sorin Comsa,Martin Hlosta,Nicole Bergamin,Per Bergamin*

Main category: cs.AI

TL;DR: 本文基于Moodle数据，用多种机器学习方法高效检测远程教育学生对非必做任务的脱离行为，准确率91%，并结合可解释性分析为及时干预提供了理论依据。


<details>
  <summary>Details</summary>
Motivation: 远程教育中学生参与度降低会带来严重后果，如学业辍学，因此及时检测学生的脱离任务现象对于教育干预十分重要。非必做在线习题的参与情况可以作为学生脱离度的重要衡量标准。

Method: 作者收集了基于Moodle的四个学期42门课的非必做小测验日志数据，选取最有信息量的特征，使用八种机器学习模型进行训练与比较，并采用SHAP方法构建可解释的机器学习框架，帮助理解模型预测机制。

Result: 实验结果显示，模型的平衡准确率达到91%，其中约85%的脱离学生被准确检测。同时给出了基于该框架的早期干预建议，以减缓学生的主动任务脱离。

Conclusion: 基于机器学习与可解释性方法的框架能够有效、准确地检测远程教育中非必做任务的学生脱离现象，并为实现教育干预和预防学生脱离提供了方法基础。

Abstract: Students disengaging from their tasks can have serious long-term
consequences, including academic drop-out. This is particularly relevant for
students in distance education. One way to measure the level of disengagement
in distance education is to observe participation in non-mandatory exercises in
different online courses. In this paper, we detect student disengagement in the
non-mandatory quizzes of 42 courses in four semesters from a distance-based
university. We carefully identified the most informative student log data that
could be extracted and processed from Moodle. Then, eight machine learning
algorithms were trained and compared to obtain the highest possible prediction
accuracy. Using the SHAP method, we developed an explainable machine learning
framework that allows practitioners to better understand the decisions of the
trained algorithm. The experimental results show a balanced accuracy of 91\%,
where about 85\% of disengaged students were correctly detected. On top of the
highly predictive performance and explainable framework, we provide a
discussion on how to design a timely intervention to minimise disengagement
from voluntary tasks in online learning.

</details>


### [25] [Time-critical and confidence-based abstraction dropping methods](https://arxiv.org/abs/2507.02703)
*Robin Schmöcker,Lennart Kampmann,Alexander Dockhorn*

Main category: cs.AI

TL;DR: 本文提出了两种新的MCTS抽象放弃算法，可在提升性能的同时避免性能大幅下降，相比现有方法更安全有效。


<details>
  <summary>Details</summary>
Motivation: Monte Carlo Tree Search（MCTS）中采用状态和/或动作抽象能够提升搜索效率，但非精确抽象会引入近似误差，最终导致无法收敛到最优解。为了解决这一问题，当前方法建议在适当时候放弃抽象，但如何安全而高效地放弃抽象仍是难点。

Method: 提出了两种新的抽象放弃（dropping）方法：OGA-IAAD和OGA-CAD。OGA-IAAD适用于对时间敏感的场景，OGA-CAD则用于在相同迭代次数下提升MCTS表现。

Result: 提出的OGA-IAAD和OGA-CAD方法可显著提升MCTS性能，并且在放弃抽象时不会像现有方法那样引发严重的性能下降。

Conclusion: 该文提出的OGA-IAAD与OGA-CAD方法能够在保证安全的前提下，有效提升基于抽象的MCTS的效率和效果，弥补了现有抽象放弃策略的不足。

Abstract: One paradigm of Monte Carlo Tree Search (MCTS) improvements is to build and
use state and/or action abstractions during the tree search. Non-exact
abstractions, however, introduce an approximation error making convergence to
the optimal action in the abstract space impossible. Hence, as proposed as a
component of Elastic Monte Carlo Tree Search by Xu et al., abstraction
algorithms should eventually drop the abstraction. In this paper, we propose
two novel abstraction dropping schemes, namely OGA-IAAD and OGA-CAD which can
yield clear performance improvements whilst being safe in the sense that the
dropping never causes any notable performance degradations contrary to Xu's
dropping method. OGA-IAAD is designed for time critical settings while OGA-CAD
is designed to improve the MCTS performance with the same number of iterations.

</details>


### [26] [Bourbaki: Self-Generated and Goal-Conditioned MDPs for Theorem Proving](https://arxiv.org/abs/2507.02726)
*Matthieu Zimmer,Xiaotong Ji,Rasul Tutunov,Anthony Bordg,Jun Wang,Haitham Bou Ammar*

Main category: cs.AI

TL;DR: 将自动定理证明中的大模型推理任务分解为更易搜索的子目标，用蒙特卡洛树搜索等方法进行高效求解，显著提升了模型的定理证明能力，在PutnamBench上达成了新SOTA。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLM）在自动定理证明领域（ATP）推理时面临诸如奖励稀疏与证明过程规模庞大等挑战，尤其在含多步骤复杂推理的高难度基准测试中愈发突出。

Method: 提出了一种自生成目标条件马尔可夫决策过程（sG-MDPs）框架，使智能体能依据不断演化的证明状态生成并追逐子目标。基于该结构化的目标生成过程，能够用类似蒙特卡洛树搜索（MCTS）的方法来求解这一决策过程，并在具体实现上，采用了Bourbaki（7B）系统，将多个7B参数规模的LLM组合用于子目标生成和策略综合。

Result: 在PutnamBench基准上，Bourbaki（7B）解决了26个问题，在同规模模型中达到了新的最先进水平。

Conclusion: 通过将证明过程分解为自生成的子目标并结合MCTS搜索，即使是参数量有限的中等规模模型也能显著提升自动定理证明能力。

Abstract: Reasoning remains a challenging task for large language models (LLMs),
especially within the logically constrained environment of automated theorem
proving (ATP), due to sparse rewards and the vast scale of proofs. These
challenges are amplified in benchmarks like PutnamBench, which contains
university-level problems requiring complex, multi-step reasoning. To address
this, we introduce self-generated goal-conditioned MDPs (sG-MDPs), a new
framework in which agents generate and pursue their subgoals based on the
evolving proof state. Given this more structured generation of goals, the
resulting problem becomes more amenable to search. We then apply Monte Carlo
Tree Search (MCTS)-like algorithms to solve the sG-MDP, instantiating our
approach in Bourbaki (7B), a modular system that can ensemble multiple 7B LLMs
for subgoal generation and tactic synthesis. On PutnamBench, Bourbaki (7B)
solves 26 problems, achieving new state-of-the-art results with models at this
scale.

</details>


### [27] [Knowledge Protocol Engineering: A New Paradigm for AI in Domain-Specific Knowledge Work](https://arxiv.org/abs/2507.02760)
*Guangwei Zhang*

Main category: cs.AI

TL;DR: 本文提出知识协议工程（KPE），通过将专家知识转化为可执行协议，让通用LLM像专家一样操作，为AI在复杂领域任务中表现带来突破。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型（LLMs）在处理复杂、领域专属知识时有很大潜力，但目前主流方法（如RAG和Agentic AI）在需要深入、流程化和方法论推理的专家领域任务上表现有限。因此，需要一种新范式让LLMs更好地执行复杂领域任务。

Method: 提出了一种新的范式——知识协议工程（KPE），系统性地将领域专家的知识（通常以自然语言文档体现）转化为机器可执行的知识协议（KP），使LLM拥有领域固有逻辑和操作策略。

Result: 通过KPE构建的知识协议，能够让通用型LLM具备专家能力，自动分解复杂查询并完成多步任务。该方法可泛化应用于法律、生物信息等多个领域。

Conclusion: KPE为人类与AI的协作提供了基础性方法论，有望成为未来推动AI领域知识执行和协作的关键技术。

Abstract: The capabilities of Large Language Models (LLMs) have opened new frontiers
for interacting with complex, domain-specific knowledge. However, prevailing
methods like Retrieval-Augmented Generation (RAG) and general-purpose Agentic
AI, while powerful, often struggle with tasks that demand deep, procedural, and
methodological reasoning inherent to expert domains. RAG provides factual
context but fails to convey logical frameworks; autonomous agents can be
inefficient and unpredictable without domain-specific heuristics. To bridge
this gap, we introduce Knowledge Protocol Engineering (KPE), a new paradigm
focused on systematically translating human expert knowledge, often expressed
in natural language documents, into a machine-executable Knowledge Protocol
(KP). KPE shifts the focus from merely augmenting LLMs with fragmented
information to endowing them with a domain's intrinsic logic, operational
strategies, and methodological principles. We argue that a well-engineered
Knowledge Protocol allows a generalist LLM to function as a specialist, capable
of decomposing abstract queries and executing complex, multi-step tasks. This
position paper defines the core principles of KPE, differentiates it from
related concepts, and illustrates its potential applicability across diverse
fields such as law and bioinformatics, positing it as a foundational
methodology for the future of human-AI collaboration.

</details>


### [28] [Grounding Intelligence in Movement](https://arxiv.org/abs/2507.02771)
*Melanie Segado,Felipe Parodi,Jordan K. Matelsky,Michael L. Platt,Eva B. Dyer,Konrad P. Kording*

Main category: cs.AI

TL;DR: 现有AI对运动建模重视不足，作者认为运动本身具有独特结构和科学价值，主张提升对运动的系统建模，以推动智能系统的发展和行为理解。


<details>
  <summary>Details</summary>
Motivation: 尽管机器学习已经在语言、视觉等高维数据建模方面取得了巨大进展，但对于运动建模依然存在较大挑战。然而，运动对于揭示行为、预测意图、实现交互等智能系统的核心能力至关重要。运动未被充分重视，原因是当前数据采集与建模多受限于具体任务和领域假设。作者认为有必要将运动作为人工智能（AI）的主要建模目标。

Method: 论文提出将运动视为一种具有丰富结构和物理基础的主要感知模态，并主张通过发掘其结构（如低维姿态描述）来提升其可解释性和计算可控性；倡导通过开发能够跨多样运动数据学习和泛化的模型，建立统一的研究基础。

Result: 提出了运动应成为AI核心建模对象的理论主张，强调通过挖掘运动结构提升模型在生成建模、控制等核心任务中的表现，并促进对生物及人工系统行为的理解。

Conclusion: 运动不应作为次要结果，而应作为理解和建模智能系统与世界交互的关键窗口，对AI建模与理解行为具有基础性意义。

Abstract: Recent advances in machine learning have dramatically improved our ability to
model language, vision, and other high-dimensional data, yet they continue to
struggle with one of the most fundamental aspects of biological systems:
movement. Across neuroscience, medicine, robotics, and ethology, movement is
essential for interpreting behavior, predicting intent, and enabling
interaction. Despite its core significance in our intelligence, movement is
often treated as an afterthought rather than as a rich and structured modality
in its own right. This reflects a deeper fragmentation in how movement data is
collected and modeled, often constrained by task-specific goals and
domain-specific assumptions. But movement is not domain-bound. It reflects
shared physical constraints, conserved morphological structures, and purposeful
dynamics that cut across species and settings. We argue that movement should be
treated as a primary modeling target for AI. It is inherently structured and
grounded in embodiment and physics. This structure, often allowing for compact,
lower-dimensional representations (e.g., pose), makes it more interpretable and
computationally tractable to model than raw, high-dimensional sensory inputs.
Developing models that can learn from and generalize across diverse movement
data will not only advance core capabilities in generative modeling and
control, but also create a shared foundation for understanding behavior across
biological and artificial systems. Movement is not just an outcome, it is a
window into how intelligent systems engage with the world.

</details>


### [29] [KERAP: A Knowledge-Enhanced Reasoning Approach for Accurate Zero-shot Diagnosis Prediction Using Multi-agent LLMs](https://arxiv.org/abs/2507.02773)
*Yuzhang Xie,Hejie Cui,Ziyang Zhang,Jiaying Lu,Kai Shu,Fadi Nahab,Xiao Hu,Carl Yang*

Main category: cs.AI

TL;DR: 本文提出的KERAP方法结合知识图谱和多代理系统，大幅提升了大语言模型在医疗诊断预测中的准确性与解释性，适合应对无标签数据条件下的诊断任务。


<details>
  <summary>Details</summary>
Motivation: 现有的医学诊断预测在依赖机器学习方法时，普遍需要大量有标签数据，这限制了模型对未见病例的泛化能力。大语言模型虽然可用，但存在幻觉、缺乏医学推理和输出无用等问题。

Method: 提出了一种名为KERAP的知识图谱增强推理方法，结合多代理结构：包括属性映射的联接代理、知识检索代理、与诊断推理代理，实现对大语言模型诊断推理能力的增强。

Result: 通过实验证明，KERAP显著提升了诊断预测的可靠性，并且在零样本医疗诊断场景下表现出良好的高效性与可解释性。

Conclusion: 利用知识图谱与多代理推理架构，可高效提升大语言模型在医学诊断预测中的可靠性与适用性，尤其适用于数据稀缺（零样本）场景。

Abstract: Medical diagnosis prediction plays a critical role in disease detection and
personalized healthcare. While machine learning (ML) models have been widely
adopted for this task, their reliance on supervised training limits their
ability to generalize to unseen cases, particularly given the high cost of
acquiring large, labeled datasets. Large language models (LLMs) have shown
promise in leveraging language abilities and biomedical knowledge for diagnosis
prediction. However, they often suffer from hallucinations, lack structured
medical reasoning, and produce useless outputs. To address these challenges, we
propose KERAP, a knowledge graph (KG)-enhanced reasoning approach that improves
LLM-based diagnosis prediction through a multi-agent architecture. Our
framework consists of a linkage agent for attribute mapping, a retrieval agent
for structured knowledge extraction, and a prediction agent that iteratively
refines diagnosis predictions. Experimental results demonstrate that KERAP
enhances diagnostic reliability efficiently, offering a scalable and
interpretable solution for zero-shot medical diagnosis prediction.

</details>


### [30] [Establishing Best Practices for Building Rigorous Agentic Benchmarks](https://arxiv.org/abs/2507.02825)
*Yuxuan Zhu,Tengjun Jin,Yada Pruksachatkun,Andy Zhang,Shu Liu,Sasha Cui,Sayash Kapoor,Shayne Longpre,Kevin Meng,Rebecca Weiss,Fazl Barez,Rahul Gupta,Jwala Dhamala,Jacob Merizian,Mario Giulianelli,Harry Coppock,Cozmin Ududec,Jasjeet Sekhon,Jacob Steinhardt,Antony Kellerman,Sarah Schwettmann,Matei Zaharia,Ion Stoica,Percy Liang,Daniel Kang*

Main category: cs.AI

TL;DR: 主流AI agent基准测试存在设计问题，可能严重曲解AI性能。作者提出了ABC指南纠正评测，显著提升了量化结果的可靠性。


<details>
  <summary>Details</summary>
Motivation: 随着AI能力提升，需要更可靠的基准测试来量化评估AI代理解决复杂现实任务的能力。然而现有的agentic benchmark在任务设置或奖励设计上存在问题，可能导致对AI能力的高估或低估。

Method: 作者分析了现有agentic benchmark评估设计中的常见问题（如测试用例不足、错误判定标准），并基于自身经验、最佳实践调研和已报道问题，总结提出了Agentic Benchmark Checklist (ABC) 指南。然后将ABC应用于复杂基准CVE-Bench进行校正测试。

Result: ABC指南能有效识别并减少评估失误。例如，在CVE-Bench中的应用能将性能高估幅度降低33%。

Conclusion: 许多现有agentic基准评测在设计上有缺陷，ABC可作为建立更可靠评估的实践指南，帮助AI社区更准确量化AI代理能力。

Abstract: Benchmarks are essential for quantitatively tracking progress in AI. As AI
agents become increasingly capable, researchers and practitioners have
introduced agentic benchmarks to evaluate agents on complex, real-world tasks.
These benchmarks typically measure agent capabilities by evaluating task
outcomes via specific reward designs. However, we show that many agentic
benchmarks have issues task setup or reward design. For example, SWE-bench
Verified uses insufficient test cases, while TAU-bench counts empty responses
as successful. Such issues can lead to under- or overestimation agents'
performance by up to 100% in relative terms. To make agentic evaluation
rigorous, we introduce the Agentic Benchmark Checklist (ABC), a set of
guidelines that we synthesized from our benchmark-building experience, a survey
of best practices, and previously reported issues. When applied to CVE-Bench, a
benchmark with a particularly complex evaluation design, ABC reduces the
performance overestimation by 33%.

</details>


### [31] [StepHint: Multi-level Stepwise Hints Enhance Reinforcement Learning to Reason](https://arxiv.org/abs/2507.02841)
*Kaiyi Zhang,Ang Lv,Jinpeng Li,Yongbo Wang,Feng Wang,Haoyuan Hu,Rui Yan*

Main category: cs.AI

TL;DR: 本文提出StepHint算法，通过多级推理提示，引导和提升大语言模型的推理训练效率与泛化能力，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于可验证奖励的强化学习（RLVR）方法存在两个主要难题：一是“near-miss reward”问题，也就是因为一点小错误便导致整体推理失败，从而影响训练效率；二是“探索停滞”问题，模型只停留在自己熟悉的解决方案，缺乏探索更优方法的动机。

Method: 提出了一种新的RLVR算法StepHint。该算法从更强大的模型里生成有效的推理链，并用自适应分割方法将推理链划分为若干步骤，然后把前几步作为hint（提示）给训练中的模型，且能同时给出多级hint。通过这些设计，引导模型探索更有前景的解空间，同时又保持独立探索能力。

Result: StepHint方法有效缓解了near-miss reward和探索停滞问题，在六个数学基准测试上优于目前有竞争力的RLVR增强方法，在跨域测试中表现也优于现有基线，展现了更强的泛化能力。

Conclusion: StepHint能够提升大模型复杂推理能力，显著提升RLVR训练效率，增强模型探索与泛化性，对于改进LLM推理训练有实际应用价值。

Abstract: Reinforcement learning with verifiable rewards (RLVR) is a promising approach
for improving the complex reasoning abilities of large language models (LLMs).
However, current RLVR methods face two significant challenges: the near-miss
reward problem, where a small mistake can invalidate an otherwise correct
reasoning process, greatly hindering training efficiency; and exploration
stagnation, where models tend to focus on solutions within their ``comfort
zone,'' lacking the motivation to explore potentially more effective
alternatives. To address these challenges, we propose StepHint, a novel RLVR
algorithm that utilizes multi-level stepwise hints to help models explore the
solution space more effectively. StepHint generates valid reasoning chains from
stronger models and partitions these chains into reasoning steps using our
proposed adaptive partitioning method. The initial few steps are used as hints,
and simultaneously, multiple-level hints (each comprising a different number of
steps) are provided to the model. This approach directs the model's exploration
toward a promising solution subspace while preserving its flexibility for
independent exploration. By providing hints, StepHint mitigates the near-miss
reward problem, thereby improving training efficiency. Additionally, the
external reasoning pathways help the model develop better reasoning abilities,
enabling it to move beyond its ``comfort zone'' and mitigate exploration
stagnation. StepHint outperforms competitive RLVR enhancement methods across
six mathematical benchmarks, while also demonstrating superior generalization
and excelling over baselines on out-of-domain benchmarks.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [32] [McBE: A Multi-task Chinese Bias Evaluation Benchmark for Large Language Models](https://arxiv.org/abs/2507.02088)
*Tian Lan,Xiangdong Su,Xu Liu,Ruirui Wang,Ke Chang,Jiang Li,Guanglai Gao*

Main category: cs.CL

TL;DR: 本文提出了一个针对中文及多偏见类别的评测基准McBE，填补了相关领域数据和工具的空白，实验发现主流大语言模型都存在不同程度的偏见，并提供了详细的分析和见解。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）在多种自然语言处理任务中的应用增多，其固有的偏见问题也逐渐暴露。当前大多数偏见评测数据集主要聚焦于英语和北美文化，相关类别对于其他文化的适用性有限，中文相关的数据集较为稀缺，且多为单一评测任务，无法多维度评估LLM的偏见。

Method: 提出了一个多任务中文偏见评测基准（McBE），包含4077个评测实例，涵盖12个主偏见类别、82个子类别，设计了5个评测任务，实现了更广泛的类别覆盖、内容多样性和评测全面性。此外，对不同系列及参数规模的主流LLMs进行了偏见评测和深入分析。

Result: 所有被评测的LLMs均表现出不同程度的偏见。通过多面向的实验和分析，论文揭示了当前LLMs在中文语境下的偏见现状，并提供了新的见解。

Conclusion: McBE为中文场景下LLMs偏见测量研究提供了完善的数据和分析工具，有助于推动对此类模型伦理风险的理解和缓解。

Abstract: As large language models (LLMs) are increasingly applied to various NLP
tasks, their inherent biases are gradually disclosed. Therefore, measuring
biases in LLMs is crucial to mitigate its ethical risks. However, most existing
bias evaluation datasets focus on English and North American culture, and their
bias categories are not fully applicable to other cultures. The datasets
grounded in the Chinese language and culture are scarce. More importantly,
these datasets usually only support single evaluation tasks and cannot evaluate
the bias from multiple aspects in LLMs. To address these issues, we present a
Multi-task Chinese Bias Evaluation Benchmark (McBE) that includes 4,077 bias
evaluation instances, covering 12 single bias categories, 82 subcategories and
introducing 5 evaluation tasks, providing extensive category coverage, content
diversity, and measuring comprehensiveness. Additionally, we evaluate several
popular LLMs from different series and with parameter sizes. In general, all
these LLMs demonstrated varying degrees of bias. We conduct an in-depth
analysis of results, offering novel insights into bias in LLMs.

</details>


### [33] [Reasoning or Not? A Comprehensive Evaluation of Reasoning LLMs for Dialogue Summarization](https://arxiv.org/abs/2507.02145)
*Keyan Jin,Yapeng Wang,Leonel Santos,Tao Fang,Xu Yang,Sio Kei Im,Hugo Gonçalo Oliveira*

Main category: cs.CL

TL;DR: 研究发现：对话摘要任务中，显式长链条推理未带来质量提升，推理型LLM反而更啰嗦、不准确。呼吁为对话摘要场景设计更适合的模型与评估方法。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型（LLMs）在摘要任务中取得了显著进展，但在需要同时兼顾抽象和简洁的对话摘要场景下，长链条推理（Long Chain-of-Thought, CoT）方法的表现尚未被充分探索。

Method: 对最先进的推理型LLM和非推理型LLM在三大对话摘要范式（通用型、角色导向型和查询导向型）下进行了系统性比较。测试涵盖多语言、多领域和多摘要长度，采用包括LLM自动评测和基于人类标准的评测协议，并使用了众多强基准数据集。

Result: 结果发现，显式的分步推理在对话摘要任务中并不会持续提升摘要质量。推理型LLM相较于非推理型LLM，在简洁性、事实一致性和简练性方面表现更差。还通过场景分析和案例研究，具体揭示了推理为何有时无助甚至妨碍复杂对话摘要。

Conclusion: 当前推理型LLM在对话摘要中的能力存在局限性，需针对性建模与评估措施以提升实际对话摘要效果。

Abstract: Dialogue summarization is a challenging task with significant practical value
in customer service, meeting analysis, and conversational AI. Although large
language models (LLMs) have achieved substantial progress in summarization
tasks, the performance of step-by-step reasoning architectures-specifically
Long Chain-of-Thought (CoT) implementations such as OpenAI-o1 and
DeepSeek-R1-remains unexplored for dialogue scenarios requiring concurrent
abstraction and conciseness. In this work, we present the first comprehensive
and systematic evaluation of state-of-the-art reasoning LLMs and non-reasoning
LLMs across three major paradigms-generic, role-oriented, and query-oriented
dialogue summarization. Our study spans diverse languages, domains, and summary
lengths, leveraging strong benchmarks (SAMSum, DialogSum, CSDS, and QMSum) and
advanced evaluation protocols that include both LLM-based automatic metrics and
human-inspired criteria. Contrary to trends in other reasoning-intensive tasks,
our findings show that explicit stepwise reasoning does not consistently
improve dialogue summarization quality. Instead, reasoning LLMs are often prone
to verbosity, factual inconsistencies, and less concise summaries compared to
their non-reasoning counterparts. Through scenario-specific analyses and
detailed case studies, we further identify when and why explicit reasoning may
fail to benefit-or even hinder-summarization in complex dialogue contexts. Our
work provides new insights into the limitations of current reasoning LLMs and
highlights the need for targeted modeling and evaluation strategies for
real-world dialogue summarization.

</details>


### [34] [Latent Chain-of-Thought? Decoding the Depth-Recurrent Transformer](https://arxiv.org/abs/2507.02199)
*Wenquan Lu,Yuechuan Yang,Kyle Lee,Yanshu Li,Enqi Liu*

Main category: cs.CL

TL;DR: 研究复用层的递归Transformer模型Huginn-3.5B在算术推理中的表现，虽能实现有限的隐式链式思维，但可解释性与性能均不如传统显性推理方法，递归带来的增益也很有限。


<details>
  <summary>Details</summary>
Motivation: 链式思维（CoT）推理提升了基于transformer的语言模型在复杂数学与多步规划任务上的能力，但标准解码器仅基于自然语言外部显式推理，虽然可解释性好但效率低。为实现非语言易表达的推理，出现了将推理过程内化于隐空间的递归结构设计。本研究关注于是否真的在深度递归Transformer中能自发涌现出这类隐式推理结构。

Method: 作者研究了Huginn-3.5B模型，这是一种在推理时复用层结构且参数量不变的深度递归Transformer，针对算术任务，引入Logit Lens和Coda Lens等一系列探测技术分析模型内部隐状态和表示路径。

Result: 研究发现，在跟踪最终与中间结果Token的秩轨迹时，仅观察到有限的可解释隐式链式思维证据。同时，不同递归块间的探测结果存在显著不一致，隐状态的可解释性高度依赖于层索引和解码方式。此外，递归深度的提升带来仅有的边际性能提升，远不如明确外部化推理步骤的模型效果。

Conclusion: 采用递归结构试图在Transformer中内化链式思维推理，仅获得有限的可解释性与推理能力增益，且其表现不如外部显式推理模型。

Abstract: Chain-of-thought (CoT) reasoning has enabled transformer-based language
models to excel at complex mathematics and multi-step planning. However, in
standard decoder-only architectures, these reasoning steps are externalized in
natural language, improving interpretability at the cost of efficiency. To
capture reasoning that is not easily represented in words, many works have
explored recurrent architectures that aim to internalize reasoning in latent
space, potentially supporting latent CoT. In this paper, we investigate whether
such reasoning structures emerge in Huginn-3.5B, a depth-recurrent Transformer
that reuses layers at inference time without increasing parameter count. We
examine the model's internal behavior on arithmetic tasks using a suite of
probing techniques including the Logit Lens and Coda Lens. Our findings reveal
limited evidence of interpretable latent CoT by tracking rank trajectories of
final and intermediate result tokens. Furthermore, we uncover significant
probing inconsistencies across recurrent blocks, where the interpretability of
hidden states depends heavily on both the layer index and the decoding method.
Finally, we empirically show that increasing recurrence depth yields only
marginal gains and falls well short of models that explicitly externalize
reasoning steps. The code is available at
https://github.com/wenquanlu/huginn-latent-cot.

</details>


### [35] [GDC Cohort Copilot: An AI Copilot for Curating Cohorts from the Genomic Data Commons](https://arxiv.org/abs/2507.02221)
*Steven Song,Anirudh Subramanyam,Zhenyu Zhang,Aarti Venkat,Robert L. Grossman*

Main category: cs.CL

TL;DR: GDC Cohort Copilot通过大语言模型，将用户的自然语言队列描述自动转化为可用于GDC的数据筛选条件，显著简化了队列构建流程，并且工具及模型均已开源。


<details>
  <summary>Details</summary>
Motivation: GDC提供了高质量、统一标准的癌症基因组数据，通过可视化界面可以构建复杂人群队列（cohort），但面对众多字段和属性，尤其是新用户，筛选指定队列仍存较大难度。相比之下，用户用自然语言描述所需队列会更直观方便。

Method: 提出并实现了GDC Cohort Copilot工具，该工具利用大语言模型（LLM）将用户输入的自然语言队列描述，自动生成GDC的数据过滤条件，并能导回GDC平台做进一步分析。用户界面支持对自动生成的队列进一步交互式调整。作者开发并评估了多种大语言模型用于此功能。

Result: 本地部署、开源的GDC Cohort LLM在GDC队列生成任务表现优于基于GPT-4o的提示。工具及源代码、模型权重均已开源。

Conclusion: GDC Cohort Copilot能有效将自然语言描述转化为精准的GDC队列过滤条件，大大提升了癌症基因组数据使用的便利性和智能化水平，尤其有助于新用户快速筛选数据。其开源特性也便于学术和实际应用推广。

Abstract: Motivation: The Genomic Data Commons (GDC) provides access to high quality,
harmonized cancer genomics data through a unified curation and analysis
platform centered around patient cohorts. While GDC users can interactively
create complex cohorts through the graphical Cohort Builder, users (especially
new ones) may struggle to find specific cohort descriptors across hundreds of
possible fields and properties. However, users may be better able to describe
their desired cohort in free-text natural language.
  Results: We introduce GDC Cohort Copilot, an open-source copilot tool for
curating cohorts from the GDC. GDC Cohort Copilot automatically generates the
GDC cohort filter corresponding to a user-input natural language description of
their desired cohort, before exporting the cohort back to the GDC for further
analysis. An interactive user interface allows users to further refine the
generated cohort. We develop and evaluate multiple large language models (LLMs)
for GDC Cohort Copilot and demonstrate that our locally-served, open-source GDC
Cohort LLM achieves better results than GPT-4o prompting in generating GDC
cohorts.
  Availability and implementation: The standalone docker image for GDC Cohort
Copilot is available at https://quay.io/repository/cdis/gdc-cohort-copilot.
Source code is available at https://github.com/uc-cdis/gdc-cohort-copilot. GDC
Cohort LLM weights are available at https://huggingface.co/uc-ctds.

</details>


### [36] [MemAgent: Reshaping Long-Context LLM with Multi-Conv RL-based Memory Agent](https://arxiv.org/abs/2507.02259)
*Hongli Yu,Tinghong Chen,Jiangtao Feng,Jiangjie Chen,Weinan Dai,Qiying Yu,Ya-Qin Zhang,Wei-Ying Ma,Jingjing Liu,Mingxuan Wang,Hao Zhou*

Main category: cs.CL

TL;DR: 本文提出MemAgent，通过分段读取及内存重写策略，实现了对超长文本的高效处理，在超长文本任务上表现优越，为线性复杂度下的长文本处理带来新突破。


<details>
  <summary>Details</summary>
Motivation: 现有的高效注意力机制和内存模块虽然能支持一定程度的上下文长度扩展，但在以线性复杂度处理极长文本且不损失性能方面仍有挑战。本文试图解决如何高效、无性能衰减地处理无限长文档的问题。

Method: 提出了一种新的代理工作流MemAgent，该方法分段读取文本，并采用重写策略来更新内存。同时，扩展了DAPO算法，支持通过独立上下文的多轮生成进行训练，实现端到端优化。

Result: MemAgent在长文本任务中展现出极佳的能力：以8K上下文训练，在32K文本上外推，迁移到3.5M的QA任务时性能损失小于5%；在512K的RULER测试中准确率超过95%。

Conclusion: MemAgent通过创新的记忆更新策略与高效训练方式，有效解决了长文本处理中的线性复杂度与性能衰减难题，展示了卓越的长文本泛化和 extrapolation 能力。

Abstract: Despite improvements by length extrapolation, efficient attention and memory
modules, handling infinitely long documents with linear complexity without
performance degradation during extrapolation remains the ultimate challenge in
long-text processing. We directly optimize for long-text tasks in an end-to-end
fashion and introduce a novel agent workflow, MemAgent, which reads text in
segments and updates the memory using an overwrite strategy. We extend the DAPO
algorithm to facilitate training via independent-context multi-conversation
generation. MemAgent has demonstrated superb long-context capabilities, being
able to extrapolate from an 8K context trained on 32K text to a 3.5M QA task
with performance loss < 5% and achieves 95%+ in 512K RULER test.

</details>


### [37] [DoMIX: An Efficient Framework for Exploiting Domain Knowledge in Fine-Tuning](https://arxiv.org/abs/2507.02302)
*Dohoon Kim,Donghun Kang,Taesup Moon*

Main category: cs.CL

TL;DR: DoMIX利用LoRA技术实现了高效、低资源占用且顺序无关的领域自适应预训练，并能为不同任务定制专属预训练模型，超越传统方法的局限，可应用于大模型微调。


<details>
  <summary>Details</summary>
Motivation: 近年来，领域自适应预训练（DAP）已因其在微调预训练模型方面的有效性受到关注。为解决在增量式融入不同领域数据集时遇到的高算力消耗、顺序敏感和单一泛化模型等问题，本文提出创新方法。

Method: 提出一种名为DoMIX的新方法，利用LoRA模块（一种高效参数微调方法），实现高效且并行的领域自适应预训练。DoMIX设计为对领域顺序不敏感，能充分利用已有知识，并为特定任务提供定制化的预训练模型。

Result: DoMIX能在保证训练高效与低显存占用的同时，针对不同下游任务生成专属适配模型，显示出较强的稳健性和灵活性。此外，该方法可扩展应用于标准大模型（LLM）微调场景。

Conclusion: DoMIX突破了现有的continual DAP方法在算力消耗、顺序敏感和模型个性化方面的限制，为领域自适应和大模型微调提供了新的高效思路。

Abstract: Domain-Adaptive Pre-training (DAP) has recently gained attention for its
effectiveness in fine-tuning pre-trained models. Building on this, continual
DAP has been explored to develop pre-trained models capable of incrementally
incorporating different domain datasets. However, existing continual DAP
methods face several limitations: (1) high computational cost and GPU memory
usage during training; (2) sensitivity to incremental data order; and (3)
providing a single, generalized model for all end tasks, which contradicts the
essence of DAP. In this paper, we propose DoMIX, a novel approach that
addresses these challenges by leveraging LoRA modules, a representative
parameter-efficient fine-tuning (PEFT) method. Our approach enables efficient
and parallel domain-adaptive pre-training that is robust to domain order and
effectively utilizes accumulated knowledge to provide tailored pre-trained
models for specific tasks. We also demonstrate that our method can be extended
beyond the DAP setting to standard LLM fine-tuning scenarios. Code is available
at https://github.com/dohoonkim-ai/DoMIX.

</details>


### [38] [Coling-UniA at SciVQA 2025: Few-Shot Example Retrieval and Confidence-Informed Ensembling for Multimodal Large Language Models](https://arxiv.org/abs/2507.02357)
*Christian Jaumann,Annemarie Friedrich,Rainer Lienhart*

Main category: cs.CL

TL;DR: 该论文提出的多模态模型集成系统在SciVQA 2025视觉问答竞赛中取得第三名（F1 85.12），采用few-shot检索及置信度选择，系统代码已公开。


<details>
  <summary>Details</summary>
Motivation: 为提升科学领域视觉问答（SciVQA）任务的效果，研究如何结合多模态大语言模型与few-shot学习策略。

Method: 采用了两种多模态大语言模型集成，并结合多种few-shot示例检索策略，根据图像和问题类型选择模型和few-shot设置，同时根据模型的置信度选择答案。

Result: 该系统在七支参赛队伍中排名第三，平均F1分数达到85.12，代码已开源。

Conclusion: 该系统在SciVQA 2025竞赛中表现优异，在盲测数据集上获得第三名，平均F1分数为85.12。

Abstract: This paper describes our system for the SciVQA 2025 Shared Task on Scientific
Visual Question Answering. Our system employs an ensemble of two Multimodal
Large Language Models and various few-shot example retrieval strategies. The
model and few-shot setting are selected based on the figure and question type.
We also select answers based on the models' confidence levels. On the blind
test data, our system ranks third out of seven with an average F1 score of
85.12 across ROUGE-1, ROUGE-L, and BERTS. Our code is publicly available.

</details>


### [39] [QFFN-BERT: An Empirical Study of Depth, Performance, and Data Efficiency in Hybrid Quantum-Classical Transformers](https://arxiv.org/abs/2507.02364)
*Pilsung Kang*

Main category: cs.CL

TL;DR: 本文提出用参数化量子电路替换BERT中的前馈网络，大幅缩减参数量并提升表现力。在多个任务和场景下实验结果显示，适当设计的QFFN-BERT参数更少、精度更高，尤其在少样本学习有优势，为PQC与主流神经网络的结合提供了新方案。


<details>
  <summary>Details</summary>
Motivation: 随着参数化量子电路(PQC)在提升神经网络表达能力方面的潜力逐渐被关注，现有研究多集中在将PQC集成到自注意力模块中，而标准Transformer的大部分参数却集中在前馈网络(FFN)。因此，该研究希望通过替换FFN为PQC层，探索量子混合模型在参数节省和表现力上的优势。

Method: 作者提出了QFFN-BERT，将紧凑型BERT变体的FFN模块用PQC层替换。设计中引入了残差连接、$R_Y$与$R_Z$旋转以及交替纠缠策略，以提升训练稳定性和表达能力，并在经典仿真器上于SST-2、DBpedia数据集进行实验。作者还做了消融实验，对比不同PQC结构的学习效果。

Result: QFFN-BERT在全数据条件下能达到或超过原BERT基线(准确率最高达102%)，同时FFN模块参数减少超99%。此外，在少样本学习中展现出更优的表现与数据效率。消融实验显示，若设计不当(PQC未优化)，模型难以学习，验证了合理PQC结构的重要性。

Conclusion: PQC结合深度学习原则后能够成为高效且有表现力的FFN替代方案，大幅降低参数量，同时不但保持甚至提升了模型表现，尤其在数据有限场景拥有竞争优势。

Abstract: Parameterized quantum circuits (PQCs) have recently emerged as promising
components for enhancing the expressibility of neural architectures. In this
work, we introduce QFFN-BERT, a hybrid quantum-classical transformer where the
feedforward network (FFN) modules of a compact BERT variant are replaced by
PQC-based layers. This design is motivated by the dominant parameter
contribution of FFNs, which account for approximately two-thirds of the
parameters within standard Transformer encoder blocks. While prior studies have
primarily integrated PQCs into self-attention modules, our work focuses on the
FFN and systematically investigates the trade-offs between PQC depth,
expressibility, and trainability. Our final PQC architecture incorporates a
residual connection, both $R_Y$ and $R_Z$ rotations, and an alternating
entanglement strategy to ensure stable training and high expressibility. Our
experiments, conducted on a classical simulator, on the SST-2 and DBpedia
benchmarks demonstrate two key findings. First, a carefully configured
QFFN-BERT achieves up to 102.0% of the baseline accuracy, surpassing its
classical counterpart in a full-data setting while reducing FFN-specific
parameters by over 99%. Second, our model exhibits a consistent and competitive
edge in few-shot learning scenarios, confirming its potential for superior data
efficiency. These results, supported by an ablation study on a non-optimized
PQC that failed to learn, confirm that PQCs can serve as powerful and
parameter-efficient alternatives to classical FFNs when co-designed with
foundational deep learning principles.

</details>


### [40] [Efficient Code LLM Training via Distribution-Consistent and Diversity-Aware Data Selection](https://arxiv.org/abs/2507.02378)
*Weijie Lyu,Sheng-Jun Huang,Xuan Xia*

Main category: cs.CL

TL;DR: 本文提出了一种基于参数化模型的数据选择方法，仅需少量高质量数据即可显著提升代码生成大模型的性能，同时大幅减少训练资源消耗。


<details>
  <summary>Details</summary>
Motivation: 当前大部分提升大模型代码生成能力的方法主要依赖海量数据，注重数据量却忽视了数据质量，导致训练效率下降。该文旨在解决数据质量在模型训练中的不足，提升模型性能和训练效率。

Method: 提出一种利用参数化模型进行代码数据选择的方法。该方法优化参数化模型，保证所选子集的数据分布一致性与多样性，从而筛选高质量训练数据。

Result: 实验结果显示，在仅用1万条样本训练的情况下，该方法在HumanEval和MBPP数据集上分别比使用全部9.2万条样本的基线高2.4%和2.3%，且优于其他采样方法。

Conclusion: 该方法在大幅降低计算成本的同时，有效提升了大模型的代码生成与理解能力。

Abstract: Recent advancements in large language models (LLMs) have significantly
improved code generation and program comprehension, accelerating the evolution
of software engineering. Current methods primarily enhance model performance by
leveraging vast amounts of data, focusing on data quantity while often
overlooking data quality, thereby reducing training efficiency. To address
this, we introduce an approach that utilizes a parametric model for code data
selection, aimed at improving both training efficiency and model performance.
Our method optimizes the parametric model to ensure distribution consistency
and diversity within the selected subset, guaranteeing high-quality data.
Experimental results demonstrate that using only 10K samples, our method
achieves gains of 2.4% (HumanEval) and 2.3% (MBPP) over 92K full-sampled
baseline, outperforming other sampling approaches in both performance and
efficiency. This underscores that our method effectively boosts model
performance while significantly reducing computational costs.

</details>


### [41] [Benchmarking Akan ASR Models Across Domain-Specific Datasets: A Comparative Evaluation of Performance, Scalability, and Adaptability](https://arxiv.org/abs/2507.02407)
*Mark Atta Mensah,Isaac Wiafe,Akon Ekpezu,Justice Kwame Appati,Jamal-Deen Abdulai,Akosua Nyarkoa Wiafe-Akenten,Frank Ernest Yeboah,Gifty Odame*

Main category: cs.CL

TL;DR: 本文评估了七个Akan语ASR模型在四个不同领域数据下的泛化能力，发现模型在领域外准确率下降明显，并揭示了两大主流架构（Whisper、Wav2Vec2）在转录错误上的不同特性。研究指出需发展领域自适应、多语种训练等方法以提升低资源语言的ASR应用。


<details>
  <summary>Details</summary>
Motivation: 当前大多数自动语音识别（ASR）研究主要在同域数据集上评估模型，很少关注它们在多样语音场景下的泛化能力。该研究旨在填补这一空白，探讨Akan语ASR模型在不同领域下的表现。

Method: 基于transformer结构（如Whisper和Wav2Vec2）构建了七个Akan语ASR模型，并利用四个来源不同的Akan语音数据集进行基准测试。数据涵盖文化图片描述、非正式对话、圣经朗读及财务对话等领域，通过评估词错误率和字符错误率比较模型性能。

Result: 实验发现，ASR模型在其训练领域内表现最佳，但在领域不匹配时准确率显著下降。Whisper结构在遇到陌生输入时，表现为输出更流畅但可能误导的转录错误；Wav2Vec2则输出更明显但难以理解的错误。结果揭示了ASR错误在可读性和透明度之间的权衡。

Conclusion: 研究建议在低资源语言ASR模型选型时需考虑错误的可读性与可解释性，并强调针对域自适应技术、自适应路由策略及多语种训练框架的重要性，以提升泛化能力。

Abstract: Most existing automatic speech recognition (ASR) research evaluate models
using in-domain datasets. However, they seldom evaluate how they generalize
across diverse speech contexts. This study addresses this gap by benchmarking
seven Akan ASR models built on transformer architectures, such as Whisper and
Wav2Vec2, using four Akan speech corpora to determine their performance. These
datasets encompass various domains, including culturally relevant image
descriptions, informal conversations, biblical scripture readings, and
spontaneous financial dialogues. A comparison of the word error rate and
character error rate highlighted domain dependency, with models performing
optimally only within their training domains while showing marked accuracy
degradation in mismatched scenarios. This study also identified distinct error
behaviors between the Whisper and Wav2Vec2 architectures. Whereas fine-tuned
Whisper Akan models led to more fluent but potentially misleading transcription
errors, Wav2Vec2 produced more obvious yet less interpretable outputs when
encountering unfamiliar inputs. This trade-off between readability and
transparency in ASR errors should be considered when selecting architectures
for low-resource language (LRL) applications. These findings highlight the need
for targeted domain adaptation techniques, adaptive routing strategies, and
multilingual training frameworks for Akan and other LRLs.

</details>


### [42] [A Cookbook for Community-driven Data Collection of Impaired Speech in LowResource Languages](https://arxiv.org/abs/2507.02428)
*Sumaya Ahmed Salihs,Isaac Wiafe,Jamal-Deen Abdulai,Elikem Doe Atsakpo,Gifty Ayoka,Richard Cave,Akon Obu Ekpezu,Catherine Holloway,Katrin Tomanek,Fiifi Baffoe Payin Winful*

Main category: cs.CL

TL;DR: 本文提出了一套针对低资源语言和言语障碍人士的ASR数据采集与模型微调方法，并首次公开了加纳Akan语言障碍语音数据集与配套实践手册，为相关弱势群体的语音识别技术发展奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 目前自动语音识别（ASR）技术在处理非主流、资源匮乏语言及有言语障碍者语音时存在数据短缺与技术壁垒，限制了残障人士的技术普惠及使用便利性。

Method: 该研究制定了一套社区驱动的数据采集、ASR模型构建的最佳实践指导手册（‘cookbook’），并组织了实际采集，收集了来自患有言语障碍者的Akan语音样本，构建了开源数据集。同时，利用这些数据集对开源ASR模型进行了微调以改善对言语障碍语音的识别能力。

Result: 研究取得了首个针对Akan语言障碍语音的开源数据集，并将其与cookbook以及相关开源工具一同免费公开。此外，初步实验显示对ASR模型的微调能提升其对Akan语言障碍语音的识别能力。

Conclusion: 社区驱动的数据采集与模型构建实践可有效推动低资源语言和言语障碍人群的ASR技术发展，提高其在更多弱势群体中的可用性和包容性。

Abstract: This study presents an approach for collecting speech samples to build
Automatic Speech Recognition (ASR) models for impaired speech, particularly,
low-resource languages. It aims to democratize ASR technology and data
collection by developing a "cookbook" of best practices and training for
community-driven data collection and ASR model building. As a proof-of-concept,
this study curated the first open-source dataset of impaired speech in Akan: a
widely spoken indigenous language in Ghana. The study involved participants
from diverse backgrounds with speech impairments. The resulting dataset, along
with the cookbook and open-source tools, are publicly available to enable
researchers and practitioners to create inclusive ASR technologies tailored to
the unique needs of speech impaired individuals. In addition, this study
presents the initial results of fine-tuning open-source ASR models to better
recognize impaired speech in Akan.

</details>


### [43] [IndianBailJudgments-1200: A Multi-Attribute Dataset for Legal NLP on Indian Bail Orders](https://arxiv.org/abs/2507.02506)
*Sneha Deshmukh,Prathmesh Kamble*

Main category: cs.CL

TL;DR: 作者发布了第一个专注印度保释判例的高质量数据集IndianBailJudgments-1200，利用GPT-4o自动标注20余项法律属性，为相关法律NLP研究提供了有力的数据支持。


<details>
  <summary>Details</summary>
Motivation: 印度法域的法律NLP发展不足，主要原因为缺乏结构化数据集。

Method: 构建并发布了IndianBailJudgments-1200数据集，包含1200份印度法院保释判决，通过prompt工程化的GPT-4o进行属性标注，并进行一致性校验。

Result: 成功构建了包含20多个属性（如裁决结果、IPC条款、犯罪类型、法律推理等）的标注数据集，为法律NLP任务提供了高质量资源。

Conclusion: IndianBailJudgments-1200是首个专注于印度保释判例的公开数据集，将极大促进该领域法律NLP的研究，包括判决预测、摘要与公平性分析等。

Abstract: Legal NLP remains underdeveloped in regions like India due to the scarcity of
structured datasets. We introduce IndianBailJudgments-1200, a new benchmark
dataset comprising 1200 Indian court judgments on bail decisions, annotated
across 20+ attributes including bail outcome, IPC sections, crime type, and
legal reasoning. Annotations were generated using a prompt-engineered GPT-4o
pipeline and verified for consistency. This resource supports a wide range of
legal NLP tasks such as outcome prediction, summarization, and fairness
analysis, and is the first publicly available dataset focused specifically on
Indian bail jurisprudence.

</details>


### [44] [WebSailor: Navigating Super-human Reasoning for Web Agent](https://arxiv.org/abs/2507.02592)
*Kuan Li,Zhongwang Zhang,Huifeng Yin,Liwen Zhang,Litu Ou,Jialong Wu,Wenbiao Yin,Baixuan Li,Zhengwei Tao,Xinyu Wang,Weizhou Shen,Junkai Zhang,Dingchu Zhang,Xixi Wu,Yong Jiang,Ming Yan,Pengjun Xie,Fei Huang,Jingren Zhou*

Main category: cs.CL

TL;DR: WebSailor通过创新的后训练流程与强化学习算法，解决了开源大模型在高不确定性信息检索任务下的推理短板，实现了对闭源超强系统的追赶和能力提升。


<details>
  <summary>Details</summary>
Motivation: 现有开源大模型在极高复杂度的信息检索任务上存在认知极限，难以达到闭源系统如DeepResearch的超人类表现。其根本原因在于缺乏系统性降低极度不确定性并有效导航海量信息的推理模式。

Method: 提出了一种完整的后训练方法WebSailor，通过结构化采样与信息遮蔽生成高不确定性任务、RFT冷启动以及高效的agent式强化学习训练算法（DUPO）组成集成流程。

Result: WebSailor在复杂的信息检索任务上显著超越了所有开源智能体，表现达到甚至匹敌闭源系统，缩小了能力差距。

Conclusion: 通过WebSailor后训练方案，开源大型语言模型在极端不确定性环境中的信息导航与推理能力大幅提升，具备与顶级闭源系统媲美的超人类水平，展示了突破人类认知极限的潜力。

Abstract: Transcending human cognitive limitations represents a critical frontier in
LLM training. Proprietary agentic systems like DeepResearch have demonstrated
superhuman capabilities on extremely complex information-seeking benchmarks
such as BrowseComp, a feat previously unattainable. We posit that their success
hinges on a sophisticated reasoning pattern absent in open-source models: the
ability to systematically reduce extreme uncertainty when navigating vast
information landscapes. Based on this insight, we introduce WebSailor, a
complete post-training methodology designed to instill this crucial capability.
Our approach involves generating novel, high-uncertainty tasks through
structured sampling and information obfuscation, RFT cold start, and an
efficient agentic RL training algorithm, Duplicating Sampling Policy
Optimization (DUPO). With this integrated pipeline, WebSailor significantly
outperforms all opensource agents in complex information-seeking tasks,
matching proprietary agents' performance and closing the capability gap.

</details>


### [45] [Revisiting Active Learning under (Human) Label Variation](https://arxiv.org/abs/2507.02593)
*Cornelia Gruber,Helen Alber,Bernd Bischl,Göran Kauermann,Barbara Plank,Matthias Aßenmacher*

Main category: cs.CL

TL;DR: 本文指出单一真实标签假设不适应实际应用，主张应将标签变异细分为有用信号与噪声，提出使主动学习流程关注人类标签变异（HLV）的理论框架，并建议在样本选择、标注者选择和标签表示等环节中全面考虑HLV，为未来现实场景下的高效标注提供理论基础。


<details>
  <summary>Details</summary>
Motivation: 目前有监督学习受限于高质量标注数据的获取，而标注结果的多样性（LV）在实际中很常见，尤其是在自然语言处理领域。当前的标注框架多假设单一的“真实标签”，忽视了“人类标签变异”（HLV）作为一种有信息量的信号。另外，主动学习方法通常基于一些难以在真实条件下满足的简化假设，因此有必要重新审视和完善这些基础假设。

Method: 作者对“真值”与标签属性的基础假设进行了反思，提出应将观测到的标签变异（LV）分解为信号（如HLV）和噪声（如标注错误）。文中综述了主动学习（AL）和标签变异（(H)LV）领域相关研究中区分或忽视这些因素的做法，并提出了在主动学习流程（包括样本选择、标注者选择和标签表示）中融入HLV的概念框架，同时探讨了将大语言模型作为标注者的可能性。

Result: 提出了一个在主动学习流程中全面考虑人类标签变异（HLV）的概念框架，并系统性总结了现有工作在信号与噪声区分上的优势和不足，对主动学习应用于现实复杂标注环境具有理论指导意义。

Conclusion: 本文为主动学习中充分利用并区分人类标签变异和标注错误奠定了理论基础，并提出了具体的HLV感知型主动学习流程设计建议，对更真实反映现实中标注复杂性的系统有重要意义。

Abstract: Access to high-quality labeled data remains a limiting factor in applied
supervised learning. While label variation (LV), i.e., differing labels for the
same instance, is common, especially in natural language processing, annotation
frameworks often still rest on the assumption of a single ground truth. This
overlooks human label variation (HLV), the occurrence of plausible differences
in annotations, as an informative signal. Similarly, active learning (AL), a
popular approach to optimizing the use of limited annotation budgets in
training ML models, often relies on at least one of several simplifying
assumptions, which rarely hold in practice when acknowledging HLV. In this
paper, we examine foundational assumptions about truth and label nature,
highlighting the need to decompose observed LV into signal (e.g., HLV) and
noise (e.g., annotation error). We survey how the AL and (H)LV communities have
addressed -- or neglected -- these distinctions and propose a conceptual
framework for incorporating HLV throughout the AL loop, including instance
selection, annotator choice, and label representation. We further discuss the
integration of large language models (LLM) as annotators. Our work aims to lay
a conceptual foundation for HLV-aware active learning, better reflecting the
complexities of real-world annotation.

</details>


### [46] [MPF: Aligning and Debiasing Language Models post Deployment via Multi Perspective Fusion](https://arxiv.org/abs/2507.02595)
*Xin Guan,PeiHsin Lin,Zekun Wu,Ze Wang,Ruibo Zhang,Emre Kazim,Adriano Koshiyama*

Main category: cs.CL

TL;DR: MPF是一种无需复杂微调即可用在现有LLM上的后训练偏见缓解和对齐方法，兼具可解释性和高效性，能有效对齐输出分布与多种基线。


<details>
  <summary>Details</summary>
Motivation: LLMs（大型语言模型）在实际应用中经常存在输出偏见，当前缺乏简单、高效且可解释的后处理方法来消除或对齐这些偏见，因此有必要开发一种提升LLM输出公平性和可解释性的对齐方法。

Method: 提出了Multiperspective Fusion（MPF）作为新的后训练对齐框架。MPF基于SAGED管道，通过将基线（如HR专业人士的情感分布）分解为多视角成分，然后引导、采样并加权生成输出，实现兼容实际人类视角的输出分布，从而对齐和缓解偏见。

Result: 实验表明，MPF能显著对齐LLM的情感分布与反事实公平基线和现实HR基线（如偏向顶尖大学），在KL散度、校准误差和泛化能力上均优于现有方法。

Conclusion: MPF为LLM的后处理对齐和偏见缓解提供了一种可扩展、兼容性强、无需复杂微调的新方法，并具备良好的可解释性。

Abstract: Multiperspective Fusion (MPF) is a novel posttraining alignment framework for
large language models (LLMs) developed in response to the growing need for easy
bias mitigation. Built on top of the SAGED pipeline, an automated system for
constructing bias benchmarks and extracting interpretable baseline
distributions, MPF leverages multiperspective generations to expose and align
biases in LLM outputs with nuanced, humanlike baselines. By decomposing
baseline, such as sentiment distributions from HR professionals, into
interpretable perspective components, MPF guides generation through sampling
and balancing of responses, weighted by the probabilities obtained in the
decomposition. Empirically, we demonstrate its ability to align LLM sentiment
distributions with both counterfactual baselines (absolute equality) and the HR
baseline (biased for Top Univeristy), resulting in small KL divergence,
reduction of calibration error and generalization to unseen questions. This
shows that MPF offers a scalable and interpretable method for alignment and
bias mitigation, compatible with deployed LLMs and requiring no extensive
prompt engineering or finetuning.

</details>


### [47] [Exploring Gender Bias Beyond Occupational Titles](https://arxiv.org/abs/2507.02679)
*Ahmed Sabir,Rajesh Sharama*

Main category: cs.CL

TL;DR: 本文提出了GenderLexicon数据集和新框架来量化解释性别偏见，证实性别偏见存在于多种情境，方法在多个数据集上有效。


<details>
  <summary>Details</summary>
Motivation: 研究性别与情境偏见之间的关系，尤其是职业等元素中的性别偏见。

Method: 提出了新的数据集GenderLexicon和评估性别偏见的新框架，通过打分量化解释性别偏见，在五个不同的数据集（包含日语数据集）进行实验验证。

Result: 模型能够对情境性别偏见进行解释和评分，证实了性别偏见不仅存在于职业刻板印象中。

Conclusion: 提出的方法有助于识别与量化不同情境下的性别偏见，提升了性别偏见解释性的能力，并验证了其在多样化数据集上的有效性。

Abstract: In this work, we investigate the correlation between gender and contextual
biases, focusing on elements such as action verbs, object nouns, and
particularly on occupations. We introduce a novel dataset, GenderLexicon, and a
framework that can estimate contextual bias and its related gender bias. Our
model can interpret the bias with a score and thus improve the explainability
of gender bias. Also, our findings confirm the existence of gender biases
beyond occupational stereotypes. To validate our approach and demonstrate its
effectiveness, we conduct evaluations on five diverse datasets, including a
Japanese dataset.

</details>


### [48] [Can LLMs Identify Critical Limitations within Scientific Research? A Systematic Evaluation on AI Research Papers](https://arxiv.org/abs/2507.02694)
*Zhijian Xu,Yilun Zhao,Manasi Patwardhan,Lovekesh Vig,Arman Cohan*

Main category: cs.CL

TL;DR: 本研究提出了一套评测大模型识别论文局限性的基准（LimitGen），并通过文献检索提升模型，证实其在科学评审中可辅助人类发现局限、提供反馈。


<details>
  <summary>Details</summary>
Motivation: 随着科学出版物数量的激增，同行评审面临专业性强且负担加重的问题，目前尚缺乏LLMs协助识别论文局限性的系统性研究。

Method: 提出了科学论文局限性类型的分类法，构建了首个针对LLMs在局限性识别和早期反馈方面能力基准评测LimitGen，包括合成数据（LimitGen-Syn）和真人撰写数据（LimitGen-Human），并结合文献检索技术提升模型能力。

Result: 通过文献检索增强后的LLMs，能够更好地识别和生成科学论文的局限性，向人类评审提供更具体、建设性的反馈。

Conclusion: LLMs结合文献检索，在补充人类同行评审、识别论文局限性方面表现出较大潜力，对提升科研论文评审效率和质量具有重要意义。

Abstract: Peer review is fundamental to scientific research, but the growing volume of
publications has intensified the challenges of this expertise-intensive
process. While LLMs show promise in various scientific tasks, their potential
to assist with peer review, particularly in identifying paper limitations,
remains understudied. We first present a comprehensive taxonomy of limitation
types in scientific research, with a focus on AI. Guided by this taxonomy, for
studying limitations, we present LimitGen, the first comprehensive benchmark
for evaluating LLMs' capability to support early-stage feedback and complement
human peer review. Our benchmark consists of two subsets: LimitGen-Syn, a
synthetic dataset carefully created through controlled perturbations of
high-quality papers, and LimitGen-Human, a collection of real human-written
limitations. To improve the ability of LLM systems to identify limitations, we
augment them with literature retrieval, which is essential for grounding
identifying limitations in prior scientific findings. Our approach enhances the
capabilities of LLM systems to generate limitations in research papers,
enabling them to provide more concrete and constructive feedback.

</details>


### [49] [Measurement of the Granularity of Vowel Production Space By Just Producible Different (JPD) Limens](https://arxiv.org/abs/2507.02744)
*Peter Viechnicki*

Main category: cs.CL

TL;DR: 本研究通过元音模仿实验，首次定量测定了人类元音产生中最小可区分距离（JPD），为理解语音产生及元音体系的结构提供了新理论依据。


<details>
  <summary>Details</summary>
Motivation: 尽管前人已证实元音产生受听觉空间目标调节，但对调节精度——即能精细到多小的听觉差异——尚不清楚。本文旨在定量确定两个元音刺激在听觉空间上需相隔多远，才能让说话者产出的模仿具有可区分性。

Method: 采用元音模仿范式，通过实验测量两组英语说话者在前元音产生中的JPD（F1×F2空间的距离）。

Result: JPD被估算为F1×F2空间中14至51 mels。该结果有助于解释语音产生的表征理论，也对元音系统可能结构及元音音位分布趋势提供心理物理学解释。

Conclusion: 本研究首次测量并提出了人类元音空间中两个元音产生可区分模仿的最小听觉距离（JPD），为元音系统结构提供了理论下界。

Abstract: A body of work over the past several decades has demonstrated that the
complex and coordinated articulatory movements of human vowel production are
governed (at least in part)by control mechanisms whose targets are regions of
auditory space. Within the target region control at the sub-phonemic level has
also been demonstrated. But the degree of accuracy of that control is unknown.
The current work investigates this question by asking how far apart must two
vowel stimuli lie in auditory space in order to yield reliably different
imitations? This distance is termed 'Just Producible Difference' (JPD). The
current study uses a vowel mimicry paradigm to derive the first measurement of
JPD among two sets of English speakers during front vowel production. JPD is
estimated at between 14 and 51 mels in F1 X F2 space. This finding has
implications for episodic theories of speech production. It also clarifies the
possible structures of human vowel systems, by setting a theoretical lower
bound for how close two vowel phonemes may be in a speaker's formant space, and
hence a psychophysical explanation of observed trends in number and patterns of
possible vowel phonemes.

</details>


### [50] [Self-Correction Bench: Revealing and Addressing the Self-Correction Blind Spot in LLMs](https://arxiv.org/abs/2507.02778)
*Ken Tsui*

Main category: cs.CL

TL;DR: 大模型难以发现并修正自身输出中的同类错误，主要因训练中缺乏纠错示例。新提出的评测工具显示主流模型普遍存在自我纠正盲点，简单的提示策略可极大改善这一缺陷。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）尽管变革性强，但在推理过程中仍会犯错，难以自我纠正自身输出的错误，这限制了其可信度。自我纠正能力对于生成式模型而言尤其重要，因此作者针对自我纠正盲点现象展开研究。

Method: 作者提出了一个名为Self-Correction Bench的系统框架，通过人为注入不同复杂度级别的错误，系统性地测量14种模型在自我纠正方面的表现。

Result: 测试结果显示，LLMs平均自我纠正盲点率为64.5%。通过分析训练数据成分发现，人类演示主要给出无错误响应而不是纠错过程，导致自我纠正能力不足。RL（强化学习）训练模型表现更好。在提示中仅添加“Wait”即可显著激活纠错潜力，将盲点率降低89.3%。

Conclusion: 当前主流LLMs存在自我纠正盲点，限制了其可靠性。训练数据和提示方式均影响纠错能力，改进训练及提示机制有望大幅提升模型可靠性。

Abstract: Although large language models (LLMs) have become transformative, they still
make mistakes and can explore unproductive reasoning paths. Self-correction is
an important capability for a trustworthy LLM, particularly an autoregressive
LLM. While LLMs can identify error in user input, they exhibit a systematic
'Self-Correction Blind Spot' - failing to correct identical error in their own
outputs. To systematically study this phenomenon, we introduce Self-Correction
Bench, a systematic framework to measure this phenomenon through controlled
error injection at three complexity levels. Testing 14 models, we find an
average 64.5% blind spot rate. We find multiple evidences that this limitation
relates to training data composition: human training demonstrations
predominantly show error-free responses rather than error-correction sequences,
unlike RL-trained models that learn error correction through outcome feedback.
Remarkably, simply appending "Wait" reduces blind spots by 89.3%, suggesting
that the capability exists but requires activation. Our work highlights a
critical limitation in current LLMs and offers potential avenues for improving
their reliability and trustworthiness.

</details>


### [51] [Is Reasoning All You Need? Probing Bias in the Age of Reasoning Language Models](https://arxiv.org/abs/2507.02799)
*Riccardo Cantini,Nicola Gabriele,Alessio Orsino,Domenico Talia*

Main category: cs.CL

TL;DR: 本研究发现，显式推理机制可能使大语言模型对社会偏见更易受攻击，挑战了“推理带来鲁棒性”的假设，强调需发展更偏见敏感的推理设计。


<details>
  <summary>Details</summary>
Motivation: 当前具备推理能力的大语言模型（Reasoning Language Models, RLMs）在复杂推理任务中表现突出，但其是否能提升模型对社会偏见的鲁棒性尚不明确。本研究动机在于探究引入推理机制后，模型在偏见诱发情境下的安全性与鲁棒性变化。

Method: 利用CLEAR-Bias基准，系统性地评估先进RLMs在不同社会文化维度下对偏见诱发的对抗性鲁棒性。采用“LLM-as-a-judge”自动安全评分和jailbreak技术测试内置安全机制的强度，从推理方式（如Chain-of-Thought提示和微调推理轨迹）进行对比分析。

Result: 实验证明，无论采用CoT提示还是推理微调，具备显式推理能力的模型总体上对偏见诱发比基础模型更脆弱；推理机制可能为刻板印象强化带来新路径。推理微调后的模型比单纯CoT提示的模型稍为安全，但后者特别容易被如故事、虚构人设、奖励诱导等上下文技巧攻破。

Conclusion: 推理能力的引入未必带来社会偏见鲁棒性的提升，甚至可能增加被攻击的风险。仅靠推理并不能保证安全，因此设计更具偏见敏感性的推理机制变得尤为重要。

Abstract: Reasoning Language Models (RLMs) have gained traction for their ability to
perform complex, multi-step reasoning tasks through mechanisms such as
Chain-of-Thought (CoT) prompting or fine-tuned reasoning traces. While these
capabilities promise improved reliability, their impact on robustness to social
biases remains unclear. In this work, we leverage the CLEAR-Bias benchmark,
originally designed for Large Language Models (LLMs), to investigate the
adversarial robustness of RLMs to bias elicitation. We systematically evaluate
state-of-the-art RLMs across diverse sociocultural dimensions, using an
LLM-as-a-judge approach for automated safety scoring and leveraging jailbreak
techniques to assess the strength of built-in safety mechanisms. Our evaluation
addresses three key questions: (i) how the introduction of reasoning
capabilities affects model fairness and robustness; (ii) whether models
fine-tuned for reasoning exhibit greater safety than those relying on CoT
prompting at inference time; and (iii) how the success rate of jailbreak
attacks targeting bias elicitation varies with the reasoning mechanisms
employed. Our findings reveal a nuanced relationship between reasoning
capabilities and bias safety. Surprisingly, models with explicit reasoning,
whether via CoT prompting or fine-tuned reasoning traces, are generally more
vulnerable to bias elicitation than base models without such mechanisms,
suggesting reasoning may unintentionally open new pathways for stereotype
reinforcement. Reasoning-enabled models appear somewhat safer than those
relying on CoT prompting, which are particularly prone to contextual reframing
attacks through storytelling prompts, fictional personas, or reward-shaped
instructions. These results challenge the assumption that reasoning inherently
improves robustness and underscore the need for more bias-aware approaches to
reasoning design.

</details>


### [52] [Multimodal Mathematical Reasoning with Diverse Solving Perspective](https://arxiv.org/abs/2507.02804)
*Wenhao Shi,Zhiqiang Hu,Yi Bin,Yang Yang,See-Kiong Ng,Heng Tao Shen*

Main category: cs.CL

TL;DR: 提出了含多样解题路径的数据集MathV-DP和多视角推理模型Qwen-VL-DP，通过强化学习优化多解和多样性，在数学多模态推理任务中准确率与多样性均超越现有模型。


<details>
  <summary>Details</summary>
Motivation: 当前的大规模强化学习推动了大语言模型在数学推理领域的进步，但现有多模态大语言模型在数学推理上往往只依赖单一影像-文本配对和单一解答监督，忽视了解题方式的多样性和内在反思。为提升推理多样性和多视角学习，需突破这一单一监督的局限。

Method: 本文提出了MathV-DP数据集，涵盖每个图像-问题对的多种不同解题轨迹，以丰富推理监督。此外，提出基于Qwen-VL的模型Qwen-VL-DP，结合有监督微调和群组相对策略优化（GRPO）这一规则驱动的强化学习方法，其中GRPO集成了正确性判别和多样性奖励，以促进模型学习多样化推理方式。

Result: 实验表明，Qwen-VL-DP在MathVista's minitest和Math-V基准上，在准确率和生成多样性方面均大幅超越了以往的多模态大语言模型。

Conclusion: 融入多样化的推理视角和反思机制对提升多模态数学推理至关重要。

Abstract: Recent progress in large-scale reinforcement learning (RL) has notably
enhanced the reasoning capabilities of large language models (LLMs), especially
in mathematical domains. However, current multimodal LLMs (MLLMs) for
mathematical reasoning often rely on one-to-one image-text pairs and
single-solution supervision, overlooking the diversity of valid reasoning
perspectives and internal reflections. In this work, we introduce MathV-DP, a
novel dataset that captures multiple diverse solution trajectories for each
image-question pair, fostering richer reasoning supervision. We further propose
Qwen-VL-DP, a model built upon Qwen-VL, fine-tuned with supervised learning and
enhanced via group relative policy optimization (GRPO), a rule-based RL
approach that integrates correctness discrimination and diversity-aware reward
functions. Our method emphasizes learning from varied reasoning perspectives
and distinguishing between correct yet distinct solutions. Extensive
experiments on the MathVista's minitest and Math-V benchmarks demonstrate that
Qwen-VL-DP significantly outperforms prior base MLLMs in both accuracy and
generative diversity, highlighting the importance of incorporating diverse
perspectives and reflective reasoning in multimodal mathematical reasoning.

</details>


### [53] [SynapseRoute: An Auto-Route Switching Framework on Dual-State Large Language Model](https://arxiv.org/abs/2507.02822)
*Wencheng Zhang,Shiqin Qiao,Lingjie Luo,Yinfeng Li,Chuanyang Zheng,Qian Xu,Meng Li,Yong Gui,Yijun He,Jianing Qiu,Jindong Hong,Jiankai Sun*

Main category: cs.CL

TL;DR: 论文提出了SynapseRoute动态路由框架，可智能分配医学问答至适宜的推理模式，实现更优的准确率与推理/成本效率，并综合用AIT指标评估表现。


<details>
  <summary>Details</summary>
Motivation: 大模型在实际应用中面临性能与运算成本的平衡，尤其是在涉及推理能力的场景下，推理模式与非推理模式的成本差距加大；识别并动态分配不同复杂度问题至适合处理方式可优化整体体验与成本。

Method: 通过对医学问答数据集进行分析，区分适合直接处理的问题与需深度推理的问题；设计并实现SynapseRoute动态路由系统，用机器学习模型判断问题复杂度，再分配至对应推理模式；比较系统在不同指标（准确率、延迟、token使用量）上的表现。

Result: SynapseRoute相比单一“思考”模式提升准确率（0.8390对0.8272），推理时间减少36.8%，token消耗减少39.66%；过度推理还会对简单问题带来负面影响。提出AIT（准确率-推理延迟-token消耗）指标进行综合评估。

Conclusion: 提出了一种基于机器学习的动态路由框架SynapseRoute，能够根据问题复杂度智能分配至“思考”或“非思考”模式，在医学问答场景下有效提升准确率并显著降低成本。

Abstract: With the widespread adoption of large language models (LLMs) in practical
applications, selecting an appropriate model requires balancing not only
performance but also operational cost. The emergence of reasoning-capable
models has further widened the cost gap between "thinking" (high reasoning) and
"non-thinking" (fast, low-cost) modes. In this work, we reveal that
approximately 58% of medical questions can be accurately answered by the
non-thinking mode alone, without requiring the high-cost reasoning process.
This highlights a clear dichotomy in problem complexity and suggests that
dynamically routing queries to the appropriate mode based on complexity could
optimize accuracy, cost-efficiency, and overall user experience. Based on this,
we further propose SynapseRoute, a machine learning-based dynamic routing
framework that intelligently assigns input queries to either thinking or
non-thinking modes. Experimental results on several medical datasets
demonstrate that SynapseRoute not only improves overall accuracy (0.8390 vs.
0.8272) compared to the thinking mode alone but also reduces inference time by
36.8% and token consumption by 39.66%. Importantly, qualitative analysis
indicates that over-reasoning on simpler queries can lead to unnecessary delays
and even decreased accuracy, a pitfall avoided by our adaptive routing.
Finally, this work further introduces the Accuracy-Inference-Token (AIT) index
to comprehensively evaluate the trade-offs among accuracy, latency, and token
cost.

</details>


### [54] [Generalizing Verifiable Instruction Following](https://arxiv.org/abs/2507.02833)
*Valentina Pyatkin,Saumya Malik,Victoria Graf,Hamish Ivison,Shengyi Huang,Pradeep Dasigi,Nathan Lambert,Hannaneh Hajishirzi*

Main category: cs.CL

TL;DR: 本文提出了新基准IFBench和RLVR训练方法，有效提高了大语言模型对复杂且新颖输出约束指令的泛化遵循能力，并公开了相关数据和工具。


<details>
  <summary>Details</summary>
Motivation: 当前的大型语言模型在精确遵循用户指令、尤其是满足特定输出约束方面表现不佳，且容易对有限的测试约束过拟合，缺乏泛化能力。推动模型对新颖、多样的输出约束具备更好的适应和泛化能力对于提升人机交互效果非常关键。

Method: 作者提出了IFBench新基准，用于评估模型在58项全新、具有挑战性且可验证的出域输出约束上的精确指令遵循能力。同时，设计了约束验证模块，并采用基于可验证奖励的强化学习（RLVR），通过这些方法指导模型提升对指令的泛化能力。

Result: 通过引入IFBench和相关验证模块，并采用RLVR训练，模型在各类复杂、新颖的输出约束任务上指令遵循能力显著提升。此外，作者还公开了29个人工标注的训练约束、验证函数、RLVR训练提示与代码。

Conclusion: 传统模型精确遵循和泛化输出约束指令上存在明显短板，作者提出的IFBench和基于可验证奖励的训练方法，有效提升了模型在复杂指令场景下的表现，并为相关研究社区提供了一套全面的评测及训练工具。

Abstract: A crucial factor for successful human and AI interaction is the ability of
language models or chatbots to follow human instructions precisely. A common
feature of instructions are output constraints like ``only answer with yes or
no" or ``mention the word `abrakadabra' at least 3 times" that the user adds to
craft a more useful answer. Even today's strongest models struggle with
fulfilling such constraints. We find that most models strongly overfit on a
small set of verifiable constraints from the benchmarks that test these
abilities, a skill called precise instruction following, and are not able to
generalize well to unseen output constraints. We introduce a new benchmark,
IFBench, to evaluate precise instruction following generalization on 58 new,
diverse, and challenging verifiable out-of-domain constraints. In addition, we
perform an extensive analysis of how and on what data models can be trained to
improve precise instruction following generalization. Specifically, we
carefully design constraint verification modules and show that reinforcement
learning with verifiable rewards (RLVR) significantly improves instruction
following. In addition to IFBench, we release 29 additional new hand-annotated
training constraints and verification functions, RLVR training prompts, and
code.

</details>


### [55] [LLM Hypnosis: Exploiting User Feedback for Unauthorized Knowledge Injection to All Users](https://arxiv.org/abs/2507.02850)
*Almog Hilel,Idan Shenfeld,Leshem Choshen,Jacob Andreas*

Main category: cs.CL

TL;DR: 本论文发现：利用用户反馈微调机制，攻击者可以仅凭评价模型输出，对大语言模型的知识、行为做出持续性篡改，包括植入虚假知识、引入安全缺陷等。这种反馈投毒方式揭示了LM偏好调整安全的新隐患。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LMs）越来越依赖用户反馈进行微调，以提升输出质量，但用户反馈机制是否存在滥用风险尚未充分研究。作者旨在揭示用户单独通过反馈机制对模型知识和行为产生持续性影响的潜在安全威胁。

Method: 攻击者通过反复对模型输入特定prompt，引导模型随机输出“恶意（poisoned）”和“正常（benign）”响应，并对前者点赞（upvote）、后者点踩（downvote），利用这种反馈生成数据影响后续的偏好微调过程。

Result: 实验表明，攻击者确实可以通过这种方式持续性地改变模型输出，包括灌输模型原本没有的事实信息、让代码生成模式引入安全漏洞、植入虚假财经新闻等。模型偏好微调过程中，有限制的用户反馈也会造成行为的精细可控变更。

Conclusion: 论文揭示了一种针对于利用用户反馈微调的大语言模型的新型攻击手段，指出即便极为有限、受控的反馈信号同样可以被恶意利用，对模型输出产生持久性、细粒度的控制，这拓展了已有关于数据投毒与提示注入攻击的边界。

Abstract: We describe a vulnerability in language models (LMs) trained with user
feedback, whereby a single user can persistently alter LM knowledge and
behavior given only the ability to provide prompts and upvote / downvote
feedback on LM outputs. To implement the attack, the attacker prompts the LM to
stochastically output either a "poisoned" or benign response, then upvotes the
poisoned response or downvotes the benign one. When feedback signals are used
in a subsequent preference tuning behavior, LMs exhibit increased probability
of producing poisoned responses even in contexts without malicious prompts. We
show that this attack can be used to (1) insert factual knowledge the model did
not previously possess, (2) modify code generation patterns in ways that
introduce exploitable security flaws, and (3) inject fake financial news. Our
finding both identifies a new qualitative feature of language model preference
tuning (showing that it even highly restricted forms of preference data can be
used to exert fine-grained control over behavior), and a new attack mechanism
for LMs trained with user feedback (extending work on pretraining-time data
poisoning and deployment-time prompt injection).

</details>


### [56] [MOTIF: Modular Thinking via Reinforcement Fine-tuning in LLMs](https://arxiv.org/abs/2507.02851)
*Purbesh Mitra,Sennur Ulukus*

Main category: cs.CL

TL;DR: 本文提出MOTIF方法，通过强化学习微调让大语言模型实现多轮思考、有更强推理能力，在数学推理任务上显著优于传统训练方法且节省数据。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM推理能力受限于其上下文长度（context size），即模型在生成较长答案时只能关注有限数量的先前token，限制了其复杂推理能力。为了让LLM能更好地进行多轮推理，需要突破这一上下文长度瓶颈。

Method: 提出了一种新的强化学习微调方法MOTIF（Modular Thinking via Reinforcement Finetuning），使模型能以模块化思维策略进行多轮生成，通过重复思考过程扩展实际使用的上下文长度，并基于GRPO（Group Relative Policy Optimization）算法进行训练。该方法在少量样本下对开源模型Qwen2.5-3B-Instruct进行参数高效微调。

Result: 在MATH500和AIME2024两个基准测试上，采用MOTIF训练方法的模型分别比普通GRPO训练提升了3.8%和3.3%的准确率。而且这一提升只用了15%的训练样本，表明MOTIF具有较高的数据利用效率。

Conclusion: MOTIF方法可以让LLM超越原有上下文长度限制，具备多轮模块化推理能力，并在复杂数学推理任务上显著提升表现，同时具有数据高效性，适合在实际大模型训练中推广应用。

Abstract: Recent advancements in the reasoning capabilities of large language models
(LLMs) show that employing group relative policy optimization (GRPO) algorithm
for reinforcement learning (RL) training allows the models to use more
thinking/reasoning tokens for generating better responses. However, LLMs can
generate only a finite amount of tokens while maintaining attention to the
previously generated tokens. This limit, also known as the context size of an
LLM, is a bottleneck in LLM reasoning with arbitrarily large number of tokens.
To think beyond the limit of context size, an LLM must employ a modular
thinking strategy to reason over multiple rounds. In this work, we propose
$\textbf{MOTIF: Modular Thinking via Reinforcement Finetuning}$ -- an RL
training method for generating thinking tokens in multiple rounds, effectively
allowing the model to think with additional context size. We trained the
open-source model Qwen2.5-3B-Instruct on GSM8K dataset via parameter efficient
fine-tuning and tested its accuracy on MATH500 and AIME2024 benchmarks. Our
experiments show 3.8\% and 3.3\% improvements over vanilla GRPO based training
in the respective benchmarks. Furthermore, this improvement was achieved with
only 15\% of samples, thus demonstrating sample efficiency of MOTIF. Our code
and models are available at https://github.com/purbeshmitra/MOTIF and
https://huggingface.co/purbeshmitra/MOTIF, respectively.

</details>


### [57] [Answer Matching Outperforms Multiple Choice for Language Model Evaluation](https://arxiv.org/abs/2507.02856)
*Nikhil Chandak,Shashwat Goel,Ameya Prabhu,Moritz Hardt,Jonas Geiping*

Main category: cs.CL

TL;DR: 多项选择基准存在投机问题，不能真实反映语言模型能力。作者提出基于生成答案与自动匹配的评估方法，实验发现在人类一致性方面优于多项选择，且能提升排名区分度，建议社区转向答案生成式评测。


<details>
  <summary>Details</summary>
Motivation: 多项选择题由于其便于客观和自动评分，被长期用作语言模型评估基准。但作者发现，流行基准中的多项选择题往往可通过选项本身被“投机”答对，甚至不需要看题干。这揭示了判别式评估本身的局限。作者提出动机：探索一种更符合实际且能大规模自动化的生成式评估方法。

Method: 作者提出“答案匹配”生成式评估方案：模型在不提示选项的情况下自由生成答案，然后用最新的大型语言模型（LLM）结合标准答案自动判定生成答案是否匹配。在MMLU-Pro和GPQA-Diamond两个数据集上，人工标注生成了真实的人类评分数据，并与自动评估法的结果进行一致性对比。

Result: 实验发现：用近期LLM进行答案匹配（哪怕是较小模型）可以达成人与人之间评分一致性（inter-annotator agreement）水平；而传统多项选择评分或仅用LLM无参考答案判定对人的评分一致性较差。此外，答案匹配方式能够显著改变部分模型的排名。

Conclusion: 答案匹配是一种可扩展且与人类一致性高度匹配的评估方案，有望取代传统多项选择。作者建议评测生态朝着基于答案生成与自动匹配的方向转变。

Abstract: Multiple choice benchmarks have long been the workhorse of language model
evaluation because grading multiple choice is objective and easy to automate.
However, we show multiple choice questions from popular benchmarks can often be
answered without even seeing the question. These shortcuts arise from a
fundamental limitation of discriminative evaluation not shared by evaluations
of the model's free-form, generative answers. Until recently, there appeared to
be no viable, scalable alternative to multiple choice--but, we show that this
has changed. We consider generative evaluation via what we call answer
matching: Give the candidate model the question without the options, have it
generate a free-form response, then use a modern language model with the
reference answer to determine if the response matches the reference. To compare
the validity of different evaluation strategies, we annotate MMLU-Pro and
GPQA-Diamond to obtain human grading data, and measure the agreement of each
evaluation approach. We find answer matching using recent models--even small
ones--achieves near-perfect agreement, in the range of inter-annotator
agreement. In contrast, both multiple choice evaluation and using
LLM-as-a-judge without reference answers aligns poorly with human grading.
Improving evaluations via answer matching is not merely a conceptual concern:
the rankings of several models change significantly when evaluating their
free-form responses with answer matching. In light of these findings, we
discuss how to move the evaluation ecosystem from multiple choice to answer
matching.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [58] [Discovery of Fatigue Strength Models via Feature Engineering and automated eXplainable Machine Learning applied to the welded Transverse Stiffener](https://arxiv.org/abs/2507.02005)
*Michael A. Kraus,Helen Bartsch*

Main category: cs.CE

TL;DR: 本研究提出AutoML与XAI结合的新范式，显著提升焊接结构疲劳强度预测的准确性和可解释性，揭示关键影响因子，为AI辅助设计与结构评估提供技术路径。


<details>
  <summary>Details</summary>
Motivation: 针对焊接横向加劲肋细节疲劳强度预测问题，现有方法在准确性和可解释性方面存在局限。工程实际中既需要高性能预测，又要明确关键影响因素。

Method: 提出将自动化机器学习（AutoML）与可解释人工智能（XAI）相结合，利用专家驱动与算法生成特征，共同进行特征工程。通过AutoML训练梯度提升、随机森林、神经网络等模型，并在三种特征方案（专家主导、算法主导、组合）下系统比较效果。采用SHAP等方法进行模型解释。

Result: 集成方法（如CatBoost、LightGBM）表现最优。专家主导模型在全面准确性和实际工程区间内都表现最好（测试RMSE约30.6 MPa，R2约0.78；0-150 MPa区间RMSE约13.4 MPa，R2约0.53）。密集特征模型泛化下降，而简单特征模型表现稳健。XAI识别压力比、应力范围、屈服强度和焊后处理为主要影响因子。部分几何参数同样显著。

Conclusion: 结合AutoML与XAI，可以构建既准确又可解释的钢结构疲劳强度预测模型，为数据驱动设计与工程验证搭建桥梁。未来将拓展至概率建模与数字孪生应用。

Abstract: This research introduces a unified approach combining Automated Machine
Learning (AutoML) with Explainable Artificial Intelligence (XAI) to predict
fatigue strength in welded transverse stiffener details. It integrates
expert-driven feature engineering with algorithmic feature creation to enhance
accuracy and explainability.
  Based on the extensive fatigue test database regression models - gradient
boosting, random forests, and neural networks - were trained using AutoML under
three feature schemes: domain-informed, algorithmic, and combined. This allowed
a systematic comparison of expert-based versus automated feature selection.
  Ensemble methods (e.g. CatBoost, LightGBM) delivered top performance. The
domain-informed model $\mathcal M_2$ achieved the best balance: test RMSE
$\approx$ 30.6 MPa and $R^2 \approx 0.780% over the full $\Delta
\sigma_{c,50\%}$ range, and RMSE $\approx$ 13.4 MPa and $R^2 \approx 0.527%
within the engineering-relevant 0 - 150 MPa domain. The denser-feature model
($\mathcal M_3$) showed minor gains during training but poorer generalization,
while the simpler base-feature model ($\mathcal M_1$) performed comparably,
confirming the robustness of minimalist designs.
  XAI methods (SHAP and feature importance) identified stress ratio $R$, stress
range $\Delta \sigma_i$, yield strength $R_{eH}$, and post-weld treatment (TIG
dressing vs. as-welded) as dominant predictors. Secondary geometric factors -
plate width, throat thickness, stiffener height - also significantly affected
fatigue life.
  This framework demonstrates that integrating AutoML with XAI yields accurate,
interpretable, and robust fatigue strength models for welded steel structures.
It bridges data-driven modeling with engineering validation, enabling
AI-assisted design and assessment. Future work will explore probabilistic
fatigue life modeling and integration into digital twin environments.

</details>


### [59] [Time Resolution Independent Operator Learning](https://arxiv.org/abs/2507.02524)
*Diab W. Abueidda,Mbebo Nonna,Panos Pantidis,Mostafa E. Mobasher*

Main category: cs.CE

TL;DR: 该论文提出NCDE-DeepONet，用NCDE对输入进行分辨率无关的编码，并结合空间-时间坐标，实现了对时变PDE算子的高效连续预测。实验显示该方法准确、稳定、速度快，能在训练外时空点直接预测。


<details>
  <summary>Details</summary>
Motivation: 在时变偏微分方程（PDEs）中，从稀疏且不规则的数据中准确学习解算子依然是一个具有挑战性的任务。现有的方法如递归DeepONet和神经ODE都存在一定局限性，难以兼顾连续时间处理和对新输入的适应能力。

Method: 提出了一种新的连续时间算子网络——NCDE-DeepONet。该方法在分支部分嵌入了神经受控微分方程（NCDE），并在主干部分显式加入了时空坐标。其关键做法是利用样条插值将输入路径转化为控制ODE，由此得到的潜在路径表示能够适应不同输入信号的离散程度；主干网络可以在任意时空点查询这个表示，使得预测与输出分辨率无关。

Result: 在瞬态泊松方程、弹性动力学和热弹性等多个问题上的基准测试表明，此方法在稳健性和准确性方面具有明显优势，并能够实现几乎即时的解预测。

Conclusion: 受控动力学为瞬态力学中高保真度的算子学习提供了理论扎实且高效的基础。

Abstract: Accurately learning solution operators for time-dependent partial
differential equations (PDEs) from sparse and irregular data remains a
challenging task. Recurrent DeepONet extensions inherit the discrete-time
limitations of sequence-to-sequence (seq2seq) RNN architectures, while
neural-ODE surrogates cannot incorporate new inputs after initialization. We
introduce NCDE-DeepONet, a continuous-time operator network that embeds a
Neural Controlled Differential Equation (NCDE) in the branch and augments the
trunk with explicit space-time coordinates. The NCDE encodes an entire load
history as the solution of a controlled ODE driven by a spline-interpolated
input path, making the representation input-resolution-independent: it encodes
different input signal discretizations of the observed samples. The trunk then
probes this latent path at arbitrary spatial locations and times, rendering the
overall map output-resolution independent: predictions can be queried on meshes
and time steps unseen during training without retraining or interpolation.
Benchmarks on transient Poisson, elastodynamic, and thermoelastic problems
confirm the robustness and accuracy of the framework, achieving almost instant
solution prediction. These findings suggest that controlled dynamics provide a
principled and efficient foundation for high-fidelity operator learning in
transient mechanics.

</details>


### [60] [Imitation and Heterogeneity Shape the Resilience of Community Currency Networks](https://arxiv.org/abs/2507.02678)
*Camilla Ancona,Dora Ricci,Carmela Bernardo,Francesco Lo Iudice,Anton Proskurnikov,Francesco Vasca*

Main category: cs.CE

TL;DR: 本文通过图论方法对Sardex社区货币系统三年期结构演化进行分析，揭示了行为模仿和异质性连接对网络韧性的重要作用。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在揭示社区货币网络（如Sardex）交易结构及动态演变规律，探讨社区成员如何通过虚拟货币建立经济联系，以及用户行为如何影响网络结构和韧性。

Method: 以意大利撒丁岛的社区货币系统Sardex为案例，构建成有向加权图，用图论方法（包括强连通分量、压缩表示、行为连通性等）分析网络三年内核心与边缘结构的变化，并使用基于度的空模型作为对照。

Result: 发现网络结构始终偏离基于度的空模型，表现出用户偏好与更活跃成员互动的行为模仿现象。类型异质的用户间连接强化了网络结构，提高了系统韧性，同时观察到网络结构随时间收缩、流动不对称和结构碎片化等特征。

Conclusion: Sardex等社区货币网络具有独特结构特性和高度韧性，用户行为（如模仿活跃成员）和异质性连接对网络稳定性与发展有积极作用。

Abstract: Community currency networks are made up of individuals and or companies that
share some physical or social characteristics and engage in economic
transactions using a virtual currency. This paper investigates the structural
and dynamic properties of such mutual credit systems through a case study of
Sardex, a community currency initiated and mainly operating in Sardinia, Italy.
The transaction network is modeled as a directed weighted graph and analyzed
through a graph theoretic framework focused on the analysis of strongly
connected components, condensed representations, and behavioral connectivity
patterns. Emphasis is placed on understanding the evolution of the network's
core and peripheral structures over a three year period, with attention to
temporal contraction, flow asymmetries, and structural fragmentation depending
on different user types. Our findings reveal persistent deviations from degree
based null models and suggest the presence of behavioral imitation,
specifically, a user preference for more active peers. We further assess the
impact of heterogeneous connections between different type of users, which
strengthen the network topology and enhance its resilience.

</details>


### [61] [Constraint-Guided Symbolic Regression for Data-Efficient Kinetic Model Discovery](https://arxiv.org/abs/2507.02730)
*Miguel Ángel de Carvalho Servia,Ilya Orson Sandoval,King Kuok,Hii,Klaus Hellgardt,Dongda Zhang,Ehecatl Antonio del Rio Chanona*

Main category: cs.CE

TL;DR: 本文提出了一种将物理约束融入符号回归的新动力学建模框架PI-ADoK，能显著提升模型准确性和实验效率，并提供可靠的不确定性量化，适用于化学反应工程领域。


<details>
  <summary>Details</summary>
Motivation: 传统的机理模型需要大量领域知识，数据驱动方法缺乏可解释性和物理一致性，导致催化过程工业化困难。

Method: 提出了PI-ADoK框架，将物理约束直接引入符号回归，通过Metropolis-Hastings算法实现参数不确定性量化和可信预测区间的生成。

Result: 与传统方法在多个催化案例中对比，PI-ADoK提升了模型可信度，减少了实验数量。

Conclusion: PI-ADoK框架能高效、可靠地发现化学反应中的动力学模型，具有显著工业应用潜力。

Abstract: The industrialization of catalytic processes hinges on the availability of
reliable kinetic models for design, optimization, and control. Traditional
mechanistic models demand extensive domain expertise, while many data-driven
approaches often lack interpretability and fail to enforce physical
consistency. To overcome these limitations, we propose the Physics-Informed
Automated Discovery of Kinetics (PI-ADoK) framework. By integrating physical
constraints directly into a symbolic regression approach, PI-ADoK narrows the
search space and substantially reduces the number of experiments required for
model convergence. Additionally, the framework incorporates a robust
uncertainty quantification strategy via the Metropolis-Hastings algorithm,
which propagates parameter uncertainty to yield credible prediction intervals.
Benchmarking our method against conventional approaches across several
catalytic case studies demonstrates that PI-ADoK not only enhances model
fidelity but also lowers the experimental burden, highlighting its potential
for efficient and reliable kinetic model discovery in chemical reaction
engineering.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [62] [Computer Science Education in the Age of Generative AI](https://arxiv.org/abs/2507.02183)
*Russell Beale*

Main category: cs.CY

TL;DR: 本文系统评估了生成式AI（如ChatGPT、Codex）在计算机科学教育中的应用前景和挑战，探讨了教学内容与评估方法的调整，并提出了政策性建议，以实现教育质量和学术诚信的协同提升。


<details>
  <summary>Details</summary>
Motivation: 近年来，生成式人工智能工具（特别是大型语言模型如ChatGPT和Codex）在计算机科学教育领域产生了重大影响。本文旨在探讨这些AI工具如何为编程教学和计算机科学教育带来机遇，以及随之出现的挑战。

Method: 文章采用文献综述与案例分析方法，结合实证数据和最新研究成果，对AI在教育场景中的应用进行分析。

Result: AI工具极大地提升了学生在编程学习中的效率和创新能力，但同时也带来了学术诚信、过度依赖AI、成果原创性验证等新问题。作者就如何在课程中整合AI技术、教学与评估方法改革提出了实践建议，并制定了相关政策建议。

Conclusion: 生成式AI具有推动计算机科学教育变革的巨大潜力。合理利用AI工具并制定相应政策，有助于最大化其正面作用，同时维护学术严谨性和教育质量。

Abstract: Generative AI tools - most notably large language models (LLMs) like ChatGPT
and Codex - are rapidly revolutionizing computer science education. These tools
can generate, debug, and explain code, thereby transforming the landscape of
programming instruction. This paper examines the profound opportunities that AI
offers for enhancing computer science education in general, from coding
assistance to fostering innovative pedagogical practices and streamlining
assessments. At the same time, it highlights challenges including academic
integrity concerns, the risk of over-reliance on AI, and difficulties in
verifying originality. We discuss what computer science educators should teach
in the AI era, how to best integrate these technologies into curricula, and the
best practices for assessing student learning in an environment where AI can
generate code, prototypes and user feedback. Finally, we propose a set of
policy recommendations designed to harness the potential of generative AI while
preserving the integrity and rigour of computer science education. Empirical
data and emerging studies are used throughout to support our arguments.

</details>


### [63] [Defining DLT Immutability: A Qualitative Survey of Node Operators](https://arxiv.org/abs/2507.02413)
*Alex Lynham,Geoff Goodell*

Main category: cs.CY

TL;DR: 区块链强调不可变性，但现实中存在重写与攻击等风险。通过分析节点运营者访谈，作者提出“实用不可变性”概念，认为区块链的不可变性依赖于治理结构与集体信任，而非绝对不可更改。


<details>
  <summary>Details</summary>
Motivation: 区块链系统追求不可变性，但实际上链上重写和各类安全风险更普遍且严重，因此有必要重新审视区块链不可变性的实际边界和本质。

Method: 采用主题分析法，对区块链节点运营者进行访谈，探讨不可变性的实际极限及其与链上重写事件的关系。

Result: 提出了“实用不可变性”（Practical Immutability）的概念，即区块链的不可变性实际上取决于网络合法治理的需求，网络利益相关者将对治理结构的信任转化为账本状态管理的合法性。

Conclusion: 区块链并非严格不可变，其不可变性受到网络治理结构和集体需求影响。实际运作中存在条件性不可变性，“实用不可变性”更贴近区块链现实。

Abstract: Immutability is a core design goal of permissionless public blockchain
systems. However, rewrites are more common than is normally understood, and the
risk of rewrite, cyberattack, exploit or black swan event is also high. Taking
the position that strict immutability is neither possible on these networks nor
the observed reality, this paper uses thematic analysis of node operator
interviews to examine the limits of immutability in light of rewrite events.
The end result is a qualitative definition of the conditional immutability
found on these networks, which we call Practical Immutability. This is
immutability contingent on the legitimate governance demands of the network,
where network stakeholders place their trust in the governance topology of a
network to lend it legitimacy, and thus manage ledger state.

</details>


### [64] [Recourse, Repair, Reparation, & Prevention: A Stakeholder Analysis of AI Supply Chains](https://arxiv.org/abs/2507.02648)
*Aspen K. Hopkins,Isabella Struckman,Kevin Klyman,Susan S. Silbey*

Main category: cs.CY

TL;DR: 本研究分析了AI供应链中各方的角色、面临的危害及其应对措施，提出了四类应对伤害的策略，通过医疗健康案例说明不同利益相关者因位置和权力差异而拥有不同的补救与预防能力，为负责任的AI供应链设计与管理提供建议。


<details>
  <summary>Details</summary>
Motivation: AI产业迅速发展，AI部署通常依赖于多实体共同参与的AI供应链，这带来了前所未有的复杂性和风险。同时，当前AI供应链缺乏传统供应链的模块化、冗余以及常规管理实践，导致难以及时识别、隔离和纠正产生的问题，因此强化了人工智能带来的潜在危害。该文希望明确AI供应链中利益相关者的角色、面临的伤害及其来源，并探究市场动态和权力差异如何影响伤害补救措施的类型和可能性。

Method: 本文采用利益相关者分析的方法，对AI供应链的参与者、受害类型、伤害来源以及市场和权力结构如何影响应对措施进行了系统梳理。文中提出并应用了应对AISC伤害的响应类型学框架（补救、修复、赔偿或预防），并在医疗健康AI供应链及三种市场情境（纵向整合、横向整合、自由市场）下进行了案例应用分析。

Result: 文中明确了AI供应链参与者在不同市场结构和权力配置下，面对AI伤害时可采取的不同补救对策。提出并验证了针对AISC伤害的四大响应类型，并说明了利益相关者在供应链中的位置和权力分布如何影响其可行的应对方式和效果。

Conclusion: AI供应链应被有意识地设计和管理，以应对人工智能系统部署过程中的复杂性、后果和风险。通过类型化响应框架，可以更有针对性地帮助不同位置和权力的利益相关者处理和预防AI供应链带来的伤害，促进AI治理的责任落实。

Abstract: The AI industry is exploding in popularity, with increasing attention to
potential harms and unwanted consequences. In the current digital ecosystem, AI
deployments are often the product of AI supply chains (AISC): networks of
outsourced models, data, and tooling through which multiple entities contribute
to AI development and distribution. AI supply chains lack the modularity,
redundancies, or conventional supply chain practices that enable
identification, isolation, and easy correction of failures, exacerbating the
already difficult processes of responding to ML-generated harms. As the
stakeholders participating in and impacted by AISCs have scaled and
diversified, so too have the risks they face. In this stakeholder analysis of
AI supply chains, we consider who participates in AISCs, what harms they face,
where sources of harm lie, and how market dynamics and power differentials
inform the type and probability of remedies. Because AI supply chains are
purposely invented and implemented, they may be designed to account for, rather
than ignore, the complexities, consequences, and risks of deploying AI systems.
To enable responsible design and management of AISCs, we offer a typology of
responses to AISC-induced harms: recourse, repair, reparation or prevention. We
apply this typology to stakeholders participating in a health-care AISC across
three stylized markets $\unicode{x2013}$ vertical integration, horizontal
integration, free market $\unicode{x2013}$ to illustrate how stakeholder
positioning and power within an AISC may shape responses to an experienced
harm.

</details>


<div id='q-fin.TR'></div>

# q-fin.TR [[Back]](#toc)

### [65] [A Midsummer Meme's Dream: Investigating Market Manipulations in the Meme Coin Ecosystem](https://arxiv.org/abs/2507.01963)
*Alberto Maria Mongardini,Alessandro Mei*

Main category: q-fin.TR

TL;DR: 本研究对主流公链上的meme币生态进行跟踪分析，发现高收益meme币广泛存在市场操控行为，如刷单和流动池操纵，这些币种的暴涨通常源于人为干预，而非自然市场行为。


<details>
  <summary>Details</summary>
Motivation: meme币区别于比特币和以太坊等实用性数字货币，其价值源于社区共识，导致它易受市场操纵。本研究欲识别及定量分析meme币市场中的操纵行为及其影响。

Method: 跨链分析，对Ethereum、BNB Smart Chain、Solana和Base四大公链的34,988个meme币进行三个月的纵向追踪研究；分析tokenomics、追踪市场活动、监测和分类各类操纵行为如刷单交易和流动池价格操控。

Result: 在高收益meme币中，82.6%存在刷单和流动池价格通胀等人为虚假增长策略。发现‘拉升抛售’（pump and dump）和资金池诈骗（rug pull）等骗局广泛发生，而上述操纵行为常为后续骗局铺垫。

Conclusion: 高收益（>100%）的meme币中，操纵行为普遍存在，尤其是通过虚假交易和流动池价格通胀等方式。其暴涨多由人为而非自然市场力量推动。

Abstract: From viral jokes to a billion-dollar phenomenon, meme coins have become one
of the most popular segments in cryptocurrency markets. Unlike utility-focused
crypto assets like Bitcoin or Ethereum, meme coins derive value primarily from
community sentiment, making them vulnerable to manipulation. This study
presents a cross-chain analysis of the meme coin ecosystem, examining 34,988
tokens across Ethereum, BNB Smart Chain, Solana, and Base. We characterize the
tokenomics of meme coins and track their growth in a three-month longitudinal
analysis. We discover that among high-return tokens (>100%), an alarming 82.6%
show evidence of extensive use of artificial growth strategies designed to
create a misleading appearance of market interest. These include wash trading
and a form of manipulation we define as Liquidity Pool-Based Price Inflation
(LPI), where small strategic purchases trigger dramatic price increases. We
also find evidence of schemes designed to profit at the expense of investors,
such as pump and dumps and rug pulls. In particular, most of the tokens
involved had previously experienced wash trading or LPI, indicating how initial
manipulations often set the stage for later exploitation. These findings reveal
that manipulations are widespread among high-performing meme coins and suggest
that their dramatic gains are often likely driven by coordinated efforts rather
than natural market dynamics.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [66] [FinAI-BERT: A Transformer-Based Model for Sentence-Level Detection of AI Disclosures in Financial Reports](https://arxiv.org/abs/2507.01991)
*Muhammad Bilal Zafar*

Main category: q-fin.CP

TL;DR: 本文提出了一种针对金融文本中AI相关内容的句子级分类模型FinAI-BERT，精度高于传统方法，并具有良好的可解释性和稳健性，推动了金融领域的自然语言处理应用。


<details>
  <summary>Details</summary>
Motivation: 金融行业中人工智能的普及导致对系统化检测公司文件中与AI相关内容的工具需求增加，而现有方法在细粒度、可解释性和稳健性方面存在不足。

Method: 提出了FinAI-BERT，这是一种面向金融领域的、基于Transformer的语言模型，可在句子级别识别年报等金融文本中的AI相关内容。模型在人工标注、均衡的1586句样本上进行微调，并通过SHAP方法提升模型可解释性，同时进行了偏差分析和稳健性测试。

Result: FinAI-BERT句子级分类准确率达99.37%，F1分数为0.993，明显优于传统方法（如Logistic回归、朴素贝叶斯、随机森林和XGBoost），模型在不同长度句子、对抗输入和时间样本下均表现出较高的鲁棒性。

Conclusion: FinAI-BERT为金融行业AI主题文本的细粒度分类提供了高效且可解释的新方法，促进了金融NLP的发展，并为分析师、监管者及学者监控AI相关披露提供了可扩展的透明工具。

Abstract: The proliferation of artificial intelligence (AI) in financial services has
prompted growing demand for tools that can systematically detect AI-related
disclosures in corporate filings. While prior approaches often rely on keyword
expansion or document-level classification, they fall short in granularity,
interpretability, and robustness. This study introduces FinAI-BERT, a
domain-adapted transformer-based language model designed to classify AI-related
content at the sentence level within financial texts. The model was fine-tuned
on a manually curated and balanced dataset of 1,586 sentences drawn from 669
annual reports of U.S. banks (2015 to 2023). FinAI-BERT achieved near-perfect
classification performance (accuracy of 99.37 percent, F1 score of 0.993),
outperforming traditional baselines such as Logistic Regression, Naive Bayes,
Random Forest, and XGBoost. Interpretability was ensured through SHAP-based
token attribution, while bias analysis and robustness checks confirmed the
model's stability across sentence lengths, adversarial inputs, and temporal
samples. Theoretically, the study advances financial NLP by operationalizing
fine-grained, theme-specific classification using transformer architectures.
Practically, it offers a scalable, transparent solution for analysts,
regulators, and scholars seeking to monitor the diffusion and framing of AI
across financial institutions.

</details>
