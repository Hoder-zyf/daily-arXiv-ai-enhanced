{"id": "2507.11582", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.11582", "abs": "https://arxiv.org/abs/2507.11582", "authors": ["Kazuyoshi Otsuka"], "title": "Subjective Evaluation Profile Analysis of Science Fiction Short Stories and its Critical-Theoretical Significance", "comment": "38 pages. Manuscript submitted for review to the Journal of\n  Computational Literary Studies (JCLS)", "summary": "This study positions large language models (LLMs) as \"subjective literary\ncritics\" to explore aesthetic preferences and evaluation patterns in literary\nassessment. Ten Japanese science fiction short stories were translated into\nEnglish and evaluated by six state-of-the-art LLMs across seven independent\nsessions. Principal component analysis and clustering techniques revealed\nsignificant variations in evaluation consistency ({\\alpha} ranging from 1.00 to\n0.35) and five distinct evaluation patterns. Additionally, evaluation variance\nacross stories differed by up to 4.5-fold, with TF-IDF analysis confirming\ndistinctive evaluation vocabularies for each model. Our seven-session\nwithin-day protocol using an original Science Fiction corpus strategically\nminimizes external biases, allowing us to observe implicit value systems shaped\nby RLHF and their influence on literary judgment. These findings suggest that\nLLMs may possess individual evaluation characteristics similar to human\ncritical schools, rather than functioning as neutral benchmarkers.", "AI": {"tldr": "\u5927\u6a21\u578b\u5728\u6587\u5b66\u8bc4\u4ef7\u4e0a\u7684\u4e3b\u89c2\u6027\u8f83\u5f3a\uff0c\u4e0d\u540c\u6a21\u578b\u95f4\u8868\u73b0\u51fa\u72ec\u7279\u98ce\u683c\u548c\u503e\u5411\uff0c\u7c7b\u4f3c\u4eba\u7c7b\u6279\u8bc4\u6d41\u6d3e\u3002", "motivation": "\u63a2\u7d22\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u6587\u5b66\u8bc4\u4ef7\u4e2d\u7684\u4e3b\u89c2\u6027\u548c\u504f\u597d\uff0c\u8003\u5bdf\u5176\u662f\u5426\u62e5\u6709\u7c7b\u4f3c\u4eba\u7c7b\u6279\u8bc4\u6d41\u6d3e\u7684\u72ec\u7279\u4ef7\u503c\u4f53\u7cfb\uff0c\u800c\u975e\u5355\u7eaf\u4e2d\u7acb\u7684\u8bc4\u5224\u5de5\u5177\u3002", "method": "\u5c06\u5341\u90e8\u65e5\u8bed\u79d1\u5e7b\u77ed\u7bc7\u7ffb\u8bd1\u4e3a\u82f1\u6587\uff0c\u8ba9\u516d\u79cd\u5148\u8fdb\u7684LLM\u5728\u4e03\u4e2a\u4e0d\u540c\u65f6\u95f4\u6bb5\u72ec\u7acb\u6253\u5206\u3002\u901a\u8fc7\u4e3b\u6210\u5206\u5206\u6790\u548c\u805a\u7c7b\u65b9\u6cd5\u5206\u6790\u4e00\u81f4\u6027\u548c\u8bc4\u4ef7\u6a21\u5f0f\uff0c\u5e76\u5229\u7528TF-IDF\u5206\u6790\u4e0d\u540c\u6a21\u578b\u7684\u7528\u8bed\u7279\u5f81\u3002", "result": "\uff081\uff09LLM\u95f4\u8bc4\u4ef7\u4e00\u81f4\u6027\u5dee\u5f02\u663e\u8457\uff08\u03b1\u4ece1.00\u52300.35\uff09\uff1b\uff082\uff09\u627e\u51fa\u4e94\u79cd\u4e0d\u540c\u7684\u8bc4\u4ef7\u6a21\u5f0f\uff1b\uff083\uff09\u4e0d\u540c\u4f5c\u54c1\u7684\u8bc4\u4ef7\u65b9\u5dee\u9ad8\u8fbe4.5\u500d\uff1b\uff084\uff09\u6bcf\u4e2a\u6a21\u578b\u7684\u8bc4\u4ef7\u8bed\u8a00\u5177\u6709\u72ec\u7279\u6027\u3002", "conclusion": "LLM\u5728\u6587\u5b66\u6279\u8bc4\u4e2d\u7684\u8868\u73b0\u663e\u793a\u51fa\u7c7b\u4f3c\u4eba\u7c7b\u6279\u8bc4\u5b66\u6d3e\u7684\u4e2a\u4f53\u8bc4\u4ef7\u7279\u5f81\uff0c\u4e0d\u662f\u5b8c\u5168\u4e2d\u7acb\u7684\u8bc4\u5224\u8005\uff0c\u5176\u4ef7\u503c\u4f53\u7cfb\u53d7RLHF\u53ca\u8bad\u7ec3\u8fc7\u7a0b\u5f71\u54cd\u663e\u8457\u3002"}}
{"id": "2507.11640", "categories": ["cs.CE", "76-10, 68T07 (Primary) 76D05, 35Q68 (Secondary)"], "pdf": "https://arxiv.org/pdf/2507.11640", "abs": "https://arxiv.org/abs/2507.11640", "authors": ["Veronika Tr\u00e1vn\u00edkov\u00e1", "Eric von Lieres", "Marek Behr"], "title": "Quantifying data needs in surrogate modeling for flow fields in 2D stirred tanks with physics-informed neural networks (PINNs)", "comment": "24 pages, 18 figures", "summary": "Stirred tanks are vital in chemical and biotechnological processes,\nparticularly as bioreactors. Although computational fluid dynamics (CFD) is\nwidely used to model the flow in stirred tanks, its high computational\ncost$-$especially in multi-query scenarios for process design and\noptimization$-$drives the need for efficient data-driven surrogate models.\nHowever, acquiring sufficiently large datasets can be costly. Physics-informed\nneural networks (PINNs) offer a promising solution to reduce data requirements\nwhile maintaining accuracy by embedding underlying physics into neural network\n(NN) training. This study quantifies the data requirements of vanilla PINNs for\ndeveloping surrogate models of a flow field in a 2D stirred tank. We compare\nthese requirements with classical supervised neural networks and\nboundary-informed neural networks (BINNs). Our findings demonstrate that\nsurrogate models can achieve prediction errors around 3% across Reynolds\nnumbers from 50 to 5000 using as few as six datapoints. Moreover, employing an\napproximation of the velocity profile in place of real data labels leads to\nprediction errors of around 2.5%. These results indicate that even with limited\nor approximate datasets, PINNs can be effectively trained to deliver high\naccuracy comparable to high-fidelity data.", "AI": {"tldr": "\u5728CFD\u8ba1\u7b97\u4ee3\u4ef7\u9ad8\u6602\u3001\u6570\u636e\u83b7\u53d6\u56f0\u96be\u7684\u80cc\u666f\u4e0b\uff0cPINNs\u53ea\u9700\u6781\u5c11\u6570\u636e\u5c31\u80fd\u9ad8\u7cbe\u5ea6\u5efa\u6a21\u6405\u62cc\u7f50\u6d41\u573a\uff0c\u9002\u5408\u9ad8\u6548\u5de5\u7a0b\u4f18\u5316\u3002", "motivation": "\u6405\u62cc\u7f50\u5728\u5316\u5de5\u548c\u751f\u7269\u5de5\u827a\u8fc7\u7a0b\u4e2d\u975e\u5e38\u91cd\u8981\uff0c\u4f46\u7528CFD\u6a21\u62df\u6d41\u52a8\u867d\u7136\u51c6\u786e\uff0c\u4f46\u8ba1\u7b97\u5f00\u9500\u5927\uff0c\u5c24\u5176\u662f\u5728\u9700\u8981\u591a\u6b21\u67e5\u8be2\u7528\u4e8e\u8fc7\u7a0b\u8bbe\u8ba1\u548c\u4f18\u5316\u65f6\u3002\u56e0\u6b64\uff0c\u5f00\u53d1\u9ad8\u6548\u7684\u6570\u636e\u9a71\u52a8\u4ee3\u7406\u6a21\u578b\u53d8\u5f97\u5fc5\u8981\u3002\u7136\u800c\uff0c\u83b7\u5f97\u8db3\u591f\u5927\u578b\u7684\u6570\u636e\u96c6\u53c8\u5f88\u6602\u8d35\u3002\u4e3a\u6b64\uff0c\u5bfb\u627e\u4e00\u79cd\u65e2\u80fd\u51cf\u5c11\u6570\u636e\u9700\u6c42\u53c8\u80fd\u4fdd\u6301\u7cbe\u5ea6\u7684\u65b0\u65b9\u6cd5\u6210\u4e3a\u4e3b\u8981\u52a8\u673a\u3002", "method": "\u672c\u6587\u91c7\u7528\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08PINNs\uff09\u4f5c\u4e3a\u6838\u5fc3\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u7269\u7406\u89c4\u5f8b\u5d4c\u5165\u795e\u7ecf\u7f51\u7edc\u7684\u8bad\u7ec3\u8fc7\u7a0b\u6765\u964d\u4f4e\u5bf9\u6570\u636e\u89c4\u6a21\u7684\u9700\u6c42\u3002\u7814\u7a76\u91cf\u5316\u4e86PINNs\u4f5c\u4e3a\u6405\u62cc\u7f50\u4e8c\u7ef4\u6d41\u573a\u4ee3\u7406\u6a21\u578b\u7684\u6570\u636e\u9700\u6c42\uff0c\u5e76\u4e0e\u4f20\u7edf\u76d1\u7763\u795e\u7ecf\u7f51\u7edc\u53ca\u8fb9\u754c\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08BINNs\uff09\u8fdb\u884c\u4e86\u5bf9\u6bd4\u3002\u90e8\u5206\u5b9e\u9a8c\u8fd8\u5c1d\u8bd5\u7528\u901f\u5ea6\u5256\u9762\u7684\u8fd1\u4f3c\u503c\u4ee3\u66ff\u771f\u5b9e\u6807\u7b7e\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u4f7f\u7528\u4ec5\u516d\u4e2a\u6570\u636e\u70b9\uff0cPINN\u4ee3\u7406\u6a21\u578b\u5728Reynolds\u657050\u81f35000\u8303\u56f4\u5185\u7684\u9884\u6d4b\u8bef\u5dee\u7ea6\u4e3a3%\uff1b\u82e5\u7528\u901f\u5ea6\u5256\u9762\u8fd1\u4f3c\u503c\u66ff\u6362\u771f\u5b9e\u6807\u7b7e\uff0c\u9884\u6d4b\u8bef\u5dee\u8fd8\u80fd\u964d\u81f32.5%\u5de6\u53f3\u3002\u8fd9\u8bf4\u660e\u5373\u4f7f\u6570\u636e\u6709\u9650\u6216\u4e0d\u7cbe\u51c6\uff0cPINNs\u4f9d\u7136\u80fd\u83b7\u5f97\u9ad8\u7cbe\u5ea6\u7684\u4ee3\u7406\u6a21\u578b\u3002", "conclusion": "\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08PINNs\uff09\u80fd\u591f\u4ee5\u6781\u5c11\u7684\u6570\u636e\u91cf\u8bad\u7ec3\u51fa\u9ad8\u7cbe\u5ea6\u7684\u6405\u62cc\u7f50\u6d41\u573a\u4ee3\u7406\u6a21\u578b\uff0c\u5373\u4fbf\u6570\u636e\u4e3a\u8fd1\u4f3c\u503c\u4e5f\u80fd\u4fdd\u8bc1\u7cbe\u5ea6\u3002\u76f8\u8f83\u4e8e\u4f20\u7edf\u795e\u7ecf\u7f51\u7edc\uff0cPINNs\u5728\u6570\u636e\u53d7\u9650\u573a\u666f\u4e0b\u5c24\u5176\u6709\u6548\u3002"}}
{"id": "2507.11625", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11625", "abs": "https://arxiv.org/abs/2507.11625", "authors": ["Varun Srivastava", "Fan Lei", "Srija Mukhopadhyay", "Vivek Gupta", "Ross Maciejewski"], "title": "MapIQ: Benchmarking Multimodal Large Language Models for Map Question Answering", "comment": "Published as a conference paper at COLM 2025", "summary": "Recent advancements in multimodal large language models (MLLMs) have driven\nresearchers to explore how well these models read data visualizations, e.g.,\nbar charts, scatter plots. More recently, attention has shifted to visual\nquestion answering with maps (Map-VQA). However, Map-VQA research has primarily\nfocused on choropleth maps, which cover only a limited range of thematic\ncategories and visual analytical tasks. To address these gaps, we introduce\nMapIQ, a benchmark dataset comprising 14,706 question-answer pairs across three\nmap types: choropleth maps, cartograms, and proportional symbol maps spanning\ntopics from six distinct themes (e.g., housing, crime). We evaluate multiple\nMLLMs using six visual analytical tasks, comparing their performance against\none another and a human baseline. An additional experiment examining the impact\nof map design changes (e.g., altered color schemes, modified legend designs,\nand removal of map elements) provides insights into the robustness and\nsensitivity of MLLMs, their reliance on internal geographic knowledge, and\npotential avenues for improving Map-VQA performance.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u5305\u542b\u591a\u5730\u56fe\u7c7b\u578b\u4e0e\u4e30\u5bcc\u4e3b\u9898\u7684MapIQ\u57fa\u51c6\uff0c\u7528\u4e8e\u5168\u9762\u8bc4\u4f30MLLMs\u5728\u5730\u56fe\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u6a21\u578b\u5728\u4e0d\u540c\u4efb\u52a1\u4e0e\u5730\u56fe\u8bbe\u8ba1\u4e0a\u7684\u8868\u73b0\u4e0d\u4e00\uff0c\u63ed\u793a\u4e86\u6539\u8fdb\u7a7a\u95f4\u3002", "motivation": "\u4ee5\u5f80\u7684\u5730\u56fe\u89c6\u89c9\u95ee\u7b54\uff08Map-VQA\uff09\u7814\u7a76\u8fc7\u4e8e\u805a\u7126\u4e8e\u6655\u67d3\u5730\u56fe\uff0c\u672a\u80fd\u8986\u76d6\u66f4\u591a\u7c7b\u578b\u7684\u5730\u56fe\u4e0e\u66f4\u4e30\u5bcc\u7684\u4e3b\u9898\u53ca\u5206\u6790\u4efb\u52a1\uff0c\u5bf9MLLMs\u5728\u66f4\u5e7f\u6cdb\u5730\u56fe\u89c6\u89c9\u5206\u6790\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\u7f3a\u4e4f\u5168\u9762\u8bc4\u4f30\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86MapIQ\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u542b14706\u7ec4\u6db5\u76d6\u6655\u67d3\u5730\u56fe\u3001\u53d8\u5f62\u5730\u56fe\u548c\u6bd4\u4f8b\u7b26\u53f7\u5730\u56fe\u4e09\u79cd\u7c7b\u578b\u3001\u6a2a\u8de8\u516d\u5927\u4e3b\u9898\u7684\u95ee\u9898-\u7b54\u6848\u5bf9\u3002\u901a\u8fc7\u516d\u79cd\u89c6\u89c9\u5206\u6790\u4efb\u52a1\uff0c\u8bc4\u4f30\u591a\u79cdMLLMs\u7684\u8868\u73b0\uff0c\u4e0e\u4eba\u7c7b\u57fa\u51c6\u8fdb\u884c\u5bf9\u6bd4\uff0c\u5e76\u901a\u8fc7\u6539\u53d8\u91cf\u56fe\u8bbe\u8ba1\uff08\u5982\u914d\u8272\u3001\u56fe\u4f8b\u3001\u5143\u7d20\u79fb\u9664\uff09\u5b9e\u9a8c\uff0c\u5206\u6790\u5176\u9c81\u68d2\u6027\u3001\u5730\u7406\u77e5\u8bc6\u4f9d\u8d56\u53ca\u6539\u8fdb\u65b9\u5411\u3002", "result": "\u591a\u4e2aMLLMs\u5728\u4e0d\u540c\u7c7b\u578b\u5730\u56fe\u53ca\u89c6\u89c9\u5206\u6790\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u6709\u8f83\u5927\u5dee\u5f02\uff0c\u4e0e\u4eba\u7c7b\u57fa\u51c6\u4ecd\u6709\u5dee\u8ddd\u3002\u5730\u56fe\u8bbe\u8ba1\u7684\u53d8\u5316\u663e\u8457\u5f71\u54cd\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\u53ca\u89e3\u9898\u80fd\u529b\u3002\u7814\u7a76\u63ed\u793a\u4e86\u5f53\u524dMLLMs\u5728\u5904\u7406\u591a\u6837\u5730\u56fe\u53ca\u590d\u6742\u89c6\u89c9\u4efb\u52a1\u4e0a\u7684\u4e0d\u8db3\u53ca\u4f9d\u8d56\u5185\u5728\u5730\u7406\u77e5\u8bc6\u7684\u60c5\u51b5\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u901a\u8fc7\u63d0\u51fa\u591a\u7c7b\u578b\u3001\u8de8\u4e3b\u9898\u7684\u5730\u56fe\u89c6\u89c9\u95ee\u7b54\u57fa\u51c6\uff0c\u6709\u6548\u5f25\u8865\u4e86\u73b0\u6709Map-VQA\u7814\u7a76\u7684\u5c40\u9650\uff0c\u4e3a\u672a\u6765\u63d0\u5347MLLM\u5730\u56fe\u611f\u77e5\u4e0e\u95ee\u7b54\u80fd\u529b\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u4e0e\u6570\u636e\u8d44\u6e90\u3002"}}
