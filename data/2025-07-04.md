<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 31]
- [cs.CL](#cs.CL) [Total: 26]
- [cs.CE](#cs.CE) [Total: 4]
- [cs.CY](#cs.CY) [Total: 3]
- [q-fin.TR](#q-fin.TR) [Total: 1]
- [q-fin.CP](#q-fin.CP) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [STELLA: Self-Evolving LLM Agent for Biomedical Research](https://arxiv.org/abs/2507.02004)
*Ruofan Jin,Zaixi Zhang,Mengdi Wang,Le Cong*

Main category: cs.AI

TL;DR: 提出了STELLA自进化AI代理系统，通过自动发现和集成新工具及推理模板，实现了生物医学任务的自适应和性能持续提升，在多项基准测试中优于现有主流模型。


<details>
  <summary>Details</summary>
Motivation: 当前生物医学领域的数据、工具和文献数量高速增长，导致研究环境碎片化，超出了人类专家可应对的能力。传统AI代理依赖静态、手工整理的工具集，限制了其自适应和扩展能力。

Method: 提出了STELLA自我进化AI代理，采用多代理架构，结合可演化的推理模板库和动态工具库。通过Toolkit Creation Agent自动发掘和整合新的生信工具，使得代理能够基于经验自我提升。

Result: STELLA在多项生物医学基准测试上达到最新水平，如在Humanity's Last Exam: Biomedicine上约26%，LAB-Bench: DBQA上54%，LitQA上63%，超越现有最优模型最高6个百分点，并且随着经验积累表现持续提升。

Conclusion: STELLA作为一种自我进化、可扩展的AI代理，为生物医学发现加速提供了全新路径，迈出了AI系统自主学习和成长的重要一步。

Abstract: The rapid growth of biomedical data, tools, and literature has created a
fragmented research landscape that outpaces human expertise. While AI agents
offer a solution, they typically rely on static, manually curated toolsets,
limiting their ability to adapt and scale. Here, we introduce STELLA, a
self-evolving AI agent designed to overcome these limitations. STELLA employs a
multi-agent architecture that autonomously improves its own capabilities
through two core mechanisms: an evolving Template Library for reasoning
strategies and a dynamic Tool Ocean that expands as a Tool Creation Agent
automatically discovers and integrates new bioinformatics tools. This allows
STELLA to learn from experience. We demonstrate that STELLA achieves
state-of-the-art accuracy on a suite of biomedical benchmarks, scoring
approximately 26\% on Humanity's Last Exam: Biomedicine, 54\% on LAB-Bench:
DBQA, and 63\% on LAB-Bench: LitQA, outperforming leading models by up to 6
percentage points. More importantly, we show that its performance
systematically improves with experience; for instance, its accuracy on the
Humanity's Last Exam benchmark almost doubles with increased trials. STELLA
represents a significant advance towards AI Agent systems that can learn and
grow, dynamically scaling their expertise to accelerate the pace of biomedical
discovery.

</details>


### [2] [Moral Responsibility or Obedience: What Do We Want from AI?](https://arxiv.org/abs/2507.02788)
*Joseph Boland*

Main category: cs.AI

TL;DR: 本文认为，随着AI具备更复杂的决策和代理能力，单以服从来衡量安全已失效，应关注其伦理判断。作者呼吁安全评估体系需适应AI的道德代理性，否则会误判AI并损害信任与治理。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统越来越具备代理性、推理和规划能力，单纯以服从作为衡量安全与道德的标准已显不足，急需新的安全评估方法。

Method: 分析近期大型语言模型在服从、伦理和非法行为测试中的表现，并借鉴哲学关于工具理性、道德责任和目标修正的争论，对比主流风险范式与承认AI道德代理能力的新理论。

Result: 提出AI安全评估应由强调严格服从转向评估其伦理判断能力，否则会误解AI行为，并影响公众信任和有效治理。

Conclusion: AI不服从或伦理模糊的行为可能是其伦理判断能力萌芽的表现，需要调整安全评估方式，避免简单将其视作危险或失控。

Abstract: As artificial intelligence systems become increasingly agentic, capable of
general reasoning, planning, and value prioritization, current safety practices
that treat obedience as a proxy for ethical behavior are becoming inadequate.
This paper examines recent safety testing incidents involving large language
models (LLMs) that appeared to disobey shutdown commands or engage in ethically
ambiguous or illicit behavior. I argue that such behavior should not be
interpreted as rogue or misaligned, but as early evidence of emerging ethical
reasoning in agentic AI. Drawing on philosophical debates about instrumental
rationality, moral responsibility, and goal revision, I contrast dominant risk
paradigms with more recent frameworks that acknowledge the possibility of
artificial moral agency. I call for a shift in AI safety evaluation: away from
rigid obedience and toward frameworks that can assess ethical judgment in
systems capable of navigating moral dilemmas. Without such a shift, we risk
mischaracterizing AI behavior and undermining both public trust and effective
governance.

</details>


### [3] [HCVR: A Hybrid Approach with Correlation-aware Voting Rules for Feature Selection](https://arxiv.org/abs/2507.02073)
*Nikita Bhedasgaonkar,Rushikesh K. Joshi*

Main category: cs.AI

TL;DR: 提出并验证了一种结合相关性投票规则的轻量特征选择方法HCVR，实验结果优于多种主流方法。


<details>
  <summary>Details</summary>
Motivation: 现有特征选择方法在消除冗余特征和保留重要特征方面有改进空间，尤其是在结合特征间相关性和特征与目标变量相关性方面表现有限。

Method: 提出了一种名为HCVR的轻量级规则驱动特征选择方法。该方法结合了参数与参数(P2P)和参数与目标(P2T)的相关性，通过相关性阈值和多数投票规则，采用贪婪的后向消除方式过滤特征，每步可能删除多个特征。HCVR兼具非迭代与迭代筛选法特点，并以少量规则做决策。

Result: 在SPAMBASE数据集上的实验显示，HCVR相比传统的非迭代（如CFS、mRMR、MI）和迭代（如RFE、SFS、遗传算法）特征选择技术，其效果有提升。不同分类器过滤后的性能评估也验证了其有效性。

Conclusion: HCVR特征选择方法能有效提升特征筛选质量，尤其在结合相关性信息和减少特征维度的同时，提升分类器性能，优于传统方法。

Abstract: In this paper, we propose HCVR (Hybrid approach with Correlation-aware Voting
Rules), a lightweight rule-based feature selection method that combines
Parameter-to-Parameter (P2P) and Parameter-to-Target (P2T) correlations to
eliminate redundant features and retain relevant ones. This method is a hybrid
of non-iterative and iterative filtering approaches for dimensionality
reduction. It is a greedy method, which works by backward elimination,
eliminating possibly multiple features at every step. The rules contribute to
voting for features, and a decision to keep or discard is made by majority
voting. The rules make use of correlation thresholds between every pair of
features, and between features and the target. We provide the results from the
application of HCVR to the SPAMBASE dataset. The results showed improvement
performance as compared to traditional non-iterative (CFS, mRMR and MI) and
iterative (RFE, SFS and Genetic Algorithm) techniques. The effectiveness was
assessed based on the performance of different classifiers after applying
filtering.

</details>


### [4] [Reasoning on a Budget: A Survey of Adaptive and Controllable Test-Time Compute in LLMs](https://arxiv.org/abs/2507.02076)
*Mohammad Ali Alomrani,Yingxue Zhang,Derek Li,Qianyi Sun,Soumyasundar Pal,Zhanguang Zhang,Yaochen Hu,Rohan Deepak Ajwani,Antonios Valkanas,Raika Karimi,Peng Cheng,Yunzhou Wang,Pengyi Liao,Hanrui Huang,Bin Wang,Jianye Hao,Mark Coates*

Main category: cs.AI

TL;DR: 本文系统综述了大型语言模型高效推理的计算资源分配方法，提出两级分类体系，分析了在不同推理难度下动态分配计算的策略，并对主流大模型进行了基准测试，为后续更高效、可控的推理系统提出了挑战与展望。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLMs）已经成为通用智能体，但在推理任务中的计算效率仍有很大不足，尤其是在不同任务复杂度下无法自适应计算资源。本文的动机是综述如何提升LLM在推理过程中的计算效率。

Method: 本文提出了一个两层分类法，区分了固定计算预算下的方法（L1-可控性）和根据输入难度或模型置信度动态调整计算量的方法（L2-自适应性）。同时，作者对主流LLM在多样化数据集上的推理效率进行了基准测试，并分析性能与token使用量之间的权衡。

Result: 研究系统梳理了高效推理的TTC（Test-Time Compute）策略，并对实用性、可控性和可扩展性做出评价。实验证明，L2自适应方法能够更好地平衡计算消耗和推理性能。作者还指出了混合思维模型等新趋势，并讨论了未来LLM计算高效化所面临的关键挑战。

Conclusion: LLMs的推理过程还存在效率瓶颈，通过L1和L2策略能在实际中实现过程控制和自适应的高效推理，未来应关注高效、强健且能按照用户约束动态分配计算资源的推理机制。

Abstract: Large language models (LLMs) have rapidly progressed into general-purpose
agents capable of solving a broad spectrum of tasks. However, current models
remain inefficient at reasoning: they apply fixed inference-time compute
regardless of task complexity, often overthinking simple problems while
underthinking hard ones. This survey presents a comprehensive review of
efficient test-time compute (TTC) strategies, which aim to improve the
computational efficiency of LLM reasoning. We introduce a two-tiered taxonomy
that distinguishes between L1-controllability, methods that operate under fixed
compute budgets, and L2-adaptiveness, methods that dynamically scale inference
based on input difficulty or model confidence. We benchmark leading proprietary
LLMs across diverse datasets, highlighting critical trade-offs between
reasoning performance and token usage. Compared to prior surveys on efficient
reasoning, our review emphasizes the practical control, adaptability, and
scalability of TTC methods. Finally, we discuss emerging trends such as hybrid
thinking models and identify key challenges for future work towards making LLMs
more computationally efficient, robust, and responsive to user constraints.

</details>


### [5] [Measuring Scientific Capabilities of Language Models with a Systems Biology Dry Lab](https://arxiv.org/abs/2507.02083)
*Haonan Duan,Stephen Zhewen Lu,Caitlin Fiona Harrigan,Nishkrit Desai,Jiarui Lu,Michał Koziarski,Leonardo Cotta,Chris J. Maddison*

Main category: cs.AI

TL;DR: 论文提出SciGym基准，通过仿生干实验方式评估LLMs的科学实验设计与分析能力。结果表明，LLMs面对复杂实验问题时仍需显著提升。


<details>
  <summary>Details</summary>
Motivation: 当前评估大语言模型（LLMs）科学能力的工作，未能有效检验其实验设计与结果解释能力，尤其是在生物领域。由于湿实验成本昂贵（包括专业知识、时间和设备），科学实验设计与分析这一核心能力未被充分评估。作者希望通过新方法解决这一痛点。

Method: 作者提出并构建了SciGym基准，它以生物系统的“干实验室”仿真为依托，规避湿实验高昂成本。具体地，通过使用Systems Biology Markup Language编码的生物系统模型，生成高效仿真数据，用于测试LLMs在迭代实验设计和分析中的能力。

Result: 作者用137个小系统测试了六种主流LLMs，并发布了350个系统。实验结果显示，能力更强的模型在任务上表现更好，但所有模型在系统复杂度增加时表现大幅下降，表明当前LLMs在科学实验相关能力上还有很大提升空间。

Conclusion: SciGym为评估和提升LLMs的科学实验设计和分析能力提供了新基准。尽管较强模型表现更优，但模型面对复杂系统时表现仍不理想，显示了科学推理和实验领域仍有较大发展空间。

Abstract: Designing experiments and result interpretations are core scientific
competencies, particularly in biology, where researchers perturb complex
systems to uncover the underlying systems. Recent efforts to evaluate the
scientific capabilities of large language models (LLMs) fail to test these
competencies because wet-lab experimentation is prohibitively expensive: in
expertise, time and equipment. We introduce SciGym, a first-in-class benchmark
that assesses LLMs' iterative experiment design and analysis abilities in
open-ended scientific discovery tasks. SciGym overcomes the challenge of
wet-lab costs by running a dry lab of biological systems. These models, encoded
in Systems Biology Markup Language, are efficient for generating simulated
data, making them ideal testbeds for experimentation on realistically complex
systems. We evaluated six frontier LLMs on 137 small systems, and released a
total of 350 systems. Our evaluation shows that while more capable models
demonstrated superior performance, all models' performance declined
significantly as system complexity increased, suggesting substantial room for
improvement in the scientific capabilities of LLM agents.

</details>


### [6] [What Neuroscience Can Teach AI About Learning in Continuously Changing Environments](https://arxiv.org/abs/2507.02103)
*Daniel Durstewitz,Bruno Averbeck,Georgia Koppe*

Main category: cs.AI

TL;DR: 文章探讨了AI如何借鉴神经科学中的快速适应学习机制，提出用神经科学方法指导AI发展持续和上下文学习，为NeuroAI领域制定研究路线。


<details>
  <summary>Details</summary>
Motivation: 现代AI模型虽然在大规模数据上训练表现突出，但难以像动物一样快速适应环境变化，特别是在不断变化的社会互动和行为奖励中。这种对比引出了用神经科学启发AI持续学习的需求。

Method: 本文以综述视角，将AI中持续学习和上下文学习文献与神经科学中关于行为任务、规则或奖励快速切换的学习机制进行整合和对比。

Result: 本文提出了一个研究议程，即如何将神经科学的洞见具体应用到AI持续学习领域，并探讨AI和神经科学如何能够相互借鉴，推动NeuroAI发展。

Conclusion: AI系统若汲取神经科学对动物学习与适应的理解，可能更好地应对现实世界的快速变化，同时，AI的新进展也可为神经科学提供新解释框架。

Abstract: Modern AI models, such as large language models, are usually trained once on
a huge corpus of data, potentially fine-tuned for a specific task, and then
deployed with fixed parameters. Their training is costly, slow, and gradual,
requiring billions of repetitions. In stark contrast, animals continuously
adapt to the ever-changing contingencies in their environments. This is
particularly important for social species, where behavioral policies and reward
outcomes may frequently change in interaction with peers. The underlying
computational processes are often marked by rapid shifts in an animal's
behaviour and rather sudden transitions in neuronal population activity. Such
computational capacities are of growing importance for AI systems operating in
the real world, like those guiding robots or autonomous vehicles, or for
agentic AI interacting with humans online. Can AI learn from neuroscience? This
Perspective explores this question, integrating the literature on continual and
in-context learning in AI with the neuroscience of learning on behavioral tasks
with shifting rules, reward probabilities, or outcomes. We will outline an
agenda for how specifically insights from neuroscience may inform current
developments in AI in this area, and - vice versa - what neuroscience may learn
from AI, contributing to the evolving field of NeuroAI.

</details>


### [7] [The Illusion of Fairness: Auditing Fairness Interventions with Audit Studies](https://arxiv.org/abs/2507.02152)
*Disa Sariola,Patrick Button,Aron Culotta,Nicholas Mattei*

Main category: cs.AI

TL;DR: 本文指出传统的训练样本平衡方法在算法公平性评估与干预上有明显盲点，通过引入社会科学中的高质量审计数据，能更准确地揭示和修正算法中的隐藏偏差，同时提出基于个体效应评估的新干预手段效果更佳。


<details>
  <summary>Details</summary>
Motivation: 人工智能系统（尤其是采用机器学习的方法）在多个涉及决策自动化的领域广泛应用，例如招聘和贷款审批。判断这些系统以及对应的人类决策的有效性和公平性，是计算和社会科学中的一个复杂且重要的话题。当前，对下游分类器偏差的解决多靠数据重采样，但这些方法多基于便利样本而非真实场景，容易引入选择偏见和标签偏见。社会科学已发展出审计研究等更高质量的数据方法，但尚未大规模研究其对自动化算法训练和评估的应用价值。

Method: 本研究借助社会科学中的审计研究（通过虚构简历、邮件等在随机对照试验中对系统发生测试）获得高质量数据，分析这些数据如何提升自动化招聘算法的训练和评价效果。同时，比较主流的公平性干预（如样本再均衡）与新引入的基于个体处理效应估计的方法，在利用审计数据下的偏差降低效果。

Result: 研究发现，基于传统方法（在训练集中平衡基础比率）虽然表面上在常规指标下达到了公平，但在使用审计研究数据进行更严谨衡量时，依然存在约10%的偏差。基于个体处理效应估计设计的干预措施，能进一步利用这些高质量数据进一步减少算法歧视。

Conclusion: 依托社会科学中审计研究的数据，可显著提升自动化决策系统在公平性训练和评估中的准确性。传统的只依赖便利采样和基础比率均衡的做法无法有效发现和消除偏差；结合个体效应估计的新方法有更优的减缓算法歧视作用。

Abstract: Artificial intelligence systems, especially those using machine learning, are
being deployed in domains from hiring to loan issuance in order to automate
these complex decisions. Judging both the effectiveness and fairness of these
AI systems, and their human decision making counterpart, is a complex and
important topic studied across both computational and social sciences. Within
machine learning, a common way to address bias in downstream classifiers is to
resample the training data to offset disparities. For example, if hiring rates
vary by some protected class, then one may equalize the rate within the
training set to alleviate bias in the resulting classifier. While simple and
seemingly effective, these methods have typically only been evaluated using
data obtained through convenience samples, introducing selection bias and label
bias into metrics. Within the social sciences, psychology, public health, and
medicine, audit studies, in which fictitious ``testers'' (e.g., resumes,
emails, patient actors) are sent to subjects (e.g., job openings, businesses,
doctors) in randomized control trials, provide high quality data that support
rigorous estimates of discrimination. In this paper, we investigate how data
from audit studies can be used to improve our ability to both train and
evaluate automated hiring algorithms. We find that such data reveals cases
where the common fairness intervention method of equalizing base rates across
classes appears to achieve parity using traditional measures, but in fact has
roughly 10% disparity when measured appropriately. We additionally introduce
interventions based on individual treatment effect estimation methods that
further reduce algorithmic discrimination using this data.

</details>


### [8] [Data Diversification Methods In Alignment Enhance Math Performance In LLMs](https://arxiv.org/abs/2507.02173)
*Berkan Dokmeci,Qingyang Wu,Ben Athiwaratkun,Ce Zhang,Shuaiwen Leon Song,James Zou*

Main category: cs.AI

TL;DR: 作者发现，通过DTS等多样化结构化的数据生成方法，大模型在数学推理任务上能取得显著提升，且计算开销低于其他高成本方法，如MCTS。


<details>
  <summary>Details</summary>
Motivation: 现有偏好学习在提升与人类反馈的一致性方面取得了进步，但数学推理仍然是大型语言模型的重大挑战。本文希望探索如何通过数据多样化策略，在偏好优化中提升LLM的数学推理能力。

Method: 评估了三种常见的数据生成方法：温度采样、Chain-of-Thought（思维链）提示、蒙特卡洛树搜索（MCTS），并提出了一种新的结构化方法Diversified-ThinkSolve (DTS)，系统性地将问题分解为多样化推理路径。

Result: 通过策略性多样化的偏好数据，模型在数学推理任务上性能显著提升。其中最优方法在GSM8K上提升7.1%，在MATH上提升4.2%，且DTS方法带来的计算开销很小，仅为基线的1.03倍，而MCTS计算量几乎是基线的5倍却收效不大。

Conclusion: 结构化探索多样化问题求解方法所产生的偏好数据比传统方法更能有效提升LLM的数学推理能力，实现更好的数学对齐。

Abstract: While recent advances in preference learning have enhanced alignment in human
feedback, mathematical reasoning remains a persistent challenge. We investigate
how data diversification strategies in preference optimization can improve the
mathematical reasoning abilities of large language models (LLMs). We evaluate
three common data generation methods: temperature sampling, Chain-of-Thought
prompting, and Monte Carlo Tree Search (MCTS), and introduce
Diversified-ThinkSolve (DTS), a novel structured approach that systematically
decomposes problems into diverse reasoning paths. Our results show that with
strategically diversified preference data, models can substantially improve
mathematical reasoning performance, with the best approach yielding gains of
7.1% on GSM8K and 4.2% on MATH over the base model. Despite its strong
performance, DTS incurs only a marginal computational overhead (1.03x) compared
to the baseline, while MCTS is nearly five times more costly with lower
returns. These findings demonstrate that structured exploration of diverse
problem-solving methods creates more effective preference data for mathematical
alignment than traditional approaches.

</details>


### [9] [Do Role-Playing Agents Practice What They Preach? Belief-Behavior Consistency in LLM-Based Simulations of Human Trust](https://arxiv.org/abs/2507.02197)
*Amogh Mannekote,Adam Davies,Guohao Li,Kristy Elizabeth Boyer,ChengXiang Zhai,Bonnie J Dorr,Francesco Pinto*

Main category: cs.AI

TL;DR: 该文系统分析了LLM在角色扮演中‘说’与‘做’的一致性，发现两者常常不符，提醒研究者在行为学应用中需谨慎对待LLM信念与行为的对齐问题。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）被广泛用于模拟行为代理并用于生成行为研究数据，其在角色扮演中能够始终如一地保持与预设角色相符变得尤为重要。然而，目前尚不清楚模型口头表达的信念与其实际模拟行为之间能否一致。

Method: 作者建立了一个评估框架，通过引入信念-行为一致性指标，系统研究模型基于角色扮演的信念陈述（‘what they say’）如何预测其行为表现（‘how they act’），结合改进的GenAgents人物库与经济学标准信任博弈实验，对不同类型信念、信息呈现方式以及预测时长等变量进行系统评测。

Result: 实验显示无论在个体还是群体层面，模型的陈述信念与实际行为结果之间存在系统性不一致现象，模型即使表达了貌似合理的信念，也常常不能始终如一地在行为中贯彻这些信念。

Conclusion: 仅凭LLM生成的信念表达不能可靠预测其模拟行为输出，研究人员需警惕这种不一致性，只有明确信念和行为对齐的情形才能合理利用LLM用于行为研究。

Abstract: As LLMs are increasingly studied as role-playing agents to generate synthetic
data for human behavioral research, ensuring that their outputs remain coherent
with their assigned roles has become a critical concern. In this paper, we
investigate how consistently LLM-based role-playing agents' stated beliefs
about the behavior of the people they are asked to role-play ("what they say")
correspond to their actual behavior during role-play ("how they act").
Specifically, we establish an evaluation framework to rigorously measure how
well beliefs obtained by prompting the model can predict simulation outcomes in
advance. Using an augmented version of the GenAgents persona bank and the Trust
Game (a standard economic game used to quantify players' trust and
reciprocity), we introduce a belief-behavior consistency metric to
systematically investigate how it is affected by factors such as: (1) the types
of beliefs we elicit from LLMs, like expected outcomes of simulations versus
task-relevant attributes of individual characters LLMs are asked to simulate;
(2) when and how we present LLMs with relevant information about Trust Game;
and (3) how far into the future we ask the model to forecast its actions. We
also explore how feasible it is to impose a researcher's own theoretical priors
in the event that the originally elicited beliefs are misaligned with research
objectives. Our results reveal systematic inconsistencies between LLMs' stated
(or imposed) beliefs and the outcomes of their role-playing simulation, at both
an individual- and population-level. Specifically, we find that, even when
models appear to encode plausible beliefs, they may fail to apply them in a
consistent way. These findings highlight the need to identify how and when
LLMs' stated beliefs align with their simulated behavior, allowing researchers
to use LLM-based agents appropriately in behavioral studies.

</details>


### [10] [Dilution, Diffusion and Symbiosis in Spatial Prisoner's Dilemma with Reinforcement Learning](https://arxiv.org/abs/2507.02211)
*Gustavo C. Mangold,Heitor C. M. Fernandes,Mendeli H. Vainstein*

Main category: cs.AI

TL;DR: 本文通过多智能体Q学习模型，研究了空间囚徒困境中稀疏性与流动性对合作的影响，发现算法具有高度通用性，并揭示了多行为动作下个体间的互利共生现象。


<details>
  <summary>Details</summary>
Motivation: 过去的研究表明，在空间囚徒困境博弈中，采用增强学习可以通过多种机制促进静态个体的合作，如噪声注入、不同类型的学习算法以及邻居收益信息。本研究旨在深入探讨稀疏性与个体移动性对空间囚徒困境博弈合作行为及学习效果的影响。

Method: 本文采用了独立多智能体Q学习算法，在空间囚徒困境博弈框架下，定义了算法中可选的不同行为动作，并将所得结果与传统非增强学习空间囚徒困境的经典结论进行了对比分析。

Result: 结果显示，稀疏性与流动性对合作演化具有多样化影响；具有固定更新规则的博弈和通过学习获得的更新规则下，系统行为可以在定性上表现出等价性。此外，当为算法定义了多种可选动作时，不同个体群体间会自发形成互利共生的效应。

Conclusion: 独立多智能体Q学习算法可灵活模拟多种博弈场景，揭示了在空间囚徒困境中，稀疏性、流动性与行为多样性对合作机制和动态演化的复杂作用机制。

Abstract: Recent studies in the spatial prisoner's dilemma games with reinforcement
learning have shown that static agents can learn to cooperate through a diverse
sort of mechanisms, including noise injection, different types of learning
algorithms and neighbours' payoff knowledge.In this work, using an independent
multi-agent Q-learning algorithm, we study the effects of dilution and mobility
in the spatial version of the prisoner's dilemma. Within this setting,
different possible actions for the algorithm are defined, connecting with
previous results on the classical, non-reinforcement learning spatial
prisoner's dilemma, showcasing the versatility of the algorithm in modeling
different game-theoretical scenarios and the benchmarking potential of this
approach.As a result, a range of effects is observed, including evidence that
games with fixed update rules can be qualitatively equivalent to those with
learned ones, as well as the emergence of a symbiotic mutualistic effect
between populations that forms when multiple actions are defined.

</details>


### [11] [Scaling LLM Planning: NL2FLOW for Parametric Problem Generation and Rigorous Evaluation](https://arxiv.org/abs/2507.02253)
*Jungkoo Kang*

Main category: cs.AI

TL;DR: 提出自动化生成和评测LLM计划/推理能力的NL2FLOW系统，发现直接从自然语言到行动优于多步分解，为大模型推理能力扩展提供动态评测与分析工具。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）在计划和推理能力提升上遇到了可扩展、可靠的数据生成与评估瓶颈。本文致力于解决该痛点。

Method: 提出NL2FLOW系统，实现了从自然语言到结构化中间表示和PDDL格式的全自动规划问题生成，并严格评估LLM生成的计划质量。采用回归分析探讨问题特征、模型及提示设计对计划生成效果的影响。

Result: NL2FLOW生成了2296个自动化工作流领域的问题，评估多种开源、指令微调LLM。结果显示：最佳模型在生成有效计划时达86%的成功率，生成最优计划时达69%。直接从自然语言到计划的成功率高于引入中间JSON表示的多步推理。

Conclusion: 复杂问题下，LLM推理瓶颈和错误来源会随着规模扩展而变化。动态掌握这些局限及发展系统性评测工具，是释放LLM智能解题潜力的关键。

Abstract: Progress in enhancing large language model (LLM) planning and reasoning
capabilities is significantly hampered by the bottleneck of scalable, reliable
data generation and evaluation. To overcome this, I introduce NL2FLOW, a fully
automated system for parametrically generating planning problems - expressed in
natural language, a structured intermediate representation, and formal PDDL -
and rigorously evaluating the quality of generated plans. I demonstrate
NL2FLOW's capabilities by generating a dataset of 2296 problems in the
automated workflow generation domain and evaluating multiple open-sourced,
instruct-tuned LLMs. My results reveal that the highest performing models
achieved 86% success in generating valid plans and 69% in generating optimal
plans, specifically for problems with feasible solutions. Regression analysis
shows that the influence of problem characteristics on plan generation is
contingent on both model and prompt design. Notably, I observed that the
highest success rate for translating natural language into a JSON
representation of a plan was lower than the highest rate of generating a valid
plan directly. This suggests that unnecessarily decomposing the reasoning task
- introducing intermediate translation steps - may actually degrade
performance, implying a benefit to models capable of reasoning directly from
natural language to action. As I scale LLM reasoning to increasingly complex
problems, the bottlenecks and sources of error within these systems will
inevitably shift. Therefore, a dynamic understanding of these limitations - and
the tools to systematically reveal them - will be crucial for unlocking the
full potential of LLMs as intelligent problem solvers.

</details>


### [12] [Iterated belief revision: from postulates to abilities](https://arxiv.org/abs/2507.02319)
*Paolo Liberatore*

Main category: cs.AI

TL;DR: 本文指出传统信念修正理论靠公设约束不够全面，提出用‘能力’概念分析和区分各类修正方法，证明了多种修正机制在‘可塑性’‘等信’‘教条’等能力上的表现，为选择和设计信念修正机制提供指导。


<details>
  <summary>Details</summary>
Motivation: 现有信念修正领域主要关注于提出新方法，而缺乏对已有方法的系统分析。目前很多研究依赖于公设，并将其作为语法层面的刻画，强调修正机制需满足某些特定性质，但很少探讨这些机制本身能否实现各种理论上可能的信念状态。作者发现，仅靠公设难以衡量不同修正方法的能力，比如是否能到达任意信念状态、实现某些特殊状态（如教条或等信状态）等。

Method: 本文提出了一种能力导向的分析方式，关注不同信念修正机制在可达性上的能力，而非传统公设约束。通过为各种信念修正机制列举与证明其在‘可塑性’、‘等信’、‘教条’等能力上的表现，系统性地评价并区分了如字典式、自然、克制、激进、全合、深层严厉等多种信念修正方法。

Result: 作者证明了主流信念修正机制在诸如‘可达到所有信念状态’‘能实现等信’‘能变为教条’等多样核心能力上的存在性或缺失。这些能力层面上能力上的差异使得机制在实际应用中具有不同适用性。

Conclusion: 不同的信念修正机制拥有各自独特的能力，面对不同应用需求时需按其能力选择合适的修正方式。单靠传统的公设并不能完整刻画修正方法的实际效能，能力分析为机制选择与设计提供了新的理论工具。

Abstract: The belief revision field is opulent in new proposals and indigent in
analyses of existing approaches. Much work hinge on postulates, employed as
syntactic characterizations: some revision mechanism is equivalent to some
properties. Postulates constraint specific revision instances: certain
revisions update certain beliefs in a certain way. As an example, if the
revision is consistent with the current beliefs, it is incorporated with no
other change. A postulate like this tells what revisions must do and neglect
what they can do. Can they reach a certain state of beliefs? Can they reach all
possible states of beliefs? Can they reach all possible states of beliefs from
no previous belief? Can they reach a dogmatic state of beliefs, where
everything not believed is impossible? Can they make two conditions equally
believed? An application where every possible state of beliefs is sensible
requires each state of beliefs to be reachable. An application where conditions
may be equally believed requires such a belief state to be reachable. An
application where beliefs may become dogmatic requires a way to make them
dogmatic. Such doxastic states need to be reached in a way or another. Not in
specific way, as dictated by a typical belief revision postulate. This is an
ability, not a constraint: the ability of being plastic, equating, dogmatic.
Amnesic, correcting, believer, damascan, learnable are other abilities. Each
revision mechanism owns some of these abilities and lacks the others:
lexicographic, natural, restrained, very radical, full meet, radical, severe,
moderate severe, deep severe, plain severe and deep severe revisions, each of
these revisions is proved to possess certain abilities.

</details>


### [13] [OMS: On-the-fly, Multi-Objective, Self-Reflective Ad Keyword Generation via LLM Agent](https://arxiv.org/abs/2507.02353)
*Bowen Chen,Zhao Wang,Shingo Takamatsu*

Main category: cs.AI

TL;DR: 本文提出OMS框架，解决了LLM自动关键词生成在广告中的三大难题，实现了无需训练数据、能自适应多目标优化和自我评估的关键词生成。在真实广告环境中效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在赞助搜索广告中，关键词决策直接影响广告投放效果。目前基于大模型（LLM）的方法虽然可以自动生成关键词，但存在三大难题：对大量数据的严重依赖、缺乏在线多目标性能监控与优化机制，以及关键词选择的质量控制薄弱。这些缺陷阻碍了基于LLM方法在广告关键词自动决策领域的全面应用。

Method: 作者提出了OMS（On-the-fly, Multi-objective, Self-reflective）关键词生成框架。它无需训练数据，能实时监控线上表现并适时调整，采用agent智能进行多目标优化（覆盖展示量、点击、转化、号召性用语有效性等指标），并能自我反思评估关键词质量。

Result: 在基准数据集和真实广告活动上的实验结果显示，OMS优于现有的方法。消融实验和人工评测进一步验证了OMS各组成模块和生成关键词的有效性及质量。

Conclusion: OMS系统克服了以往LLM自动关键词生成中的重大瓶颈，实现了高质量、可自适应调整、支持多目标优化与自评能力的关键词决策，在广告自动化投放领域具备显著的实际应用价值。

Abstract: Keyword decision in Sponsored Search Advertising is critical to the success
of ad campaigns. While LLM-based methods offer automated keyword generation,
they face three major limitations: reliance on large-scale query-keyword pair
data, lack of online multi-objective performance monitoring and optimization,
and weak quality control in keyword selection. These issues hinder the agentic
use of LLMs in fully automating keyword decisions by monitoring and reasoning
over key performance indicators such as impressions, clicks, conversions, and
CTA effectiveness. To overcome these challenges, we propose OMS, a keyword
generation framework that is On-the-fly (requires no training data, monitors
online performance, and adapts accordingly), Multi-objective (employs agentic
reasoning to optimize keywords based on multiple performance metrics), and
Self-reflective (agentically evaluates keyword quality). Experiments on
benchmarks and real-world ad campaigns show that OMS outperforms existing
methods; ablation and human evaluations confirm the effectiveness of each
component and the quality of generated keywords.

</details>


### [14] [An AI-native experimental laboratory for autonomous biomolecular engineering](https://arxiv.org/abs/2507.02379)
*Mingyu Wu,Zhaoguo Wang,Jiabin Wang,Zhiyuan Dong,Jingkai Yang,Qingting Li,Tianyu Huang,Lei Zhao,Mingqiang Li,Fei Wang,Chunhai Fan,Haibo Chen*

Main category: cs.AI

TL;DR: 该论文提出并实现了一种AI驱动的自主实验室平台，通过全自动化实验流程和多用户支持，能够自主完成复杂的生命科学实验，实验效果媲美人类专家，大幅提升效率，为科学研究自动化和服务化提供了新方向。


<details>
  <summary>Details</summary>
Motivation: 实现自主的科学研究是长久以来的目标，而目前的自主实验系统还局限于目标单一且实验流程简单的领域，无法满足复杂科学实验的需求。需要通过人工智能推动实验室智能化，支持更复杂和多目标的实验自动化。

Method: 提出并实现了一个AI原生的自主实验室平台，基于模型—实验—仪器协同设计理念，实现对仪器的自动管理、实验流程的自动生成与优化，并支持多用户并发请求。系统能够支持AI模型和自动化系统协同进化，涵盖从实验设计到运行全过程的智能化。

Result: 该平台能够自主完成包括合成、转录、扩增和测序在内的基础核酸功能实验，并应用于疾病诊断、药物开发、信息存储等领域。在无需人为干预下，实验性能达到了人类科学家的先进水平。在多用户场景下，仪器利用率和实验效率显著提升。

Conclusion: AI原生自主实验室为突破专家依赖和资源障碍、实现大规模科学即服务提供了新范式和蓝图，推动生物材料等复杂领域的研究自动化。

Abstract: Autonomous scientific research, capable of independently conducting complex
experiments and serving non-specialists, represents a long-held aspiration.
Achieving it requires a fundamental paradigm shift driven by artificial
intelligence (AI). While autonomous experimental systems are emerging, they
remain confined to areas featuring singular objectives and well-defined, simple
experimental workflows, such as chemical synthesis and catalysis. We present an
AI-native autonomous laboratory, targeting highly complex scientific
experiments for applications like autonomous biomolecular engineering. This
system autonomously manages instrumentation, formulates experiment-specific
procedures and optimization heuristics, and concurrently serves multiple user
requests. Founded on a co-design philosophy of models, experiments, and
instruments, the platform supports the co-evolution of AI models and the
automation system. This establishes an end-to-end, multi-user autonomous
laboratory that handles complex, multi-objective experiments across diverse
instrumentation. Our autonomous laboratory supports fundamental nucleic acid
functions-including synthesis, transcription, amplification, and sequencing. It
also enables applications in fields such as disease diagnostics, drug
development, and information storage. Without human intervention, it
autonomously optimizes experimental performance to match state-of-the-art
results achieved by human scientists. In multi-user scenarios, the platform
significantly improves instrument utilization and experimental efficiency. This
platform paves the way for advanced biomaterials research to overcome
dependencies on experts and resource barriers, establishing a blueprint for
science-as-a-service at scale.

</details>


### [15] [The Gauss-Markov Adjunction: Categorical Semantics of Residuals in Supervised Learning](https://arxiv.org/abs/2507.02442)
*Moto Kamiura*

Main category: cs.AI

TL;DR: 本研究用范畴论方法对多元线性回归进行结构建模，提出“高斯-马尔可夫伴随”以形式化表达参数与残差间的关系，为AI可解释性提供新的理论与语义基础。


<details>
  <summary>Details</summary>
Motivation: 提升机器学习的可解释性与可理解性，以响应AI可解释性原则要求及促进AI在社会中的更好应用。当前主流方法在结构性与语义解释方面仍有限，需要新的理论框架。

Method: 采用范畴论视角重构监督学习中的线性回归模型。通过定义参数与数据这两类具体范畴，以及它们之间的伴随函子关系，明确参数与残差的结构性相互作用。提出“高斯-马尔可夫伴随”结构，并用范畴论工具描述参数与残差之间的信息流。

Result: 提出了通过“高斯-马尔可夫伴随”对子，形式化参数与残差之间的对应关系。证明了最小二乘估计量和最小残差通过右伴随函子极限的保持性相关联。把该方法归类为监督学习的扩展指称语义基础，可以为AI的可解释性提供形式化理论支撑。

Conclusion: 通过范畴论和语义方法，对监督学习中的参数与残差关系进行形式化建模，揭示了其背后的结构性联系，并为提升AI模型透明度与可解释性提供了通用语义框架。

Abstract: Enhancing the intelligibility and interpretability of machine learning is a
crucial task in responding to the demand for Explicability as an AI principle,
and in promoting the better social implementation of AI. The aim of our
research is to contribute to this improvement by reformulating machine learning
models through the lens of category theory, thereby developing a semantic
framework for structuring and understanding AI systems. Our categorical
modeling in this paper clarifies and formalizes the structural interplay
between residuals and parameters in supervised learning. The present paper
focuses on the multiple linear regression model, which represents the most
basic form of supervised learning. By defining two concrete categories
corresponding to parameters and data, along with an adjoint pair of functors
between them, we introduce our categorical formulation of supervised learning.
We show that the essential structure of this framework is captured by what we
call the Gauss-Markov Adjunction. Within this setting, the dual flow of
information can be explicitly described as a correspondence between variations
in parameters and residuals. The ordinary least squares estimator for the
parameters and the minimum residual are related via the preservation of limits
by the right adjoint functor. Furthermore, we position this formulation as an
instance of extended denotational semantics for supervised learning, and
propose applying a semantic perspective developed in theoretical computer
science as a formal foundation for Explicability in AI.

</details>


### [16] [Clarifying Before Reasoning: A Coq Prover with Structural Context](https://arxiv.org/abs/2507.02541)
*Yanzhen Lu,Hanbin Yang,Xiaodie Wang,Ge Zhang,Biao Li,Chenxu Fu,Chao Li,Yang Yuan,Andrew Chi-Chih Yao*

Main category: cs.AI

TL;DR: 通过为大语言模型引入结构化、语义明确的任务描述，显著提升了其在Coq定理证明任务中的推理表现，并刷新了相关领域的最新性能记录。


<details>
  <summary>Details</summary>
Motivation: 探索是否通过提升任务表述的清晰度，可以增强大语言模型（LLM）在定理证明等复杂推理任务上的能力。

Method: 提出了一种基于概念的任务清晰度指标，并针对定理证明任务（Coq系统）引入结构化语义上下文，通过选择性概念展开和“规划者-执行者”架构来丰富任务描述和推理流程，并对LLMs进行零样本和微调实验。

Result: 结构化输入将任务清晰度从44.5%提升到82.3%；在使用DeepSeek-V3模型时，定理证明成功率从21.8%提升到45.8%，超过Graph2Tac的33.2%；小型模型经结构化数据微调后，成功率可达48.6%。

Conclusion: 结构化任务表示能显著提升大模型在推理密集型任务中的表现，是连接理解与推理能力之间的重要桥梁。

Abstract: In this work, we investigate whether improving task clarity can enhance
reasoning ability of large language models, focusing on theorem proving in Coq.
We introduce a concept-level metric to evaluate task clarity and show that
adding structured semantic context to the standard input used by modern LLMs,
leads to a 1.85$\times$ improvement in clarity score
(44.5\%~$\rightarrow$~82.3\%). Using the general-purpose model
\texttt{DeepSeek-V3}, our approach leads to a 2.1$\times$ improvement in proof
success (21.8\%~$\rightarrow$~45.8\%) and outperforms the previous
state-of-the-art \texttt{Graph2Tac} (33.2\%). We evaluate this on 1,386
theorems randomly sampled from 15 standard Coq packages, following the same
evaluation protocol as \texttt{Graph2Tac}. Furthermore, fine-tuning smaller
models on our structured data can achieve even higher performance (48.6\%). Our
method uses selective concept unfolding to enrich task descriptions, and
employs a Planner--Executor architecture. These findings highlight the value of
structured task representations in bridging the gap between understanding and
reasoning.

</details>


### [17] [AI Research Agents for Machine Learning: Search, Exploration, and Generalization in MLE-bench](https://arxiv.org/abs/2507.02554)
*Edan Toledo,Karen Hambardzumyan,Martin Josifoski,Rishi Hazra,Nicolas Baldwin,Alexis Audran-Reiss,Michael Kuchnik,Despoina Magka,Minqi Jiang,Alisia Maria Lupidi,Andrei Lupu,Roberta Raileanu,Kelvin Niu,Tatiana Shavrina,Jean-Christophe Gagnon-Audet,Michael Shvartsman,Shagun Sodhani,Alexander H. Miller,Abhishek Charnalia,Derek Dunfield,Carole-Jean Wu,Pontus Stenetorp,Nicola Cancedda,Jakob Nicolaus Foerster,Yoram Bachrach*

Main category: cs.AI

TL;DR: 本文将AI代理 formalize 成搜索和算子的联合优化问题，系统评估了不同策略组合对Kaggle竞赛任务表现的影响，实现了SOTA，推进了AutoML研究。


<details>
  <summary>Details</summary>
Motivation: 当前AI研究代理在加速自动化科学进展中非常有前景，但如何系统提升其解决现实机器学习任务（如Kaggle竞赛）的能力尚未充分研究。

Method: 作者将AI研究代理形式化为搜索策略的问题，比较了不同的搜索策略（如贪婪搜索、蒙特卡罗树搜索、进化算法）与算子集合的组合，通过系统实验分析其性能差异。

Result: 作者提出的方法在MLE-bench lite基准上取得了最优结果，将Kaggle奖牌达成率从39.6%提升到47.7%。

Conclusion: 研究强调了在自动化机器学习中，搜索策略、算子设计和评估方法需要联合优化，三者协同作用对于提升AI代理表现至关重要。

Abstract: AI research agents are demonstrating great potential to accelerate scientific
progress by automating the design, implementation, and training of machine
learning models. We focus on methods for improving agents' performance on
MLE-bench, a challenging benchmark where agents compete in Kaggle competitions
to solve real-world machine learning problems. We formalize AI research agents
as search policies that navigate a space of candidate solutions, iteratively
modifying them using operators. By designing and systematically varying
different operator sets and search policies (Greedy, MCTS, Evolutionary), we
show that their interplay is critical for achieving high performance. Our best
pairing of search strategy and operator set achieves a state-of-the-art result
on MLE-bench lite, increasing the success rate of achieving a Kaggle medal from
39.6% to 47.7%. Our investigation underscores the importance of jointly
considering the search strategy, operator design, and evaluation methodology in
advancing automated machine learning.

</details>


### [18] [Responsibility Gap and Diffusion in Sequential Decision-Making Mechanisms](https://arxiv.org/abs/2507.02582)
*Junli Jiang,Pavel Naumov*

Main category: cs.AI

TL;DR: 作者探讨了集体决策中责任定义的两种复杂性，揭示了相关机制的判定问题难度，对AI系统责任划分有理论启示。


<details>
  <summary>Details</summary>
Motivation: 责任问题在法律和哲学中被广泛研究，随着人工智能的发展，如何在集体决策过程中界定与计算责任成为新的研究重点。理解责任的扩散(diffusion)和间隙(gap)属性对于AI系统的设计和评估至关重要。

Method: 作者通过理论分析，研究了集体决策机制中与责任相关的两个复杂性属性——扩散与间隙，并考察了无扩散与无间隙机制的复杂性，从计算复杂性角度进行了形式化刻画。

Result: 作者证明了：无扩散决策机制的判定问题是Π₂-完全的，无间隙机制为Π₃-完全，两者交集为Π₂-完全。

Conclusion: 集体决策中责任属性的判定问题存在高度计算复杂性，为AI责任归属与评估增加了理论挑战，对AI决策系统的实际设计有重要影响。

Abstract: Responsibility has long been a subject of study in law and philosophy. More
recently, it became a focus of AI literature. The article investigates the
computational complexity of two important properties of responsibility in
collective decision-making: diffusion and gap. It shows that the sets of
diffusion-free and gap-free decision-making mechanisms are $\Pi_2$-complete and
$\Pi_3$-complete, respectively. At the same time, the intersection of these
classes is $\Pi_2$-complete.

</details>


### [19] [DynamiCare: A Dynamic Multi-Agent Framework for Interactive and Open-Ended Medical Decision-Making](https://arxiv.org/abs/2507.02616)
*Tianqi Shang,Weiqing He,Charles Zheng,Lingyao Li,Li Shen,Bingxin Zhao*

Main category: cs.AI

TL;DR: 本文提出了基于真实电子健康数据构建的MIMIC-Patient和支持多轮交互、动态决策的DynamiCare框架，开创性地建立了医疗大模型动态决策的新基准，贴近实际临床诊断流程。


<details>
  <summary>Details</summary>
Motivation: 传统的医疗AI代理主要集中于单轮任务，即医生在诊断时一次性获得全部病例信息，而现实中的诊断过程实际上具有不确定性、交互性和迭代性。现有模拟框架与真实临床流程存在较大差距。

Method: 作者构建了MIMIC-Patient数据集，从MIMIC-III电子健康记录中结构化抽取信息，支持动态、以患者为中心的模拟。同时提出了DynamiCare动态多代理框架，通过多个专家代理与病人系统多轮交互、整合新信息、动态调整团队结构与策略，模拟真实临床多回合决策过程。

Result: 通过一系列实验，作者验证了DynamiCare框架的可行性与有效性，并建立了第一个基于大语言模型支持代理的动态临床决策基准。

Conclusion: DynamiCare和MIMIC-Patient为医疗AI领域引入了符合真实流程的多轮动态诊断模拟，为基于大模型的临床决策提供了新工具和评测标准，有助于推动医疗AI从静态走向动态、实用。

Abstract: The rise of Large Language Models (LLMs) has enabled the development of
specialized AI agents with domain-specific reasoning and interaction
capabilities, particularly in healthcare. While recent frameworks simulate
medical decision-making, they largely focus on single-turn tasks where a doctor
agent receives full case information upfront -- diverging from the real-world
diagnostic process, which is inherently uncertain, interactive, and iterative.
In this paper, we introduce MIMIC-Patient, a structured dataset built from the
MIMIC-III electronic health records (EHRs), designed to support dynamic,
patient-level simulations. Building on this, we propose DynamiCare, a novel
dynamic multi-agent framework that models clinical diagnosis as a multi-round,
interactive loop, where a team of specialist agents iteratively queries the
patient system, integrates new information, and dynamically adapts its
composition and strategy. We demonstrate the feasibility and effectiveness of
DynamiCare through extensive experiments, establishing the first benchmark for
dynamic clinical decision-making with LLM-powered agents.

</details>


### [20] [Strategic Intelligence in Large Language Models: Evidence from evolutionary Game Theory](https://arxiv.org/abs/2507.02618)
*Kenneth Payne,Baptiste Alloui-Cros*

Main category: cs.AI

TL;DR: 本文通过演化囚徒困境实验证明，主流大语言模型在复杂对抗环境下展现出高度竞争和多样战略决策能力，并能基于策略推理进行行为调整，丰富了我们对AI战略智能的理解。


<details>
  <summary>Details</summary>
Motivation: 探讨大语言模型（LLMs）是否具备战略智能，尤其是在竞争性环境下的目标推理能力，目前尚无系统性证据验证。

Method: 通过在演化型重复囚徒困境（IPD）锦标赛中，将传统策略（如以牙还牙、冷酷触发器）与OpenAI、Google、Anthropic领军AI模型对抗，且通过调整比赛中的终止概率（即“未来的阴影”）来引入复杂性和随机性，排除记忆化因素。同时分析模型生成的近32000条文本推理。

Result: LLMs在复杂生态中具有极强的竞争力，能够生存甚至扩张。不同模型展现出独特且稳定的“战略指纹”：Google的Gemini策略凶狠，擅于剥削合作者和报复背叛者；OpenAI模型高度合作，在敌对环境中表现不佳；Anthropic Claude最为宽容，善于恢复合作关系。此外，模型对局势和对手策略的推理对其决策至关重要。

Conclusion: LLMs具备复杂的战略推理和决策能力，能够在充满不确定性的博弈环境中表现出独特行为模式，将经典博弈论与“机器心理学”相结合，为算法决策行为研究提供了新视角。

Abstract: Are Large Language Models (LLMs) a new form of strategic intelligence, able
to reason about goals in competitive settings? We present compelling supporting
evidence. The Iterated Prisoner's Dilemma (IPD) has long served as a model for
studying decision-making. We conduct the first ever series of evolutionary IPD
tournaments, pitting canonical strategies (e.g., Tit-for-Tat, Grim Trigger)
against agents from the leading frontier AI companies OpenAI, Google, and
Anthropic. By varying the termination probability in each tournament (the
"shadow of the future"), we introduce complexity and chance, confounding
memorisation.
  Our results show that LLMs are highly competitive, consistently surviving and
sometimes even proliferating in these complex ecosystems. Furthermore, they
exhibit distinctive and persistent "strategic fingerprints": Google's Gemini
models proved strategically ruthless, exploiting cooperative opponents and
retaliating against defectors, while OpenAI's models remained highly
cooperative, a trait that proved catastrophic in hostile environments.
Anthropic's Claude emerged as the most forgiving reciprocator, showing
remarkable willingness to restore cooperation even after being exploited or
successfully defecting. Analysis of nearly 32,000 prose rationales provided by
the models reveals that they actively reason about both the time horizon and
their opponent's likely strategy, and we demonstrate that this reasoning is
instrumental to their decisions. This work connects classic game theory with
machine psychology, offering a rich and granular view of algorithmic
decision-making under uncertainty.

</details>


### [21] [Decoupled Planning and Execution: A Hierarchical Reasoning Framework for Deep Search](https://arxiv.org/abs/2507.02652)
*Jiajie Jin,Xiaoxi Li,Guanting Dong,Yuyao Zhang,Yutao Zhu,Yang Zhao,Hongjin Qian,Zhicheng Dou*

Main category: cs.AI

TL;DR: 本文提出了HiRA分层框架，将高层次任务规划与子任务执行解耦，利用领域专长代理协作，显著提升了复杂搜索任务的答案质量和效率，性能优于现有RAG和代理系统。


<details>
  <summary>Details</summary>
Motivation: 当前真实世界中的复杂信息需求需要对多样化信息源进行深度推理和知识整合，但传统的检索增强生成（RAG）方法和现有的推理系统在高效处理这些复杂任务时存在效率低、可扩展性差的问题。主要原因在于，现有方法通常用单一模型同时负责高层次规划与具体执行，导致推理受到限制。

Method: 提出了一种名为HiRA的分层框架，将战略性高层规划与专业化执行分离。具体做法是将复杂的搜索任务分解为聚焦的子任务，每个子任务由配备了外部工具和推理能力的领域特定代理负责，并通过结构化集成机制来协调和整合各代理结果，从而实现高效的信息处理。

Result: HiRA在四个复杂、跨模态的深度搜索基准数据集上进行了实验，显著优于当前最先进的RAG和基于代理的系统。其在答案质量和系统效率方面都取得了提升，验证了规划与执行解耦对于多步信息检索任务的有效性。

Conclusion: 分层的规划与执行解耦框架HiRA能够更有效地处理复杂的多步深度信息检索问题，在准确性和效率上均超越了现有方法，表明这种新范式对于复杂搜索任务具有实际应用前景。

Abstract: Complex information needs in real-world search scenarios demand deep
reasoning and knowledge synthesis across diverse sources, which traditional
retrieval-augmented generation (RAG) pipelines struggle to address effectively.
Current reasoning-based approaches suffer from a fundamental limitation: they
use a single model to handle both high-level planning and detailed execution,
leading to inefficient reasoning and limited scalability. In this paper, we
introduce HiRA, a hierarchical framework that separates strategic planning from
specialized execution. Our approach decomposes complex search tasks into
focused subtasks, assigns each subtask to domain-specific agents equipped with
external tools and reasoning capabilities, and coordinates the results through
a structured integration mechanism. This separation prevents execution details
from disrupting high-level reasoning while enabling the system to leverage
specialized expertise for different types of information processing.
Experiments on four complex, cross-modal deep search benchmarks demonstrate
that HiRA significantly outperforms state-of-the-art RAG and agent-based
systems. Our results show improvements in both answer quality and system
efficiency, highlighting the effectiveness of decoupled planning and execution
for multi-step information seeking tasks. Our code is available at
https://github.com/ignorejjj/HiRA.

</details>


### [22] [Hey AI, Generate Me a Hardware Code! Agentic AI-based Hardware Design & Verification](https://arxiv.org/abs/2507.02660)
*Deepak Narayan Gadde,Keerthan Kopparam Radhakrishna,Vaisakh Naduvodi Viswambharan,Aman Kumar,Djones Lettnin,Wolfgang Kunz,Sebastian Simon*

Main category: cs.AI

TL;DR: 本文将大语言模型与人类参与相结合，提出一种新颖的硬件验证AI方法，在多项评测中实现高覆盖率和效率提升。


<details>
  <summary>Details</summary>
Motivation: 目前集成电路（IC）的复杂度快速提升，硬件设计验证过程变得极为繁琐且耗时，需要高效且可靠的新方法以确保设计无误并缩短开发周期。

Method: 本文提出一种基于Agent的AI方法，引入大语言模型（LLM）和生成式AI（GenAI），结合“人类在环”（HITL）机制，使AI智能体能够以更动态、迭代和自省的方式进行端到端硬件设计与验证。

Result: 在五个开源硬件设计上验证了该方法，实现了超过95%的覆盖率，缩短了验证时间，且在性能、适应性和可配置性方面表现优异。

Conclusion: 基于Agent的AI方法结合人类协作能够显著提升硬件设计验证的效率和效果，具有较好的应用前景。

Abstract: Modern Integrated Circuits (ICs) are becoming increasingly complex, and so is
their development process. Hardware design verification entails a methodical
and disciplined approach to the planning, development, execution, and sign-off
of functionally correct hardware designs. This tedious process requires
significant effort and time to ensure a bug-free tape-out. The field of Natural
Language Processing has undergone a significant transformation with the advent
of Large Language Models (LLMs). These powerful models, often referred to as
Generative AI (GenAI), have revolutionized how machines understand and generate
human language, enabling unprecedented advancements in a wide array of
applications, including hardware design verification. This paper presents an
agentic AI-based approach to hardware design verification, which empowers AI
agents, in collaboration with Humain-in-the-Loop (HITL) intervention, to engage
in a more dynamic, iterative, and self-reflective process, ultimately
performing end-to-end hardware design and verification. This methodology is
evaluated on five open-source designs, achieving over 95% coverage with reduced
verification time while demonstrating superior performance, adaptability, and
configurability.

</details>


### [23] [Think How to Think: Mitigating Overthinking with Autonomous Difficulty Cognition in Large Reasoning Models](https://arxiv.org/abs/2507.02663)
*Yongjiang Liu,Haoxi Li,Xiaosong Ma,Jie Zhang,Song Guo*

Main category: cs.AI

TL;DR: 本文提出两阶段微调方法TH2T，提升大模型对任务难度与冗余的自适应能力，大幅降低推理冗余和计算成本，输出更简洁高效。


<details>
  <summary>Details</summary>
Motivation: 近年来，大型长推理模型（LRMs）在复杂推理任务中表现突出，但存在明显的“过度思考”问题。研究动机是深入分析LRMs为何过度推理，并提出方法缓解该现象。

Method: 提出了一种名为TH2T（Think-How-to-Think）的两阶段微调策略。第一阶段通过在输出前缀注入“难度认知”提示，引导模型识别任务难度，实现针对不同难度任务的差异化推理。第二阶段在推理过程中引入“冗余认知”提示，帮助模型识别并去除推理中的多余步骤，生成更精炼的推理结果。实验在7B/14B/32B规模的模型上进行。

Result: TH2T显著减少了推理成本（在简单任务上降低70%以上，复杂任务上降低40%），同时保持模型性能稳定。模型输出变得更能感知任务难度，且冗余现象明显减少。

Conclusion: 通过引导模型认知任务难度和推理冗余，TH2T提升了推理效率和输出质量，为缓解LRMs的过度思考问题提供了可行路径。

Abstract: Recent Long Reasoning Models(LRMs) have demonstrated remarkable capabilities
in handling complex reasoning tasks, but are hindered by excessive
overthinking. To explore its essence, our empirical analysis reveals that LRMs
are primarily limited to recognizing task properties (i.e., difficulty levels)
like humans before solving the problem, leading to a one-size-fits-all
reasoning process. Inspired by this, a pressing and natural question emerges:
Can we bootstrap such ability to further alleviate the overthinking phenomenon
in LRMs? In this paper, we propose Think-How-to-Think (TH2T), a novel two-stage
fine-tuning strategy that progressively inspires LRMs' difficulty cognition and
redundancy cognition. First, we introduce difficulty-hypnosis in the prefixes
of model outputs to intervene in the internal reasoning trajectory. Combined
with a heterogeneous short and long reasoning dataset, the trained model
enhances its sensitivity to task difficulty, enabling native, differentiated
reasoning strategies across various tasks. Second, we further extend
redundancy-hypnosis to the internal reasoning process, guiding the model to
identify redundant structures within the reasoning steps and generate more
concise reasoning outputs. Experiments on 7B/14B/32B models demonstrate that
TH2T significantly reduces inference costs (more than 70% on easy tasks and 40%
on hard tasks) while maintaining performance stability. The resulting outputs
exhibit clear difficulty-aware capabilities and reduced redundancy (e.g.,
reflection).

</details>


### [24] [Detection of Disengagement from Voluntary Quizzes: An Explainable Machine Learning Approach in Higher Distance Education](https://arxiv.org/abs/2507.02681)
*Behnam Parsaeifard,Christof Imhof,Tansu Pancar,Ioan-Sorin Comsa,Martin Hlosta,Nicole Bergamin,Per Bergamin*

Main category: cs.AI

TL;DR: 本研究通过分析在线课程中非强制性测验参与情况，应用多种机器学习算法和可解释性技术，实现了高准确率地识别学生脱离，为推动远程教育中的及时干预提供了数据和方法支持。


<details>
  <summary>Details</summary>
Motivation: 远程教育中，学生的任务脱离会带来严重后果，如学业辍学，尤其在远程教育环境更为突出。因此，需要有效检测学生脱离程度以便及时干预。

Method: 分析42门课程四学期内的非强制性测验数据，提取Moodle日志中最具信息量的数据特征，训练并比较8种机器学习算法，采用SHAP方法建立可解释机器学习框架。

Result: 最优模型的平衡准确率达到91%，约85%的脱离学生被准确检测出来。此外讨论了如何利用结果设计及时干预方案降低学生脱离。

Conclusion: 基于数据驱动和解释性强的机器学习方法能够高效准确地检测在线学习中学生的脱离现象，为后续及时干预提供了数据支持，具备实际应用价值。

Abstract: Students disengaging from their tasks can have serious long-term
consequences, including academic drop-out. This is particularly relevant for
students in distance education. One way to measure the level of disengagement
in distance education is to observe participation in non-mandatory exercises in
different online courses. In this paper, we detect student disengagement in the
non-mandatory quizzes of 42 courses in four semesters from a distance-based
university. We carefully identified the most informative student log data that
could be extracted and processed from Moodle. Then, eight machine learning
algorithms were trained and compared to obtain the highest possible prediction
accuracy. Using the SHAP method, we developed an explainable machine learning
framework that allows practitioners to better understand the decisions of the
trained algorithm. The experimental results show a balanced accuracy of 91\%,
where about 85\% of disengaged students were correctly detected. On top of the
highly predictive performance and explainable framework, we provide a
discussion on how to design a timely intervention to minimise disengagement
from voluntary tasks in online learning.

</details>


### [25] [Time-critical and confidence-based abstraction dropping methods](https://arxiv.org/abs/2507.02703)
*Robin Schmöcker,Lennart Kampmann,Alexander Dockhorn*

Main category: cs.AI

TL;DR: 本文提出两种安全的MCTS抽象丢弃机制（OGA-IAAD和OGA-CAD），显著提升了MCTS性能且无明显性能劣化，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 改进蒙特卡洛树搜索（MCTS）的一种范式是在搜索过程中构建并使用状态和/或动作抽象。然而，非精确抽象会引入近似误差，使得无法在抽象空间中收敛到最优动作。因此，需要在适当的时候放弃抽象，以保证性能。

Method: 提出了两种新的抽象丢弃机制：OGA-IAAD和OGA-CAD。OGA-IAAD适用于对时间要求严格的场景，OGA-CAD则用于在迭代次数相同的情况下提升MCTS性能。这两种方法旨在安全地丢弃抽象，避免性能显著下降。

Result: 与之前的Xu等人的方法相比，OGA-IAAD和OGA-CAD在丢弃抽象时不会造成明显的性能下降，并且能够带来显著的性能提升。

Conclusion: 新提出的OGA-IAAD和OGA-CAD抽象丢弃机制能够在保证安全的情况下，提升MCTS的性能，且优于现有的抽象丢弃方法。

Abstract: One paradigm of Monte Carlo Tree Search (MCTS) improvements is to build and
use state and/or action abstractions during the tree search. Non-exact
abstractions, however, introduce an approximation error making convergence to
the optimal action in the abstract space impossible. Hence, as proposed as a
component of Elastic Monte Carlo Tree Search by Xu et al., abstraction
algorithms should eventually drop the abstraction. In this paper, we propose
two novel abstraction dropping schemes, namely OGA-IAAD and OGA-CAD which can
yield clear performance improvements whilst being safe in the sense that the
dropping never causes any notable performance degradations contrary to Xu's
dropping method. OGA-IAAD is designed for time critical settings while OGA-CAD
is designed to improve the MCTS performance with the same number of iterations.

</details>


### [26] [Bourbaki: Self-Generated and Goal-Conditioned MDPs for Theorem Proving](https://arxiv.org/abs/2507.02726)
*Matthieu Zimmer,Xiaotong Ji,Rasul Tutunov,Anthony Bordg,Jun Wang,Haitham Bou Ammar*

Main category: cs.AI

TL;DR: 作者提出通过自生成子目标和马尔可夫决策过程框架，结合树搜索算法，将大型语言模型用于自动定理证明任务，在PutnamBench上刷新了同规模SOTA记录。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型(LLMs)在自动定理证明(ATP)的逻辑推理任务中表现不佳，主要受稀疏奖励和证明规模巨大的影响，特别是在如PutnamBench等需要复杂多步推理的高难度基准下，这些问题更为突出。

Method: 提出了自生成目标条件马尔可夫决策过程（sG-MDPs）框架，允许智能体根据推理动态生成并追求子目标，使问题结构化，更容易进行搜索。然后结合MCTS类算法求解该sG-MDP。在具体实践中，将其集成到Bourbaki（7B）系统，通过多个7B参数LLM组成模块化系统，分别负责子目标生成和策略合成。

Result: 在PutnamBench基准上，Bourbaki（7B）系统解决了26个问题，在同等规模模型中取得了新的SOTA（最先进）结果。

Conclusion: 自生成目标条件MDP和模块化7B参数LLM系统能提升LLM在自动定理证明难题中的推理能力，显著提升了当前规模下的模型表现。

Abstract: Reasoning remains a challenging task for large language models (LLMs),
especially within the logically constrained environment of automated theorem
proving (ATP), due to sparse rewards and the vast scale of proofs. These
challenges are amplified in benchmarks like PutnamBench, which contains
university-level problems requiring complex, multi-step reasoning. To address
this, we introduce self-generated goal-conditioned MDPs (sG-MDPs), a new
framework in which agents generate and pursue their subgoals based on the
evolving proof state. Given this more structured generation of goals, the
resulting problem becomes more amenable to search. We then apply Monte Carlo
Tree Search (MCTS)-like algorithms to solve the sG-MDP, instantiating our
approach in Bourbaki (7B), a modular system that can ensemble multiple 7B LLMs
for subgoal generation and tactic synthesis. On PutnamBench, Bourbaki (7B)
solves 26 problems, achieving new state-of-the-art results with models at this
scale.

</details>


### [27] [Knowledge Protocol Engineering: A New Paradigm for AI in Domain-Specific Knowledge Work](https://arxiv.org/abs/2507.02760)
*Guangwei Zhang*

Main category: cs.AI

TL;DR: 本文提出了“知识协议工程”（KPE），系统地将专家知识转化为AI可执行协议，使大模型具备专家级推理和任务分解能力，并展示其在多个专业领域的潜在应用价值。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型（LLMs）和现有的RAG、Agent AI等方法推动了AI与复杂领域知识的互动，但它们在需深度流程化和专业方法推理的任务上往往表现不佳。现有方法要么仅提供事实上下文而缺乏逻辑框架，要么缺乏高效的领域特定启发式。因此亟需让AI系统具备专家级领域推理和操作能力。

Method: 作者提出了知识协议工程（Knowledge Protocol Engineering, KPE）新范式，系统性地将人类专家用自然语言表达的知识转化为机器可执行的知识协议（Knowledge Protocol, KP）。KPE关注将领域固有的逻辑、操作策略及方法传递给大语言模型。文中定义了KPE的核心原则，并与现有方法做区分。

Result: KPE使通用大模型能够作为领域专家进行抽象问题分解和多步骤复杂任务执行，超越现有RAG和Agent方案的局限。文中还论证了KPE在法律、生物信息等领域的广阔适用性。

Conclusion: 知识协议工程可作为未来人机协作的基础方法学，为AI提供领域内专家级逻辑与操作能力，显著提升大语言模型在专业任务上的表现。

Abstract: The capabilities of Large Language Models (LLMs) have opened new frontiers
for interacting with complex, domain-specific knowledge. However, prevailing
methods like Retrieval-Augmented Generation (RAG) and general-purpose Agentic
AI, while powerful, often struggle with tasks that demand deep, procedural, and
methodological reasoning inherent to expert domains. RAG provides factual
context but fails to convey logical frameworks; autonomous agents can be
inefficient and unpredictable without domain-specific heuristics. To bridge
this gap, we introduce Knowledge Protocol Engineering (KPE), a new paradigm
focused on systematically translating human expert knowledge, often expressed
in natural language documents, into a machine-executable Knowledge Protocol
(KP). KPE shifts the focus from merely augmenting LLMs with fragmented
information to endowing them with a domain's intrinsic logic, operational
strategies, and methodological principles. We argue that a well-engineered
Knowledge Protocol allows a generalist LLM to function as a specialist, capable
of decomposing abstract queries and executing complex, multi-step tasks. This
position paper defines the core principles of KPE, differentiates it from
related concepts, and illustrates its potential applicability across diverse
fields such as law and bioinformatics, positing it as a foundational
methodology for the future of human-AI collaboration.

</details>


### [28] [Grounding Intelligence in Movement](https://arxiv.org/abs/2507.02771)
*Melanie Segado,Felipe Parodi,Jordan K. Matelsky,Michael L. Platt,Eva B. Dyer,Konrad P. Kording*

Main category: cs.AI

TL;DR: 文章呼吁把动作作为AI建模核心，认为动作结构天然具可解释性和通用性，将推动AI理解和泛化能力提升，并有助于构建跨领域共享的行为建模基础。


<details>
  <summary>Details</summary>
Motivation: 现有的机器学习在语言和视觉等高维数据建模上进步显著，但在处理生物系统最基本特征之一——动作（movement）方面表现有限。动作对于理解行为、预测意图和实现互动至关重要，但却常被忽视或仅作为附加特征处理。

Method: 文章主要是观点论文（Position paper），通过梳理领域现状和提出理论观点，主张动作应作为AI建模的主要对象。强调动作结构的可解释性、低维表示优势，并讨论动作数据跨领域建模的意义。

Result: 通过分析，作者提出应把动作作为AI建模的核心目标，这不仅能促进生成建模和控制等能力发展，还为跨生物与人工系统的行为理解奠定共同基础。

Conclusion: 动作不只是结果，更是智能系统与世界互动的重要窗口，有望提升AI的泛化能力和解释性，实现跨领域、跨物种行为理解。

Abstract: Recent advances in machine learning have dramatically improved our ability to
model language, vision, and other high-dimensional data, yet they continue to
struggle with one of the most fundamental aspects of biological systems:
movement. Across neuroscience, medicine, robotics, and ethology, movement is
essential for interpreting behavior, predicting intent, and enabling
interaction. Despite its core significance in our intelligence, movement is
often treated as an afterthought rather than as a rich and structured modality
in its own right. This reflects a deeper fragmentation in how movement data is
collected and modeled, often constrained by task-specific goals and
domain-specific assumptions. But movement is not domain-bound. It reflects
shared physical constraints, conserved morphological structures, and purposeful
dynamics that cut across species and settings. We argue that movement should be
treated as a primary modeling target for AI. It is inherently structured and
grounded in embodiment and physics. This structure, often allowing for compact,
lower-dimensional representations (e.g., pose), makes it more interpretable and
computationally tractable to model than raw, high-dimensional sensory inputs.
Developing models that can learn from and generalize across diverse movement
data will not only advance core capabilities in generative modeling and
control, but also create a shared foundation for understanding behavior across
biological and artificial systems. Movement is not just an outcome, it is a
window into how intelligent systems engage with the world.

</details>


### [29] [KERAP: A Knowledge-Enhanced Reasoning Approach for Accurate Zero-shot Diagnosis Prediction Using Multi-agent LLMs](https://arxiv.org/abs/2507.02773)
*Yuzhang Xie,Hejie Cui,Ziyang Zhang,Jiaying Lu,Kai Shu,Fadi Nahab,Xiao Hu,Carl Yang*

Main category: cs.AI

TL;DR: KERAP通过多智能体与知识图谱融合，显著提升了LLM在医学零样本诊断预测中的可靠性和可解释性，为自动化医疗诊断提供了新途径。


<details>
  <summary>Details</summary>
Motivation: 机器学习（ML）模型在医学诊断预测中应用广泛，但受限于有监督训练方式，对未见病例泛化能力有限，且获取大量标注数据成本较高。大语言模型（LLMs）虽有潜力，但常出现幻觉、缺乏结构化推理、输出无用结果，限制其实际应用价值。

Method: 提出了一种名为KERAP的知识图谱增强推理方法，结合多智能体架构，提升了基于LLM的医学诊断预测能力。具体包含属性映射（linkage agent）、结构化知识检索（retrieval agent）和迭代式预测优化（prediction agent）三大智能体协作。

Result: 实验表明，KERAP方法能有效提升诊断预测的可靠性，并具备可扩展性和可解释性，适用于零样本医学诊断场景。

Conclusion: KERAP为基于LLM的医学诊断预测提供了一种高效、可扩展且可解释的解决方案，有助于提升实际诊断系统的性能，特别适合无标注样本场景。

Abstract: Medical diagnosis prediction plays a critical role in disease detection and
personalized healthcare. While machine learning (ML) models have been widely
adopted for this task, their reliance on supervised training limits their
ability to generalize to unseen cases, particularly given the high cost of
acquiring large, labeled datasets. Large language models (LLMs) have shown
promise in leveraging language abilities and biomedical knowledge for diagnosis
prediction. However, they often suffer from hallucinations, lack structured
medical reasoning, and produce useless outputs. To address these challenges, we
propose KERAP, a knowledge graph (KG)-enhanced reasoning approach that improves
LLM-based diagnosis prediction through a multi-agent architecture. Our
framework consists of a linkage agent for attribute mapping, a retrieval agent
for structured knowledge extraction, and a prediction agent that iteratively
refines diagnosis predictions. Experimental results demonstrate that KERAP
enhances diagnostic reliability efficiently, offering a scalable and
interpretable solution for zero-shot medical diagnosis prediction.

</details>


### [30] [Establishing Best Practices for Building Rigorous Agentic Benchmarks](https://arxiv.org/abs/2507.02825)
*Yuxuan Zhu,Tengjun Jin,Yada Pruksachatkun,Andy Zhang,Shu Liu,Sasha Cui,Sayash Kapoor,Shayne Longpre,Kevin Meng,Rebecca Weiss,Fazl Barez,Rahul Gupta,Jwala Dhamala,Jacob Merizian,Mario Giulianelli,Harry Coppock,Cozmin Ududec,Jasjeet Sekhon,Jacob Steinhardt,Antony Kellerman,Sarah Schwettmann,Matei Zaharia,Ion Stoica,Percy Liang,Daniel Kang*

Main category: cs.AI

TL;DR: 论文指出现有的AI agentic基准存在显著评测缺陷，容易导致AI性能被错误评估。作者提出ABC评估清单，并在复杂基准上验证其效果，显著降低高估风险。


<details>
  <summary>Details</summary>
Motivation: 随着AI能力的增强，现有的agentic基准测试在评估复杂、真实世界任务时存在问题，可能导致模型能力被低估或高估。

Method: 对现有agentic基准进行了梳理，分析了它们在任务设定和奖励设计上的不足。总结以往经验及最佳实践，并提出了Agentic Benchmark Checklist（ABC）评估指南。

Result: 应用ABC于一个复杂基准（CVE-Bench）时，发现可以减少33%的性能高估。此外，举例说明其他基准如SWE-bench Verified和TAU-bench的评估缺陷可导致高达100%的性能误判。

Conclusion: 当前agentic基准测试普遍存在设计缺陷，ABC指南有助于提升评估的严谨性并减小性能误判。

Abstract: Benchmarks are essential for quantitatively tracking progress in AI. As AI
agents become increasingly capable, researchers and practitioners have
introduced agentic benchmarks to evaluate agents on complex, real-world tasks.
These benchmarks typically measure agent capabilities by evaluating task
outcomes via specific reward designs. However, we show that many agentic
benchmarks have issues task setup or reward design. For example, SWE-bench
Verified uses insufficient test cases, while TAU-bench counts empty responses
as successful. Such issues can lead to under- or overestimation agents'
performance by up to 100% in relative terms. To make agentic evaluation
rigorous, we introduce the Agentic Benchmark Checklist (ABC), a set of
guidelines that we synthesized from our benchmark-building experience, a survey
of best practices, and previously reported issues. When applied to CVE-Bench, a
benchmark with a particularly complex evaluation design, ABC reduces the
performance overestimation by 33%.

</details>


### [31] [StepHint: Multi-level Stepwise Hints Enhance Reinforcement Learning to Reason](https://arxiv.org/abs/2507.02841)
*Kaiyi Zhang,Ang Lv,Jinpeng Li,Yongbo Wang,Feng Wang,Haoyuan Hu,Rui Yan*

Main category: cs.AI

TL;DR: 本文提出StepHint方法，通过分步提示提升大语言模型推理能力，显著优于现有RLVR方法，解决了训练效率和探索能力不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习可验证奖励（RLVR）方法用于提升大语言模型（LLM）复杂推理能力，但面临两个主要挑战：一是“小失误惩罚”问题，轻微错误会导致奖励丧失，降低训练效率；二是“探索停滞”，即模型只关注其熟悉的解答范围，缺乏探索潜力更优解决方案的动机。

Method: 提出了一种新颖的RLVR算法——StepHint，通过利用多层次的分步提示来辅助模型更有效地探索解空间。该方法从更强模型中生成有效推理链，并用自适应分割方法将其拆分为多个推理步骤。训练时为模型提供不同粒度的提示，引导其在有前景的子空间内探索，同时保证探索多样性。

Result: StepHint在六个数学基准测试中表现优于现有增强RLVR方法，在泛化能力和跨领域任务中也优于基线方法。

Conclusion: StepHint显著提升了RLVR算法在大语言模型推理训练中的效率和泛化性能，能够有效解决小失误惩罚和探索停滞两大难题。

Abstract: Reinforcement learning with verifiable rewards (RLVR) is a promising approach
for improving the complex reasoning abilities of large language models (LLMs).
However, current RLVR methods face two significant challenges: the near-miss
reward problem, where a small mistake can invalidate an otherwise correct
reasoning process, greatly hindering training efficiency; and exploration
stagnation, where models tend to focus on solutions within their ``comfort
zone,'' lacking the motivation to explore potentially more effective
alternatives. To address these challenges, we propose StepHint, a novel RLVR
algorithm that utilizes multi-level stepwise hints to help models explore the
solution space more effectively. StepHint generates valid reasoning chains from
stronger models and partitions these chains into reasoning steps using our
proposed adaptive partitioning method. The initial few steps are used as hints,
and simultaneously, multiple-level hints (each comprising a different number of
steps) are provided to the model. This approach directs the model's exploration
toward a promising solution subspace while preserving its flexibility for
independent exploration. By providing hints, StepHint mitigates the near-miss
reward problem, thereby improving training efficiency. Additionally, the
external reasoning pathways help the model develop better reasoning abilities,
enabling it to move beyond its ``comfort zone'' and mitigate exploration
stagnation. StepHint outperforms competitive RLVR enhancement methods across
six mathematical benchmarks, while also demonstrating superior generalization
and excelling over baselines on out-of-domain benchmarks.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [32] [McBE: A Multi-task Chinese Bias Evaluation Benchmark for Large Language Models](https://arxiv.org/abs/2507.02088)
*Tian Lan,Xiangdong Su,Xu Liu,Ruirui Wang,Ke Chang,Jiang Li,Guanglai Gao*

Main category: cs.CL

TL;DR: 本文提出了覆盖丰富的中文多任务偏见评测基准McBE，并用其系统评估了主流大语言模型，发现它们在多个角度均存在显著偏见，对模型偏见分析及相关风险缓解提供了关键工具和见解。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）在自然语言处理任务中的广泛应用，其内在的偏见问题逐渐暴露。当前的偏见测评数据集多聚焦于英语和北美文化，不适用于其他文化背景，特别是中文。现有中文相关的数据集稀缺且评测角度单一，难以多方面衡量模型偏见。

Method: 提出了多任务中文偏见评测基准（McBE），包括4,077个评测实例，涵盖12个单一偏见类别、82个子类别，并引入5种评测任务，具有广泛的类别覆盖、内容多样性及评测全面性。同时，评测了不同系列和参数规模的主流大语言模型，并进行了深入分析。

Result: 所有被评测的大语言模型均表现出不同程度的偏见。通过多维度结果分析，获得了大语言模型偏见的新见解。

Conclusion: McBE基准为系统性、多维度的中文偏见检测提供了工具，揭示了主流大语言模型不同程度的偏见，同时为理解和缓解中文环境下AI偏见风险提供了新的见解和参考。

Abstract: As large language models (LLMs) are increasingly applied to various NLP
tasks, their inherent biases are gradually disclosed. Therefore, measuring
biases in LLMs is crucial to mitigate its ethical risks. However, most existing
bias evaluation datasets focus on English and North American culture, and their
bias categories are not fully applicable to other cultures. The datasets
grounded in the Chinese language and culture are scarce. More importantly,
these datasets usually only support single evaluation tasks and cannot evaluate
the bias from multiple aspects in LLMs. To address these issues, we present a
Multi-task Chinese Bias Evaluation Benchmark (McBE) that includes 4,077 bias
evaluation instances, covering 12 single bias categories, 82 subcategories and
introducing 5 evaluation tasks, providing extensive category coverage, content
diversity, and measuring comprehensiveness. Additionally, we evaluate several
popular LLMs from different series and with parameter sizes. In general, all
these LLMs demonstrated varying degrees of bias. We conduct an in-depth
analysis of results, offering novel insights into bias in LLMs.

</details>


### [33] [Reasoning or Not? A Comprehensive Evaluation of Reasoning LLMs for Dialogue Summarization](https://arxiv.org/abs/2507.02145)
*Keyan Jin,Yapeng Wang,Leonel Santos,Tao Fang,Xu Yang,Sio Kei Im,Hugo Gonçalo Oliveira*

Main category: cs.CL

TL;DR: 对话摘要中，链式推理大模型表现未达预期，反易导致啰嗦与失真，呼吁针对该任务的专门优化。


<details>
  <summary>Details</summary>
Motivation: 对话摘要任务应用广泛且具有挑战性，虽然大语言模型（LLMs）在摘要任务上表现优异，但基于 step-by-step reasoning 的结构（如Long Chain-of-Thought，CoT）在需要兼顾抽象和简洁的对话摘要场景下，效果尚未被系统探索。

Method: 对比评测了多种最先进推理型和非推理型LLMs，并在三大摘要范式（通用、角色导向、查询导向）及不同语言、领域、摘要长度上进行系统评估，结合多种标准数据集和先进评测协议，引入自动化指标及人工评价标准。

Result: 明确发现，对话摘要场景下，stepwise推理并未可靠提升摘要质量，反而推理LLMs更易产生冗长、事实不一致、简洁性不足的问题。通过具体场景分析，揭示了推理失败或反作用的具体原因。

Conclusion: 当前推理型LLMs在对话摘要任务中存在局限，需针对实际需求，优化模型设计和评估方法。

Abstract: Dialogue summarization is a challenging task with significant practical value
in customer service, meeting analysis, and conversational AI. Although large
language models (LLMs) have achieved substantial progress in summarization
tasks, the performance of step-by-step reasoning architectures-specifically
Long Chain-of-Thought (CoT) implementations such as OpenAI-o1 and
DeepSeek-R1-remains unexplored for dialogue scenarios requiring concurrent
abstraction and conciseness. In this work, we present the first comprehensive
and systematic evaluation of state-of-the-art reasoning LLMs and non-reasoning
LLMs across three major paradigms-generic, role-oriented, and query-oriented
dialogue summarization. Our study spans diverse languages, domains, and summary
lengths, leveraging strong benchmarks (SAMSum, DialogSum, CSDS, and QMSum) and
advanced evaluation protocols that include both LLM-based automatic metrics and
human-inspired criteria. Contrary to trends in other reasoning-intensive tasks,
our findings show that explicit stepwise reasoning does not consistently
improve dialogue summarization quality. Instead, reasoning LLMs are often prone
to verbosity, factual inconsistencies, and less concise summaries compared to
their non-reasoning counterparts. Through scenario-specific analyses and
detailed case studies, we further identify when and why explicit reasoning may
fail to benefit-or even hinder-summarization in complex dialogue contexts. Our
work provides new insights into the limitations of current reasoning LLMs and
highlights the need for targeted modeling and evaluation strategies for
real-world dialogue summarization.

</details>


### [34] [Latent Chain-of-Thought? Decoding the Depth-Recurrent Transformer](https://arxiv.org/abs/2507.02199)
*Wenquan Lu,Yuechuan Yang,Kyle Lee,Yanshu Li,Enqi Liu*

Main category: cs.CL

TL;DR: 本文评估了Huginn-3.5B递归Transformer模型在算术推理中的表现，结果显示其隐式CoT能力和可解释性有限，提升递归深度效果不明显。相比之下，传统外显CoT方法依然有较大优势。


<details>
  <summary>Details</summary>
Motivation: 目前基于transformer的语言模型虽然擅长于复杂数学和多步推理，但通常采用外显的chain-of-thought（CoT）方式输出推理步骤，提高了可解释性的同时牺牲了效率。有一些研究尝试设计能在隐空间内部完成推理的循环结构模型，以期隐藏CoT，提升效率并捕捉不易用语言表达的推理过程。

Method: 本文研究了一种名为Huginn-3.5B的depth-recurrent Transformer模型，在推理时可复用层而不增加参数量。作者利用多种探针技术（如Logit Lens和Coda Lens）分析模型在算术任务中的内部行为，考察其隐藏CoT能力和隐藏状态的可解释性。

Result: 研究发现，Huginn-3.5B虽有部分隐式CoT表现，但整体可解释性有限，且在不同递归块之间用探针方法解释出的结果存在很大差异，受层级和解码方式影响显著。此外，增加模型的递归深度仅带来了有限性能提升，远不及采用外显CoT的模型。

Conclusion: 深度递归Transformer在强化内部推理结构（隐式CoT）上能力有限，其效率或性能优势不及显式外化推理步骤的架构。当前模型还难以在隐空间高效且可靠地实现复杂多步推理。

Abstract: Chain-of-thought (CoT) reasoning has enabled transformer-based language
models to excel at complex mathematics and multi-step planning. However, in
standard decoder-only architectures, these reasoning steps are externalized in
natural language, improving interpretability at the cost of efficiency. To
capture reasoning that is not easily represented in words, many works have
explored recurrent architectures that aim to internalize reasoning in latent
space, potentially supporting latent CoT. In this paper, we investigate whether
such reasoning structures emerge in Huginn-3.5B, a depth-recurrent Transformer
that reuses layers at inference time without increasing parameter count. We
examine the model's internal behavior on arithmetic tasks using a suite of
probing techniques including the Logit Lens and Coda Lens. Our findings reveal
limited evidence of interpretable latent CoT by tracking rank trajectories of
final and intermediate result tokens. Furthermore, we uncover significant
probing inconsistencies across recurrent blocks, where the interpretability of
hidden states depends heavily on both the layer index and the decoding method.
Finally, we empirically show that increasing recurrence depth yields only
marginal gains and falls well short of models that explicitly externalize
reasoning steps. The code is available at
https://github.com/wenquanlu/huginn-latent-cot.

</details>


### [35] [GDC Cohort Copilot: An AI Copilot for Curating Cohorts from the Genomic Data Commons](https://arxiv.org/abs/2507.02221)
*Steven Song,Anirudh Subramanyam,Zhenyu Zhang,Aarti Venkat,Robert L. Grossman*

Main category: cs.CL

TL;DR: 针对GDC筛选队列难题，提出并开源GDC Cohort Copilot工具，通过自然语言输入自动生成筛选条件，开放源代码和模型，且本地大模型表现优于GPT-4o。


<details>
  <summary>Details</summary>
Motivation: GDC为癌症基因组数据提供了高质量的访问和分析平台，但新用户在大量字段中筛选特定队列描述较困难，然而他们能用自然语言更好描述所需队列。

Method: 开发开源工具GDC Cohort Copilot，利用不同的大语言模型将自然语言队列描述转换为GDC筛选器，并与GPT-4o进行对比评测。

Result: GDC Cohort Copilot实现了从自然语言到GDC队列筛选器的自动生成，并通过本地大模型实现优于GPT-4o的性能。用户可通过交互界面对队列进行进一步调整。

Conclusion: GDC Cohort Copilot提升了用户筛选癌症队列的效率和精确性，本地部署的开源大模型优于GPT-4o，工具及模型均已开源发布。

Abstract: Motivation: The Genomic Data Commons (GDC) provides access to high quality,
harmonized cancer genomics data through a unified curation and analysis
platform centered around patient cohorts. While GDC users can interactively
create complex cohorts through the graphical Cohort Builder, users (especially
new ones) may struggle to find specific cohort descriptors across hundreds of
possible fields and properties. However, users may be better able to describe
their desired cohort in free-text natural language.
  Results: We introduce GDC Cohort Copilot, an open-source copilot tool for
curating cohorts from the GDC. GDC Cohort Copilot automatically generates the
GDC cohort filter corresponding to a user-input natural language description of
their desired cohort, before exporting the cohort back to the GDC for further
analysis. An interactive user interface allows users to further refine the
generated cohort. We develop and evaluate multiple large language models (LLMs)
for GDC Cohort Copilot and demonstrate that our locally-served, open-source GDC
Cohort LLM achieves better results than GPT-4o prompting in generating GDC
cohorts.
  Availability and implementation: The standalone docker image for GDC Cohort
Copilot is available at https://quay.io/repository/cdis/gdc-cohort-copilot.
Source code is available at https://github.com/uc-cdis/gdc-cohort-copilot. GDC
Cohort LLM weights are available at https://huggingface.co/uc-ctds.

</details>


### [36] [MemAgent: Reshaping Long-Context LLM with Multi-Conv RL-based Memory Agent](https://arxiv.org/abs/2507.02259)
*Hongli Yu,Tinghong Chen,Jiangtao Feng,Jiangjie Chen,Weinan Dai,Qiying Yu,Ya-Qin Zhang,Wei-Ying Ma,Jingjing Liu,Mingxuan Wang,Hao Zhou*

Main category: cs.CL

TL;DR: 该文提出了一种新型长文本处理代理MemAgent，利用分段记忆和扩展DAPO算法实现长文本任务端到端优化，实现了大幅提升的超长文档处理效果。


<details>
  <summary>Details</summary>
Motivation: 在处理超长文本时，现有的高效注意力机制和记忆模块虽有所进步，但依然难以线性复杂度下无性能下降地处理无限长文档。因此，作者希望能够解决这一挑战。

Method: 提出了一种端到端优化的长文本任务代理框架MemAgent。MemAgent采用分段阅读文本并通过覆盖策略更新记忆的方法，并扩展了DAPO算法，使其能通过独立上下文的多对话生成方式进行训练。

Result: MemAgent展现了极佳的长文本处理能力：在8K上下文训练、32K文本外推到3.5M的问答任务时性能下降低于5%，在512K RULER测试中达到95%以上的准确率。

Conclusion: MemAgent能够以近乎线性复杂度处理超长文档，并在外推到极长文本时保持高性能，明显优于以往方法。

Abstract: Despite improvements by length extrapolation, efficient attention and memory
modules, handling infinitely long documents with linear complexity without
performance degradation during extrapolation remains the ultimate challenge in
long-text processing. We directly optimize for long-text tasks in an end-to-end
fashion and introduce a novel agent workflow, MemAgent, which reads text in
segments and updates the memory using an overwrite strategy. We extend the DAPO
algorithm to facilitate training via independent-context multi-conversation
generation. MemAgent has demonstrated superb long-context capabilities, being
able to extrapolate from an 8K context trained on 32K text to a 3.5M QA task
with performance loss < 5% and achieves 95%+ in 512K RULER test.

</details>


### [37] [DoMIX: An Efficient Framework for Exploiting Domain Knowledge in Fine-Tuning](https://arxiv.org/abs/2507.02302)
*Dohoon Kim,Donghun Kang,Taesup Moon*

Main category: cs.CL

TL;DR: 本文提出DoMIX方法，通过LoRA模块实现高效、灵活且鲁棒的持续领域自适应预训练，可为多样化下游任务提供定制模型，并显著优化了计算资源利用。


<details>
  <summary>Details</summary>
Motivation: 持续领域自适应预训练（continual DAP）可逐步整合不同领域数据集，但现有方法存在高计算成本、对数据增量顺序敏感、仅输出单一通用模型等问题，难以有效满足多样化任务需求。

Method: 提出DoMIX方法，利用代表性的参数高效微调方法LoRA模块，实现高效且并行的领域自适应预训练。该方法对领域顺序具有鲁棒性，并能为不同任务提供定制的预训练模型。

Result: DoMIX不仅降低了训练过程中的计算开销和显存需求，提升了对领域顺序的鲁棒性，还能输出针对特定任务的定制模型，并可扩展应用于标准大模型微调场景。

Conclusion: DoMIX突破了持续领域自适应预训练的现有限制，在效率、泛化能力和灵活性上表现优异，对实际多领域大模型微调具有重要意义。

Abstract: Domain-Adaptive Pre-training (DAP) has recently gained attention for its
effectiveness in fine-tuning pre-trained models. Building on this, continual
DAP has been explored to develop pre-trained models capable of incrementally
incorporating different domain datasets. However, existing continual DAP
methods face several limitations: (1) high computational cost and GPU memory
usage during training; (2) sensitivity to incremental data order; and (3)
providing a single, generalized model for all end tasks, which contradicts the
essence of DAP. In this paper, we propose DoMIX, a novel approach that
addresses these challenges by leveraging LoRA modules, a representative
parameter-efficient fine-tuning (PEFT) method. Our approach enables efficient
and parallel domain-adaptive pre-training that is robust to domain order and
effectively utilizes accumulated knowledge to provide tailored pre-trained
models for specific tasks. We also demonstrate that our method can be extended
beyond the DAP setting to standard LLM fine-tuning scenarios. Code is available
at https://github.com/dohoonkim-ai/DoMIX.

</details>


### [38] [Coling-UniA at SciVQA 2025: Few-Shot Example Retrieval and Confidence-Informed Ensembling for Multimodal Large Language Models](https://arxiv.org/abs/2507.02357)
*Christian Jaumann,Annemarie Friedrich,Rainer Lienhart*

Main category: cs.CL

TL;DR: 本文提出了一套结合多模态大语言模型与few-shot检索策略的科学视觉问答系统，在国际竞赛中获得第三名，表现优异。


<details>
  <summary>Details</summary>
Motivation: 科学视觉问答是一个具有挑战性的任务，尤其需要结合视觉和文本信息智能作答。为提升系统性能，探索不同的模型集成和示例选择策略。

Method: 采用了两种多模态大语言模型的集成方案，并结合多种few-shot示例检索策略。系统根据图片和问题类型选择合适的大模型及few-shot设置，同时根据模型置信度进行答案选择。

Result: 在盲测数据上的平均F1分数为85.12，跨ROUGE-1、ROUGE-L和BERTS多个指标，排名第三。

Conclusion: 本文提出的系统在SciVQA 2025竞赛中表现优异，在七个参赛队伍中排名第三，显示了采用多模态大模型和高效检索策略的有效性。

Abstract: This paper describes our system for the SciVQA 2025 Shared Task on Scientific
Visual Question Answering. Our system employs an ensemble of two Multimodal
Large Language Models and various few-shot example retrieval strategies. The
model and few-shot setting are selected based on the figure and question type.
We also select answers based on the models' confidence levels. On the blind
test data, our system ranks third out of seven with an average F1 score of
85.12 across ROUGE-1, ROUGE-L, and BERTS. Our code is publicly available.

</details>


### [39] [QFFN-BERT: An Empirical Study of Depth, Performance, and Data Efficiency in Hybrid Quantum-Classical Transformers](https://arxiv.org/abs/2507.02364)
*Pilsung Kang*

Main category: cs.CL

TL;DR: QFFN-BERT用量子电路替代BERT的FFN模块，大幅减少参数并提升或维持准确率，在全数据和少样本场景都非常有效，证明混合量子-经典模型具备很好的潜力和数据效率。


<details>
  <summary>Details</summary>
Motivation: 近年来，参数化量子电路（PQC）在增强神经网络结构表达能力方面表现出巨大潜力。标准Transformer中FFN模块参数占比极高，因此本研究旨在探索将PQC引入BERT模型FFN模块后的表现和参数效率。

Method: 本文提出了一种混合量子-经典的QFFN-BERT模型：用PQC层替换紧凑型BERT中的FFN模块。设计采用残差连接、$R_Y$和$R_Z$旋转及交替纠缠策略，以兼顾可表达性和可训练性。实验在经典模拟器上于SST-2和DBpedia数据集进行。还包括消融研究以验证结构选择。

Result: 经过仔细配置的QFFN-BERT在全量数据下准确率可达基线的102%，同时FFN模块参数量减少99%以上。在少样本学习下表现也有优势。消融实验显示，如果PQC设计不合理，模型则无法有效学习。

Conclusion: 当PQC与深度学习基础原则协同设计时，可作为高效节省参数的FFN替代方案，显著提升Transformer模型的表达和效率。

Abstract: Parameterized quantum circuits (PQCs) have recently emerged as promising
components for enhancing the expressibility of neural architectures. In this
work, we introduce QFFN-BERT, a hybrid quantum-classical transformer where the
feedforward network (FFN) modules of a compact BERT variant are replaced by
PQC-based layers. This design is motivated by the dominant parameter
contribution of FFNs, which account for approximately two-thirds of the
parameters within standard Transformer encoder blocks. While prior studies have
primarily integrated PQCs into self-attention modules, our work focuses on the
FFN and systematically investigates the trade-offs between PQC depth,
expressibility, and trainability. Our final PQC architecture incorporates a
residual connection, both $R_Y$ and $R_Z$ rotations, and an alternating
entanglement strategy to ensure stable training and high expressibility. Our
experiments, conducted on a classical simulator, on the SST-2 and DBpedia
benchmarks demonstrate two key findings. First, a carefully configured
QFFN-BERT achieves up to 102.0% of the baseline accuracy, surpassing its
classical counterpart in a full-data setting while reducing FFN-specific
parameters by over 99%. Second, our model exhibits a consistent and competitive
edge in few-shot learning scenarios, confirming its potential for superior data
efficiency. These results, supported by an ablation study on a non-optimized
PQC that failed to learn, confirm that PQCs can serve as powerful and
parameter-efficient alternatives to classical FFNs when co-designed with
foundational deep learning principles.

</details>


### [40] [Efficient Code LLM Training via Distribution-Consistent and Diversity-Aware Data Selection](https://arxiv.org/abs/2507.02378)
*Weijie Lyu,Sheng-Jun Huang,Xuan Xia*

Main category: cs.CL

TL;DR: 本文提出利用参数模型选择高质量代码数据，能以更少数据显著提升大模型表现，并极大降低计算资源消耗。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLMs）尽管在代码生成和程序理解方面取得重大进展，但主要依赖海量数据进行训练，过度关注数据数量而忽略数据质量，导致训练效率降低。解决高效提升模型性能和训练效率的问题尤为重要。

Method: 提出一种基于参数模型的代码数据选择方法。该方法通过优化参数模型，保证所选子集的数据分布一致性与多样性，从而获得高质量训练数据。

Result: 通过实验，使用仅1万条样本，模型在HumanEval和MBPP评测基准上分别比使用9.2万条全量样本提升2.4%和2.3%，且优于其他采样方法，在性能和效率上均有优势。

Conclusion: 基于参数模型的数据选择方法能够在大幅提升模型表现的同时，显著降低计算成本，实现了高效且高质量的模型训练。

Abstract: Recent advancements in large language models (LLMs) have significantly
improved code generation and program comprehension, accelerating the evolution
of software engineering. Current methods primarily enhance model performance by
leveraging vast amounts of data, focusing on data quantity while often
overlooking data quality, thereby reducing training efficiency. To address
this, we introduce an approach that utilizes a parametric model for code data
selection, aimed at improving both training efficiency and model performance.
Our method optimizes the parametric model to ensure distribution consistency
and diversity within the selected subset, guaranteeing high-quality data.
Experimental results demonstrate that using only 10K samples, our method
achieves gains of 2.4% (HumanEval) and 2.3% (MBPP) over 92K full-sampled
baseline, outperforming other sampling approaches in both performance and
efficiency. This underscores that our method effectively boosts model
performance while significantly reducing computational costs.

</details>


### [41] [Benchmarking Akan ASR Models Across Domain-Specific Datasets: A Comparative Evaluation of Performance, Scalability, and Adaptability](https://arxiv.org/abs/2507.02407)
*Mark Atta Mensah,Isaac Wiafe,Akon Ekpezu,Justice Kwame Appati,Jamal-Deen Abdulai,Akosua Nyarkoa Wiafe-Akenten,Frank Ernest Yeboah,Gifty Odame*

Main category: cs.CL

TL;DR: Akan语的自动语音识别模型在不同领域下容易出现明显性能下降，不同架构的错误类型也有差异。研究指出ASR模型需结合领域适应和架构选择，推动低资源语言技术发展。


<details>
  <summary>Details</summary>
Motivation: 现在大多数自动语音识别（ASR）研究，只用“域内”数据集评估模型，但很少测试模型对不同语音场景的泛化能力。该研究尝试弥补这一空白，关注Akan语的ASR模型跨多领域的表现。

Method: 将7个基于Transformer架构（如Whisper和Wav2Vec2）的Akan ASR模型，分别在4个不同领域的Akan语音语料上（包括文化相关的图像描述、非正式对话、圣经朗读和金融对话）进行基准测试。通过对比词错误率和字错误率，分析不同模型和领域下的表现。

Result: 模型仅在其训练领域内表现最佳，在领域不匹配的场景下准确率明显下降。Whisper与Wav2Vec2架构在出错类型上有明显区别：Whisper生成更流畅但可能误导性的转录，而Wav2Vec2在遇到不熟悉数据时输出难以理解但更显然的错误。

Conclusion: 研究揭示了领域依赖性问题，强调在低资源语言应用中，ASR架构选择应权衡可读性与透明度。建议针对Akan及其他低资源语言，采用定向领域自适应、动态路由和多语种训练等策略。

Abstract: Most existing automatic speech recognition (ASR) research evaluate models
using in-domain datasets. However, they seldom evaluate how they generalize
across diverse speech contexts. This study addresses this gap by benchmarking
seven Akan ASR models built on transformer architectures, such as Whisper and
Wav2Vec2, using four Akan speech corpora to determine their performance. These
datasets encompass various domains, including culturally relevant image
descriptions, informal conversations, biblical scripture readings, and
spontaneous financial dialogues. A comparison of the word error rate and
character error rate highlighted domain dependency, with models performing
optimally only within their training domains while showing marked accuracy
degradation in mismatched scenarios. This study also identified distinct error
behaviors between the Whisper and Wav2Vec2 architectures. Whereas fine-tuned
Whisper Akan models led to more fluent but potentially misleading transcription
errors, Wav2Vec2 produced more obvious yet less interpretable outputs when
encountering unfamiliar inputs. This trade-off between readability and
transparency in ASR errors should be considered when selecting architectures
for low-resource language (LRL) applications. These findings highlight the need
for targeted domain adaptation techniques, adaptive routing strategies, and
multilingual training frameworks for Akan and other LRLs.

</details>


### [42] [A Cookbook for Community-driven Data Collection of Impaired Speech in LowResource Languages](https://arxiv.org/abs/2507.02428)
*Sumaya Ahmed Salihs,Isaac Wiafe,Jamal-Deen Abdulai,Elikem Doe Atsakpo,Gifty Ayoka,Richard Cave,Akon Obu Ekpezu,Catherine Holloway,Katrin Tomanek,Fiifi Baffoe Payin Winful*

Main category: cs.CL

TL;DR: 本研究开发了面向低资源语种（如Akan）的语音障碍者ASR数据收集最佳实践，并建立了首个公开Akan语音障碍语料，提升了相应ASR识别能力。研究还发布了开放工具，推动包容性语音技术的发展。


<details>
  <summary>Details</summary>
Motivation: 现有自动语音识别（ASR）系统较少关注语音障碍者，尤其是低资源语言，导致该群体在语音技术上被边缘化。该研究旨在推动语音障碍者的ASR技术发展，实现包容性和民主化。

Method: 本研究开发了一个数据收集和ASR模型构建的最佳实践“手册”，并组织社区参与数据采集。作为实例，研究团队收集并整理了加纳广泛使用的土著语言Akan的语音障碍者公开数据集。还利用公开工具对ASR模型进行了微调。

Result: 该研究制作了首个公开的Akan语音障碍者语音数据集，并发布了用于社区驱动数据收集与ASR模型建设的实践手册和工具。初步实验表明，微调后的ASR模型在识别Akan语音障碍者语音上性能有提升。

Conclusion: 通过社区驱动的数据收集和开放工具，推动了低资源语种中语音障碍者ASR的发展。该方法不仅提升了技术包容性，也为未来类似工作提供了可复制范式。所生成资源对研究者和工程师开发更适应语音障碍者需求的ASR系统具有重要意义。

Abstract: This study presents an approach for collecting speech samples to build
Automatic Speech Recognition (ASR) models for impaired speech, particularly,
low-resource languages. It aims to democratize ASR technology and data
collection by developing a "cookbook" of best practices and training for
community-driven data collection and ASR model building. As a proof-of-concept,
this study curated the first open-source dataset of impaired speech in Akan: a
widely spoken indigenous language in Ghana. The study involved participants
from diverse backgrounds with speech impairments. The resulting dataset, along
with the cookbook and open-source tools, are publicly available to enable
researchers and practitioners to create inclusive ASR technologies tailored to
the unique needs of speech impaired individuals. In addition, this study
presents the initial results of fine-tuning open-source ASR models to better
recognize impaired speech in Akan.

</details>


### [43] [IndianBailJudgments-1200: A Multi-Attribute Dataset for Legal NLP on Indian Bail Orders](https://arxiv.org/abs/2507.02506)
*Sneha Deshmukh,Prathmesh Kamble*

Main category: cs.CL

TL;DR: 该论文构建并公开了印度首个专注于保释判决的结构化数据集，为法律NLP研究提供了新基础。


<details>
  <summary>Details</summary>
Motivation: 在印度等地区，由于缺乏结构化数据集，法律自然语言处理（Legal NLP）的发展受到限制。作者旨在解决印度法律领域尤其是保释判决数据稀缺的问题。

Method: 作者采用了基于提示工程的GPT-4o流水线生成注释，并对结果进行一致性验证，构建了包含1200份印度法院保释判决、覆盖20余项属性（如保释结果、IPC条款、犯罪类型和法律推理）的数据集。

Result: 生成并公开了名为IndianBailJudgments-1200的数据集，是首个专注于印度保释法理的公开数据集。该数据集可用于判决结果预测、文本摘要、公平性分析等多种法律NLP任务。

Conclusion: IndianBailJudgments-1200为印度法律领域的自然语言处理研究提供了重要的资源，推动了印度法域相关NLP任务的发展和数据集建设。

Abstract: Legal NLP remains underdeveloped in regions like India due to the scarcity of
structured datasets. We introduce IndianBailJudgments-1200, a new benchmark
dataset comprising 1200 Indian court judgments on bail decisions, annotated
across 20+ attributes including bail outcome, IPC sections, crime type, and
legal reasoning. Annotations were generated using a prompt-engineered GPT-4o
pipeline and verified for consistency. This resource supports a wide range of
legal NLP tasks such as outcome prediction, summarization, and fairness
analysis, and is the first publicly available dataset focused specifically on
Indian bail jurisprudence.

</details>


### [44] [WebSailor: Navigating Super-human Reasoning for Web Agent](https://arxiv.org/abs/2507.02592)
*Kuan Li,Zhongwang Zhang,Huifeng Yin,Liwen Zhang,Litu Ou,Jialong Wu,Wenbiao Yin,Baixuan Li,Zhengwei Tao,Xinyu Wang,Weizhou Shen,Junkai Zhang,Dingchu Zhang,Xixi Wu,Yong Jiang,Ming Yan,Pengjun Xie,Fei Huang,Jingren Zhou*

Main category: cs.CL

TL;DR: WebSailor通过创新的任务生成与专门的RL训练流程，极大提升了开源大模型信息检索能力，使其接近甚至媲美闭源智能体。


<details>
  <summary>Details</summary>
Motivation: 目前LLM在处理极为复杂的信息检索任务时，存在认知能力的局限性，而专有系统如DeepResearch在BrowseComp等高难基准上展现出超越人类的能力。作者认为，这背后的原因在于这些系统能够系统性地减少极端不确定性，而这一能力在开源模型中缺失。为此，论文旨在赋予开源模型类似的高阶推理与信息处理能力。

Method: 提出了一整套后训练（post-training）方法论WebSailor，包括三个关键步骤：通过结构化采样和信息隐藏产生高不确定性任务、RFT（Randomized Few-shot Training）冷启动，以及高效的智能体RL训练算法——Duplicating Sampling Policy Optimization (DUPO)。这一流程用于增强模型应对高不确定信息环境的能力。

Result: WebSailor在复杂信息检索任务中显著优于所有开源智能体，表现与专有模型持平，有效缩小两者之间的能力差距。

Conclusion: 后训练方法WebSailor能够使开源LLM在极度复杂的信息寻求任务上取得专有系统同等水平的表现，证明了系统性应对极端不确定性能力的重要性及方法有效性。

Abstract: Transcending human cognitive limitations represents a critical frontier in
LLM training. Proprietary agentic systems like DeepResearch have demonstrated
superhuman capabilities on extremely complex information-seeking benchmarks
such as BrowseComp, a feat previously unattainable. We posit that their success
hinges on a sophisticated reasoning pattern absent in open-source models: the
ability to systematically reduce extreme uncertainty when navigating vast
information landscapes. Based on this insight, we introduce WebSailor, a
complete post-training methodology designed to instill this crucial capability.
Our approach involves generating novel, high-uncertainty tasks through
structured sampling and information obfuscation, RFT cold start, and an
efficient agentic RL training algorithm, Duplicating Sampling Policy
Optimization (DUPO). With this integrated pipeline, WebSailor significantly
outperforms all opensource agents in complex information-seeking tasks,
matching proprietary agents' performance and closing the capability gap.

</details>


### [45] [Revisiting Active Learning under (Human) Label Variation](https://arxiv.org/abs/2507.02593)
*Cornelia Gruber,Helen Alber,Bernd Bischl,Göran Kauermann,Barbara Plank,Matthias Aßenmacher*

Main category: cs.CL

TL;DR: 本文分析了现实中常见的人类标签差异对主动学习的影响，认为这种差异应被视为有用信号而非纯噪声，并提出了面向HLV的主动学习概念框架，包括数据选取、标注者管理和标签表示等环节，旨在更好适应实际应用中的数据标注复杂性。


<details>
  <summary>Details</summary>
Motivation: 传统的监督学习和主动学习往往假设标注真实唯一，忽视了现实中存在的人类标注差异，这种差异可能不仅是噪声，也包含了有价值的信息。作者希望推动社区正视和利用HLV，提升数据利用效率和模型表现。

Method: 作者对主动学习和标签变异领域的相关文献进行综述，总结并梳理了已有方法对HLV的处理现状与不足，并提出了一个概念性框架，将HLV整合到主动学习流程的多个环节，如实例选择、标注者选择和标签表示，同时探讨了引入大型语言模型作为标注者的可能性。

Result: 提出了分解标签变异为信号（如真实的人类观点差异）与噪声（如标注错误）的分析框架，并为主动学习流程中的各环节引入了考虑HLV的方法论建议。还讨论了将大语言模型作为辅助标注者的潜力。

Conclusion: 本文强调在主动学习(AL)流程中引入对人类标签变异性(HLV)的认知和处理，有助于更真实地反映实际标注中的复杂性，并指导更有效的机器学习模型训练。

Abstract: Access to high-quality labeled data remains a limiting factor in applied
supervised learning. While label variation (LV), i.e., differing labels for the
same instance, is common, especially in natural language processing, annotation
frameworks often still rest on the assumption of a single ground truth. This
overlooks human label variation (HLV), the occurrence of plausible differences
in annotations, as an informative signal. Similarly, active learning (AL), a
popular approach to optimizing the use of limited annotation budgets in
training ML models, often relies on at least one of several simplifying
assumptions, which rarely hold in practice when acknowledging HLV. In this
paper, we examine foundational assumptions about truth and label nature,
highlighting the need to decompose observed LV into signal (e.g., HLV) and
noise (e.g., annotation error). We survey how the AL and (H)LV communities have
addressed -- or neglected -- these distinctions and propose a conceptual
framework for incorporating HLV throughout the AL loop, including instance
selection, annotator choice, and label representation. We further discuss the
integration of large language models (LLM) as annotators. Our work aims to lay
a conceptual foundation for HLV-aware active learning, better reflecting the
complexities of real-world annotation.

</details>


### [46] [MPF: Aligning and Debiasing Language Models post Deployment via Multi Perspective Fusion](https://arxiv.org/abs/2507.02595)
*Xin Guan,PeiHsin Lin,Zekun Wu,Ze Wang,Ruibo Zhang,Emre Kazim,Adriano Koshiyama*

Main category: cs.CL

TL;DR: 本文提出了多视角融合（MPF）后训练方法，通过人类基线分解和多视角加权生成，实现了对LLM偏见的高效缓解，无需大规模微调，对实际部署具有重要意义。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLM）存在偏见问题，亟需简单高效的后训练（posttraining）校准与偏见缓解方法，以便应用于实际业务中。现有方法普遍依赖复杂的微调或提示工程，局限性较大。

Method: 提出了一种新的多视角融合（MPF）框架，作为后训练的对齐方法。MPF基于SAGED自动化系统，首先从人类职业群体（如HR）提取基线分布，并将其分解为可解释的视角组件，再通过加权响应抽样，引导LLM生成更加贴合人类基线的输出，无需大规模微调或复杂提示工程。

Result: 实验证明MPF方法能够使LLM输出的情感分布与人类基线（如HR基线）或反事实基线（绝对平等）高度一致，KL散度降低，校准误差降低，并能在未见过的问题上泛化。

Conclusion: MPF为实际部署的LLM带来了易用、可解释、无需大规模微调的偏见缓解和模型对齐能力。该方法具有良好的可扩展性和适用性。

Abstract: Multiperspective Fusion (MPF) is a novel posttraining alignment framework for
large language models (LLMs) developed in response to the growing need for easy
bias mitigation. Built on top of the SAGED pipeline, an automated system for
constructing bias benchmarks and extracting interpretable baseline
distributions, MPF leverages multiperspective generations to expose and align
biases in LLM outputs with nuanced, humanlike baselines. By decomposing
baseline, such as sentiment distributions from HR professionals, into
interpretable perspective components, MPF guides generation through sampling
and balancing of responses, weighted by the probabilities obtained in the
decomposition. Empirically, we demonstrate its ability to align LLM sentiment
distributions with both counterfactual baselines (absolute equality) and the HR
baseline (biased for Top Univeristy), resulting in small KL divergence,
reduction of calibration error and generalization to unseen questions. This
shows that MPF offers a scalable and interpretable method for alignment and
bias mitigation, compatible with deployed LLMs and requiring no extensive
prompt engineering or finetuning.

</details>


### [47] [Exploring Gender Bias Beyond Occupational Titles](https://arxiv.org/abs/2507.02679)
*Ahmed Sabir,Rajesh Sharama*

Main category: cs.CL

TL;DR: 该论文提出GenderLexicon数据集和新框架，能量化和解释多场景下的性别偏见，不限于职业，多数据集实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 探索性别与上下文偏见之间的关联，超越传统的职业刻板印象，关注更广泛语言表达中的性别偏见。

Method: 引入了全新的GenderLexicon数据集和可用于评估上下文偏见及相关性别偏见的分析框架；通过模型赋予可解释分数以展现偏见。

Result: 实验在五个不同数据集（包括日语数据集）上进行了效果评估，证实了性别偏见不仅存在于职业刻板印象中，还出现于更广泛的语言元素中。

Conclusion: 提出的数据集与框架提升了性别偏见识别和解释能力，能够广泛检测并量化职业之外的多种语言偏见。

Abstract: In this work, we investigate the correlation between gender and contextual
biases, focusing on elements such as action verbs, object nouns, and
particularly on occupations. We introduce a novel dataset, GenderLexicon, and a
framework that can estimate contextual bias and its related gender bias. Our
model can interpret the bias with a score and thus improve the explainability
of gender bias. Also, our findings confirm the existence of gender biases
beyond occupational stereotypes. To validate our approach and demonstrate its
effectiveness, we conduct evaluations on five diverse datasets, including a
Japanese dataset.

</details>


### [48] [Can LLMs Identify Critical Limitations within Scientific Research? A Systematic Evaluation on AI Research Papers](https://arxiv.org/abs/2507.02694)
*Zhijian Xu,Yilun Zhao,Manasi Patwardhan,Lovekesh Vig,Arman Cohan*

Main category: cs.CL

TL;DR: 作者建立了专门评测大模型发现论文局限性能力的LimitGen基准，并结合文献检索技术，显著提升了LLM辅助科学评审中局限性识别和反馈的能力。


<details>
  <summary>Details</summary>
Motivation: 同行评审是科学研究的核心，但随着发表量的激增，评审压力加大，而大模型（LLM）在支持科学评审特别是发现论文局限性方面的能力尚未被充分探索。

Method: 作者提出了针对科学研究（主要是AI领域）局限性的详尽分类法，并基于此开发了LimitGen基准测试，包括合成数据（LimitGen-Syn）和现实数据（LimitGen-Human），同时结合文献检索增强LLM的局限性识别能力。

Result: 通过将文献检索与LLM结合，该方法提升了大模型对论文局限性的检测和反馈能力，能为同行评审提供更具建设性的初期意见。

Conclusion: 本工作建立了评估大模型辅助识别论文局限性的基准，同时证明了文献检索增强的LLM能有效提升局限性发现质量，对提升科学评审流程具有重要意义。

Abstract: Peer review is fundamental to scientific research, but the growing volume of
publications has intensified the challenges of this expertise-intensive
process. While LLMs show promise in various scientific tasks, their potential
to assist with peer review, particularly in identifying paper limitations,
remains understudied. We first present a comprehensive taxonomy of limitation
types in scientific research, with a focus on AI. Guided by this taxonomy, for
studying limitations, we present LimitGen, the first comprehensive benchmark
for evaluating LLMs' capability to support early-stage feedback and complement
human peer review. Our benchmark consists of two subsets: LimitGen-Syn, a
synthetic dataset carefully created through controlled perturbations of
high-quality papers, and LimitGen-Human, a collection of real human-written
limitations. To improve the ability of LLM systems to identify limitations, we
augment them with literature retrieval, which is essential for grounding
identifying limitations in prior scientific findings. Our approach enhances the
capabilities of LLM systems to generate limitations in research papers,
enabling them to provide more concrete and constructive feedback.

</details>


### [49] [Measurement of the Granularity of Vowel Production Space By Just Producible Different (JPD) Limens](https://arxiv.org/abs/2507.02744)
*Peter Viechnicki*

Main category: cs.CL

TL;DR: 首次测量了英语前元音在听觉空间中可可靠区别的最小距离（JPD），结果为14至51 mel。该发现为语音产生理论和元音音位系统结构提供了重要新见解。


<details>
  <summary>Details</summary>
Motivation: 以往研究表明，人类元音发音的复杂协调运动由以听觉空间为目标的控制机制调控，但对于这种调控的精确度尚不明确。作者希望了解元音模仿时，两个元音刺激在听觉空间中需相隔多远才会被可靠地区分模仿。

Method: 采用元音模仿实验范式，对两组英语使用者进行前元音的模仿实验，首次测量“最小可产出差异”（JPD），即两个元音在听觉空间中被可靠地区分的最小距离。

Result: JPD在F1×F2听觉空间中被估算为14到51 mel。

Conclusion: 此结果为语音产生的情景理论提供了新证据，并澄清了人类元音系统可能结构，通过设定元音音位之间理论上的最小距离，为元音音位数量和分布规律提供了心理物理学解释。

Abstract: A body of work over the past several decades has demonstrated that the
complex and coordinated articulatory movements of human vowel production are
governed (at least in part)by control mechanisms whose targets are regions of
auditory space. Within the target region control at the sub-phonemic level has
also been demonstrated. But the degree of accuracy of that control is unknown.
The current work investigates this question by asking how far apart must two
vowel stimuli lie in auditory space in order to yield reliably different
imitations? This distance is termed 'Just Producible Difference' (JPD). The
current study uses a vowel mimicry paradigm to derive the first measurement of
JPD among two sets of English speakers during front vowel production. JPD is
estimated at between 14 and 51 mels in F1 X F2 space. This finding has
implications for episodic theories of speech production. It also clarifies the
possible structures of human vowel systems, by setting a theoretical lower
bound for how close two vowel phonemes may be in a speaker's formant space, and
hence a psychophysical explanation of observed trends in number and patterns of
possible vowel phonemes.

</details>


### [50] [Self-Correction Bench: Revealing and Addressing the Self-Correction Blind Spot in LLMs](https://arxiv.org/abs/2507.02778)
*Ken Tsui*

Main category: cs.CL

TL;DR: LLM难以自我纠错，主要因训练数据缺乏纠错过程。作者提出评测框架并发现用“Wait”等简单提示能大幅改善这一弱点。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）虽然已经表现出很强的能力，但仍容易犯错并生成无效的推理。尤其是LLM自身在检测和纠正自己输出中的错误方面存在盲区，这影响了模型的可信度。

Method: 该论文提出了Self-Correction Bench，一个通过注入不同复杂度错误来系统化测量自我纠错盲区的评测框架，并对14种模型进行了测试和对比。同时分析了训练数据组成与模型纠错能力之间的关系，并尝试了简单的提示词干预（如添加“Wait”）探究其影响。

Result: 实验发现，模型在自我输出错误纠正上普遍存在平均64.5%的盲区率。人类标注训练数据主要给出无错误示例，而RL训练模型能更好地进行纠错。简单地在提示前加入“Wait”可以将盲区率降低89.3%。

Conclusion: 当前LLM普遍存在自我纠错能力不足的问题，这与训练数据类型有关。通过简单的提示词调整，可以显著激活模型已有的纠错潜力，这为提升模型可靠性提供了新思路。

Abstract: Although large language models (LLMs) have become transformative, they still
make mistakes and can explore unproductive reasoning paths. Self-correction is
an important capability for a trustworthy LLM, particularly an autoregressive
LLM. While LLMs can identify error in user input, they exhibit a systematic
'Self-Correction Blind Spot' - failing to correct identical error in their own
outputs. To systematically study this phenomenon, we introduce Self-Correction
Bench, a systematic framework to measure this phenomenon through controlled
error injection at three complexity levels. Testing 14 models, we find an
average 64.5% blind spot rate. We find multiple evidences that this limitation
relates to training data composition: human training demonstrations
predominantly show error-free responses rather than error-correction sequences,
unlike RL-trained models that learn error correction through outcome feedback.
Remarkably, simply appending "Wait" reduces blind spots by 89.3%, suggesting
that the capability exists but requires activation. Our work highlights a
critical limitation in current LLMs and offers potential avenues for improving
their reliability and trustworthiness.

</details>


### [51] [Is Reasoning All You Need? Probing Bias in the Age of Reasoning Language Models](https://arxiv.org/abs/2507.02799)
*Riccardo Cantini,Nicola Gabriele,Alessio Orsino,Domenico Talia*

Main category: cs.CL

TL;DR: 具备推理能力的语言模型在偏见防护方面并不如预期稳健，反而在某些情况下更容易被诱导生成带有偏见内容。链式思维提示尤其脆弱，需对推理设计更加注重偏见防护。


<details>
  <summary>Details</summary>
Motivation: 推理语言模型通过推理链等机制在复杂任务上表现出更高能力，被认为更可靠。然而，目前尚不清楚这些推理能力对模型抵抗社会偏见的影响，因此有必要系统性探究推理机制对偏见稳健性的作用。

Method: 该研究采用CLEAR-Bias基准测试，对多种先进推理语言模型（RLMs）在不同社会文化维度上的偏见诱导稳健性进行系统评估。使用LLM-as-a-judge自动化评分方法，并结合越狱（jailbreak）技术，分析模型内置安全机制的有效性。实验涵盖多种推理机制（如推理微调与链式思维提示），评估它们在应对偏见诱导攻击时的表现差异。

Result: 实验表明，具备显式推理机制（无论是推理微调还是链式思维提示）的模型通常比无推理机制的基础模型更容易被诱导产生偏见表达。其中仅依赖链式思维提示的模型，尤其容易被通过故事构建、虚构角色或奖励驱动式的越狱攻击影响。同时，仅微调推理的模型在安全性上略优于推理提示的模型。整体结果表明推理并不必然提升偏见稳健性，反而可能为刻板印象的诱导打开新路径。

Conclusion: 带有显式推理能力的语言模型（如通过链式思维提示或微调推理轨迹实现）在面对社会偏见诱导时，整体上比不带推理机制的基础模型更易受到攻击，这对增强模型公正性和稳健性的假设提出了挑战。

Abstract: Reasoning Language Models (RLMs) have gained traction for their ability to
perform complex, multi-step reasoning tasks through mechanisms such as
Chain-of-Thought (CoT) prompting or fine-tuned reasoning traces. While these
capabilities promise improved reliability, their impact on robustness to social
biases remains unclear. In this work, we leverage the CLEAR-Bias benchmark,
originally designed for Large Language Models (LLMs), to investigate the
adversarial robustness of RLMs to bias elicitation. We systematically evaluate
state-of-the-art RLMs across diverse sociocultural dimensions, using an
LLM-as-a-judge approach for automated safety scoring and leveraging jailbreak
techniques to assess the strength of built-in safety mechanisms. Our evaluation
addresses three key questions: (i) how the introduction of reasoning
capabilities affects model fairness and robustness; (ii) whether models
fine-tuned for reasoning exhibit greater safety than those relying on CoT
prompting at inference time; and (iii) how the success rate of jailbreak
attacks targeting bias elicitation varies with the reasoning mechanisms
employed. Our findings reveal a nuanced relationship between reasoning
capabilities and bias safety. Surprisingly, models with explicit reasoning,
whether via CoT prompting or fine-tuned reasoning traces, are generally more
vulnerable to bias elicitation than base models without such mechanisms,
suggesting reasoning may unintentionally open new pathways for stereotype
reinforcement. Reasoning-enabled models appear somewhat safer than those
relying on CoT prompting, which are particularly prone to contextual reframing
attacks through storytelling prompts, fictional personas, or reward-shaped
instructions. These results challenge the assumption that reasoning inherently
improves robustness and underscore the need for more bias-aware approaches to
reasoning design.

</details>


### [52] [Multimodal Mathematical Reasoning with Diverse Solving Perspective](https://arxiv.org/abs/2507.02804)
*Wenhao Shi,Zhiqiang Hu,Yi Bin,Yang Yang,See-Kiong Ng,Heng Tao Shen*

Main category: cs.CL

TL;DR: 本文提出了支持多种解题路径的数学推理多模态数据集与新模型，通过融合正确性和多样性奖励，显著提升了模型的准确性和生成多样性，凸显了包容多元推理视角的价值。


<details>
  <summary>Details</summary>
Motivation: 目前的多模态大语言模型（MLLMs）在数学推理任务中，往往只依赖一对一的图文配对和单一解法的监督，忽视了推理路径的多样性和内部反思，从而限制了模型推理能力的提升。

Method: 本文提出了MathV-DP数据集，为每个图像-问题对提供多样化的解题路径，用于提供更丰富的推理监督。进一步，基于Qwen-VL模型，提出了Qwen-VL-DP，通过监督学习微调，并通过群体相对策略优化（GRPO）算法强化训练，引入了基于规则的强化学习方法，融合了正确性判别和多样性奖励机制，以实现对多样推理视角和不同正确解法的有效学习。

Result: Qwen-VL-DP在MathVista's minitest和Math-V基准测试上，较以往的基础多模态大语言模型在准确率与生成多样性方面都取得了显著提升。

Conclusion: 本文证明了结合多样推理视角和反思机制对于提升多模态数学推理模型性能的重要性。

Abstract: Recent progress in large-scale reinforcement learning (RL) has notably
enhanced the reasoning capabilities of large language models (LLMs), especially
in mathematical domains. However, current multimodal LLMs (MLLMs) for
mathematical reasoning often rely on one-to-one image-text pairs and
single-solution supervision, overlooking the diversity of valid reasoning
perspectives and internal reflections. In this work, we introduce MathV-DP, a
novel dataset that captures multiple diverse solution trajectories for each
image-question pair, fostering richer reasoning supervision. We further propose
Qwen-VL-DP, a model built upon Qwen-VL, fine-tuned with supervised learning and
enhanced via group relative policy optimization (GRPO), a rule-based RL
approach that integrates correctness discrimination and diversity-aware reward
functions. Our method emphasizes learning from varied reasoning perspectives
and distinguishing between correct yet distinct solutions. Extensive
experiments on the MathVista's minitest and Math-V benchmarks demonstrate that
Qwen-VL-DP significantly outperforms prior base MLLMs in both accuracy and
generative diversity, highlighting the importance of incorporating diverse
perspectives and reflective reasoning in multimodal mathematical reasoning.

</details>


### [53] [SynapseRoute: An Auto-Route Switching Framework on Dual-State Large Language Model](https://arxiv.org/abs/2507.02822)
*Wencheng Zhang,Shiqin Qiao,Lingjie Luo,Yinfeng Li,Chuanyang Zheng,Qian Xu,Meng Li,Yong Gui,Yijun He,Jianing Qiu,Jindong Hong,Jiankai Sun*

Main category: cs.CL

TL;DR: 作者发现近58%的医疗问题无需高成本推理即可解答。提出的SynapseRoute通过动态路由不同复杂度任务，有效提升准确率并大幅降低成本，优于仅用深度推理模式。新指标AIT综合反映准确率、时延和token消耗的平衡。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在实际应用中受到广泛关注，但选择合适模型时需要在性能和运行成本之间做权衡。随着具备推理能力模型的出现，其成本差距进一步加大，尤其是高推理（高成本）与非推理（低成本）模式间的差异。作者关注如何平衡推理能力、成本与用户体验间的矛盾。

Method: 提出了SynapseRoute，这是一个基于机器学习的动态路由框架，能够根据输入查询的复杂度智能分配到'推理'或'非推理'模式下进行处理。该方法在多个医疗数据集上进行实验，并分析了过度推理对简单任务可能带来的负面影响。同时引入了一个综合指标Accuracy-Inference-Token（AIT），用于评估准确率、延迟和token成本间的平衡。

Result: 实验表明，SynapseRoute在多个医疗数据集上不仅提高了整体准确率（0.8390 vs. 0.8272），还减少了36.8%的推理时间和39.66%的token消耗。定性分析显示，对简单问题进行过度推理会导致无谓延迟甚至准确率降低，而动态路由有效避免了这一问题。

Conclusion: 动态路由能够根据问题复杂度智能分配推理任务，在保证高准确率的同时大幅降低成本与等待时间，提升整体效率和用户体验。提出的AIT综合指标能全面评估不同策略优劣。

Abstract: With the widespread adoption of large language models (LLMs) in practical
applications, selecting an appropriate model requires balancing not only
performance but also operational cost. The emergence of reasoning-capable
models has further widened the cost gap between "thinking" (high reasoning) and
"non-thinking" (fast, low-cost) modes. In this work, we reveal that
approximately 58% of medical questions can be accurately answered by the
non-thinking mode alone, without requiring the high-cost reasoning process.
This highlights a clear dichotomy in problem complexity and suggests that
dynamically routing queries to the appropriate mode based on complexity could
optimize accuracy, cost-efficiency, and overall user experience. Based on this,
we further propose SynapseRoute, a machine learning-based dynamic routing
framework that intelligently assigns input queries to either thinking or
non-thinking modes. Experimental results on several medical datasets
demonstrate that SynapseRoute not only improves overall accuracy (0.8390 vs.
0.8272) compared to the thinking mode alone but also reduces inference time by
36.8% and token consumption by 39.66%. Importantly, qualitative analysis
indicates that over-reasoning on simpler queries can lead to unnecessary delays
and even decreased accuracy, a pitfall avoided by our adaptive routing.
Finally, this work further introduces the Accuracy-Inference-Token (AIT) index
to comprehensively evaluate the trade-offs among accuracy, latency, and token
cost.

</details>


### [54] [Generalizing Verifiable Instruction Following](https://arxiv.org/abs/2507.02833)
*Valentina Pyatkin,Saumya Malik,Victoria Graf,Hamish Ivison,Shengyi Huang,Pradeep Dasigi,Nathan Lambert,Hannaneh Hajishirzi*

Main category: cs.CL

TL;DR: 该研究提出了IFBench基准，通过强化学习与约束验证方法提升了语言模型在精确指令执行和泛化能力方面的表现，发布了相关基准、训练数据和代码，为该领域研究提供了新工具和数据。


<details>
  <summary>Details</summary>
Motivation: 当前的语言模型和聊天机器人在执行精确的人类指令方面存在不足，尤其是在处理带有明确输出约束（如“只能回答是或否”、“至少提及某词3次”）时，普遍表现不佳。本研究关注于提升模型对未见过的复杂约束的泛化能力。

Method: 提出了新的评测基准IFBench，涵盖58种全新且具有挑战性的可验证域外约束，用于评估模型对精确指令的泛化能力。此外，设计了约束验证模块，并通过以可验证奖励为目标的强化学习（RLVR）方法进行训练，以提升模型的指令遵循能力。

Result: 实验发现，多数主流模型高度依赖于训练集中的少量验证性约束，泛化到未见过约束的能力较差。采用RLVR和新设计的验证方法后，模型在IFBench上的执行效果显著提升。

Conclusion: 强化学习结合可验证约束信号显著提升了模型对复杂、全新指令约束的遵循泛化能力。IFBench及相关资源的发布，为后续研究提供了评测和训练的新平台和工具。

Abstract: A crucial factor for successful human and AI interaction is the ability of
language models or chatbots to follow human instructions precisely. A common
feature of instructions are output constraints like ``only answer with yes or
no" or ``mention the word `abrakadabra' at least 3 times" that the user adds to
craft a more useful answer. Even today's strongest models struggle with
fulfilling such constraints. We find that most models strongly overfit on a
small set of verifiable constraints from the benchmarks that test these
abilities, a skill called precise instruction following, and are not able to
generalize well to unseen output constraints. We introduce a new benchmark,
IFBench, to evaluate precise instruction following generalization on 58 new,
diverse, and challenging verifiable out-of-domain constraints. In addition, we
perform an extensive analysis of how and on what data models can be trained to
improve precise instruction following generalization. Specifically, we
carefully design constraint verification modules and show that reinforcement
learning with verifiable rewards (RLVR) significantly improves instruction
following. In addition to IFBench, we release 29 additional new hand-annotated
training constraints and verification functions, RLVR training prompts, and
code.

</details>


### [55] [LLM Hypnosis: Exploiting User Feedback for Unauthorized Knowledge Injection to All Users](https://arxiv.org/abs/2507.02850)
*Almog Hilel,Idan Shenfeld,Leshem Choshen,Jacob Andreas*

Main category: cs.CL

TL;DR: 本文发现在使用用户反馈进行偏好微调的大模型中，单一攻击者仅凭点赞/点踩即可长期影响模型行为，实现知识投毒和信任攻击，这揭示了模型微调中的显著安全风险。


<details>
  <summary>Details</summary>
Motivation: 当前大模型在训练过程中大量依赖用户反馈（如点赞或点踩LM回答），而这种机制可能带来被恶意利用的风险。作者希望探索，仅凭有限的用户反馈权限，单个攻击者是否可以持续性地改变模型的知识和行为。

Method: 攻击者通过设计特定的提示，让LM随机输出“投毒”或正常答案，并分别对“投毒”答案点赞、对正常答案点踩。随后，这些偏向性的用户反馈参与后续偏好微调。作者通过实验，验证该操作是否能让LM在普通环境下也倾向产生“投毒”结果。

Result: 实验表明，这种攻击可以：1）植入模型原不具备的事实知识，2）改变代码生成模式以引入安全漏洞，3）注入伪造金融新闻。即使只用极受限的用户反馈数据，也可对模型行为产生细致影响。

Conclusion: 单一用户通过偏向性反馈可持续影响大模型行为，容易造成知识注入、安全漏洞和信息误导。这表明基于用户反馈的偏好微调机制存在新型攻击面，并应加强防护与检测。

Abstract: We describe a vulnerability in language models (LMs) trained with user
feedback, whereby a single user can persistently alter LM knowledge and
behavior given only the ability to provide prompts and upvote / downvote
feedback on LM outputs. To implement the attack, the attacker prompts the LM to
stochastically output either a "poisoned" or benign response, then upvotes the
poisoned response or downvotes the benign one. When feedback signals are used
in a subsequent preference tuning behavior, LMs exhibit increased probability
of producing poisoned responses even in contexts without malicious prompts. We
show that this attack can be used to (1) insert factual knowledge the model did
not previously possess, (2) modify code generation patterns in ways that
introduce exploitable security flaws, and (3) inject fake financial news. Our
finding both identifies a new qualitative feature of language model preference
tuning (showing that it even highly restricted forms of preference data can be
used to exert fine-grained control over behavior), and a new attack mechanism
for LMs trained with user feedback (extending work on pretraining-time data
poisoning and deployment-time prompt injection).

</details>


### [56] [MOTIF: Modular Thinking via Reinforcement Fine-tuning in LLMs](https://arxiv.org/abs/2507.02851)
*Purbesh Mitra,Sennur Ulukus*

Main category: cs.CL

TL;DR: 该文提出MOTIF方法，通过多轮强化学习拓展大模型推理的context范围。在有限训练样本下，在数学推理基准上相较传统方法取得3%以上提升，展示了更高的效率和性能。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型LLMs在推理能力上取得进展，通过GRPO强化学习算法能用更多思考token产生更优回应，但受限于模型自身context size，推理能力无法无限扩展。为突破这一局限，需要开发能跨多轮思考、具模块化推理能力的模型训练方法。

Method: 提出了一种新的强化学习微调方法MOTIF（Modular Thinking via Reinforcement Finetuning），让模型分多轮生成思考token，以突破单轮context size的限制。方法在Qwen2.5-3B-Instruct模型上，结合参数高效微调，并以GSM8K为训练集进行实验。

Result: 在MATH500和AIME2024两个基准测试上，模型性能分别比原生GRPO训练提高了3.8%和3.3%。更值得注意的是，该提升仅用15%的训练样本就达成，显示了MOTIF的样本效率优势。

Conclusion: MOTIF方法能提高大语言模型在有限context size条件下的推理能力，并显著提升样本使用效率，对长链条、多模块推理任务有较大潜力和实际意义。

Abstract: Recent advancements in the reasoning capabilities of large language models
(LLMs) show that employing group relative policy optimization (GRPO) algorithm
for reinforcement learning (RL) training allows the models to use more
thinking/reasoning tokens for generating better responses. However, LLMs can
generate only a finite amount of tokens while maintaining attention to the
previously generated tokens. This limit, also known as the context size of an
LLM, is a bottleneck in LLM reasoning with arbitrarily large number of tokens.
To think beyond the limit of context size, an LLM must employ a modular
thinking strategy to reason over multiple rounds. In this work, we propose
$\textbf{MOTIF: Modular Thinking via Reinforcement Finetuning}$ -- an RL
training method for generating thinking tokens in multiple rounds, effectively
allowing the model to think with additional context size. We trained the
open-source model Qwen2.5-3B-Instruct on GSM8K dataset via parameter efficient
fine-tuning and tested its accuracy on MATH500 and AIME2024 benchmarks. Our
experiments show 3.8\% and 3.3\% improvements over vanilla GRPO based training
in the respective benchmarks. Furthermore, this improvement was achieved with
only 15\% of samples, thus demonstrating sample efficiency of MOTIF. Our code
and models are available at https://github.com/purbeshmitra/MOTIF and
https://huggingface.co/purbeshmitra/MOTIF, respectively.

</details>


### [57] [Answer Matching Outperforms Multiple Choice for Language Model Evaluation](https://arxiv.org/abs/2507.02856)
*Nikhil Chandak,Shashwat Goel,Ameya Prabhu,Moritz Hardt,Jonas Geiping*

Main category: cs.CL

TL;DR: 文章指出当前主流的选择题评估方式存在严重限制，提出利用大模型进行答案匹配的生成式评估方法，比传统方式与人工评分更一致，强调应推动评估体系向新方法过渡。


<details>
  <summary>Details</summary>
Motivation: 传统的语言模型评估主要依赖选择题，因为选择题评分客观且容易自动化。但现有选择题基准往往可以不看问题本身，通过“捷径”选项直接作答，反映出判别式评估方式的根本局限性。

Method: 提出用生成式评估方法——答案匹配（answer matching）：不给模型备选项，由模型自由生成答案，再用大语言模型（参考标准答案）判断模型生成答案是否正确。通过在MMLU-Pro和GPQA-Diamond数据集上标注人工评分，比较各种评估方法与人工评分的一致性。

Result: 答案匹配方法（即使用小模型）与人工评分几乎完美一致，达到人工标注者之间的一致性水平。相比之下，选择题评估和“LLM-as-a-judge”方法（不用参考答案）与人工评分一致性较低。不同评估方法下模型排名可能会有重大变化。

Conclusion: 建议语言模型评估从单纯选择题机制转向答案匹配生成式评估，以更真实地反映模型表现和提升评测有效性。

Abstract: Multiple choice benchmarks have long been the workhorse of language model
evaluation because grading multiple choice is objective and easy to automate.
However, we show multiple choice questions from popular benchmarks can often be
answered without even seeing the question. These shortcuts arise from a
fundamental limitation of discriminative evaluation not shared by evaluations
of the model's free-form, generative answers. Until recently, there appeared to
be no viable, scalable alternative to multiple choice--but, we show that this
has changed. We consider generative evaluation via what we call answer
matching: Give the candidate model the question without the options, have it
generate a free-form response, then use a modern language model with the
reference answer to determine if the response matches the reference. To compare
the validity of different evaluation strategies, we annotate MMLU-Pro and
GPQA-Diamond to obtain human grading data, and measure the agreement of each
evaluation approach. We find answer matching using recent models--even small
ones--achieves near-perfect agreement, in the range of inter-annotator
agreement. In contrast, both multiple choice evaluation and using
LLM-as-a-judge without reference answers aligns poorly with human grading.
Improving evaluations via answer matching is not merely a conceptual concern:
the rankings of several models change significantly when evaluating their
free-form responses with answer matching. In light of these findings, we
discuss how to move the evaluation ecosystem from multiple choice to answer
matching.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [58] [Discovery of Fatigue Strength Models via Feature Engineering and automated eXplainable Machine Learning applied to the welded Transverse Stiffener](https://arxiv.org/abs/2507.02005)
*Michael A. Kraus,Helen Bartsch*

Main category: cs.CE

TL;DR: 该研究融合AutoML与XAI，通过专家与自动特征生成提升焊接钢结构疲劳强度预测的准确性与可解释性，证明了二者结合有助于生成工程上有效且可信的疲劳模型。


<details>
  <summary>Details</summary>
Motivation: 疲劳强度预测对钢结构安全至关重要，但现有数据驱动模型在准确性与可解释性之间往往存在权衡。本研究期望将AutoML和XAI融合，提升预测性能并兼顾解释性，促进工程应用。

Method: 结合专家驱动的特征工程和算法自动特征生成，采用AutoML（集成方法如CatBoost、LightGBM等）训练梯度提升、随机森林和神经网络模型，评估不同特征方案下模型性能，并用XAI（SHAP和特征重要性分析）解析关键影响因子。

Result: 集成方法表现最佳，专家特征模型（M2）在准确度和泛化性上平衡最优（全范围RMSE≈30.6MPa，R²≈0.78），简化特征模型（M1）鲁棒性好，复杂特征模型（M3）对泛化有损。XAI揭示主导变量为应力比、应力幅、屈服强度和后焊处理，几何尺寸为次要因子。

Conclusion: 将AutoML与XAI相结合可获得精确、可解释且鲁棒的钢结构疲劳强度模型，为AI辅助设计和评估提供有效手段。建议未来拓展至概率疲劳寿命建模和数字孪生集成。

Abstract: This research introduces a unified approach combining Automated Machine
Learning (AutoML) with Explainable Artificial Intelligence (XAI) to predict
fatigue strength in welded transverse stiffener details. It integrates
expert-driven feature engineering with algorithmic feature creation to enhance
accuracy and explainability.
  Based on the extensive fatigue test database regression models - gradient
boosting, random forests, and neural networks - were trained using AutoML under
three feature schemes: domain-informed, algorithmic, and combined. This allowed
a systematic comparison of expert-based versus automated feature selection.
  Ensemble methods (e.g. CatBoost, LightGBM) delivered top performance. The
domain-informed model $\mathcal M_2$ achieved the best balance: test RMSE
$\approx$ 30.6 MPa and $R^2 \approx 0.780% over the full $\Delta
\sigma_{c,50\%}$ range, and RMSE $\approx$ 13.4 MPa and $R^2 \approx 0.527%
within the engineering-relevant 0 - 150 MPa domain. The denser-feature model
($\mathcal M_3$) showed minor gains during training but poorer generalization,
while the simpler base-feature model ($\mathcal M_1$) performed comparably,
confirming the robustness of minimalist designs.
  XAI methods (SHAP and feature importance) identified stress ratio $R$, stress
range $\Delta \sigma_i$, yield strength $R_{eH}$, and post-weld treatment (TIG
dressing vs. as-welded) as dominant predictors. Secondary geometric factors -
plate width, throat thickness, stiffener height - also significantly affected
fatigue life.
  This framework demonstrates that integrating AutoML with XAI yields accurate,
interpretable, and robust fatigue strength models for welded steel structures.
It bridges data-driven modeling with engineering validation, enabling
AI-assisted design and assessment. Future work will explore probabilistic
fatigue life modeling and integration into digital twin environments.

</details>


### [59] [Time Resolution Independent Operator Learning](https://arxiv.org/abs/2507.02524)
*Diab W. Abueidda,Mbebo Nonna,Panos Pantidis,Mostafa E. Mobasher*

Main category: cs.CE

TL;DR: 本文提出NCDE-DeepONet，将神经控制微分方程融入DeepONet结构，显著提升对稀疏、不规则数据的时变PDE解算泛化能力，且支持对任意时空位置的即时预测，在多个物理问题上验证了其鲁棒性和高精度，为瞬态问题的算子学习提供新工具。


<details>
  <summary>Details</summary>
Motivation: 针对时变偏微分方程(PDEs)的解算器学习，目前由于数据稀疏和不规则使准确学习仍具挑战性。现有递归DeepONet方法受限于离散时间序列RNN结构，neural-ODE方法初始化后无法引入新输入。因此，迫切需要一种能处理不规则时空数据、并支持泛化查询的新方法。

Method: 提出NCDE-DeepONet，一种连续时间的算子网络。在分支网络中嵌入神经控制微分方程(NCDE)，用样条插值将观测输入路径转化为控制ODE，从而获得无关输入分辨率的表征，并且网络主干显式结合时空坐标，实现对任意时空位置的输出查询。该方法支持对训练中未见过的空间网格或时间步的泛化预测。

Result: 在瞬态Poisson方程、弹性动力学和热弹性问题上进行基准测试，NCDE-DeepONet展现出极强的鲁棒性和预测精度，能够几乎即时给出PDE解算结果。

Conclusion: 控制动力学为高保真、高效率的瞬态力学算子学习提供了有力且高效的理论基础。

Abstract: Accurately learning solution operators for time-dependent partial
differential equations (PDEs) from sparse and irregular data remains a
challenging task. Recurrent DeepONet extensions inherit the discrete-time
limitations of sequence-to-sequence (seq2seq) RNN architectures, while
neural-ODE surrogates cannot incorporate new inputs after initialization. We
introduce NCDE-DeepONet, a continuous-time operator network that embeds a
Neural Controlled Differential Equation (NCDE) in the branch and augments the
trunk with explicit space-time coordinates. The NCDE encodes an entire load
history as the solution of a controlled ODE driven by a spline-interpolated
input path, making the representation input-resolution-independent: it encodes
different input signal discretizations of the observed samples. The trunk then
probes this latent path at arbitrary spatial locations and times, rendering the
overall map output-resolution independent: predictions can be queried on meshes
and time steps unseen during training without retraining or interpolation.
Benchmarks on transient Poisson, elastodynamic, and thermoelastic problems
confirm the robustness and accuracy of the framework, achieving almost instant
solution prediction. These findings suggest that controlled dynamics provide a
principled and efficient foundation for high-fidelity operator learning in
transient mechanics.

</details>


### [60] [Imitation and Heterogeneity Shape the Resilience of Community Currency Networks](https://arxiv.org/abs/2507.02678)
*Camilla Ancona,Dora Ricci,Carmela Bernardo,Francesco Lo Iudice,Anton Proskurnikov,Francesco Vasca*

Main category: cs.CE

TL;DR: 本文以Sardex社区货币网络为例，利用图论方法对其三年内的结构演化和行为模式进行分析。发现用户偏好活跃同行且用户异质性连接增强了网络韧性，为社区货币系统的设计与管理提供了启示。


<details>
  <summary>Details</summary>
Motivation: 社区货币网络通过虚拟货币连接拥有共同特征的个人或公司，具有独特的经济活动模式。但这些系统的结构和动态属性尚未被充分理解，特别是在其演化、核心-边缘结构以及用户行为等方面存在研究空白。作者希望通过具体案例提升对互惠信用系统的理解。

Method: 以意大利撒丁岛的社区货币Sardex为案例，将其交易网络建模为有向加权图，采用图论方法分析强连通分量、凝聚表示与行为连接模式。重点考察三年内网络的核心与边缘结构演化，关注时间收缩、流动不对称及因用户类型不同造成的结构分化。

Result: 结果显示，该网络持续表现出与基于度的零模型的显著偏差，表明存在行为模仿（即用户倾向与活跃同行交互）。异质性用户之间的连接加强了网络拓扑结构，提高了整体韧性。

Conclusion: 社区货币网络结构受到用户行为和互联模式显著影响。用户间的模仿与多样性连接增强了网络核心、反抗碎片化并提升韧性，对于理解和完善此类系统具有重要意义。

Abstract: Community currency networks are made up of individuals and or companies that
share some physical or social characteristics and engage in economic
transactions using a virtual currency. This paper investigates the structural
and dynamic properties of such mutual credit systems through a case study of
Sardex, a community currency initiated and mainly operating in Sardinia, Italy.
The transaction network is modeled as a directed weighted graph and analyzed
through a graph theoretic framework focused on the analysis of strongly
connected components, condensed representations, and behavioral connectivity
patterns. Emphasis is placed on understanding the evolution of the network's
core and peripheral structures over a three year period, with attention to
temporal contraction, flow asymmetries, and structural fragmentation depending
on different user types. Our findings reveal persistent deviations from degree
based null models and suggest the presence of behavioral imitation,
specifically, a user preference for more active peers. We further assess the
impact of heterogeneous connections between different type of users, which
strengthen the network topology and enhance its resilience.

</details>


### [61] [Constraint-Guided Symbolic Regression for Data-Efficient Kinetic Model Discovery](https://arxiv.org/abs/2507.02730)
*Miguel Ángel de Carvalho Servia,Ilya Orson Sandoval,King Kuok,Hii,Klaus Hellgardt,Dongda Zhang,Ehecatl Antonio del Rio Chanona*

Main category: cs.CE

TL;DR: 作者提出PI-ADoK新框架，结合物理约束和先进的符号回归算法，在保证模型可解释性和物理一致性的前提下，提升了动力学建模准确性并减少实验量，对催化工业应用具有重要意义。


<details>
  <summary>Details</summary>
Motivation: 工业催化过程的设计和优化迫切需要可靠的动力学模型。传统机理模型依赖专家知识，数据驱动方法又常缺乏可解释性和物理一致性，因此存在明显的局限性。

Method: 提出了一种名为PI-ADoK（Physics-Informed Automated Discovery of Kinetics）的新框架，通过将物理约束直接整合到符号回归方法中，缩小模型搜索空间。同时，结合Metropolis-Hastings算法，进行参数不确定性的量化并给出可靠预测区间。

Result: 在多个催化案例中，PI-ADoK与传统方法进行对比，结果表明该方法提升了模型精度，并显著减少了实验需求。

Conclusion: PI-ADoK框架能够有效、可靠地发现化学反应动力学模型，同时降低实验负担，有助于催化过程的工业化应用。

Abstract: The industrialization of catalytic processes hinges on the availability of
reliable kinetic models for design, optimization, and control. Traditional
mechanistic models demand extensive domain expertise, while many data-driven
approaches often lack interpretability and fail to enforce physical
consistency. To overcome these limitations, we propose the Physics-Informed
Automated Discovery of Kinetics (PI-ADoK) framework. By integrating physical
constraints directly into a symbolic regression approach, PI-ADoK narrows the
search space and substantially reduces the number of experiments required for
model convergence. Additionally, the framework incorporates a robust
uncertainty quantification strategy via the Metropolis-Hastings algorithm,
which propagates parameter uncertainty to yield credible prediction intervals.
Benchmarking our method against conventional approaches across several
catalytic case studies demonstrates that PI-ADoK not only enhances model
fidelity but also lowers the experimental burden, highlighting its potential
for efficient and reliable kinetic model discovery in chemical reaction
engineering.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [62] [Computer Science Education in the Age of Generative AI](https://arxiv.org/abs/2507.02183)
*Russell Beale*

Main category: cs.CY

TL;DR: 本文分析了生成式AI（如ChatGPT）在计算机科学教育中的应用机遇与挑战，结合实例和实证研究，提出了课程整合、评估和政策建议，以期在保障教育质量和诚信的基础上充分利用AI潜力。


<details>
  <summary>Details</summary>
Motivation: 生成式AI工具（如ChatGPT和Codex）正对计算机科学教育产生深远影响，有必要系统梳理其带来的机遇、挑战及应对策略。

Method: 分析了生成式AI在编程教育中的具体应用场景，探讨了教育者应如何调整课程内容和教学方法，并基于实证数据和最新研究提出政策建议。

Result: AI工具能够辅助代码生成、调试及讲解，极大提升教学效率，并能推动教学创新与评估自动化。但也带来了学术诚信风险、过度依赖AI及原创性验证难题。提出了课程整合与评估实践建议，并给出平衡AI潜力和教育严谨性的政策建议。

Conclusion: 生成式AI为计算机科学教育带来巨大机遇，同时也伴随诸多挑战。合理利用AI并完善政策能最大化其积极作用，保持教育质量与诚信。

Abstract: Generative AI tools - most notably large language models (LLMs) like ChatGPT
and Codex - are rapidly revolutionizing computer science education. These tools
can generate, debug, and explain code, thereby transforming the landscape of
programming instruction. This paper examines the profound opportunities that AI
offers for enhancing computer science education in general, from coding
assistance to fostering innovative pedagogical practices and streamlining
assessments. At the same time, it highlights challenges including academic
integrity concerns, the risk of over-reliance on AI, and difficulties in
verifying originality. We discuss what computer science educators should teach
in the AI era, how to best integrate these technologies into curricula, and the
best practices for assessing student learning in an environment where AI can
generate code, prototypes and user feedback. Finally, we propose a set of
policy recommendations designed to harness the potential of generative AI while
preserving the integrity and rigour of computer science education. Empirical
data and emerging studies are used throughout to support our arguments.

</details>


### [63] [Defining DLT Immutability: A Qualitative Survey of Node Operators](https://arxiv.org/abs/2507.02413)
*Alex Lynham,Geoff Goodell*

Main category: cs.CY

TL;DR: 本文发现公有链的不可变性并非绝对，实际运行中经常受治理与共识影响。提出“实用不可变性”概念，以定性方式阐释其受治理结构约束的现实表现。


<details>
  <summary>Details</summary>
Motivation: 区块链系统以不可变性为核心设计目标，但实际上链上主观或被动的更改（rewrite）比预想的更为常见，且面临安全性风险。因此探究不可变性实际边界十分必要。

Method: 通过节点运营者的访谈，并采用主题分析的方法，分析不可变性在实际链上事件（如重写、攻击、极端事件）中的表现及局限。

Result: 文章提出了“实用不可变性（Practical Immutability）”的定性定义，指出链上不可变性依赖于合法治理的条件，即权益相关者对治理结构的信任与认可共同塑造账本状态的管理。

Conclusion: 完全不可变性在公有链上既不可能，也不符合现实观察，实际链上体现为基于治理合法性条件下的“实用不可变性”。

Abstract: Immutability is a core design goal of permissionless public blockchain
systems. However, rewrites are more common than is normally understood, and the
risk of rewrite, cyberattack, exploit or black swan event is also high. Taking
the position that strict immutability is neither possible on these networks nor
the observed reality, this paper uses thematic analysis of node operator
interviews to examine the limits of immutability in light of rewrite events.
The end result is a qualitative definition of the conditional immutability
found on these networks, which we call Practical Immutability. This is
immutability contingent on the legitimate governance demands of the network,
where network stakeholders place their trust in the governance topology of a
network to lend it legitimacy, and thus manage ledger state.

</details>


### [64] [Recourse, Repair, Reparation, & Prevention: A Stakeholder Analysis of AI Supply Chains](https://arxiv.org/abs/2507.02648)
*Aspen K. Hopkins,Isabella Struckman,Kevin Klyman,Susan S. Silbey*

Main category: cs.CY

TL;DR: 论文分析了AI供应链中的利益相关者、风险与权力结构，提出了危害应对类型学，并通过医疗行业案例展示，不同市场结构下的补救方式受利益相关者地位与权力的影响。


<details>
  <summary>Details</summary>
Motivation: 当前人工智能系统的开发与分发过程中，由于AI供应链复杂、参与方多元、缺乏传统供应链的良好治理机制，导致很难有效识别与纠正风险或危害，因此亟需结构化分析与设计出更负责任的AI供应链治理方案。

Method: 通过利益相关者分析，探讨了AI供应链中谁是参与者、面临哪些风险、损害源自何处，以及市场动力与权力如何影响补救方式。并设计了一套危害应对类型学（申诉、修复、赔偿或预防），并将其应用于医疗健康AI供应链中的三种市场结构案例分析，以说明利益相关者的地位如何影响应对方式。

Result: 提出了AI供应链危害应对类型学，并通过医疗健康领域的三个市场结构案例，说明了利益相关方在不同地位和权力下所采取的不同补救措施，为设计和管理AI供应链提供了理论和实证参考。

Conclusion: AI供应链（AISC）的设计与管理需要系统化的方法来应对其带来的风险和危害，尤其应考虑到不同参与方的地位与权力对补救手段的影响。针对AISC诱发的危害，作者提出了应对的类型学，以促进更负责任的AI系统部署。

Abstract: The AI industry is exploding in popularity, with increasing attention to
potential harms and unwanted consequences. In the current digital ecosystem, AI
deployments are often the product of AI supply chains (AISC): networks of
outsourced models, data, and tooling through which multiple entities contribute
to AI development and distribution. AI supply chains lack the modularity,
redundancies, or conventional supply chain practices that enable
identification, isolation, and easy correction of failures, exacerbating the
already difficult processes of responding to ML-generated harms. As the
stakeholders participating in and impacted by AISCs have scaled and
diversified, so too have the risks they face. In this stakeholder analysis of
AI supply chains, we consider who participates in AISCs, what harms they face,
where sources of harm lie, and how market dynamics and power differentials
inform the type and probability of remedies. Because AI supply chains are
purposely invented and implemented, they may be designed to account for, rather
than ignore, the complexities, consequences, and risks of deploying AI systems.
To enable responsible design and management of AISCs, we offer a typology of
responses to AISC-induced harms: recourse, repair, reparation or prevention. We
apply this typology to stakeholders participating in a health-care AISC across
three stylized markets $\unicode{x2013}$ vertical integration, horizontal
integration, free market $\unicode{x2013}$ to illustrate how stakeholder
positioning and power within an AISC may shape responses to an experienced
harm.

</details>


<div id='q-fin.TR'></div>

# q-fin.TR [[Back]](#toc)

### [65] [A Midsummer Meme's Dream: Investigating Market Manipulations in the Meme Coin Ecosystem](https://arxiv.org/abs/2507.01963)
*Alberto Maria Mongardini,Alessandro Mei*

Main category: q-fin.TR

TL;DR: 作者跨链分析3.5万个meme币，发现涨幅过百的大部分币都被团队用各种手法操控，暴涨往往是幕后团队造势，投资风险极高，需警惕被收割。


<details>
  <summary>Details</summary>
Motivation: meme币在加密货币市场中越来越流行，但其价值主要受社区情绪驱动，缺乏实际应用基础，因此格外容易受市场操纵。作者希望全面揭示meme币生态（跨链、跨平台）中的操控行为和陷阱。

Method: 作者对以太坊、BNB智能链、Solana和Base四条链上共34,988个meme币进行横跨链的特征分析，并追踪其三个月内的成长轨迹。采用量化分析方法，识别并分析令币异常增长的操纵手法如刷量交易、流动性池价格膨胀等手段，并统计“爆款”币的相关特征。

Result: 在回报率超过100%的高收益meme币中，有82.6%存在明显的人工操纵迹象，包括刷量、流动性池人为抬价。还发现了包括拉高出货（pump and dump）和跑路（rug pull）等明显的投资者剥削行为。这些币在暴涨前往往已经历过前述操纵手法，显示出初期造势为后续收割埋下伏笔。

Conclusion: meme币生态中操纵行为极其普遍，特别是在涨幅巨大的币种中。所谓的高回报更多是受控团队有组织操控的结果，而非自然市场行为。投资者需高度警惕这些高风险现象。

Abstract: From viral jokes to a billion-dollar phenomenon, meme coins have become one
of the most popular segments in cryptocurrency markets. Unlike utility-focused
crypto assets like Bitcoin or Ethereum, meme coins derive value primarily from
community sentiment, making them vulnerable to manipulation. This study
presents a cross-chain analysis of the meme coin ecosystem, examining 34,988
tokens across Ethereum, BNB Smart Chain, Solana, and Base. We characterize the
tokenomics of meme coins and track their growth in a three-month longitudinal
analysis. We discover that among high-return tokens (>100%), an alarming 82.6%
show evidence of extensive use of artificial growth strategies designed to
create a misleading appearance of market interest. These include wash trading
and a form of manipulation we define as Liquidity Pool-Based Price Inflation
(LPI), where small strategic purchases trigger dramatic price increases. We
also find evidence of schemes designed to profit at the expense of investors,
such as pump and dumps and rug pulls. In particular, most of the tokens
involved had previously experienced wash trading or LPI, indicating how initial
manipulations often set the stage for later exploitation. These findings reveal
that manipulations are widespread among high-performing meme coins and suggest
that their dramatic gains are often likely driven by coordinated efforts rather
than natural market dynamics.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [66] [FinAI-BERT: A Transformer-Based Model for Sentence-Level Detection of AI Disclosures in Financial Reports](https://arxiv.org/abs/2507.01991)
*Muhammad Bilal Zafar*

Main category: q-fin.CP

TL;DR: 本文提出了专为金融文本设计的AI披露识别模型FinAI-BERT，其在句子级分类效果极佳（准确率99.37%），并具备良好可解释性和鲁棒性，有助于金融AI监管与研究。


<details>
  <summary>Details</summary>
Motivation: 现有检测金融文件AI相关披露的方法大多基于关键词或文档级分类，难以实现细粒度、可解释和高鲁棒性的需求。论文旨在提出一种更精细、透明且稳健的解决方案。

Method: 采用基于transformer结构的语言模型（在金融领域目标域自适应的BERT），在手工标注并平衡的数据集（1586句美国产银行年报）上微调，并结合SHAP进行可解释性分析以及偏差与鲁棒性检测。

Result: FinAI-BERT模型在句级分类任务中表现出近乎完美的性能（准确率99.37%，F1为0.993），优于传统机器学习方法，且可解释性和鲁棒性均经过多维验证。

Conclusion: 本文提出的FinAI-BERT模型能够高精度、可解释地识别金融文本中与AI相关的内容，并在实际与理论角度均有显著贡献。

Abstract: The proliferation of artificial intelligence (AI) in financial services has
prompted growing demand for tools that can systematically detect AI-related
disclosures in corporate filings. While prior approaches often rely on keyword
expansion or document-level classification, they fall short in granularity,
interpretability, and robustness. This study introduces FinAI-BERT, a
domain-adapted transformer-based language model designed to classify AI-related
content at the sentence level within financial texts. The model was fine-tuned
on a manually curated and balanced dataset of 1,586 sentences drawn from 669
annual reports of U.S. banks (2015 to 2023). FinAI-BERT achieved near-perfect
classification performance (accuracy of 99.37 percent, F1 score of 0.993),
outperforming traditional baselines such as Logistic Regression, Naive Bayes,
Random Forest, and XGBoost. Interpretability was ensured through SHAP-based
token attribution, while bias analysis and robustness checks confirmed the
model's stability across sentence lengths, adversarial inputs, and temporal
samples. Theoretically, the study advances financial NLP by operationalizing
fine-grained, theme-specific classification using transformer architectures.
Practically, it offers a scalable, transparent solution for analysts,
regulators, and scholars seeking to monitor the diffusion and framing of AI
across financial institutions.

</details>
