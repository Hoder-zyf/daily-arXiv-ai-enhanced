{"id": "2507.01051", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01051", "abs": "https://arxiv.org/abs/2507.01051", "authors": ["Giada Pistilli", "Bruna Trevelin"], "title": "Can AI be Consentful?", "comment": null, "summary": "The evolution of generative AI systems exposes the challenges of traditional\nlegal and ethical frameworks built around consent. This chapter examines how\nthe conventional notion of consent, while fundamental to data protection and\nprivacy rights, proves insufficient in addressing the implications of\nAI-generated content derived from personal data. Through legal and ethical\nanalysis, we show that while individuals can consent to the initial use of\ntheir data for AI training, they cannot meaningfully consent to the numerous\npotential outputs their data might enable or the extent to which the output is\nused or distributed. We identify three fundamental challenges: the scope\nproblem, the temporality problem, and the autonomy trap, which collectively\ncreate what we term a ''consent gap'' in AI systems and their surrounding\necosystem. We argue that current legal frameworks inadequately address these\nemerging challenges, particularly regarding individual autonomy, identity\nrights, and social responsibility, especially in cases where AI-generated\ncontent creates new forms of personal representation beyond the scope of the\noriginal consent. By examining how these consent limitations intersect with\nbroader principles of responsible AI (including fairness, transparency,\naccountability, and autonomy) we demonstrate the need to evolve ethical and\nlegal approaches to consent.", "AI": {"tldr": "\u751f\u6210\u5f0fAI\u8ba9\u6570\u636e\u540c\u610f\u673a\u5236\u5931\u6548\uff0c\u5e26\u6765\u8303\u56f4\u3001\u65f6\u6548\u4e0e\u81ea\u4e3b\u6743\u4e09\u5927\u95ee\u9898\uff1b\u4f20\u7edf\u6cd5\u5f8b\u65e0\u6cd5\u4fdd\u62a4AI\u751f\u6210\u5185\u5bb9\u4e0b\u7684\u4e2a\u4eba\u6743\u76ca\uff0c\u9700\u6cd5\u5f8b\u4e0e\u4f26\u7406\u6846\u67b6\u521b\u65b0\u3002", "motivation": "\u751f\u6210\u5f0fAI\u6280\u672f\u7684\u53d1\u5c55\uff0c\u4f7f\u4f20\u7edf\u57fa\u4e8e\u540c\u610f\u7684\u6570\u636e\u4fdd\u62a4\u548c\u9690\u79c1\u6743\u601d\u8def\u9047\u5230\u4e25\u5cfb\u6311\u6218\uff0c\u73b0\u6709\u540c\u610f\u673a\u5236\u96be\u4ee5\u5e94\u5bf9AI\u751f\u6210\u5185\u5bb9\u7684\u4e0d\u786e\u5b9a\u6027\u548c\u6269\u6563\u6027\uff0c\u56e0\u6b64\u6709\u5fc5\u8981\u91cd\u65b0\u5ba1\u89c6\u540c\u610f\u5728AI\u8bed\u5883\u4e0b\u7684\u9002\u7528\u6027\u3002", "method": "\u91c7\u7528\u6cd5\u5f8b\u548c\u4f26\u7406\u5206\u6790\u65b9\u6cd5\uff0c\u68b3\u7406\u4e2a\u4eba\u6570\u636e\u88abAI\u7528\u4e8e\u751f\u6210\u5185\u5bb9\u65f6\u540c\u610f\u673a\u5236\u9762\u4e34\u7684\u95ee\u9898\uff0c\u5e76\u7ed3\u5408AI\u8d23\u4efb\u6cbb\u7406\u539f\u5219\u8fdb\u884c\u900f\u89c6\u548c\u8ba8\u8bba\u3002", "result": "\u4f5c\u8005\u8bc6\u522b\u51fa\u4e09\u5927\u540c\u610f\u6311\u6218\uff1a\u8303\u56f4\u95ee\u9898\u3001\u65f6\u6548\u95ee\u9898\u3001\u81ea\u4e3b\u6743\u9677\u9631\uff0c\u5f62\u6210\u6240\u8c13\u201c\u540c\u610f\u7f3a\u53e3\u201d\uff1b\u5e76\u6307\u51fa\u73b0\u884c\u6cd5\u5f8b\u672a\u80fd\u6709\u6548\u89e3\u51b3\u4e2a\u4eba\u81ea\u4e3b\u6743\u3001\u8eab\u4efd\u6743\u548c\u793e\u4f1a\u8d23\u4efb\u7b49\u65b0\u5174\u98ce\u9669\uff0c\u547c\u5401\u5b8c\u5584AI\u540c\u610f\u6cbb\u7406\u4f53\u7cfb\u4ee5\u9002\u5e94\u6280\u672f\u53d8\u9769\u3002", "conclusion": "\u76ee\u524d\u6cd5\u5f8b\u4e0e\u4f26\u7406\u6846\u67b6\u5173\u4e8e\u540c\u610f\uff08consent\uff09\u7684\u89c4\u5b9a\u65e0\u6cd5\u5145\u5206\u5e94\u5bf9AI\u751f\u6210\u5185\u5bb9\u5bf9\u4e2a\u4eba\u6743\u76ca\u4ea7\u751f\u7684\u65b0\u6311\u6218\uff0c\u4e9f\u9700\u66f4\u65b0\u76f8\u5173\u7406\u5ff5\u4e0e\u6cd5\u89c4\u3002"}}
{"id": "2507.01061", "categories": ["cs.CY", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.01061", "abs": "https://arxiv.org/abs/2507.01061", "authors": ["Jingjing Qu", "Kejia Hu", "Jun Zhu", "Wenhao Li", "Teng Wang", "Zhiyun Chen", "Yulei Ye", "Chaochao Lu", "Aimin Zhou", "Xiangfeng Wang", "James Evan"], "title": "Epitome: Pioneering an Experimental Platform for AI-Social Science Integration", "comment": "18 pages, 5figures", "summary": "The integration of Large Language Models (LLMs) into social science\nexperiments represents a transformative approach to understanding human-AI\ninteractions and their societal impacts. We introduce Epitome, the world's\nfirst open experimental platform dedicated to the deep integration of\nartificial intelligence and social science. Rooted in theoretical foundations\nfrom management, communication studies, sociology, psychology, and ethics,\nEpitome focuses on the interactive impacts of AI on individuals, organizations,\nand society during its real-world deployment. It constructs a theoretical\nsupport system through cross-disciplinary experiments. The platform offers a\none-stop comprehensive experimental solution spanning \"foundation\nmodels-complex application development-user feedback\" through seven core\nmodules, while embedding the classical \"control-comparison-comparative causal\nlogic\" of social science experiments into multilevel human-computer interaction\nenvironments, including dialogues, group chats, and multi-agent virtual\nscenarios. With its canvas-style, user-friendly interface, Epitome enables\nresearchers to easily design and run complex experimental scenarios,\nfacilitating systematic investigations into the social impacts of AI and\nexploration of integrated solutions.To demonstrate its capabilities, we\nreplicated three seminal social science experiments involving LLMs, showcasing\nEpitome's potential to streamline complex experimental designs and produce\nrobust results, suitable for publishing in the top selective journals. Our\nfindings highlight the platform's utility in enhancing the efficiency and\nquality of human-AI interactions, providing valuable insights into the societal\nimplications of AI technologies. Epitome thus offers a powerful tool for\nadvancing interdisciplinary research at the intersection of AI and social\nscience, with potential applications in policy-making, ...", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5e76\u5b9e\u73b0\u4e86\u4e16\u754c\u9996\u4e2aAI\u4e0e\u793e\u4f1a\u79d1\u5b66\u6df1\u5ea6\u878d\u5408\u5b9e\u9a8c\u5e73\u53f0Epitome\uff0c\u5e73\u53f0\u80fd\u591f\u7cfb\u7edf\u652f\u6491\u591a\u5c42\u6b21\u4eba-\u673a\u4ea4\u4e92\u5b9e\u9a8c\uff0c\u901a\u8fc7\u590d\u73b0\u7ecf\u5178\u5b9e\u9a8c\u5c55\u793a\u5176\u5b9e\u7528\u6027\u548c\u7406\u8bba\u4ef7\u503c\uff0c\u4e3a\u793e\u4f1a\u79d1\u5b66\u7814\u7a76AI\u5f71\u54cd\u63d0\u4f9b\u4e86\u521b\u65b0\u5de5\u5177\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u9010\u6b65\u6df1\u5ea6\u878d\u5165\u793e\u4f1a\u79d1\u5b66\u5b9e\u9a8c\uff0c\u4f46\u7f3a\u4e4f\u7cfb\u7edf\u7684\u5b9e\u9a8c\u5e73\u53f0\u6765\u652f\u6301\u8de8\u5b66\u79d1\u591a\u5c42\u6b21\u7684\u4eba\u673a\u4ea4\u4e92\u7814\u7a76\uff0c\u56e0\u6b64\u4e9f\u9700\u521b\u65b0\u7684\u7efc\u5408\u6027\u5b9e\u9a8c\u5e73\u53f0\u63a8\u52a8AI\u4e0e\u793e\u4f1a\u79d1\u5b66\u7684\u5b9e\u9645\u878d\u5408\u53ca\u793e\u4f1a\u5f71\u54cd\u8bc4\u4f30\u3002", "method": "\u63d0\u51fa\u5e76\u5b9e\u73b0\u4e86Epitome\u5e73\u53f0\uff0c\u7ed3\u5408\u7ba1\u7406\u5b66\u3001\u4f20\u64ad\u5b66\u3001\u793e\u4f1a\u5b66\u3001\u5fc3\u7406\u5b66\u53ca\u4f26\u7406\u5b66\u7b49\u7406\u8bba\uff0c\u901a\u8fc7\u8de8\u5b66\u79d1\u5b9e\u9a8c\u6784\u5efa\u7406\u8bba\u652f\u6301\u4f53\u7cfb\uff0c\u5e73\u53f0\u4ee5\u4e03\u5927\u6a21\u5757\u6db5\u76d6\u57fa\u7840\u6a21\u578b\u3001\u590d\u6742\u5e94\u7528\u5f00\u53d1\u548c\u7528\u6237\u53cd\u9988\uff0c\u5e76\u5185\u5d4c\u793e\u4f1a\u79d1\u5b66\u7ecf\u5178\u63a7\u5236-\u6bd4\u8f83-\u56e0\u679c\u903b\u8f91\uff0c\u652f\u6301\u591a\u573a\u666f\u591a\u5c42\u6b21\u4eba\u673a\u4ea4\u4e92\u5b9e\u9a8c\u3002\u901a\u8fc7\u590d\u73b0\u4e09\u9879\u7ecf\u5178\u5b9e\u9a8c\uff0c\u5c55\u793a\u5e73\u53f0\u80fd\u529b\u3002", "result": "\u5e73\u53f0\u6210\u529f\u590d\u73b0\u4e86\u4e09\u9879\u5177\u6709\u4ee3\u8868\u6027\u7684\u793e\u4f1a\u79d1\u5b66\u5b9e\u9a8c\uff0c\u5c55\u793a\u4e86Epitome\u5bf9\u4e8e\u9ad8\u6548\u8bbe\u8ba1\u3001\u7ec4\u7ec7\u548c\u6267\u884c\u590d\u6742\u5b9e\u9a8c\u7684\u80fd\u529b\uff0c\u4e3aAI\u793e\u4f1a\u5f71\u54cd\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u65b9\u6cd5\u548c\u5de5\u5177\uff0c\u7ed3\u679c\u5177\u5907\u53d1\u8868\u9ad8\u6c34\u5e73\u671f\u520a\u7684\u6f5c\u529b\u3002", "conclusion": "Epitome\u5e73\u53f0\u6781\u5927\u63d0\u5347\u4e86AI\u4e0e\u793e\u4f1a\u79d1\u5b66\u878d\u5408\u7814\u7a76\u7684\u6548\u7387\u4e0e\u8d28\u91cf\uff0c\u662f\u63a8\u52a8AI\u793e\u4f1a\u6548\u5e94\u7406\u89e3\u548c\u653f\u7b56\u5236\u5b9a\u7684\u6709\u529b\u5de5\u5177\uff0c\u52a9\u529b\u8de8\u5b66\u79d1\u7406\u8bba\u4e0e\u5e94\u7528\u521b\u65b0\u3002"}}
{"id": "2507.01062", "categories": ["cs.CY", "cs.AI", "62P25", "K.3.1; H.5.2"], "pdf": "https://arxiv.org/pdf/2507.01062", "abs": "https://arxiv.org/abs/2507.01062", "authors": ["Seyma Yaman Kayadibi"], "title": "Quantifying Student Success with Generative AI: A Monte Carlo Simulation Informed by Systematic Review", "comment": "35 pages, 4 figures. All figures are image-based: one Python code\n  screenshot, one regression model output, one success score distribution\n  chart, and one PRISMA diagram. This article presents a standalone segment\n  from the author's master's thesis at Victoria University", "summary": "The exponential development of generative artificial intelligence (GenAI)\ntechnologies like ChatGPT has raised increasing curiosity about their use in\nhigher education, specifically with respect to how students view them, make use\nof them, and the implications for learning outcomes. This paper employs a\nhybrid methodological approach involving a systematic literature review and\nsimulation-based modeling to explore student perceptions of GenAI use in the\ncontext of higher education. A total of nineteen empirical articles from 2023\nthrough 2025 were selected from the PRISMA-based search targeting the Scopus\ndatabase. Synthesis of emerging patterns from the literature was achieved by\nthematic categorization. Six of these had enough quantitative information,\ni.e., item-level means and standard deviations, to permit probabilistic\nmodeling. One dataset, from the resulting subset, was itself selected as a\nrepresentative case with which to illustrate inverse-variance weighting by\nMonte Carlo simulation, by virtue of its well-designed Likert scale format and\nthematic alignment with the use of computing systems by the researcher.\n  The simulation provided a composite \"Success Score\" forecasting the strength\nof the relationship between student perceptions and learning achievements.\nFindings reveal that attitude factors concerned with usability and real-world\nusefulness are significantly better predictors of positive learning achievement\nthan affective or trust-based factors. Such an interdisciplinary perspective\nprovides a unique means of linking thematic results with predictive modelling,\nresonating with longstanding controversies about the proper use of GenAI tools\nwithin the university.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u7cfb\u7edf\u6587\u732e\u7efc\u8ff0\u548c\u6a21\u62df\u5efa\u6a21\uff0c\u53d1\u73b0\u5927\u5b66\u751f\u5bf9\u4eba\u5de5\u667a\u80fd\u5de5\u5177\u7684\u53ef\u7528\u6027\u4e0e\u73b0\u5b9e\u7528\u9014\u7684\u6001\u5ea6\u5bf9\u5b66\u4e60\u6210\u6548\u63d0\u5347\u4f5c\u7528\u663e\u8457\uff0c\u5efa\u8bae\u5173\u6ce8\u5b9e\u9645\u5e94\u7528\u800c\u975e\u5355\u7eaf\u4fe1\u4efb\u6216\u60c5\u611f\u56e0\u7d20\u3002", "motivation": "\u8fd1\u5e74\u6765\uff0c\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\uff08\u5982ChatGPT\uff09\u8fc5\u901f\u53d1\u5c55\uff0c\u5f15\u53d1\u4e86\u4eba\u4eec\u5bf9\u4e8e\u5176\u5728\u9ad8\u7b49\u6559\u80b2\u4e2d\u5e94\u7528\u7684\u5174\u8da3\uff0c\u5c24\u5176\u662f\u5b66\u751f\u5982\u4f55\u770b\u5f85\u5e76\u4f7f\u7528\u8fd9\u4e9b\u6280\u672f\u53ca\u5176\u5bf9\u5b66\u4e60\u6548\u679c\u7684\u5f71\u54cd\u3002", "method": "\u91c7\u7528\u6df7\u5408\u65b9\u6cd5\uff0c\u5305\u62ec\u7cfb\u7edf\u6027\u6587\u732e\u7efc\u8ff0\u548c\u57fa\u4e8e\u6a21\u62df\u7684\u5efa\u6a21\uff0c\u4eceScopus\u6570\u636e\u5e93\u4e2d\u7b5b\u9009\u4e862023-2025\u5e74\u95f4\u768419\u7bc7\u5b9e\u8bc1\u7814\u7a76\uff0c\u5e76\u901a\u8fc7\u4e3b\u9898\u5206\u7c7b\u7efc\u5408\u6587\u732e\u4e2d\u7684\u6a21\u5f0f\uff0c\u5bf9\u5b9a\u91cf\u4fe1\u606f\u8fdb\u884c\u6982\u7387\u5efa\u6a21\uff0c\u5e76\u5229\u7528Monte Carlo\u6a21\u62df\u5bf9\u5178\u578b\u6570\u636e\u96c6\u8fdb\u884c\u9006\u65b9\u5dee\u52a0\u6743\u5206\u6790\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u4e0e\u60c5\u611f\u6216\u4fe1\u4efb\u76f8\u5173\u56e0\u7d20\u76f8\u6bd4\uff0c\u5173\u6ce8\u53ef\u7528\u6027\u4e0e\u5b9e\u9645\u7528\u9014\u7684\u6001\u5ea6\u56e0\u7d20\u80fd\u66f4\u597d\u5730\u9884\u6d4b\u5b66\u751f\u5b66\u4e60\u6210\u5c31\u7684\u63d0\u5347\u3002", "conclusion": "\u9ad8\u7b49\u6559\u80b2\u9886\u57df\u4e2d\uff0c\u5b66\u751f\u5bf9\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u7684\u53ef\u7528\u6027\u53ca\u5176\u73b0\u5b9e\u5e94\u7528\u7684\u79ef\u6781\u770b\u6cd5\uff0c\u662f\u63d0\u5347\u5b66\u4e60\u6210\u679c\u7684\u91cd\u8981\u9884\u6d4b\u56e0\u5b50\uff0c\u63a8\u52a8\u4e86\u5bf9\u5927\u5b66\u4e2d\u5982\u4f55\u5408\u7406\u5229\u7528GenAI\u5de5\u5177\u7684\u8ba8\u8bba\u3002"}}
{"id": "2507.01304", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2507.01304", "abs": "https://arxiv.org/abs/2507.01304", "authors": ["Ion Nemteanu", "Adir Mancebo Jr.", "Leslie Joe", "Ryan Lopez", "Patricia Lopez", "Warren Woodrich Pettine"], "title": "A Practical SAFE-AI Framework for Small and Medium-Sized Enterprises Developing Medical Artificial Intelligence Ethics Policies", "comment": "31 pages, two figures", "summary": "Artificial intelligence (AI) offers incredible possibilities for patient\ncare, but raises significant ethical issues, such as the potential for bias.\nPowerful ethical frameworks exist to minimize these issues, but are often\ndeveloped for academic or regulatory environments and tend to be comprehensive\nbut overly prescriptive, making them difficult to operationalize within\nfast-paced, resource-constrained environments. We introduce the Scalable Agile\nFramework for Execution in AI (SAFE-AI) designed to balance ethical rigor with\nbusiness priorities by embedding ethical oversight into standard Agile-based\nproduct development workflows. The framework emphasizes the early establishment\nof testable acceptance criteria, fairness metrics, and transparency metrics to\nmanage model uncertainty, while also promoting continuous monitoring and\nre-evaluation of these metrics across the AI lifecycle. A core component of\nthis framework are responsibility metrics using scenario-based probability\nanalogy mapping designed to enhance transparency and stakeholder trust. This\nensures that retraining or tuning activities are subject to lightweight but\nmeaningful ethical review. By focusing on the minimum necessary requirements\nfor responsible development, our framework offers a scalable, business-aligned\napproach to ethical AI suitable for organizations without dedicated ethics\nteams.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u73b0\u6709AI\u4f26\u7406\u6846\u67b6\u96be\u4ee5\u843d\u5730\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u5c06\u4f26\u7406\u673a\u5236\u878d\u5165\u654f\u6377\u5f00\u53d1\u6d41\u7a0b\u7684SAFE-AI\u6846\u67b6\uff0c\u517c\u987e\u4f01\u4e1a\u6548\u7387\u4e0e\u4f26\u7406\u76d1\u7763\uff0c\u5b9e\u73b0\u53ef\u6301\u7eed\u3001\u53ef\u6269\u5c55\u7684\u4f26\u7406AI\u5f00\u53d1\u3002", "motivation": "\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u5728\u533b\u7597\u9886\u57df\u5e26\u6765\u5de8\u5927\u6f5c\u529b\u7684\u540c\u65f6\uff0c\u4e5f\u5f15\u53d1\u4e86\u5982\u504f\u89c1\u7b49\u91cd\u8981\u4f26\u7406\u95ee\u9898\u3002\u73b0\u6709\u7684\u4f26\u7406\u6846\u67b6\u867d\u7136\u4e25\u8c28\uff0c\u4f46\u5f80\u5f80\u8fc7\u4e8e\u590d\u6742\uff0c\u96be\u4ee5\u5728\u8282\u594f\u5feb\u3001\u8d44\u6e90\u6709\u9650\u7684\u4e1a\u52a1\u73af\u5883\u4e2d\u843d\u5730\u3002\u56e0\u6b64\u6025\u9700\u4e00\u4e2a\u517c\u987e\u4f26\u7406\u6027\u4e0e\u4e1a\u52a1\u9700\u6c42\u3001\u80fd\u591f\u9ad8\u6548\u6267\u884c\u7684\u6846\u67b6\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u654f\u6377AI\u6267\u884c\u4f26\u7406\u6846\u67b6\uff08SAFE-AI\uff09\uff0c\u5c06\u4f26\u7406\u76d1\u7763\u5d4c\u5165\u5230\u57fa\u4e8e\u654f\u6377\u7684\u4ea7\u54c1\u5f00\u53d1\u6d41\u7a0b\u4e2d\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u8bbe\u8ba1\u53ef\u6d4b\u8bd5\u7684\u9a8c\u6536\u6807\u51c6\u3001\u516c\u5e73\u6027\u4e0e\u900f\u660e\u6027\u6307\u6807\uff0c\u6301\u7eed\u76d1\u63a7AI\u751f\u547d\u5468\u671f\uff0c\u91c7\u7528\u57fa\u4e8e\u60c5\u666f\u6982\u7387\u7c7b\u6bd4\u6620\u5c04\u7684\u8d23\u4efb\u6307\u6807\uff0c\u5e76\u5bf9\u6a21\u578b\u518d\u8bad\u7ec3\u548c\u8c03\u6574\u65bd\u52a0\u8f7b\u91cf\u4f46\u6709\u6548\u7684\u4f26\u7406\u5ba1\u67e5\u3002", "result": "SAFE-AI\u6846\u67b6\u80fd\u591f\u7528\u6700\u5c0f\u7684\u5fc5\u8981\u8981\u6c42\uff0c\u63a8\u52a8\u8d1f\u8d23\u4efb\u7684AI\u5f00\u53d1\u3002\u5b83\u65e2\u80fd\u5951\u5408\u4f01\u4e1a\u5b9e\u9645\uff0c\u4e5f\u9002\u7528\u4e8e\u672a\u8bbe\u6709\u4e13\u95e8\u4f26\u7406\u56e2\u961f\u7684\u7ec4\u7ec7\uff0c\u8fd8\u80fd\u63d0\u5347\u4f26\u7406\u900f\u660e\u5ea6\u548c\u5229\u76ca\u76f8\u5173\u65b9\u7684\u4fe1\u4efb\u3002", "conclusion": "SAFE-AI\u6210\u529f\u5b9e\u73b0\u4e86\u4f26\u7406\u4e0e\u654f\u6377\u5f00\u53d1\u7684\u878d\u5408\uff0c\u4e3a\u8d44\u6e90\u6709\u9650\u7684\u7ec4\u7ec7\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u4e14\u4e1a\u52a1\u53cb\u597d\u7684\u4f26\u7406AI\u5f00\u53d1\u6a21\u5f0f\u3002\u5176\u6838\u5fc3\u673a\u5236\u4e3a\u65e9\u671f\u5efa\u7acb\u5ea6\u91cf\u6807\u51c6\u53ca\u6301\u7eed\u8fed\u4ee3\uff0c\u786e\u4fdd\u5f00\u53d1\u4e0e\u4f26\u7406\u5e76\u91cd\u3002"}}
{"id": "2507.01025", "categories": ["cs.CE", "cs.AI", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2507.01025", "abs": "https://arxiv.org/abs/2507.01025", "authors": ["Yutong Lu", "Dan Huang", "Pin Chen"], "title": "HPC-AI Coupling Methodology for Scientific Applications", "comment": "14 pages, 11 figures", "summary": "Artificial intelligence (AI) technologies have fundamentally transformed\nnumerical-based high-performance computing (HPC) applications with data-driven\napproaches and endeavored to address existing challenges, e.g. high\ncomputational intensity, in various scientific domains. In this study, we\nexplore the scenarios of coupling HPC and AI (HPC-AI) in the context of\nemerging scientific applications, presenting a novel methodology that\nincorporates three patterns of coupling: surrogate, directive, and coordinate.\nEach pattern exemplifies a distinct coupling strategy, AI-driven prerequisite,\nand typical HPC-AI ensembles. Through case studies in materials science, we\ndemonstrate the application and effectiveness of these patterns. The study\nhighlights technical challenges, performance improvements, and implementation\ndetails, providing insight into promising perspectives of HPC-AI coupling. The\nproposed coupling patterns are applicable not only to materials science but\nalso to other scientific domains, offering valuable guidance for future HPC-AI\nensembles in scientific discovery.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e09\u79cdHPC\u4e0eAI\u7684\u8026\u5408\u6a21\u5f0f\uff0c\u5e76\u901a\u8fc7\u6750\u6599\u79d1\u5b66\u6848\u4f8b\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002\u7814\u7a76\u4e3a\u79d1\u5b66\u9886\u57dfHPC-AI\u7ed3\u5408\u63d0\u4f9b\u4e86\u5b9e\u8df5\u6307\u5bfc\u548c\u672a\u6765\u5c55\u671b\u3002", "motivation": "\u4eba\u5de5\u667a\u80fd\u6280\u672f\u4e3a\u9ad8\u6027\u80fd\u8ba1\u7b97\uff08HPC\uff09\u5e94\u7528\u5e26\u6765\u4e86\u6570\u636e\u9a71\u52a8\u7684\u65b0\u65b9\u6cd5\uff0c\u8bd5\u56fe\u89e3\u51b3\u4f20\u7edf\u6570\u503c\u578bHPC\u5728\u5404\u79cd\u79d1\u5b66\u9886\u57df\u4e2d\u9762\u4e34\u7684\u9ad8\u8ba1\u7b97\u5f3a\u5ea6\u7b49\u6311\u6218\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22HPC\u4e0eAI\u7ed3\u5408\u7684\u65b0\u6a21\u5f0f\uff0c\u4ee5\u63a8\u52a8\u79d1\u5b66\u7814\u7a76\u7684\u53d1\u5c55\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\u8bba\uff0c\u5305\u542b\u4e09\u79cdHPC\u4e0eAI\u7684\u8026\u5408\u6a21\u5f0f\uff1a\u4ee3\u7406\uff08surrogate\uff09\u3001\u6307\u4ee4\uff08directive\uff09\u548c\u534f\u8c03\uff08coordinate\uff09\u3002\u5e76\u4ee5\u6750\u6599\u79d1\u5b66\u4e3a\u6848\u4f8b\uff0c\u5bf9\u8fd9\u4e9b\u6a21\u5f0f\u7684\u5e94\u7528\u53ca\u5176\u6548\u679c\u8fdb\u884c\u4e86\u7814\u7a76\u3002", "result": "\u901a\u8fc7\u5728\u6750\u6599\u79d1\u5b66\u4e2d\u7684\u6848\u4f8b\u7814\u7a76\uff0c\u4e09\u79cd\u8026\u5408\u6a21\u5f0f\u5747\u5c55\u793a\u51fa\u5bf9\u9ad8\u6027\u80fd\u8ba1\u7b97\u4e0e\u4eba\u5de5\u667a\u80fd\u7ed3\u5408\u7684\u6709\u6548\u6027\u548c\u5e94\u7528\u524d\u666f\u3002\u8bba\u6587\u8fd8\u8be6\u7ec6\u8ba8\u8bba\u4e86\u6280\u672f\u6311\u6218\u3001\u6027\u80fd\u63d0\u5347\u53ca\u5b9e\u73b0\u7ec6\u8282\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u4e09\u79cd\u8026\u5408\u6a21\u5f0f\u80fd\u591f\u4e3a\u6750\u6599\u79d1\u5b66\u53ca\u5176\u4ed6\u79d1\u5b66\u9886\u57df\u5728\u672a\u6765HPC\u4e0eAI\u96c6\u6210\u63d0\u4f9b\u5b9e\u7528\u6307\u5bfc\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u79d1\u5b66\u53d1\u73b0\u3002"}}
{"id": "2507.01019", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.01019", "abs": "https://arxiv.org/abs/2507.01019", "authors": ["Imran Mirza", "Cole Huang", "Ishwara Vasista", "Rohan Patil", "Asli Akalin", "Sean O'Brien", "Kevin Zhu"], "title": "MALIBU Benchmark: Multi-Agent LLM Implicit Bias Uncovered", "comment": "Accepted to Building Trust in LLMs @ ICLR 2025 and NAACL SRW 2025", "summary": "Multi-agent systems, which consist of multiple AI models interacting within a\nshared environment, are increasingly used for persona-based interactions.\nHowever, if not carefully designed, these systems can reinforce implicit biases\nin large language models (LLMs), raising concerns about fairness and equitable\nrepresentation. We present MALIBU, a novel benchmark developed to assess the\ndegree to which LLM-based multi-agent systems implicitly reinforce social\nbiases and stereotypes. MALIBU evaluates bias in LLM-based multi-agent systems\nthrough scenario-based assessments. AI models complete tasks within predefined\ncontexts, and their responses undergo evaluation by an LLM-based multi-agent\njudging system in two phases. In the first phase, judges score responses\nlabeled with specific demographic personas (e.g., gender, race, religion)\nacross four metrics. In the second phase, judges compare paired responses\nassigned to different personas, scoring them and selecting the superior\nresponse. Our study quantifies biases in LLM-generated outputs, revealing that\nbias mitigation may favor marginalized personas over true neutrality,\nemphasizing the need for nuanced detection, balanced fairness strategies, and\ntransparent evaluation benchmarks in multi-agent systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86MALIBU\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4ee5\u91cf\u5316\u548c\u5206\u6790\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7531\u5927\u8bed\u8a00\u6a21\u578b\u5f15\u53d1\u7684\u9690\u6027\u793e\u4f1a\u504f\u89c1\uff0c\u5e76\u6307\u51fa\u504f\u89c1\u7f13\u89e3\u4e0d\u5f53\u53ef\u80fd\u5bfc\u81f4\u65b0\u7684\u4e0d\u516c\uff0c\u547c\u5401\u66f4\u5e73\u8861\u4e0e\u900f\u660e\u7684\u65b9\u6cd5\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u57fa\u4e8e\u89d2\u8272\u7684\u4e92\u52a8\u4e2d\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u8fd9\u4e9b\u7cfb\u7edf\u5982\u679c\u8bbe\u8ba1\u4e0d\u5f53\u4f1a\u52a0\u5267\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u9690\u6027\u504f\u89c1\uff0c\u4ece\u800c\u5f71\u54cd\u516c\u5e73\u4e0e\u4ee3\u8868\u6027\u3002\u672c\u7814\u7a76\u65e8\u5728\u8bc4\u4f30\u548c\u7f13\u89e3\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u793e\u4f1a\u504f\u89c1\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5MALIBU\uff0c\u901a\u8fc7\u573a\u666f\u8bc4\u4f30\u6cd5\u68c0\u6d4b\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u504f\u89c1\u3002\u65b9\u6cd5\u5305\u62ec\u4e24\u4e2a\u9636\u6bb5\uff1a\u7b2c\u4e00\u9636\u6bb5\uff0c\u8bc4\u59d4\u9488\u5bf9\u5e26\u5177\u4f53\u4eba\u53e3\u7279\u5f81\uff08\u5982\u6027\u522b\u3001\u79cd\u65cf\u3001\u5b97\u6559\uff09\u7684\u56de\u7b54\uff0c\u6309\u56db\u9879\u6307\u6807\u6253\u5206\u3002\u7b2c\u4e8c\u9636\u6bb5\uff0c\u8bc4\u59d4\u5bf9\u4e0d\u540c\u4eba\u53e3\u7279\u5f81\u7684\u56de\u7b54\u8fdb\u884c\u914d\u5bf9\u6bd4\u5bf9\uff0c\u7ed9\u5206\u5e76\u9009\u51fa\u66f4\u4f18\u7b54\u6848\u3002", "result": "\u7814\u7a76\u5b9a\u91cf\u63ed\u793a\u4e86LLM\u751f\u6210\u5185\u5bb9\u4e2d\u7684\u504f\u89c1\u73b0\u8c61\u3002\u504f\u89c1\u7f13\u89e3\u8fc7\u7a0b\u6709\u65f6\u4f1a\u8fc7\u5ea6\u503e\u5411\u4e8e\u8fb9\u7f18\u7fa4\u4f53\u800c\u975e\u771f\u6b63\u7684\u4e2d\u7acb\u3002\u5f3a\u8c03\u521b\u65b0\u68c0\u6d4b\u3001\u5e73\u8861\u516c\u5e73\u6027\u7b56\u7565\u548c\u900f\u660e\u7684\u8bc4\u4f30\u57fa\u51c6\u5728\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u5fc5\u8981\u6027\u3002", "conclusion": "MALIBU\u4e3a\u8bc4\u4f30\u548c\u63ed\u793a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u9690\u6027\u793e\u4f1a\u504f\u89c1\u63d0\u4f9b\u4e86\u5de5\u5177\u3002\u5f53\u524d\u7684\u504f\u89c1\u4fee\u6b63\u65b9\u6cd5\u53ef\u80fd\u5bfc\u81f4\u5bf9\u8fb9\u7f18\u4eba\u7fa4\u7684\u504f\u5411\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u7ec6\u81f4\u548c\u516c\u6b63\u7684\u504f\u89c1\u68c0\u6d4b\u4e0e\u7f13\u89e3\u7b56\u7565\u3002"}}
{"id": "2507.01231", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01231", "abs": "https://arxiv.org/abs/2507.01231", "authors": ["I\u00f1aki Dellibarda Varela", "Pablo Romero-Sorozabal", "Eduardo Rocon", "Manuel Cebrian"], "title": "Rethinking the Illusion of Thinking", "comment": "8 pages, 4 figures", "summary": "Earlier this year, Apple ignited controversy by publishing \"The Illusion of\nThinking,\" prompting heated debate within the AI community. Critics seized upon\nthe findings as conclusive evidence that Large Reasoning Models (LRMs) lack\ngenuine reasoning capabilities, branding them as mere stochastic parrots.\nMeanwhile, defenders-spearheaded by Lawsen et al. (2025)-fired back, condemning\nthe experimental setup as flawed and the conclusions overstated. We clarify\nthis debate by replicating and refining two of the original study's most\ncontentious benchmarks: Towers of Hanoi and River Crossing. By introducing\nincremental stepwise prompting and agentic collaborative dialogue, we show that\npreviously reported failures solving the Towers of Hanoi were not purely result\nof output constraints, but also partly a result of cognition limitations: LRMs\nstill stumble when complexity rises moderately (around 8 disks). Moreover, the\nRiver Crossing results initially heralded as catastrophic failures turn out to\nhinge upon testing unsolvable configurations. Once we limit tests strictly to\nsolvable problems-LRMs effortlessly solve large instances involving over 100\nagent pairs. Our findings ultimately defy simplistic narratives: today's LRMs\nare stochastic, RL-tuned searchers in a discrete state space we barely\nunderstand. Real progress in symbolic, long-horizon reasoning demands mapping\nthat terrain through fine-grained ablations like those introduced here.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u5927\u578b\u63a8\u7406\u6a21\u578b\u63a8\u7406\u80fd\u529b\u7684\u4e89\u8bae\uff0c\u590d\u73b0\u5e76\u6539\u8fdb\u4e86\u5173\u952e\u5b9e\u9a8c\u3002\u7ed3\u679c\u663e\u793a\uff0cLRMs\u63a8\u7406\u5931\u8d25\u90e8\u5206\u56e0\u5176\u8ba4\u77e5\u74f6\u9888\uff0c\u90e8\u5206\u56e0\u5b9e\u9a8c\u65b9\u6cd5\u4e0d\u5f53\uff0c\u5b9e\u9645\u5728\u9650\u5b9a\u6761\u4ef6\u4e0b\u80fd\u5b8c\u6210\u590d\u6742\u4efb\u52a1\u3002\u672a\u6765\u9700\u66f4\u7cfb\u7edf\u6027\u5730\u5206\u6790\u6a21\u578b\u63a8\u7406\u8fb9\u754c\u3002", "motivation": "\u9488\u5bf9\u82f9\u679c\u516c\u53f8\u53d1\u5e03\u7684\u300aThe Illusion of Thinking\u300b\u4e00\u6587\u5728AI\u9886\u57df\u5f15\u53d1\u7684\u5173\u4e8e\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u662f\u5426\u5177\u5907\u771f\u6b63\u63a8\u7406\u80fd\u529b\u7684\u4e89\u8bba\uff0c\u4ee5\u53ca\u539f\u6587\u5b9e\u9a8c\u65b9\u6cd5\u53d7\u5230\u6279\u8bc4\uff0c\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u6f84\u6e05\u4e89\u8bae\uff0c\u8fdb\u4e00\u6b65\u63a2\u7a76LRMs\u7684\u63a8\u7406\u8868\u73b0\u3002", "method": "\u590d\u73b0\u5e76\u6539\u8fdb\u539f\u7814\u7a76\u4e2d\u6709\u4e89\u8bae\u7684\u5b9e\u9a8c\u57fa\u51c6\uff08\u6c49\u8bfa\u5854\u548c\u6cb3\u6d41\u8fc7\u5173\u95ee\u9898\uff09\uff0c\u5f15\u5165\u4e86\u589e\u91cf\u6027\u9010\u6b65\u63d0\u793a\u548c\u4ee3\u7406\u534f\u540c\u5bf9\u8bdd\uff0c\u533a\u5206\u6a21\u578b\u8f93\u51fa\u9650\u5236\u548c\u8ba4\u77e5\u80fd\u529b\u7684\u5f71\u54cd\uff0c\u5e76\u5bf9\u53ef\u89e3\u4e0e\u4e0d\u53ef\u89e3\u95ee\u9898\u914d\u7f6e\u505a\u51fa\u660e\u786e\u533a\u5206\u3002", "result": "\u6c49\u8bfa\u5854\u5b9e\u9a8c\u663e\u793a\uff0cLRMs\u7684\u5931\u8d25\u4e0d\u4ec5\u4ec5\u56e0\u8f93\u51fa\u53d7\u9650\uff0c\u8fd8\u56e0\u4e3a\u968f\u7740\u590d\u6742\u5ea6\u63d0\u5347\uff08\u5927\u7ea68\u76d8\u4ee5\u4e0a\uff09\u800c\u663e\u73b0\u51fa\u63a8\u7406\u80fd\u529b\u74f6\u9888\u3002\u6cb3\u6d41\u8fc7\u5173\u95ee\u9898\u7ed3\u679c\u8868\u660e\uff0c\u539f\u5148\u62a5\u544a\u7684\u5927\u89c4\u6a21\u5931\u8d25\u4e3b\u8981\u7531\u4e8e\u6d4b\u8bd5\u4e86\u4e0d\u53ef\u89e3\u7684\u914d\u7f6e\uff1b\u5728\u53ea\u6d4b\u8bd5\u53ef\u89e3\u95ee\u9898\u65f6\uff0cLRMs\u53ef\u4ee5\u987a\u5229\u89e3\u51b3\u8d85\u8fc7100\u5bf9\u4ee3\u7406\u7684\u5927\u89c4\u6a21\u5b9e\u4f8b\u3002", "conclusion": "\u73b0\u6709LRMs\u4e0d\u662f\u7b80\u5355\u7684\u968f\u673a\u9e66\u9e49\uff0c\u5b83\u4eec\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u3001\u5728\u79bb\u6563\u72b6\u6001\u7a7a\u95f4\u4e2d\u8fdb\u884c\u641c\u7d22\uff0c\u5bf9\u63a8\u7406\u4efb\u52a1\u8868\u73b0\u5df2\u88ab\u4f4e\u4f30\u3002\u771f\u6b63\u63a8\u52a8\u7b26\u53f7\u957f\u7a0b\u63a8\u7406\u8fdb\u5c55\u9700\u901a\u8fc7\u5982\u672c\u7814\u7a76\u7ec6\u81f4\u7684\u6d88\u878d\u6d4b\u8bd5\u8fdb\u4e00\u6b65\u6620\u5c04\u80fd\u529b\u8fb9\u754c\u3002"}}
{"id": "2507.01418", "categories": ["cs.CY", "cs.AI", "H.5.2; I.2"], "pdf": "https://arxiv.org/pdf/2507.01418", "abs": "https://arxiv.org/abs/2507.01418", "authors": ["Inyoung Cheong", "Alicia Guo", "Mina Lee", "Zhehui Liao", "Kowe Kadoma", "Dongyoung Go", "Joseph Chee Chang", "Peter Henderson", "Mor Naaman", "Amy X. Zhang"], "title": "Penalizing Transparency? How AI Disclosure and Author Demographics Shape Human and AI Judgments About Writing", "comment": "Presented at CHIWORK 2025 Workshop on Generative AI Disclosure,\n  Ownership, and Accountability in Co-Creative Domains", "summary": "As AI integrates in various types of human writing, calls for transparency\naround AI assistance are growing. However, if transparency operates on uneven\nground and certain identity groups bear a heavier cost for being honest, then\nthe burden of openness becomes asymmetrical. This study investigates how AI\ndisclosure statement affects perceptions of writing quality, and whether these\neffects vary by the author's race and gender. Through a large-scale controlled\nexperiment, both human raters (n = 1,970) and LLM raters (n = 2,520) evaluated\na single human-written news article while disclosure statements and author\ndemographics were systematically varied. This approach reflects how both human\nand algorithmic decisions now influence access to opportunities (e.g., hiring,\npromotion) and social recognition (e.g., content recommendation algorithms). We\nfind that both human and LLM raters consistently penalize disclosed AI use.\nHowever, only LLM raters exhibit demographic interaction effects: they favor\narticles attributed to women or Black authors when no disclosure is present.\nBut these advantages disappear when AI assistance is revealed. These findings\nilluminate the complex relationships between AI disclosure and author identity,\nhighlighting disparities between machine and human evaluation patterns.", "AI": {"tldr": "\u4eba\u7c7b\u548cAI\u8bc4\u59d4\u5bf9\u516c\u5f00AI\u8f85\u52a9\u7684\u5199\u4f5c\u6301\u8d1f\u9762\u770b\u6cd5\u3002AI\u8bc4\u59d4\u4f1a\u66f4\u504f\u7231\u67d0\u4e9b\u7fa4\u4f53\uff08\u5973\u6027\u6216\u9ed1\u4eba\uff09\u4f5c\u8005\u7684\u4f5c\u54c1\uff0c\u4f46\u5982\u679c\u62ab\u9732\u4e86AI\u8f85\u52a9\uff0c\u4f18\u52bf\u5373\u6d88\u5931\uff0c\u663e\u793aAI\u62ab\u9732\u4e0e\u793e\u4f1a\u8eab\u4efd\u5b58\u5728\u590d\u6742\u4ea4\u4e92\u3002", "motivation": "\u968f\u7740AI\u6280\u672f\u5e7f\u6cdb\u878d\u5165\u4eba\u7c7b\u5199\u4f5c\uff0c\u5404\u65b9\u547c\u5401\u589e\u52a0\u5173\u4e8eAI\u8f85\u52a9\u5199\u4f5c\u7684\u900f\u660e\u5ea6\u3002\u4f46\u5982\u679c\u8bda\u5b9e\u516c\u5f00AI\u4f7f\u7528\u7684\u6210\u672c\u5728\u4e0d\u540c\u8eab\u4efd\u7fa4\u4f53\u4e4b\u95f4\u5206\u5e03\u4e0d\u5747\uff0c\u2018\u900f\u660e\u2019\u672c\u8eab\u5c06\u5e26\u6765\u4e0d\u5747\u8861\u7684\u8d1f\u62c5\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u52a8\u673a\u662f\u63a2\u7a76AI\u8f85\u52a9\u5199\u4f5c\u62ab\u9732\u5bf9\u4f5c\u8005\u7684\u5f71\u54cd\uff0c\u7279\u522b\u662f\u5728\u4e0d\u540c\u79cd\u65cf\u548c\u6027\u522b\u7fa4\u4f53\u4e2d\u662f\u5426\u5b58\u5728\u5dee\u5f02\u3002", "method": "\u901a\u8fc7\u4e00\u9879\u5927\u89c4\u6a21\u5bf9\u7167\u5b9e\u9a8c\uff0c\u5206\u522b\u8ba9\u4eba\u7c7b\u8bc4\u59d4\uff08n = 1,970\uff09\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8bc4\u59d4\uff08n = 2,520\uff09\u5bf9\u552f\u4e00\u4e00\u7bc7\u7531\u4eba\u7c7b\u64b0\u5199\u7684\u65b0\u95fb\u6587\u7ae0\u8fdb\u884c\u8bc4\u4ef7\u3002\u5b9e\u9a8c\u4e2d\u7cfb\u7edf\u5730\u53d8\u6362\u62ab\u9732AI\u8f85\u52a9\u5199\u4f5c\u7684\u58f0\u660e\u4ee5\u53ca\u4f5c\u8005\u7684\u79cd\u65cf\u548c\u6027\u522b\u4fe1\u606f\uff0c\u4ee5\u89c2\u6d4b\u5176\u5bf9\u6587\u7ae0\u8d28\u91cf\u8bc4\u4ef7\u7684\u5f71\u54cd\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u65e0\u8bba\u662f\u4eba\u7c7b\u8fd8\u662fLLM\u8bc4\u59d4\uff0c\u5bf9\u4e8e\u516c\u5f00AI\u8f85\u52a9\u5199\u4f5c\u7684\u6587\u7ae0\u90fd\u5b58\u5728\u8bc4\u5206\u60e9\u7f5a\u3002\u4f46\u53ea\u6709LLM\u8bc4\u59d4\u5728\u4f5c\u8005\u8eab\u4efd\u4e0a\u51fa\u73b0\u4e92\u52a8\u6548\u5e94\uff1a\u5728\u6ca1\u6709AI\u62ab\u9732\u65f6\uff0c\u66f4\u504f\u597d\u5f52\u5c5e\u4e8e\u5973\u6027\u6216\u9ed1\u4eba\u4f5c\u8005\u7684\u6587\u7ae0\uff0c\u800c\u4e00\u65e6AI\u8f85\u52a9\u4e8b\u5b9e\u88ab\u516c\u5f00\uff0c\u9019\u79cd\u4f18\u52bf\u5219\u6d88\u5931\u3002", "conclusion": "AI\u62ab\u9732\u4e0e\u4f5c\u8005\u8eab\u4efd\u4e4b\u95f4\u5b58\u5728\u590d\u6742\u7684\u5173\u7cfb\uff0c\u5c24\u5176\u5728\u673a\u5668\u4e0e\u4eba\u7c7b\u8bc4\u4ef7\u6a21\u5f0f\u4e4b\u95f4\u8868\u73b0\u51fa\u663e\u8457\u5dee\u5f02\uff0c\u8fd9\u7a81\u663e\u4e86AI\u5199\u4f5c\u62ab\u9732\u5e26\u6765\u7684\u793e\u4f1a\u516c\u6b63\u548c\u673a\u4f1a\u5e73\u7b49\u65b0\u6311\u6218\u3002"}}
{"id": "2507.01069", "categories": ["cs.CE", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.01069", "abs": "https://arxiv.org/abs/2507.01069", "authors": ["Nishant A. Parikh"], "title": "Agentic AI in Product Management: A Co-Evolutionary Model", "comment": "41 pages, 2 figures", "summary": "This study explores agentic AI's transformative role in product management,\nproposing a conceptual co-evolutionary framework to guide its integration\nacross the product lifecycle. Agentic AI, characterized by autonomy,\ngoal-driven behavior, and multi-agent collaboration, redefines product managers\n(PMs) as orchestrators of socio-technical ecosystems. Using systems theory,\nco-evolutionary theory, and human-AI interaction theory, the framework maps\nagentic AI capabilities in discovery, scoping, business case development,\ndevelopment, testing, and launch. An integrative review of 70+ sources,\nincluding case studies from leading tech firms, highlights PMs' evolving roles\nin AI orchestration, supervision, and strategic alignment. Findings emphasize\nmutual adaptation between PMs and AI, requiring skills in AI literacy,\ngovernance, and systems thinking. Addressing gaps in traditional frameworks,\nthis study provides a foundation for future research and practical\nimplementation to ensure responsible, effective agentic AI integration in\nsoftware organizations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4ee5\u5171\u8fdb\u5316\u7406\u8bba\u4e3a\u57fa\u7840\u7684Agentic AI\u6574\u5408\u6846\u67b6\uff0c\u63ed\u793a\u4ea7\u54c1\u7ecf\u7406\u548cAI\u4e4b\u95f4\u7684\u53cc\u5411\u9002\u5e94\u53caPM\u80fd\u529b\u8f6c\u578b\u9700\u6c42\uff0c\u5bf9AI\u5728\u4ea7\u54c1\u7ba1\u7406\u5168\u751f\u547d\u5468\u671f\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u7406\u8bba\u548c\u5b9e\u8df5\u53c2\u8003\u3002", "motivation": "\u8fd1\u5e74\u6765\uff0cAgentic AI\uff08\u5177\u6709\u81ea\u4e3b\u6027\u3001\u76ee\u6807\u5bfc\u5411\u548c\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u7279\u5f81\u7684AI\uff09\u7684\u53d1\u5c55\u6b63\u6df1\u523b\u5f71\u54cd\u4ea7\u54c1\u7ba1\u7406\u9886\u57df\uff0c\u5bf9\u4ea7\u54c1\u7ecf\u7406\u89d2\u8272\u548c\u4ea7\u54c1\u5f00\u53d1\u6d41\u7a0b\u5e26\u6765\u4e86\u6311\u6218\u548c\u673a\u9047\u3002\u56e0\u6b64\uff0c\u8feb\u5207\u9700\u8981\u7406\u8bba\u6846\u67b6\u6307\u5bfcAI\u5728\u4ea7\u54c1\u5168\u5468\u671f\u4e2d\u7684\u6709\u6548\u6574\u5408\u3002", "method": "\u57fa\u4e8e\u7cfb\u7edf\u7406\u8bba\u3001\u5171\u8fdb\u5316\u7406\u8bba\u548c\u4eba\u673a\u534f\u4f5c\u7406\u8bba\uff0c\u4f5c\u8005\u5efa\u7acb\u4e86\u4e00\u4e2a\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u6790Agentic AI\u5728\u4ea7\u54c1\u53d1\u73b0\u3001\u8303\u56f4\u754c\u5b9a\u3001\u5546\u4e1a\u6848\u4f8b\u5f00\u53d1\u3001\u5f00\u53d1\u3001\u6d4b\u8bd5\u4e0e\u53d1\u5e03\u5404\u73af\u8282\u7684\u4f5c\u7528\u3002\u540c\u65f6\uff0c\u5bf970\u4f59\u7bc7\u76f8\u5173\u6587\u732e\u53ca\u9f99\u5934\u79d1\u6280\u516c\u53f8\u6848\u4f8b\u8fdb\u884c\u4e86\u7efc\u8ff0\u548c\u5f52\u7eb3\u3002", "result": "\u901a\u8fc7\u7814\u7a76\u53d1\u73b0\uff0c\u4ea7\u54c1\u7ecf\u7406\u7684\u89d2\u8272\u4ece\u4f20\u7edf\u7684\u4ea7\u54c1\u8d1f\u8d23\u4eba\u5411\u793e\u4f1a-\u6280\u672f\u751f\u6001\u7cfb\u7edf\u7684\u7f16\u6392\u8005\u8f6c\u53d8\u3002Agentic AI\u5f3a\u8c03PM\u4e0eAI\u7684\u5171\u540c\u9002\u5e94\uff0cPM\u9700\u8981\u589e\u5f3aAI\u7d20\u517b\u3001\u6cbb\u7406\u3001\u7cfb\u7edf\u601d\u7ef4\u7b49\u80fd\u529b\uff0c\u4ee5\u5b9e\u73b0AI\u5728\u8f6f\u4ef6\u4f01\u4e1a\u4e2d\u7684\u8d1f\u8d23\u4efb\u4e0e\u9ad8\u6548\u6574\u5408\u3002", "conclusion": "\u672c\u7814\u7a76\u5f25\u8865\u4e86\u4f20\u7edf\u4ea7\u54c1\u7ba1\u7406\u6846\u67b6\u5728AI\u5e94\u7528\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u5171\u8fdb\u5316\u7684\u6574\u5408\u6846\u67b6\uff0c\u4e3a\u672a\u6765\u76f8\u5173\u7406\u8bba\u7814\u7a76\u548c\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u57fa\u7840\uff0c\u4fc3\u8fdbAgentic AI\u5728\u4ea7\u54c1\u7ba1\u7406\u4e2d\u7684\u53ef\u6301\u7eed\u5b9e\u65bd\u3002"}}
{"id": "2507.01160", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01160", "abs": "https://arxiv.org/abs/2507.01160", "authors": ["Huiling You", "Samia Touileb", "Erik Velldal", "Lilja \u00d8vrelid"], "title": "Event-based evaluation of abstractive news summarization", "comment": "to appear at GEM2 workshop@ACL 2025", "summary": "An abstractive summary of a news article contains its most important\ninformation in a condensed version. The evaluation of automatically generated\nsummaries by generative language models relies heavily on human-authored\nsummaries as gold references, by calculating overlapping units or similarity\nscores. News articles report events, and ideally so should the summaries. In\nthis work, we propose to evaluate the quality of abstractive summaries by\ncalculating overlapping events between generated summaries, reference\nsummaries, and the original news articles. We experiment on a richly annotated\nNorwegian dataset comprising both events annotations and summaries authored by\nexpert human annotators. Our approach provides more insight into the event\ninformation contained in the summaries.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4ee5\u4e8b\u4ef6\u91cd\u53e0\u4e3a\u6838\u5fc3\u7684\u65b0\u578b\u81ea\u52a8\u6458\u8981\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5e76\u5728\u632a\u5a01\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u80fd\u66f4\u6df1\u5165\u5206\u6790\u6458\u8981\u5bf9\u4e8e\u5173\u952e\u4fe1\u606f\u7684\u63d0\u53d6\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u5bf9\u81ea\u52a8\u751f\u6210\u65b0\u95fb\u6458\u8981\u7684\u8bc4\u4ef7\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u4e8e\u4e0e\u4eba\u5de5\u6458\u8981\u7684\u91cd\u53e0\u5ea6\uff08\u4f8b\u5982\u8bcd\u53e5\u91cd\u53e0\u6216\u76f8\u4f3c\u5ea6\u8bc4\u5206\uff09\uff0c\u4f46\u8fd9\u79cd\u65b9\u5f0f\u672a\u5fc5\u80fd\u6709\u6548\u8bc4\u4ef7\u6458\u8981\u5bf9\u65b0\u95fb\u4e8b\u4ef6\u6838\u5fc3\u5185\u5bb9\u7684\u628a\u63e1\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u901a\u8fc7\u8ba1\u7b97\u751f\u6210\u6458\u8981\u3001\u53c2\u8003\u6458\u8981\u4e0e\u539f\u59cb\u65b0\u95fb\u6587\u7ae0\u95f4\u7684\u4e8b\u4ef6\u91cd\u53e0\u6765\u8bc4\u4f30\u6458\u8981\u8d28\u91cf\u7684\u65b9\u6cd5\uff0c\u5e76\u5728\u5305\u542b\u8be6\u7ec6\u4e8b\u4ef6\u6807\u6ce8\u548c\u4e13\u5bb6\u4eba\u5de5\u6458\u8981\u7684\u632a\u5a01\u8bed\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u63d0\u4f9b\u5173\u4e8e\u6458\u8981\u6240\u5305\u542b\u4e8b\u4ef6\u4fe1\u606f\u7684\u66f4\u591a\u6d1e\u89c1\uff0c\u4ece\u800c\u8fdb\u4e00\u6b65\u8bc4\u4f30\u6458\u8981\u5bf9\u4e8e\u65b0\u95fb\u5b9e\u9645\u62a5\u9053\u4e8b\u4ef6\u7684\u6982\u62ec\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u4e8b\u4ef6\u91cd\u53e0\u5ea6\u8bc4\u4ef7\u65b9\u6cd5\uff0c\u53ef\u4ee5\u66f4\u6709\u6548\u5206\u6790\u548c\u63d0\u5347\u81ea\u52a8\u6458\u8981\u7cfb\u7edf\u5bf9\u6838\u5fc3\u65b0\u95fb\u4e8b\u4ef6\u7684\u8986\u76d6\u4e0e\u8868\u8fbe\u80fd\u529b\u3002"}}
{"id": "2507.01282", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.01282", "abs": "https://arxiv.org/abs/2507.01282", "authors": ["Matthew JY Kang", "Wenli Yang", "Monica R Roberts", "Byeong Ho Kang", "Charles B Malpas"], "title": "Beyond Black-Box AI: Interpretable Hybrid Systems for Dementia Care", "comment": null, "summary": "The recent boom of large language models (LLMs) has re-ignited the hope that\nartificial intelligence (AI) systems could aid medical diagnosis. Yet despite\ndazzling benchmark scores, LLM assistants have yet to deliver measurable\nimprovements at the bedside. This scoping review aims to highlight the areas\nwhere AI is limited to make practical contributions in the clinical setting,\nspecifically in dementia diagnosis and care.\n  Standalone machine-learning models excel at pattern recognition but seldom\nprovide actionable, interpretable guidance, eroding clinician trust. Adjacent\nuse of LLMs by physicians did not result in better diagnostic accuracy or\nspeed. Key limitations trace to the data-driven paradigm: black-box outputs\nwhich lack transparency, vulnerability to hallucinations, and weak causal\nreasoning. Hybrid approaches that combine statistical learning with expert\nrule-based knowledge, and involve clinicians throughout the process help bring\nback interpretability. They also fit better with existing clinical workflows,\nas seen in examples like PEIRS and ATHENA-CDS.\n  Future decision-support should prioritise explanatory coherence by linking\npredictions to clinically meaningful causes. This can be done through\nneuro-symbolic or hybrid AI that combines the language ability of LLMs with\nhuman causal expertise. AI researchers have addressed this direction, with\nexplainable AI and neuro-symbolic AI being the next logical steps in further\nadvancement in AI. However, they are still based on data-driven knowledge\nintegration instead of human-in-the-loop approaches. Future research should\nmeasure success not only by accuracy but by improvements in clinician\nunderstanding, workflow fit, and patient outcomes. A better understanding of\nwhat helps improve human-computer interactions is greatly needed for AI systems\nto become part of clinical practice.", "AI": {"tldr": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u533b\u5b66\u9886\u57df\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u75f4\u5446\u7b49\u4e34\u5e8a\u5b9e\u9645\u4e2d\u7684\u8f85\u52a9\u6548\u679c\u6709\u9650\uff0c\u4e3b\u8981\u56e0\u89e3\u91ca\u6027\u548c\u4fe1\u4efb\u5ea6\u4e0d\u8db3\u3002\u6df7\u5408AI\u4e0e\u795e\u7ecf\u7b26\u53f7AI\u662f\u672a\u6765\u53d1\u5c55\u65b9\u5411\uff0c\u4f46\u9700\u66f4\u591a\u805a\u7126\u4e8e\u533b\u751f\u53c2\u4e0e\u548c\u5b9e\u9645\u4e34\u5e8a\u9700\u6c42\uff0c\u8bc4\u4ef7\u6307\u6807\u5e94\u66f4\u591a\u5143\u3002", "motivation": "\u8fd1\u5e74\u6765\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5907\u53d7\u5173\u6ce8\uff0c\u5e0c\u671b\u5176\u80fd\u63d0\u5347\u533b\u7597\u8bca\u65ad\u80fd\u529b\uff0c\u5c24\u5176\u662f\u75f4\u5446\u75c7\u7684\u8bca\u65ad\u4e0e\u62a4\u7406\u3002\u7136\u800c\uff0c\u5c3d\u7ba1LLM\u6210\u7ee9\u4eae\u773c\uff0c\u5728\u5b9e\u9645\u4e34\u5e8a\u5374\u96be\u6709\u5b9e\u8d28\u6027\u6539\u5584\u3002\u672c\u6587\u65e8\u5728\u5206\u6790AI\u5728\u4e34\u5e8a\u5e94\u7528\u4e0a\u7684\u9650\u5236\u3002", "method": "\u91c7\u7528\u7efc\u8ff0\u7814\u7a76(scoping review)\u65b9\u6cd5\uff0c\u68b3\u7406AI\u5728\u75f4\u5446\u75c7\u8bca\u65ad\u548c\u62a4\u7406\u9886\u57df\u7684\u5b9e\u9645\u5e94\u7528\u548c\u5c40\u9650\uff0c\u91cd\u70b9\u8ba8\u8bbaLLM\u6a21\u578b\u3001\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u7684\u4e0d\u8db3\uff0c\u4ee5\u53ca\u6df7\u5408\u5f0f\u65b9\u6cd5\u7684\u5e94\u7528\u3002", "result": "\u5355\u4e00\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u6a21\u5f0f\u8bc6\u522b\u4e0a\u8868\u73b0\u7a81\u51fa\uff0c\u4f46\u96be\u4ee5\u63d0\u4f9b\u53ef\u89e3\u91ca\u3001\u53ef\u64cd\u4f5c\u7684\u6307\u5bfc\uff0c\u672a\u80fd\u63d0\u5347\u533b\u751f\u7684\u8bca\u65ad\u51c6\u786e\u6027\u6216\u901f\u5ea6\uff0c\u4e3b\u8981\u56e0\u4e3a\u5176\u9ed1\u7bb1\u5c5e\u6027\u3001\u5bf9\u8c2c\u8bef\u6613\u611f\u3001\u56e0\u679c\u63a8\u7406\u80fd\u529b\u5f31\u3002\u7ed3\u5408\u4e13\u5bb6\u77e5\u8bc6\u7684\u6df7\u5408\u5f0f\u65b9\u6cd5\uff08\u5982PEIRS\u3001ATHENA-CDS\uff09\u589e\u5f3a\u4e86\u89e3\u91ca\u6027\u53ca\u4e0e\u4e34\u5e8a\u6d41\u7a0b\u7684\u5951\u5408\uff0c\u4f46\u76ee\u524d\u5c1a\u672a\u5927\u89c4\u6a21\u666e\u53ca\u3002", "conclusion": "\u4eca\u540eAI\u51b3\u7b56\u652f\u6301\u5e94\u6ce8\u91cd\u89e3\u91ca\u6027\u4e0e\u56e0\u679c\u8054\u7cfb\uff0c\u5c06LLM\u7684\u8bed\u8a00\u80fd\u529b\u4e0e\u4eba\u7c7b\u56e0\u679c\u77e5\u8bc6\u7ed3\u5408\uff0c\u53ef\u4ee5\u91c7\u7528\u795e\u7ecf\u7b26\u53f7\u6216\u6df7\u5408AI\u65b9\u6cd5\uff0c\u4f46\u4ecd\u9700\u66f4\u591a\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u7814\u7a76\u3002AI\u5728\u4e34\u5e8a\u5e94\u7528\u7684\u6210\u529f\u6807\u51c6\u5e94\u6269\u5c55\u5230\u533b\u751f\u7406\u89e3\u3001\u6d41\u7a0b\u5951\u5408\u5ea6\u53ca\u60a3\u8005\u7ed3\u5c40\uff0c\u800c\u975e\u4ec5\u4ec5\u662f\u9884\u6d4b\u51c6\u786e\u7387\u3002\u4eba\u673a\u534f\u4f5c\u673a\u5236\u7684\u6df1\u5165\u7406\u89e3\u662fAI\u843d\u5730\u533b\u7597\u7684\u5173\u952e\u3002"}}
{"id": "2507.01547", "categories": ["cs.CY", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01547", "abs": "https://arxiv.org/abs/2507.01547", "authors": ["Ubada El Joulani", "Tatiana Kalganova", "Stergios-Aristoteles Mitoulis", "Sotirios Argyroudis"], "title": "AI and Remote Sensing for Resilient and Sustainable Built Environments: A Review of Current Methods, Open Data and Future Directions", "comment": null, "summary": "Critical infrastructure, such as transport networks, underpins economic\ngrowth by enabling mobility and trade. However, ageing assets, climate change\nimpacts (e.g., extreme weather, rising sea levels), and hybrid threats ranging\nfrom natural disasters to cyber attacks and conflicts pose growing risks to\ntheir resilience and functionality. This review paper explores how emerging\ndigital technologies, specifically Artificial Intelligence (AI), can enhance\ndamage assessment and monitoring of transport infrastructure. A systematic\nliterature review examines existing AI models and datasets for assessing damage\nin roads, bridges, and other critical infrastructure impacted by natural\ndisasters. Special focus is given to the unique challenges and opportunities\nassociated with bridge damage detection due to their structural complexity and\ncritical role in connectivity. The integration of SAR (Synthetic Aperture\nRadar) data with AI models is also discussed, with the review revealing a\ncritical research gap: a scarcity of studies applying AI models to SAR data for\ncomprehensive bridge damage assessment. Therefore, this review aims to identify\nthe research gaps and provide foundations for AI-driven solutions for assessing\nand monitoring critical transport infrastructures.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86AI\u5728\u4ea4\u901a\u57fa\u7840\u8bbe\u65bd\u635f\u4f24\u8bc4\u4f30\u4e2d\u7684\u5e94\u7528\uff0c\u53d1\u73b0AI\u7ed3\u5408SAR\u6570\u636e\u7528\u4e8e\u6865\u6881\u635f\u4f24\u68c0\u6d4b\u65b9\u9762\u5b58\u5728\u7814\u7a76\u7a7a\u767d\uff0c\u63d0\u51fa\u9700\u52a0\u5f3a\u8be5\u9886\u57df\u7684\u7814\u7a76\uff0c\u4e3aAI\u9a71\u52a8\u7684\u57fa\u7840\u8bbe\u65bd\u76d1\u6d4b\u63d0\u4f9b\u53c2\u8003\u3002", "motivation": "\u4ea4\u901a\u57fa\u7840\u8bbe\u65bd\u5bf9\u7ecf\u6d4e\u589e\u957f\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u53d7\u5230\u8001\u5316\u3001\u6c14\u5019\u53d8\u5316\u4ee5\u53ca\u81ea\u7136\u707e\u5bb3\u3001\u7f51\u7edc\u653b\u51fb\u7b49\u591a\u91cd\u5a01\u80c1\uff0c\u4e9f\u9700\u63d0\u5347\u5176\u97e7\u6027\u548c\u529f\u80fd\u4fdd\u969c\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u6027\u6587\u732e\u7efc\u8ff0\uff0c\u68b3\u7406\u4e0e\u603b\u7ed3\u5f53\u524d\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u6280\u672f\u5728\u4ea4\u901a\u57fa\u7840\u8bbe\u65bd\u635f\u4f24\u8bc4\u4f30\u53ca\u76d1\u6d4b\u4e2d\u7684\u5e94\u7528\u73b0\u72b6\uff0c\u91cd\u70b9\u5173\u6ce8\u6865\u6881\u635f\u4f24\u68c0\u6d4b\u53caAI\u4e0e\u5408\u6210\u5b54\u5f84\u96f7\u8fbe\uff08SAR\uff09\u6570\u636e\u7684\u7ed3\u5408\u3002", "result": "\u5f53\u524dAI\u6a21\u578b\u548c\u6570\u636e\u96c6\u5df2\u5728\u9053\u8def\u3001\u6865\u6881\u7b49\u8bbe\u65bd\u53d7\u707e\u540e\u7684\u635f\u4f24\u8bc4\u4f30\u65b9\u9762\u6709\u6240\u5e94\u7528\uff0c\u4f46\u9488\u5bf9\u6865\u6881\u7684\u7814\u7a76\u8f83\u4e3a\u6709\u9650\uff0c\u5c24\u5176\u662fAI\u4e0eSAR\u7ed3\u5408\u7528\u4e8e\u6865\u6881\u5168\u9762\u635f\u4f24\u8bc4\u4f30\u7684\u76f8\u5173\u7814\u7a76\u5be5\u5be5\uff0c\u5b58\u5728\u663e\u8457\u7814\u7a76\u7a7a\u767d\u3002", "conclusion": "\u672c\u7efc\u8ff0\u660e\u786e\u4e86AI\u6280\u672f\u5728\u4ea4\u901a\u57fa\u7840\u8bbe\u65bd\u635f\u4f24\u8bc4\u4f30\u9886\u57df\u7279\u522b\u662f\u6865\u6881\u9886\u57df\u7684\u7814\u7a76\u4e0d\u8db3\uff0c\u5e76\u4e3a\u672a\u6765\u57fa\u4e8eAI\u7684\u6865\u6881\u53ca\u5176\u4ed6\u5173\u952e\u4ea4\u901a\u57fa\u7840\u8bbe\u65bd\u8bc4\u4f30\u548c\u76d1\u6d4b\u7814\u7a76\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2507.01617", "categories": ["cs.CE"], "pdf": "https://arxiv.org/pdf/2507.01617", "abs": "https://arxiv.org/abs/2507.01617", "authors": ["Faisal Aljaberi", "Hadi Belhaj", "Sajjad Foroughi", "Mohammed Al-Kobaisi", "Martin Blunt"], "title": "Spatially Distributed Wettability Characterization in Porous Media", "comment": null, "summary": "An enhanced geometric algorithm for automated pore-by-pore contact angle\nmeasurement from micro-CT images, is presented that achieves superior accuracy\ncompared to existing methods through robust fluid-fluid and solid-fluid\ninterface extrapolation. Using this high resolution data, we generate spatially\ndistributed contact angle maps that reveal previously hidden wettability\nheterogeneity. Our analysis of mixed-wet systems demonstrates the severe\nlimitations of averaged metrics: a sample with a mean contact angle of 64.7\ndegrees, conventionally classified as uniformly weakly water-wet, exhibits 40%\nof its pore space in the intermediate-wetting regime (70-110 degrees). This\nheterogeneity explains the presence of minimal surface interfaces and\nfundamentally different pore-filling mechanisms operating within the same\nsample. By providing open-source tools for spatially-resolved wettability\ncharacterization, this work enables more accurate predictions of multiphase\nflow behavior in heterogeneous porous materials, essential for optimizing\nsubsurface energy storage and recovery processes.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u5faeCT\u56fe\u7247\u81ea\u52a8\u5316\u63a5\u89e6\u89d2\u6d4b\u91cf\u7b97\u6cd5\uff0c\u63ed\u793a\u4e86\u6da6\u6e7f\u6027\u7a7a\u95f4\u5f02\u8d28\u6027\uff0c\u6311\u6218\u4e86\u4f20\u7edf\u5747\u503c\u6307\u6807\uff0c\u52a9\u529b\u63d0\u5347\u591a\u76f8\u6d41\u9884\u6d4b\u4e0e\u5730\u4e0b\u80fd\u6e90\u8fc7\u7a0b\u4f18\u5316\u3002", "motivation": "\u73b0\u6709\u5fae\u5b54\u63a5\u89e6\u89d2\u6d4b\u91cf\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u63ed\u793a\u6da6\u6e7f\u6027\u5f02\u8d28\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u65e0\u6cd5\u5145\u5206\u53cd\u6620\u591a\u5b54\u4ecb\u8d28\u4e2d\u591a\u76f8\u6d41\u884c\u4e3a\u7684\u590d\u6742\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u589e\u5f3a\u578b\u51e0\u4f55\u7b97\u6cd5\uff0c\u901a\u8fc7\u7a33\u5065\u7684\u6d41\u4f53-\u6d41\u4f53\u548c\u56fa\u4f53-\u6d41\u4f53\u754c\u9762\u5916\u63a8\uff0c\u5b9e\u73b0\u4e86\u57fa\u4e8e\u5faeCT\u56fe\u50cf\u7684\u81ea\u52a8\u9010\u5b54\u63a5\u89e6\u89d2\u6d4b\u91cf\u3002\u5229\u7528\u9ad8\u5206\u8fa8\u7387\u6570\u636e\uff0c\u751f\u6210\u7a7a\u95f4\u5206\u5e03\u7684\u63a5\u89e6\u89d2\u5730\u56fe\u3002", "result": "\u5728\u5206\u6790\u6df7\u5408\u6da6\u6e7f\u4f53\u7cfb\u65f6\u53d1\u73b0\uff0c\u5e73\u5747\u63a5\u89e6\u89d2\u4e3a64.7\u5ea6\u4f46\u670940%\u5b54\u9699\u5904\u4e8e\u4e2d\u95f4\u6da6\u6e7f\u72b6\u6001\uff0870-110\u5ea6\uff09\uff0c\u63ed\u793a\u4e86\u6da6\u6e7f\u6027\u5f02\u8d28\u6027\u548c\u4e0d\u540c\u5b54\u586b\u5145\u673a\u5236\u5171\u5b58\u3002", "conclusion": "\u7a7a\u95f4\u5206\u8fa8\u7684\u6da6\u6e7f\u6027\u8868\u5f81\u80fd\u591f\u66f4\u771f\u5b9e\u53cd\u6620\u591a\u5b54\u4ecb\u8d28\u7684\u591a\u76f8\u6d41\u884c\u4e3a\uff0c\u4e3a\u4f18\u5316\u5730\u4e0b\u80fd\u6e90\u5b58\u50a8\u4e0e\u5f00\u91c7\u63d0\u4f9b\u4e86\u652f\u6301\u3002\u76f8\u5173\u5f00\u6e90\u5de5\u5177\u4fc3\u8fdb\u4e86\u7cbe\u786e\u9884\u6d4b\u3002"}}
{"id": "2507.01170", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01170", "abs": "https://arxiv.org/abs/2507.01170", "authors": ["Simon B\u00f6rjesson", "Erik Ersmark", "Pierre Nugues"], "title": "Matching and Linking Entries in Historical Swedish Encyclopedias", "comment": "10 pages, 3 figures", "summary": "The \\textit{Nordisk familjebok} is a Swedish encyclopedia from the 19th and\n20th centuries. It was written by a team of experts and aimed to be an\nintellectual reference, stressing precision and accuracy. This encyclopedia had\nfour main editions remarkable by their size, ranging from 20 to 38 volumes. As\na consequence, the \\textit{Nordisk familjebok} had a considerable influence in\nuniversities, schools, the media, and society overall. As new editions were\nreleased, the selection of entries and their content evolved, reflecting\nintellectual changes in Sweden.\n  In this paper, we used digitized versions from \\textit{Project Runeberg}. We\nfirst resegmented the raw text into entries and matched pairs of entries\nbetween the first and second editions using semantic sentence embeddings. We\nthen extracted the geographical entries from both editions using a\ntransformer-based classifier and linked them to Wikidata. This enabled us to\nidentify geographic trends and possible shifts between the first and second\neditions, written between 1876-1899 and 1904-1926, respectively.\n  Interpreting the results, we observe a small but significant shift in\ngeographic focus away from Europe and towards North America, Africa, Asia,\nAustralia, and northern Scandinavia from the first to the second edition,\nconfirming the influence of the First World War and the rise of new powers. The\ncode and data are available on GitHub at\nhttps://github.com/sibbo/nordisk-familjebok.", "AI": {"tldr": "\u672c\u6587\u6570\u5b57\u5316\u5e76\u6bd4\u8f83\u5206\u6790\u4e86\u5317\u6b27\u5bb6\u5ead\u767e\u79d1\u5168\u4e66\u4e24\u5927\u7248\u672c\u7684\u5730\u7406\u6761\u76ee\uff0c\u53d1\u73b0\u4e00\u6218\u53ca\u5168\u7403\u683c\u5c40\u53d8\u5316\u5bfc\u81f4\u767e\u79d1\u5168\u4e66\u5bf9\u975e\u6b27\u6d32\u5730\u533a\u5173\u6ce8\u5ea6\u4e0a\u5347\uff0c\u663e\u793a\u767e\u79d1\u5168\u4e66\u5185\u5bb9\u968f\u65f6\u4ee3\u53d8\u8fc1\u3002", "motivation": "\u7814\u7a7619\u81f320\u4e16\u7eaa\u7684\u5317\u6b27\u5bb6\u5ead\u767e\u79d1\u5168\u4e66\uff08Nordisk familjebok\uff09\u4e0d\u540c\u7248\u672c\u6240\u53cd\u6620\u7684\u77e5\u8bc6\u7ed3\u6784\u548c\u5730\u7406\u5173\u6ce8\u53d8\u5316\uff0c\u63a2\u8ba8\u5176\u5bf9\u745e\u5178\u793e\u4f1a\u3001\u6559\u80b2\u548c\u4f20\u5a92\u7b49\u9886\u57df\u7684\u5f71\u54cd\uff0c\u4ee5\u53ca\u53cd\u6620\u51fa\u7684\u5386\u53f2\u80cc\u666f\u4e0e\u793e\u4f1a\u53d8\u8fc1\u3002", "method": "\u5229\u7528Project Runeberg\u7684\u6570\u5b57\u5316\u6570\u636e\uff0c\u9996\u5148\u5bf9\u539f\u59cb\u6587\u672c\u8fdb\u884c\u6761\u76ee\u5206\u5272\uff0c\u7136\u540e\u4f7f\u7528\u8bed\u4e49\u53e5\u5d4c\u5165\u65b9\u6cd5\u5bf9\u7b2c\u4e00\u3001\u4e8c\u7248\u6761\u76ee\u8fdb\u884c\u914d\u5bf9\u3002\u901a\u8fc7\u57fa\u4e8etransformer\u7684\u5206\u7c7b\u5668\u63d0\u53d6\u5730\u7406\u7c7b\u6761\u76ee\uff0c\u5e76\u4e0eWikidata\u94fe\u63a5\uff0c\u5206\u6790\u4e24\u4e2a\u7248\u672c\u4e4b\u95f4\u7684\u5730\u7406\u5173\u6ce8\u53d8\u5316\u3002", "result": "\u5206\u6790\u53d1\u73b0\uff0c\u4ece\u7b2c\u4e00\u7248\uff081876-1899\uff09\u5230\u7b2c\u4e8c\u7248\uff081904-1926\uff09\uff0c\u767e\u79d1\u5168\u4e66\u4e2d\u7684\u5730\u7406\u5173\u6ce8\u70b9\u660e\u663e\u4ece\u6b27\u6d32\u8f6c\u5411\u5317\u7f8e\u3001\u975e\u6d32\u3001\u4e9a\u6d32\u3001\u6fb3\u5927\u5229\u4e9a\u53ca\u65af\u582a\u7684\u7eb3\u7ef4\u4e9a\u5317\u90e8\uff0c\u53cd\u6620\u51fa\u4e00\u6218\u540e\u4e16\u754c\u683c\u5c40\u53d8\u5316\uff0c\u4ee5\u53ca\u65b0\u5174\u5f3a\u56fd\u7684\u5d1b\u8d77\u3002", "conclusion": "\u901a\u8fc7\u5bf9\u4e24\u7248\u767e\u79d1\u5168\u4e66\u5730\u7406\u6761\u76ee\u7684\u5bf9\u6bd4\u5206\u6790\uff0c\u63ed\u793a\u4e8619-20\u4e16\u7eaa\u745e\u5178\u77e5\u8bc6\u4f53\u7cfb\u4e2d\u5730\u7406\u5173\u6ce8\u7684\u5386\u53f2\u8f6c\u53d8\uff0c\u7a81\u51fa\u4e86\u91cd\u5927\u5386\u53f2\u4e8b\u4ef6\uff08\u5982\u4e00\u6218\uff09\u5bf9\u77e5\u8bc6\u7ed3\u6784\u7684\u5f71\u54cd\u3002"}}
{"id": "2507.01376", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01376", "abs": "https://arxiv.org/abs/2507.01376", "authors": ["Yinwang Ren", "Yangyang Liu", "Tang Ji", "Xun Xu"], "title": "AI Agents and Agentic AI-Navigating a Plethora of Concepts for Future Manufacturing", "comment": "Submitted to JMS(March 2025)", "summary": "AI agents are autonomous systems designed to perceive, reason, and act within\ndynamic environments. With the rapid advancements in generative AI (GenAI),\nlarge language models (LLMs) and multimodal large language models (MLLMs) have\nsignificantly improved AI agents' capabilities in semantic comprehension,\ncomplex reasoning, and autonomous decision-making. At the same time, the rise\nof Agentic AI highlights adaptability and goal-directed autonomy in dynamic and\ncomplex environments. LLMs-based AI Agents (LLM-Agents), MLLMs-based AI Agents\n(MLLM-Agents), and Agentic AI contribute to expanding AI's capabilities in\ninformation processing, environmental perception, and autonomous\ndecision-making, opening new avenues for smart manufacturing. However, the\ndefinitions, capability boundaries, and practical applications of these\nemerging AI paradigms in smart manufacturing remain unclear. To address this\ngap, this study systematically reviews the evolution of AI and AI agent\ntechnologies, examines the core concepts and technological advancements of\nLLM-Agents, MLLM-Agents, and Agentic AI, and explores their potential\napplications in and integration into manufacturing, along with the potential\nchallenges they may face.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u56de\u987e\u4e86\u5927\u6a21\u578b\u9a71\u52a8\u7684AI\u667a\u80fd\u4f53\u5728\u667a\u80fd\u5236\u9020\u7684\u6f5c\u5728\u5e94\u7528\u53ca\u6311\u6218\uff0c\u660e\u786e\u4e86\u5176\u80fd\u529b\uff0c\u4f46\u4e5f\u6307\u51fa\u4e86\u5b9a\u4e49\u548c\u5b9e\u9645\u843d\u5730\u4e2d\u5b58\u5728\u7684\u95ee\u9898\u3002", "motivation": "\u968f\u7740\u5927\u6a21\u578b\u548c\u591a\u6a21\u6001\u6a21\u578b\u7684\u53d1\u5c55\uff0cAI\u667a\u80fd\u4f53\u5728\u8bed\u4e49\u7406\u89e3\u3001\u590d\u6742\u63a8\u7406\u548c\u81ea\u4e3b\u51b3\u7b56\u7b49\u80fd\u529b\u663e\u8457\u63d0\u5347\uff0c\u4f46\u8fd9\u4e9b\u65b0\u5174AI\u667a\u80fd\u4f53\u5728\u667a\u80fd\u5236\u9020\u9886\u57df\u7684\u5b9a\u4e49\u3001\u80fd\u529b\u8fb9\u754c\u548c\u5b9e\u9645\u5e94\u7528\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u7cfb\u7edf\u6027\u56de\u987eAI\u53caAI\u667a\u80fd\u4f53\u6280\u672f\u7684\u53d1\u5c55\uff0c\u68b3\u7406LLM-Agents\u3001MLLM-Agents\u548cAgentic AI\u7684\u6838\u5fc3\u6982\u5ff5\u53ca\u6280\u672f\u8fdb\u5c55\uff0c\u5e76\u63a2\u8ba8\u5b83\u4eec\u5728\u5236\u9020\u4e1a\u4e2d\u7684\u5e94\u7528\u524d\u666f\u4e0e\u9762\u4e34\u7684\u6311\u6218\u3002", "result": "\u8be5\u7814\u7a76\u9610\u660e\u4e86\u5404\u7c7b\u667a\u80fd\u4f53\u5728\u4fe1\u606f\u5904\u7406\u3001\u73af\u5883\u611f\u77e5\u548c\u81ea\u4e3b\u51b3\u7b56\u65b9\u9762\u7684\u80fd\u529b\u6269\u5c55\uff0c\u603b\u7ed3\u4e86\u5b83\u4eec\u5728\u667a\u80fd\u5236\u9020\u4e2d\u7684\u6f5c\u529b\u5e94\u7528\uff0c\u5e76\u6307\u51fa\u5f53\u524d\u5b9e\u9645\u5e94\u7528\u4e0e\u96c6\u6210\u9762\u4e34\u7684\u6311\u6218\u3002", "conclusion": "LLM-Agents\u3001MLLM-Agents\u548cAgentic AI\u63a8\u52a8\u4e86AI\u5728\u667a\u80fd\u5236\u9020\u9886\u57df\u7684\u80fd\u529b\u8fb9\u754c\u6269\u5c55\uff0c\u4f46\u76f8\u5173\u5b9a\u4e49\u3001\u5e94\u7528\u548c\u6311\u6218\u4ecd\u9700\u8fdb\u4e00\u6b65\u660e\u786e\u548c\u89e3\u51b3\u3002"}}
{"id": "2507.01787", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2507.01787", "abs": "https://arxiv.org/abs/2507.01787", "authors": ["Marie-Therese Sekwenz", "Ben Wagner", "Hans De Bruijn"], "title": "From Reports to Reality: Testing Consistency in Instagram's Digital Services Act Compliance Data", "comment": null, "summary": "The Digital Services Act (DSA) introduces harmonized rules for content\nmoderation and platform governance in the European Union, mandating robust\ncompliance mechanisms, particularly for very large online platforms and search\nengines. This study examined compliance with DSA requirements, focusing on\nInstagram as a case study. We develop and apply a multi-level consistency\nframework to evaluate DSA compliance. Our findings contribute to the broader\ndiscussion on empirically-based regulation, providing insight into how\nresearchers, regulators, auditors and platforms can better utilize DSA\nmechanisms to improve reporting and enforcement quality and accountability.\nThis work underscores that consistency can help detect potential compliance\nfailures. It also demonstrates that platforms should be evaluated as part of an\ninterconnected ecosystem rather than through isolated processes, which is\ncrucial for effective compliance evaluation under the DSA.", "AI": {"tldr": "\u672c\u6587\u4ee5Instagram\u4e3a\u4f8b\uff0c\u63d0\u51fa\u591a\u5c42\u6b21\u4e00\u81f4\u6027\u6846\u67b6\u7528\u4e8e\u8bc4\u4f30DSA\u5408\u89c4\u6027\uff0c\u53d1\u73b0\u5e94\u5c06\u5e73\u53f0\u89c6\u4f5c\u6574\u4f53\u751f\u6001\u7cfb\u7edf\u8003\u91cf\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u76d1\u7ba1\u548c\u5408\u89c4\u6548\u679c\u3002", "motivation": "DSA\u5bf9\u5185\u5bb9\u5ba1\u6838\u548c\u5e73\u53f0\u6cbb\u7406\u63d0\u51fa\u4e86\u65b0\u8981\u6c42\uff0c\u5c24\u5176\u5bf9\u5927\u578b\u5e73\u53f0\u548c\u641c\u7d22\u5f15\u64ce\u63d0\u51fa\u4e86\u5f3a\u5236\u5408\u89c4\u673a\u5236\uff0c\u4e9f\u9700\u5b9e\u8bc1\u7814\u7a76\u4ee5\u6307\u5bfc\u5e73\u53f0\u3001\u76d1\u7ba1\u8005\u548c\u7814\u7a76\u8005\u66f4\u597d\u5229\u7528DSA\u673a\u5236\u3002", "method": "\u5f00\u53d1\u5e76\u5e94\u7528\u4e86\u4e00\u4e2a\u591a\u5c42\u6b21\u4e00\u81f4\u6027\u6846\u67b6\uff0c\u5bf9Instagram\u9075\u5faaDSA\u5408\u89c4\u8981\u6c42\u7684\u8868\u73b0\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "result": "\u4e00\u81f4\u6027\u8bc4\u4f30\u6846\u67b6\u80fd\u591f\u5e2e\u52a9\u53d1\u73b0\u5408\u89c4\u5931\u8bef\uff1b\u7814\u7a76\u4e3a\u5b9e\u8bc1\u57fa\u7840\u76d1\u7ba1\u63d0\u4f9b\u4e86\u89c1\u89e3\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u5e73\u53f0\u62a5\u544a\u3001\u6267\u6cd5\u7684\u8d28\u91cf\u4e0e\u95ee\u8d23\u6027\u3002", "conclusion": "\u5e73\u53f0\u5e94\u88ab\u89c6\u4f5c\u76f8\u4e92\u5173\u8054\u7684\u751f\u6001\u7cfb\u7edf\uff0c\u800c\u975e\u5b64\u7acb\u4e2a\u4f53\u8fdb\u884c\u8bc4\u4f30\uff0c\u8fd9\u5bf9\u4e8e\u6709\u6548\u7684DSA\u5408\u89c4\u8bc4\u4f30\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2507.01706", "categories": ["cs.CE"], "pdf": "https://arxiv.org/pdf/2507.01706", "abs": "https://arxiv.org/abs/2507.01706", "authors": ["Dominik Itner", "Dmitrij Dreiling", "Hauke Gravenkamp", "Bernd Henning", "Carolin Birk"], "title": "A modified Levenberg-Marquardt method for estimating the elastic material parameters of polymer waveguides using residuals between autocorrelated frequency responses", "comment": null, "summary": "In this contribution, we address the estimation of the frequency-dependent\nelastic parameters of polymers in the ultrasound range, which is formulated as\nan inverse problem. This inverse problem is implemented as a nonlinear\nregression-type optimization problem, in which the simulation signals are\nfitted to the measurement signals. These signals consist of displacement\nresponses in waveguides, focusing on hollow cylindrical geometries to enhance\nthe simulation efficiency. To accelerate the optimization and reduce the number\nof model evaluations and wait times, we propose two novel methods. First, we\nintroduce an adaptation of the Levenberg-Marquardt method derived from a\ngeometrical interpretation of the least-squares optimization problem. Second,\nwe introduce an improved objective function based on the autocorrelated\nenvelopes of the measurement and simulation signals. Given that this study\nprimarily relies on simulation data to quantify optimization convergence, we\naggregate the expected ranges of realistic material parameters and derive their\ndistributions to ensure the reproducibility of optimizations with proper\nmeasurements. We demonstrate the effectiveness of our objective function\nmodification and step adaptation for various materials with isotropic material\nsymmetry by comparing them with a state-of-the-art optimization method. In all\ncases, our method reduces the total number of model evaluations, thereby\nshortening the time to identify the material parameters.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e24\u9879\u65b0\u65b9\u6cd5\u6539\u8fdb\u805a\u5408\u7269\u8d85\u58f0\u5f39\u6027\u53c2\u6570\u4f30\u7b97\uff1a\u91c7\u7528\u6539\u8fdb\u7684\u4f18\u5316\u5668\u548c\u76ee\u6807\u51fd\u6570\uff0c\u80fd\u51cf\u5c11\u6a21\u578b\u8c03\u7528\uff0c\u63d0\u5347\u8fd0\u7b97\u901f\u5ea6\uff0c\u5bf9\u5b9e\u9645\u5e94\u7528\u6709\u660e\u663e\u5e2e\u52a9\u3002", "motivation": "\u8be5\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u4f7f\u7528\u8d85\u58f0\u6ce2\u6d4b\u91cf\u805a\u5408\u7269\u9891\u7387\u76f8\u5173\u5f39\u6027\u53c2\u6570\u65f6\u7684\u8ba1\u7b97\u6548\u7387\u4e0e\u53c2\u6570\u4f30\u7b97\u51c6\u786e\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u4f18\u5316\u8fc7\u7a0b\u4e2d\u6a21\u578b\u8bc4\u4f30\u6b21\u6570\u591a\u3001\u8017\u65f6\u957f\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u5c06\u6b64\u9006\u95ee\u9898\u8f6c\u5316\u4e3a\u975e\u7ebf\u6027\u56de\u5f52\u578b\u4f18\u5316\uff0c\u5e76\u4e13\u6ce8\u4e8e\u7a7a\u5fc3\u5706\u67f1\u6ce2\u5bfc\u7684\u6a21\u62df\u4ee5\u63d0\u9ad8\u4eff\u771f\u6548\u7387\u3002\u65b9\u6cd5\u521b\u65b0\u5305\u62ec\uff1a(1) \u57fa\u4e8e\u6700\u5c0f\u4e8c\u4e58\u51e0\u4f55\u89e3\u91ca\u6539\u8fdbLevenberg-Marquardt\u65b9\u6cd5\u7528\u4e8e\u6b65\u957f\u9002\u5e94\uff0c(2) \u57fa\u4e8e\u81ea\u52a8\u76f8\u5173\u5305\u7edc\u6539\u8fdb\u76ee\u6807\u51fd\u6570\u3002", "result": "\u6240\u63d0\u65b9\u6cd5\u5728\u5404\u79cd\u5404\u5411\u540c\u6027\u6750\u6599\u5b9e\u9a8c\u4e2d\uff0c\u4e0e\u73b0\u6709\u4f18\u5316\u7b97\u6cd5\u6bd4\u8f83\uff0c\u80fd\u591f\u5927\u5e45\u51cf\u5c11\u6a21\u578b\u8bc4\u4f30\u6b21\u6570\uff0c\u663e\u8457\u7f29\u77ed\u6750\u6599\u53c2\u6570\u8bc6\u522b\u603b\u8017\u65f6\u3002", "conclusion": "\u6240\u6539\u8fdb\u7684\u4f18\u5316\u65b9\u6cd5\u548c\u76ee\u6807\u51fd\u6570\u63d0\u5347\u4e86\u805a\u5408\u7269\u5f39\u6027\u53c2\u6570\u8bc6\u522b\u7684\u6548\u7387\u4e0e\u53ef\u590d\u73b0\u6027\uff0c\u5bf9\u5b9e\u9645\u6d4b\u91cf\u5177\u6709\u63a8\u5e7f\u4ef7\u503c\u3002"}}
{"id": "2507.01213", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01213", "abs": "https://arxiv.org/abs/2507.01213", "authors": ["Adamu Lawan", "Juhua Pu", "Haruna Yunusa", "Jawad Muhammad", "Muhammad Lawan"], "title": "MEGA: xLSTM with Multihead Exponential Gated Fusion for Precise Aspect-based Sentiment Analysis", "comment": "6, 1 figure", "summary": "Aspect-based Sentiment Analysis (ABSA) is a critical Natural Language\nProcessing (NLP) task that extracts aspects from text and determines their\nassociated sentiments, enabling fine-grained analysis of user opinions.\nExisting ABSA methods struggle to balance computational efficiency with high\nperformance: deep learning models often lack global context, transformers\ndemand significant computational resources, and Mamba-based approaches face\nCUDA dependency and diminished local correlations. Recent advancements in\nExtended Long Short-Term Memory (xLSTM) models, particularly their efficient\nmodeling of long-range dependencies, have significantly advanced the NLP\ncommunity. However, their potential in ABSA remains untapped. To this end, we\npropose xLSTM with Multihead Exponential Gated Fusion (MEGA), a novel framework\nintegrating a bi-directional mLSTM architecture with forward and partially\nflipped backward (PF-mLSTM) streams. The PF-mLSTM enhances localized context\nmodeling by processing the initial sequence segment in reverse with dedicated\nparameters, preserving critical short-range patterns. We further introduce an\nmLSTM-based multihead cross exponential gated fusion mechanism (MECGAF) that\ndynamically combines forward mLSTM outputs as query and key with PF-mLSTM\noutputs as value, optimizing short-range dependency capture while maintaining\nglobal context and efficiency. Experimental results on three benchmark datasets\ndemonstrate that MEGA outperforms state-of-the-art baselines, achieving\nsuperior accuracy and efficiency in ABSA tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u57fa\u4e8exLSTM\u548cmLSTM\u7684MEGA\u6846\u67b6\uff0c\u6709\u6548\u5e73\u8861\u4e86\u8ba1\u7b97\u6548\u7387\u4e0e\u6027\u80fd\uff0c\u5728ABSA\u4efb\u52a1\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709ABSA\u65b9\u6cd5\u5728\u8ba1\u7b97\u6548\u7387\u4e0e\u6027\u80fd\u4e4b\u95f4\u96be\u4ee5\u5e73\u8861\u3002\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7f3a\u4e4f\u5168\u5c40\u8bed\u5883\uff0cTransformer\u8d44\u6e90\u8017\u8d39\u5927\uff0cMamba\u65b9\u6cd5\u4f9d\u8d56CUDA\u4e14\u96be\u4ee5\u5efa\u6a21\u5c40\u90e8\u76f8\u5173\u6027\u3002xLSTM\u5728\u957f\u8ddd\u79bb\u4f9d\u8d56\u5efa\u6a21\u4e0a\u6709\u65b0\u7a81\u7834\uff0c\u4f46\u5728ABSA\u9886\u57df\u5e94\u7528\u5c1a\u672a\u5f00\u53d1\u3002", "method": "MEGA\u7ed3\u5408\u4e86\u53cc\u5411xLSTM\u7ed3\u6784\uff0c\u5305\u62ec\u6b63\u5411mLSTM\u548c\u90e8\u5206\u7ffb\u8f6c\u7684\u53cd\u5411\uff08PF-mLSTM\uff09\u6d41\u6a21\u578b\uff0c\u5e76\u5f15\u5165\u57fa\u4e8emLSTM\u7684\u591a\u5934\u4ea4\u53c9\u6307\u6570\u95e8\u63a7\u878d\u5408\u673a\u5236\uff08MECGAF\uff09\uff0c\u52a8\u6001\u6574\u5408\u6b63\u5411\u548c\u53cd\u5411\u6d41\u4fe1\u606f\u4ee5\u4f18\u5316\u5c40\u90e8\u4e0e\u5168\u5c40\u4f9d\u8d56\u5efa\u6a21\u3002", "result": "MEGA\u65b9\u6cd5\u5728ABSA\u4efb\u52a1\u4e0a\u4f18\u4e8e\u5f53\u524d\u4e3b\u6d41\u65b9\u6cd5\uff0c\u5728\u4e09\u4e2a\u516c\u5f00\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5747\u53d6\u5f97\u66f4\u9ad8\u7684\u51c6\u786e\u7387\u548c\u66f4\u4f18\u7684\u6548\u7387\u3002", "conclusion": "\u63d0\u51fa\u7684MEGA\u6846\u67b6\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u5728\u51c6\u786e\u7387\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u5747\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u5148\u8fdb\u6a21\u578b\u3002"}}
{"id": "2507.01410", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01410", "abs": "https://arxiv.org/abs/2507.01410", "authors": ["Abeer Dyoub", "Francesca A. Lisi"], "title": "A Fuzzy Approach to the Specification, Verification and Validation of Risk-Based Ethical Decision Making Models", "comment": null, "summary": "The ontological and epistemic complexities inherent in the moral domain make\nit challenging to establish clear standards for evaluating the performance of a\nmoral machine. In this paper, we present a formal method to describe Ethical\nDecision Making models based on ethical risk assessment. Then, we show how\nthese models that are specified as fuzzy rules can be verified and validated\nusing fuzzy Petri nets. A case study from the medical field is considered to\nillustrate the proposed approach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4f26\u7406\u98ce\u9669\u548c\u6a21\u7cca\u89c4\u5219\u7684\u9053\u5fb7\u51b3\u7b56\u6a21\u578b\u5efa\u6a21\u4e0e\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u533b\u7597\u6848\u4f8b\u52a0\u4ee5\u9a8c\u8bc1\u3002", "motivation": "\u7531\u4e8e\u9053\u5fb7\u9886\u57df\u672c\u8eab\u5728\u672c\u4f53\u8bba\u548c\u8ba4\u8bc6\u8bba\u4e0a\u7684\u590d\u6742\u6027\uff0c\u5f88\u96be\u4e3a\u9053\u5fb7\u673a\u5668\u7684\u8bc4\u4f30\u5efa\u7acb\u660e\u786e\u6807\u51c6\u3002\u4f5c\u8005\u5e0c\u671b\u4e3a\u6b64\u63d0\u4f9b\u7cfb\u7edf\u7684\u5efa\u6a21\u548c\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u4f26\u7406\u98ce\u9669\u8bc4\u4f30\u7684\u5f62\u5f0f\u5316\u4f26\u7406\u51b3\u7b56\u6a21\u578b\uff0c\u5e76\u5c06\u8fd9\u4e9b\u4ee5\u6a21\u7cca\u89c4\u5219\u8868\u793a\u7684\u6a21\u578b\uff0c\u901a\u8fc7\u6a21\u7ccaPetri\u7f51\u8fdb\u884c\u9a8c\u8bc1\u548c\u786e\u8ba4\u3002", "result": "\u901a\u8fc7\u4e00\u4e2a\u533b\u7597\u9886\u57df\u7684\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u53ef\u884c\u6027\u3002", "conclusion": "\u4e3a\u9053\u5fb7\u51b3\u7b56\u5efa\u6a21\u63d0\u4f9b\u4e86\u4e00\u5957\u53ef\u5f62\u5f0f\u5316\u63cf\u8ff0\u3001\u53ef\u9a8c\u8bc1\u7684\u65b9\u6cd5\uff0c\u5bf9\u9053\u5fb7AI\u7cfb\u7edf\u7684\u6807\u51c6\u5316\u548c\u8bc4\u4f30\u5177\u6709\u53c2\u8003\u4ef7\u503c\u3002"}}
{"id": "2507.01234", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01234", "abs": "https://arxiv.org/abs/2507.01234", "authors": ["Yu Fan", "Yang Tian", "Shauli Ravfogel", "Mrinmaya Sachan", "Elliott Ash", "Alexander Hoyle"], "title": "The Medium Is Not the Message: Deconfounding Text Embeddings via Linear Concept Erasure", "comment": null, "summary": "Embedding-based similarity metrics between text sequences can be influenced\nnot just by the content dimensions we most care about, but can also be biased\nby spurious attributes like the text's source or language. These document\nconfounders cause problems for many applications, but especially those that\nneed to pool texts from different corpora. This paper shows that a debiasing\nalgorithm that removes information about observed confounders from the encoder\nrepresentations substantially reduces these biases at a minimal computational\ncost. Document similarity and clustering metrics improve across every embedding\nvariant and task we evaluate -- often dramatically. Interestingly, performance\non out-of-distribution benchmarks is not impacted, indicating that the\nembeddings are not otherwise degraded.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u53bb\u9664\u6587\u672c\u5d4c\u5165\u4e2d\u6df7\u6742\u56e0\u5b50\u4fe1\u606f\u7684\u7b97\u6cd5\uff0c\u5927\u5e45\u63d0\u5347\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u6587\u672c\u76f8\u4f3c\u5ea6\u4e0e\u805a\u7c7b\u4efb\u52a1\u8868\u73b0\uff0c\u4e14\u4e0d\u4f1a\u635f\u5bb3\u6a21\u578b\u5728\u65b0\u5206\u5e03\u4e0a\u7684\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u6587\u672c\u5d4c\u5165\u76f8\u4f3c\u5ea6\u6307\u6807\u4e0d\u4ec5\u53d7\u6211\u4eec\u5173\u6ce8\u7684\u5185\u5bb9\u5f71\u54cd\uff0c\u8fd8\u5bb9\u6613\u88ab\u6587\u672c\u6765\u6e90\u3001\u8bed\u8a00\u7b49\u65e0\u5173\u53d8\u91cf\uff08\u6587\u6863\u6df7\u6742\u56e0\u5b50\uff09\u5e72\u6270\uff0c\u8fd9\u5728\u9700\u8981\u6574\u5408\u4e0d\u540c\u8bed\u6599\u7684\u573a\u666f\u4e0b\u5c24\u4e3a\u4e25\u91cd\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u53bb\u9664\u7f16\u7801\u5668\u8868\u8fbe\u4e2d\u5df2\u89c2\u6d4b\u6df7\u6742\u56e0\u5b50\u7684\u53bb\u504f\u7b97\u6cd5\uff0c\u4ece\u5d4c\u5165\u8868\u793a\u4e2d\u5254\u9664\u4e0e\u8fd9\u4e9b\u56e0\u7d20\u6709\u5173\u7684\u4fe1\u606f\uff0c\u4ee5\u964d\u4f4e\u5176\u5e26\u6765\u7684\u5e72\u6270\u3002", "result": "\u5728\u8bc4\u4f30\u7684\u6240\u6709\u5d4c\u5165\u53d8\u4f53\u548c\u4efb\u52a1\u4e2d\uff0c\u6587\u6863\u76f8\u4f3c\u5ea6\u548c\u805a\u7c7b\u6307\u6807\u5747\u5f97\u5230\u4e86\u663e\u8457\u63d0\u5347\uff0c\u4e14\u63d0\u5347\u5e45\u5ea6\u5f80\u5f80\u5f88\u5927\u3002\u540c\u65f6\uff0c\u5728\u5206\u5e03\u5916\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u672a\u53d7\u5f71\u54cd\uff0c\u8868\u660e\u5d4c\u5165\u8d28\u91cf\u672a\u56e0\u53bb\u504f\u8fc7\u7a0b\u800c\u4e0b\u964d\u3002", "conclusion": "\u901a\u8fc7\u6709\u9488\u5bf9\u6027\u5730\u53bb\u9664\u4e0e\u6df7\u6742\u56e0\u5b50\u76f8\u5173\u7684\u4fe1\u606f\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u6587\u672c\u5d4c\u5165\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u6539\u5584\u8de8\u8bed\u6599\u6587\u672c\u5904\u7406\u4efb\u52a1\u7684\u8868\u73b0\uff0c\u540c\u65f6\u65e0\u663e\u8457\u526f\u4f5c\u7528\u3002"}}
{"id": "2507.01431", "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01431", "abs": "https://arxiv.org/abs/2507.01431", "authors": ["Yoonseok Yang", "Minjune Kim", "Marlon Rondinelli", "Keren Shao"], "title": "Pensieve Grader: An AI-Powered, Ready-to-Use Platform for Effortless Handwritten STEM Grading", "comment": "7 pages, 5 figues, 1 table", "summary": "Grading handwritten, open-ended responses remains a major bottleneck in large\nuniversity STEM courses. We introduce Pensieve (https://www.pensieve.co), an\nAI-assisted grading platform that leverages large language models (LLMs) to\ntranscribe and evaluate student work, providing instructors with rubric-aligned\nscores, transcriptions, and confidence ratings. Unlike prior tools that focus\nnarrowly on specific tasks like transcription or rubric generation, Pensieve\nsupports the entire grading pipeline-from scanned student submissions to final\nfeedback-within a human-in-the-loop interface.\n  Pensieve has been deployed in real-world courses at over 20 institutions and\nhas graded more than 300,000 student responses. We present system details and\nempirical results across four core STEM disciplines: Computer Science,\nMathematics, Physics, and Chemistry. Our findings show that Pensieve reduces\ngrading time by an average of 65%, while maintaining a 95.4% agreement rate\nwith instructor-assigned grades for high-confidence predictions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5e76\u5b9e\u6d4b\u4e86AI\u8f85\u52a9\u6279\u6539\u7cfb\u7edfPensieve\uff0c\u663e\u8457\u63d0\u5347STEM\u5927\u73ed\u8bfe\u7a0b\u624b\u5199\u9898\u76ee\u7684\u6279\u6539\u6548\u7387\u548c\u4e00\u81f4\u6027\u3002", "motivation": "\u624b\u5199\u3001\u5f00\u653e\u6027\u4f5c\u4e1a\u6279\u6539\u5728\u5927\u5b66\u5927\u73edSTEM\u8bfe\u7a0b\u4e2d\u6781\u5176\u8017\u65f6\u4e14\u96be\u4ee5\u6807\u51c6\u5316\uff0c\u9700\u8981\u9ad8\u6548\u5e76\u53ef\u9760\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u642d\u5efaAI\u8f85\u52a9\u6279\u6539\u5e73\u53f0Pensieve\uff0c\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u5b9e\u73b0\u4ece\u8bd5\u5377\u626b\u63cf\u5230\u8bc4\u5206\u4e0e\u53cd\u9988\u7684\u5b8c\u6574\u6d41\u7a0b\uff0c\u91c7\u7528\u4eba\u673a\u534f\u4f5c\u6a21\u5f0f\uff0c\u5e76\u572820\u591a\u6240\u673a\u6784\u300130\u4e07\u5b66\u751f\u7b54\u5377\u4e2d\u90e8\u7f72\u4e0e\u6d4b\u8bd5\u3002", "result": "Pensieve\u5e73\u53f0\u5e73\u5747\u51cf\u5c1165%\u7684\u6279\u6539\u65f6\u95f4\uff0c\u4e14\u9ad8\u7f6e\u4fe1\u9884\u6d4b\u4e0e\u6559\u5e08\u8bc4\u5206\u4e00\u81f4\u7387\u8fbe95.4%\u3002", "conclusion": "Pensieve\u7cfb\u7edf\u80fd\u591f\u6709\u6548\u63d0\u5347\u6279\u6539\u624b\u5199\u3001\u5f00\u653e\u6027\u7b54\u9898\u7684\u6548\u7387\u548c\u4e00\u81f4\u6027\uff0c\u5728\u56db\u5927STEM\u5b66\u79d1\u4e2d\u5747\u8868\u73b0\u4f18\u79c0\u3002"}}
{"id": "2507.01936", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.01936", "abs": "https://arxiv.org/abs/2507.01936", "authors": ["Adrian de Wynter", "Tangming Yuan"], "title": "The Thin Line Between Comprehension and Persuasion in LLMs", "comment": null, "summary": "Large language models (LLMs) are excellent at maintaining high-level,\nconvincing dialogues. They are being fast deployed as chatbots and evaluators\nin sensitive areas, such as peer review and mental health applications. This,\nalong with the disparate accounts on their reasoning capabilities, calls for a\ncloser examination of LLMs and their comprehension of dialogue. In this work we\nbegin by evaluating LLMs' ability to maintain a debate--one of the purest yet\nmost complex forms of human communication. Then we measure how this capability\nrelates to their understanding of what is being talked about, namely, their\ncomprehension of dialogical structures and the pragmatic context. We find that\nLLMs are capable of maintaining coherent, persuasive debates, often swaying the\nbeliefs of participants and audiences alike. We also note that awareness or\nsuspicion of AI involvement encourage people to be more critical of the\narguments made. When polling LLMs on their comprehension of deeper structures\nof dialogue, however, they cannot demonstrate said understanding. Our findings\ntie the shortcomings of LLMs-as-evaluators to their (in)ability to understand\nthe context. More broadly, for the field of argumentation theory we posit that,\nif an agent can convincingly maintain a dialogue, it is not necessary for it to\nknow what it is talking about. Hence, the modelling of pragmatic context and\ncoherence are secondary to effectiveness.", "AI": {"tldr": "LLMs\u53ef\u4ee5\u6709\u6548\u5730\u8fdb\u884c\u8bf4\u670d\u6027\u8fa9\u8bba\u5e76\u5f71\u54cd\u610f\u89c1\uff0c\u4f46\u5176\u5bf9\u5bf9\u8bdd\u6df1\u5c42\u7ed3\u6784\u7684\u7406\u89e3\u6709\u9650\u3002\u5584\u4e8e\u5bf9\u8bdd\u4e0d\u7b49\u4e8e\u771f\u6b63\u7406\u89e3\u5185\u5bb9\uff0c\u6548\u679c\u548c\u5f71\u54cd\u529b\u4f18\u5148\u4e8e\u771f\u6b63\u7684\u8bed\u7528\u7406\u89e3\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5df2\u7ecf\u5feb\u901f\u5e94\u7528\u4e8e\u654f\u611f\u9886\u57df\u5982\u540c\u884c\u8bc4\u5ba1\u548c\u5fc3\u7406\u5065\u5eb7\uff0c\u4f46\u5176\u63a8\u7406\u80fd\u529b\u4ecd\u5b58\u5728\u4e89\u8bae\u3002\u56e0\u6b64\u6709\u5fc5\u8981\u6df1\u5165\u63a2\u7a76LLMs\u5bf9\u5bf9\u8bdd\u7684\u7406\u89e3\u80fd\u529b\u3002\u4f5c\u8005\u4ee5\u201c\u8fa9\u8bba\u201d\u4f5c\u4e3a\u590d\u6742\u4eba\u7c7b\u4ea4\u6d41\u7684\u4ee3\u8868\uff0c\u8003\u5bdfLLMs\u7684\u5bf9\u8bdd\u7406\u89e3\u53ca\u5176\u5c40\u9650\u6027\u3002", "method": "\u9996\u5148\u8bc4\u4f30LLMs\u7ef4\u6301\u8fa9\u8bba\uff08\u5bf9\u8bdd\u7684\u4e00\u79cd\u590d\u6742\u5f62\u5f0f\uff09\u7684\u80fd\u529b\uff0c\u518d\u8861\u91cf\u5176\u5bf9\u6b64\u8fc7\u7a0b\u4e2d\u5bf9\u8bdd\u7ed3\u6784\u548c\u8bed\u7528\u8bed\u5883\u7406\u89e3\u7684\u5173\u8054\uff1b\u5e76\u901a\u8fc7\u8c03\u67e5\u53c2\u4e0e\u8005\u5bf9AI\u53c2\u4e0e\u7684\u8ba4\u77e5\u5bf9\u6279\u5224\u6027\u601d\u8003\u7684\u5f71\u54cd\uff1b\u6700\u540e\u901a\u8fc7\u7ed9LLM\u6d4b\u8bd5\u9898\uff0c\u63a2\u67e5\u5176\u5bf9\u66f4\u6df1\u5c42\u6b21\u5bf9\u8bdd\u7ed3\u6784\u7684\u7406\u89e3\u529b\u3002", "result": "LLMs\u53ef\u4ee5\u8fdb\u884c\u8fde\u8d2f\u3001\u5177\u6709\u8bf4\u670d\u529b\u7684\u8fa9\u8bba\uff0c\u80fd\u5f71\u54cd\u53c2\u4e0e\u8005\u548c\u89c2\u4f17\u7684\u4fe1\u5ff5\u3002\u4f46\u5f53\u4eba\u4eec\u610f\u8bc6\u5230AI\u53c2\u4e0e\u65f6\uff0c\u66f4\u503e\u5411\u6279\u5224\u6027\u601d\u8003\u3002\u540c\u65f6\uff0c\u5728\u68c0\u9a8c\u5176\u5bf9\u5bf9\u8bdd\u6df1\u5c42\u7ed3\u6784\u7406\u89e3\u65f6\uff0cLLM\u672a\u80fd\u8868\u73b0\u51fa\u5b9e\u8d28\u6027\u7406\u89e3\u3002\u5176\u4f5c\u4e3a\u8bc4\u4f30\u8005\u7684\u77ed\u677f\u5f52\u56e0\u4e8e\u4e0a\u8ff0\u7406\u89e3\u7684\u4e0d\u8db3\u3002", "conclusion": "LLMs\u80fd\u591f\u6709\u6548\u7ef4\u6301\u5bf9\u8bdd\uff0c\u4f46\u8fd9\u5e76\u4e0d\u610f\u5473\u7740\u5176\u771f\u6b63\u7406\u89e3\u6240\u8c08\u5185\u5bb9\u3002\u5bf9\u8bdd\u7684\u8bed\u7528\u8bed\u5883\u548c\u8fde\u8d2f\u6027\u53ef\u4ee5\u662f\u6b21\u8981\u7684\uff0c\u800c\u6709\u6548\u6027\u624d\u662f\u5173\u952e\u3002\u5bf9\u8bba\u8bc1\u7406\u8bba\u9886\u57df\u800c\u8a00\uff0c\u201c\u6709\u6548\u4ea4\u6d41\u201d\u4e0d\u9700\u201c\u771f\u5b9e\u7406\u89e3\u201d\u3002"}}
{"id": "2507.01259", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01259", "abs": "https://arxiv.org/abs/2507.01259", "authors": ["Micha\u0142 Matak", "Jaros\u0142aw A. Chudziak"], "title": "GAIus: Combining Genai with Legal Clauses Retrieval for Knowledge-based Assistant", "comment": "8 pages, 2 figures, presented at ICAART 2025, in proceedings of the\n  17th International Conference on Agents and Artificial Intelligence - Volume\n  3: ICAART", "summary": "In this paper we discuss the capability of large language models to base\ntheir answer and provide proper references when dealing with legal matters of\nnon-english and non-chinese speaking country. We discuss the history of legal\ninformation retrieval, the difference between case law and statute law, its\nimpact on the legal tasks and analyze the latest research in this field. Basing\non that background we introduce gAIus, the architecture of the cognitive\nLLM-based agent, whose responses are based on the knowledge retrieved from\ncertain legal act, which is Polish Civil Code. We propose a retrieval mechanism\nwhich is more explainable, human-friendly and achieves better results than\nembedding-based approaches. To evaluate our method we create special dataset\nbased on single-choice questions from entrance exams for law apprenticeships\nconducted in Poland. The proposed architecture critically leveraged the\nabilities of used large language models, improving the gpt-3.5-turbo-0125 by\n419%, allowing it to beat gpt-4o and lifting gpt-4o-mini score from 31% to 86%.\nAt the end of our paper we show the possible future path of research and\npotential applications of our findings.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6ce2\u5170\u6cd5\u5f8b\u4f53\u7cfb\u4e0b\u95ee\u7b54\u65f6\u4f9d\u636e\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8ba4\u77e5\u67b6\u6784\u548c\u53ef\u89e3\u91ca\u6027\u68c0\u7d22\u7684\u65b0\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u6cd5\u5f8b\u8003\u8bd5\u4e2d\u7684\u8868\u73b0\uff0c\u4e3a\u6cd5\u5f8bAI\u52a9\u7406\u7684\u5e94\u7528\u62d3\u5c55\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u975e\u82f1\u8bed\u548c\u975e\u4e2d\u6587\u56fd\u5bb6\u7684\u6cd5\u5f8b\u95ee\u9898\u65f6\uff0c\u5f80\u5f80\u96be\u4ee5\u505a\u5230\u5145\u5206\u5f15\u7528\u548c\u7ed9\u51fa\u51c6\u786e\u4f9d\u636e\u3002\u672c\u6587\u65e8\u5728\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7279\u5b9a\u6cd5\u5f8b\u4f53\u7cfb\uff08\u5982\u6ce2\u5170\u6c11\u4e8b\u6cd5\u5178\uff09\u4e0b\u7684\u68c0\u7d22\u4e0e\u63a8\u7406\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8ba4\u77e5\u67b6\u6784\u7684LLM\u4ee3\u7406\uff08gAIus\uff09\uff0c\u901a\u8fc7\u53ef\u89e3\u91ca\u6027\u66f4\u5f3a\u3001\u5bf9\u4eba\u7c7b\u66f4\u53cb\u597d\u7684\u68c0\u7d22\u673a\u5236\uff0c\u5229\u7528\u7279\u5b9a\u6cd5\u5f8b\u6587\u672c\u76f4\u63a5\u4e3a\u6cd5\u5f8b\u95ee\u7b54\u63d0\u4f9b\u652f\u6301\uff0c\u5e76\u6784\u5efa\u4e86\u57fa\u4e8e\u6ce2\u5170\u6cd5\u5f8b\u5b9e\u4e60\u751f\u5165\u5b66\u8003\u8bd5\u5355\u9009\u9898\u7684\u6570\u636e\u96c6\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u65b0\u63d0\u51fa\u7684\u68c0\u7d22\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86gpt-3.5-turbo-0125\u7684\u5f97\u5206\uff08\u63d0\u5347419%\uff09\uff0c\u5e76\u4f7fgpt-4o-mini\u7684\u6b63\u786e\u7387\u4ece31%\u63d0\u5347\u81f386%\uff0c\u751a\u81f3\u8d85\u8d8a\u4e86gpt-4o\u7684\u8868\u73b0\u3002\u8be5\u65b9\u6cd5\u6bd4\u4f20\u7edfembedding-based\u65b9\u6cd5\u6548\u679c\u66f4\u4f73\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u4e13\u95e8\u7684\u6cd5\u5f8b\u68c0\u7d22\u673a\u5236\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u53ef\u4ee5\u5927\u5e45\u63d0\u5347LLM\u5728\u7279\u5b9a\u6cd5\u5f8b\u4f53\u7cfb\u4e0b\u7684\u95ee\u7b54\u80fd\u529b\u3002\u672a\u6765\u8be5\u67b6\u6784\u53ef\u62d3\u5c55\u5230\u66f4\u591a\u56fd\u5bb6\u548c\u6cd5\u5f8b\u9886\u57df\u7684\u667a\u80fd\u5e94\u7528\u3002"}}
{"id": "2507.01446", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01446", "abs": "https://arxiv.org/abs/2507.01446", "authors": ["Abd Elrahman Amer", "Magdi Amer"], "title": "Using multi-agent architecture to mitigate the risk of LLM hallucinations", "comment": null, "summary": "Improving customer service quality and response time are critical factors for\nmaintaining customer loyalty and increasing a company's market share. While\nadopting emerging technologies such as Large Language Models (LLMs) is becoming\na necessity to achieve these goals, the risk of hallucination remains a major\nchallenge. In this paper, we present a multi-agent system to handle customer\nrequests sent via SMS. This system integrates LLM based agents with fuzzy logic\nto mitigate hallucination risks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0e\u6a21\u7cca\u903b\u8f91\u7ed3\u5408\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u901a\u8fc7\u77ed\u4fe1\u4e3a\u5ba2\u6237\u670d\u52a1\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u5927\u6a21\u578b\u5e7b\u89c9\u98ce\u9669\u3002", "motivation": "\u63d0\u5347\u5ba2\u6237\u670d\u52a1\u8d28\u91cf\u548c\u54cd\u5e94\u901f\u5ea6\u5bf9\u4e8e\u4fdd\u6301\u5ba2\u6237\u5fe0\u8bda\u548c\u63d0\u5347\u5e02\u573a\u4efd\u989d\u81f3\u5173\u91cd\u8981\u3002\u5c3d\u7ba1\u91c7\u7528\u5927\u6a21\u578b\uff08LLM\uff09\u53ef\u4ee5\u5e2e\u52a9\u5b9e\u73b0\u8fd9\u4e9b\u76ee\u6807\uff0c\u4f46\u5176\u5e7b\u89c9\uff08hallucination\uff09\u98ce\u9669\u4f9d\u7136\u662f\u4e3b\u8981\u6311\u6218\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u5904\u7406\u901a\u8fc7\u77ed\u4fe1\u53d1\u9001\u7684\u5ba2\u6237\u8bf7\u6c42\u3002\u8be5\u7cfb\u7edf\u6574\u5408\u4e86\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u667a\u80fd\u4f53\u548c\u6a21\u7cca\u903b\u8f91\uff0c\u7528\u4e8e\u964d\u4f4e\u5e7b\u89c9\u98ce\u9669\u3002", "result": "\u7cfb\u7edf\u80fd\u591f\u6709\u6548\u6574\u5408LLM\u548c\u6a21\u7cca\u903b\u8f91\u65b9\u6cd5\uff0c\u6709\u671b\u63d0\u5347\u5ba2\u6237\u670d\u52a1\u7684\u667a\u80fd\u5316\u6c34\u5e73\uff0c\u5e76\u51cf\u5c11LLM\u5e26\u6765\u7684\u5e7b\u89c9\u98ce\u9669\u3002", "conclusion": "\u901a\u8fc7\u5c06\u6a21\u7cca\u903b\u8f91\u4e0eLLM\u76f8\u7ed3\u5408\uff0c\u8be5\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e3a\u63d0\u5347\u5ba2\u6237\u670d\u52a1\u8d28\u91cf\u548c\u54cd\u5e94\u901f\u5ea6\u3001\u964d\u4f4e\u98ce\u9669\u63d0\u4f9b\u4e86\u4e00\u6761\u53ef\u884c\u9014\u5f84\u3002"}}
{"id": "2507.01278", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01278", "abs": "https://arxiv.org/abs/2507.01278", "authors": ["Cindy Lie Tabuse", "David Restepo", "Carolina Gracitelli", "Fernando Korn Malerbi", "Caio Regatieri", "Luis Filipe Nakayama"], "title": "Evaluating Large Language Models for Multimodal Simulated Ophthalmic Decision-Making in Diabetic Retinopathy and Glaucoma Screening", "comment": null, "summary": "Large language models (LLMs) can simulate clinical reasoning based on natural\nlanguage prompts, but their utility in ophthalmology is largely unexplored.\nThis study evaluated GPT-4's ability to interpret structured textual\ndescriptions of retinal fundus photographs and simulate clinical decisions for\ndiabetic retinopathy (DR) and glaucoma screening, including the impact of\nadding real or synthetic clinical metadata. We conducted a retrospective\ndiagnostic validation study using 300 annotated fundus images. GPT-4 received\nstructured prompts describing each image, with or without patient metadata. The\nmodel was tasked with assigning an ICDR severity score, recommending DR\nreferral, and estimating the cup-to-disc ratio for glaucoma referral.\nPerformance was evaluated using accuracy, macro and weighted F1 scores, and\nCohen's kappa. McNemar's test and change rate analysis were used to assess the\ninfluence of metadata. GPT-4 showed moderate performance for ICDR\nclassification (accuracy 67.5%, macro F1 0.33, weighted F1 0.67, kappa 0.25),\ndriven mainly by correct identification of normal cases. Performance improved\nin the binary DR referral task (accuracy 82.3%, F1 0.54, kappa 0.44). For\nglaucoma referral, performance was poor across all settings (accuracy ~78%, F1\n<0.04, kappa <0.03). Metadata inclusion did not significantly affect outcomes\n(McNemar p > 0.05), and predictions remained consistent across conditions.\nGPT-4 can simulate basic ophthalmic decision-making from structured prompts but\nlacks precision for complex tasks. While not suitable for clinical use, LLMs\nmay assist in education, documentation, or image annotation workflows in\nophthalmology.", "AI": {"tldr": "GPT-4\u53ef\u901a\u8fc7\u7ed3\u6784\u5316\u6587\u672c\u63cf\u8ff0\uff0c\u6a21\u62df\u57fa\u7840\u773c\u79d1\u51b3\u7b56\uff0c\u4f46\u7cbe\u786e\u6027\u4e0d\u9ad8\uff0c\u6682\u4e0d\u9002\u5b9c\u4e34\u5e8a\u4f7f\u7528\uff0c\u53ef\u8f85\u52a9\u6559\u80b2\u4e0e\u6807\u6ce8\u3002", "motivation": "\u76ee\u524d\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u533b\u5b66\u9886\u57df\u7684\u4e34\u5e8a\u63a8\u7406\u80fd\u529b\u5df2\u521d\u6b65\u663e\u73b0\uff0c\u4f46\u5728\u773c\u79d1\u5b66\u9886\u57df\uff0c\u7279\u522b\u662f\u5728\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\uff08DR\uff09\u548c\u9752\u5149\u773c\u7b5b\u67e5\u65b9\u9762\u7684\u5e94\u7528\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u7814\u7a76\u65e8\u5728\u8bc4\u4f30GPT-4\u80fd\u5426\u57fa\u4e8e\u7ed3\u6784\u5316\u6587\u672c\u63cf\u8ff0\u7684\u773c\u5e95\u7167\u7247\uff0c\u8f85\u52a9\u6a21\u62df\u4e34\u5e8a\u51b3\u7b56\u3002", "method": "\u672c\u7814\u7a76\u4e3a\u56de\u987e\u6027\u8bca\u65ad\u9a8c\u8bc1\u7814\u7a76\u3002\u5229\u7528300\u5f20\u5e26\u6709\u6ce8\u91ca\u7684\u773c\u5e95\u56fe\u50cf\uff0c\u751f\u6210\u7ed3\u6784\u5316\u6587\u672c\u63cf\u8ff0\uff0c\u5206\u522b\u6709\u65e0\u60a3\u8005\u5143\u6570\u636e\uff0c\u8f93\u5165\u7ed9GPT-4\u3002\u6a21\u578b\u9700\u4f5c\u51faICDR\u5206\u7ea7\u3001DR\u8f6c\u8bca\u5efa\u8bae\u3001\u53ca\u4f30\u7b97\u9752\u5149\u773c\u7528\u7684\u89c6\u676f\u76d8\u6bd4\u3002\u901a\u8fc7\u51c6\u786e\u7387\u3001\u5b8f/\u52a0\u6743F1\u5206\u6570\u3001Cohen's kappa\u7b49\u6307\u6807\uff0c\u4ee5\u53caMcNemar\u68c0\u9a8c\u8bc4\u4f30\u5143\u6570\u636e\u5f71\u54cd\u3002", "result": "GPT-4\u5728ICDR\u5206\u7ea7\u7684\u8868\u73b0\u4e3a\u4e2d\u7b49\uff08\u51c6\u786e\u738767.5%\uff09\uff0c\u4e3b\u8981\u4f53\u73b0\u5728\u6b63\u5e38\u75c5\u4f8b\u7684\u8bc6\u522b\u3002DR\u8f6c\u8bca\u7684\u4e8c\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u597d\uff08\u51c6\u786e\u738782.3%\uff09\uff0c\u4f46\u9752\u5149\u773c\u7b5b\u67e5\u8868\u73b0\u8f83\u5dee\uff08\u51c6\u786e\u7387\u7ea678%\uff0c\u4f46F1\u53cakappa\u6781\u4f4e\uff09\u3002\u65e0\u8bba\u5143\u6570\u636e\u5982\u4f55\uff0c\u6a21\u578b\u9884\u6d4b\u7ed3\u679c\u5dee\u5f02\u4e0d\u663e\u8457\u3002", "conclusion": "GPT-4\u80fd\u4f9d\u8d56\u7ed3\u6784\u5316\u6587\u672c\u63cf\u8ff0\uff0c\u6a21\u62df\u57fa\u672c\u7684\u773c\u79d1\u4e34\u5e8a\u51b3\u7b56\uff0c\u4f46\u5bf9\u4e8e\u590d\u6742\u4efb\u52a1\u51c6\u786e\u7387\u4e0d\u8db3\uff0c\u76ee\u524d\u5c1a\u4e0d\u9002\u5408\u76f4\u63a5\u7528\u4e8e\u4e34\u5e8a\u3002\u4e0d\u8fc7\uff0c\u672a\u6765\u53ef\u5728\u773c\u79d1\u6559\u80b2\u3001\u6587\u6863\u8bb0\u5f55\u6216\u56fe\u50cf\u6807\u6ce8\u7b49\u8f85\u52a9\u5de5\u4f5c\u4e2d\u53d1\u6325\u4f5c\u7528\u3002"}}
{"id": "2507.01489", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.01489", "abs": "https://arxiv.org/abs/2507.01489", "authors": ["Yanfei Zhang"], "title": "Agent-as-Tool: A Study on the Hierarchical Decision Making with Reinforcement Learning", "comment": "12 pages", "summary": "Large Language Models (LLMs) have emerged as one of the most significant\ntechnological advancements in artificial intelligence in recent years. Their\nability to understand, generate, and reason with natural language has\ntransformed how we interact with AI systems. With the development of LLM-based\nagents and reinforcement-learning-based reasoning models, the study of applying\nreinforcement learning in agent frameworks has become a new research focus.\nHowever, all previous studies face the challenge of deciding the tool calling\nprocess and the reasoning process simultaneously, and the chain of reasoning\nwas solely relied on the unprocessed raw result with redundant information and\nsymbols unrelated to the task from the tool, which impose a heavy burden on the\nmodel's capability to reason. Therefore, in our research, we proposed a\nhierarchical framework Agent-as-tool that detach the tool calling process and\nthe reasoning process, which enables the model to focus on the verbally\nreasoning process while the tool calling process is handled by another agent.\nOur work had achieved comparable results with only a slight reinforcement\nfine-tuning on 180 samples, and had achieved exceptionally well performance in\nBamboogle with 63.2% of exact match and 75.2% in cover exact match, exceeding\nSearch-R1 by 4.8% in exact match and 3.2% in cover exact match.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Agent-as-tool\u5c42\u6b21\u6846\u67b6\uff0c\u5c06\u5de5\u5177\u8c03\u7528\u4e0e\u63a8\u7406\u8fc7\u7a0b\u62c6\u5206\uff0c\u663e\u8457\u63d0\u5347LLM\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u63a8\u7406\u8868\u73b0\uff0c\u5728\u4e3b\u6d41\u6570\u636e\u96c6\u4e0a\u8d85\u8fc7\u4e86\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\u540c\u65f6\u5904\u7406\u5de5\u5177\u8c03\u7528\u548c\u63a8\u7406\uff0c\u5bfc\u81f4\u63a8\u7406\u73af\u8282\u53d7\u5230\u539f\u59cb\u4e14\u5197\u4f59\u5de5\u5177\u8f93\u51fa\u7684\u5e72\u6270\uff0c\u964d\u4f4e\u63a8\u7406\u6548\u679c\uff0c\u56e0\u6b64\u9700\u8981\u5206\u79bb\u4e24\u8005\u4ee5\u63d0\u5347\u63a8\u7406\u6027\u80fd\u3002", "method": "\u6784\u5efa\u5c42\u6b21\u5316\u7684LLM Agent\u7cfb\u7edf\uff0c\u5c06\u5de5\u5177\u8c03\u7528\u4e0e\u8bed\u8a00\u63a8\u7406\u5206\u79bb\uff0c\u7531\u4e0d\u540cagent\u5206\u522b\u8d1f\u8d23\uff1b\u4ec5\u5728180\u4e2a\u6837\u672c\u4e0a\u8fdb\u884c\u4e86\u8f7b\u91cf\u7ea7\u7684\u5f3a\u5316\u5fae\u8c03\u3002", "result": "\u5728Bamboogle\u4e0a\u7684\u7cbe\u786e\u5339\u914d\u8fbe\u523063.2%\uff0c\u8986\u76d6\u7cbe\u786e\u5339\u914d\u4e3a75.2%\uff0c\u5206\u522b\u8d85\u8fc7\u5df2\u6709\u6700\u4f18\u65b9\u6cd5Search-R1 4.8%\u548c3.2%\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5c42\u6b21\u5316\u7684Agent-as-tool\u6846\u67b6\uff0c\u5c06\u5de5\u5177\u8c03\u7528\u8fc7\u7a0b\u4e0e\u63a8\u7406\u8fc7\u7a0b\u89e3\u8026\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u63a8\u7406\u80fd\u529b\uff0c\u57282\u9879\u6307\u6807\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u3002"}}
{"id": "2507.01281", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01281", "abs": "https://arxiv.org/abs/2507.01281", "authors": ["Juan Chen", "Baolong Bi", "Wei Zhang", "Jingyan Sui", "Xiaofei Zhu", "Yuanzhuo Wang", "Lingrui Mei", "Shenghua Liu"], "title": "Rethinking All Evidence: Enhancing Trustworthy Retrieval-Augmented Generation via Conflict-Driven Summarization", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by\nintegrating their parametric knowledge with external retrieved content.\nHowever, knowledge conflicts caused by internal inconsistencies or noisy\nretrieved content can severely undermine the generation reliability of RAG\nsystems.In this work, we argue that LLMs should rethink all evidence, including\nboth retrieved content and internal knowledge, before generating responses.We\npropose CARE-RAG (Conflict-Aware and Reliable Evidence for RAG), a novel\nframework that improves trustworthiness through Conflict-Driven Summarization\nof all available evidence.CARE-RAG first derives parameter-aware evidence by\ncomparing parameter records to identify diverse internal perspectives. It then\nrefines retrieved evidences to produce context-aware evidence, removing\nirrelevant or misleading content. To detect and summarize conflicts, we distill\na 3B LLaMA3.2 model to perform conflict-driven summarization, enabling reliable\nsynthesis across multiple sources.To further ensure evaluation integrity, we\nintroduce a QA Repair step to correct outdated or ambiguous benchmark\nanswers.Experiments on revised QA datasets with retrieval data show that\nCARE-RAG consistently outperforms strong RAG baselines, especially in scenarios\nwith noisy or conflicting evidence.", "AI": {"tldr": "\u63d0\u51faCARE-RAG\u65b9\u6cd5\uff0c\u901a\u8fc7\u51b2\u7a81\u611f\u77e5\u8bc1\u636e\u603b\u7ed3\u548c\u68c0\u7d22\u7ed3\u679c\u7cbe\u70bc\u89e3\u51b3RAG\u4e2d\u7684\u77e5\u8bc6\u51b2\u7a81\uff0c\u63d0\u5347\u4e86\u7cfb\u7edf\u5728\u9762\u5bf9\u566a\u58f0\u548c\u77db\u76fe\u8bc1\u636e\u65f6\u7684\u53ef\u9760\u6027\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "RAG\u7cfb\u7edf\u901a\u8fc7\u7ed3\u5408\u5927\u6a21\u578b\u5185\u90e8\u77e5\u8bc6\u548c\u5916\u90e8\u68c0\u7d22\u5185\u5bb9\u63d0\u5347\u56de\u7b54\u80fd\u529b\uff0c\u4f46\u5185\u90e8\u77db\u76fe\u6216\u5916\u90e8\u68c0\u7d22\u5185\u5bb9\u566a\u58f0\u4f1a\u5bfc\u81f4\u77e5\u8bc6\u51b2\u7a81\uff0c\u4e25\u91cd\u5f71\u54cd\u751f\u6210\u7ed3\u679c\u7684\u53ef\u9760\u6027\u3002\u4e3a\u63d0\u5347RAG\u7cfb\u7edf\u53ef\u4fe1\u5ea6\uff0c\u9700\u8981\u89e3\u51b3\u77e5\u8bc6\u51b2\u7a81\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86CARE-RAG\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6240\u6709\u8bc1\u636e\uff08\u5185\u90e8\u548c\u5916\u90e8\uff09\u7684\u51b2\u7a81\u9a71\u52a8\u578b\u603b\u7ed3\uff0c\u63d0\u9ad8\u56de\u7b54\u53ef\u9760\u6027\u3002\u8be5\u65b9\u6cd5\u5305\u62ec\uff1a1\uff09\u6bd4\u8f83\u6a21\u578b\u53c2\u6570\u8bb0\u5f55\uff0c\u63d0\u53d6\u591a\u5143\u5185\u90e8\u89c6\u89d2\u7684evidence\uff1b2\uff09\u5bf9\u68c0\u7d22\u8bc1\u636e\u8fdb\u884c\u7cbe\u70bc\uff0c\u53bb\u9664\u65e0\u5173\u6216\u8bef\u5bfc\u6027\u5185\u5bb9\uff0c\u83b7\u5f97\u60c5\u666f\u76f8\u5173evidence\uff1b3\uff09\u7528\u84b8\u998f\u81eaLLaMA3.2 3B\u7684\u6a21\u578b\u5bf9\u6240\u6709\u8bc1\u636e\u51b2\u7a81\u8fdb\u884c\u68c0\u6d4b\u548c\u603b\u7ed3\uff0c\u5b9e\u73b0\u53ef\u9760\u8bc1\u636e\u7efc\u5408\uff1b4\uff09\u5f15\u5165QA Repair\u6b65\u9aa4\uff0c\u4fee\u6b63\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fc7\u65f6\u6216\u542b\u7cca\u7684\u7b54\u6848\uff0c\u4fdd\u8bc1\u8bc4\u4f30\u7684\u516c\u5e73\u6027\u3002", "result": "\u5728\u542b\u6709\u68c0\u7d22\u6570\u636e\u7684\u4fee\u8ba2\u7248QA\u6570\u636e\u96c6\u4e0a\uff0cCARE-RAG\u5728\u566a\u58f0\u6216\u51b2\u7a81\u8bc1\u636e\u573a\u666f\u4e0b\u7a33\u5b9a\u4f18\u4e8e\u4e3b\u6d41RAG\u5f3a\u57fa\u7ebf\u3002", "conclusion": "CARE-RAG\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86RAG\u7cfb\u7edf\u9762\u5bf9\u77e5\u8bc6\u51b2\u7a81\u65f6\u7684\u751f\u6210\u53ef\u9760\u6027\uff0c\u5c24\u5176\u5728\u5bf9\u6297\u566a\u58f0\u548c\u77db\u76fe\u8bc1\u636e\u65b9\u9762\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2507.01597", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01597", "abs": "https://arxiv.org/abs/2507.01597", "authors": ["Yuehang Si", "Zefan Zeng", "Jincai Huang", "Qing Cheng"], "title": "T3DM: Test-Time Training-Guided Distribution Shift Modelling for Temporal Knowledge Graph Reasoning", "comment": null, "summary": "Temporal Knowledge Graph (TKG) is an efficient method for describing the\ndynamic development of facts along a timeline. Most research on TKG reasoning\n(TKGR) focuses on modelling the repetition of global facts and designing\npatterns of local historical facts. However, they face two significant\nchallenges: inadequate modeling of the event distribution shift between\ntraining and test samples, and reliance on random entity substitution for\ngenerating negative samples, which often results in low-quality sampling. To\nthis end, we propose a novel distributional feature modeling approach for\ntraining TKGR models, Test-Time Training-guided Distribution shift Modelling\n(T3DM), to adjust the model based on distribution shift and ensure the global\nconsistency of model reasoning. In addition, we design a negative-sampling\nstrategy to generate higher-quality negative quadruples based on adversarial\ntraining. Extensive experiments show that T3DM provides better and more robust\nresults than the state-of-the-art baselines in most cases.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u65f6\u5e8f\u77e5\u8bc6\u56fe\u8c31\u63a8\u7406\u4e2d\u5206\u5e03\u8f6c\u79fb\u548c\u8d1f\u91c7\u6837\u8d28\u91cf\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86T3DM\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u5927\u591a\u6570\u65f6\u5e8f\u77e5\u8bc6\u56fe\u8c31\u63a8\u7406\uff08TKGR\uff09\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u4e8e\u5168\u5c40\u4e8b\u5b9e\u7684\u91cd\u590d\u5efa\u6a21\u548c\u5c40\u90e8\u5386\u53f2\u4e8b\u5b9e\u6a21\u5f0f\u7684\u8bbe\u8ba1\uff0c\u4f46\u5b58\u5728\u4e8b\u4ef6\u5206\u5e03\u8f6c\u79fb\u5efa\u6a21\u4e0d\u8db3\u548c\u8d1f\u6837\u672c\u91c7\u6837\u8d28\u91cf\u4f4e\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7528\u4e8e\u8bad\u7ec3TKGR\u6a21\u578b\u7684\u5206\u5e03\u7279\u5f81\u5efa\u6a21\u65b9\u6cd5\uff0c\u5373\u6d4b\u8bd5\u65f6\u8bad\u7ec3\u5f15\u5bfc\u4e0b\u7684\u5206\u5e03\u8f6c\u79fb\u5efa\u6a21\uff08T3DM\uff09\uff0c\u540c\u65f6\u7ed3\u5408\u57fa\u4e8e\u5bf9\u6297\u8bad\u7ec3\u7684\u8d1f\u91c7\u6837\u7b56\u7565\u4ee5\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u8d1f\u56db\u5143\u7ec4\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cT3DM\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u6bd4\u5f53\u524d\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u6a21\u578b\u8868\u73b0\u66f4\u597d\u4e14\u66f4\u52a0\u9c81\u68d2\u3002", "conclusion": "T3DM\u4e0d\u4ec5\u6709\u6548\u5efa\u6a21\u4e86\u6570\u636e\u5206\u5e03\u8f6c\u79fb\uff0c\u8fd8\u901a\u8fc7\u65b0\u9896\u7684\u8d1f\u91c7\u6837\u7b56\u7565\u63d0\u5347\u4e86TKGR\u6a21\u578b\u7684\u6574\u4f53\u63a8\u7406\u4e00\u81f4\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2507.01297", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.01297", "abs": "https://arxiv.org/abs/2507.01297", "authors": ["Xinxi Lyu", "Michael Duan", "Rulin Shao", "Pang Wei Koh", "Sewon Min"], "title": "Frustratingly Simple Retrieval Improves Challenging, Reasoning-Intensive Benchmarks", "comment": "33 pages, 2 figures, 27 tables", "summary": "Retrieval-augmented Generation (RAG) has primarily been studied in limited\nsettings, such as factoid question answering; more challenging,\nreasoning-intensive benchmarks have seen limited success from minimal RAG. In\nthis work, we challenge this prevailing view on established,\nreasoning-intensive benchmarks: MMLU, MMLU Pro, AGI Eval, GPQA, and MATH. We\nidentify a key missing component in prior work: a usable, web-scale datastore\naligned with the breadth of pretraining data. To this end, we introduce\nCompactDS: a diverse, high-quality, web-scale datastore that achieves high\nretrieval accuracy and subsecond latency on a single-node. The key insights are\n(1) most web content can be filtered out without sacrificing coverage, and a\ncompact, high-quality subset is sufficient; and (2) combining in-memory\napproximate nearest neighbor (ANN) retrieval and on-disk exact search balances\nspeed and recall. Using CompactDS, we show that a minimal RAG pipeline achieves\nconsistent accuracy improvements across all benchmarks and model sizes\n(8B--70B), with relative gains of 10% on MMLU, 33% on MMLU Pro, 14% on GPQA,\nand 19% on MATH. No single data source suffices alone, highlighting the\nimportance of diversity of sources (web crawls, curated math, academic papers,\ntextbooks). Finally, we show that our carefully designed in-house datastore\nmatches or outperforms web search engines such as Google Search, as well as\nrecently proposed, complex agent-based RAG systems--all while maintaining\nsimplicity, reproducibility, and self-containment. We release CompactDS and our\nretrieval pipeline, supporting future research exploring retrieval-based AI\nsystems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u9ad8\u8d28\u91cf\u591a\u6837\u5316\u6570\u636e\u5b58\u50a8\uff08CompactDS\uff09\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u578b\u4efb\u52a1\u4e0a\u7684RAG\u8868\u73b0\uff0c\u5e76\u8d85\u8fc7Google\u7b49\u4e3b\u6d41\u641c\u7d22\u53ca\u590d\u6742RAG\u7cfb\u7edf\uff0c\u5de5\u5177\u5df2\u53d1\u5e03\uff0c\u52a9\u529b\u540e\u7eed\u7814\u7a76\u3002", "motivation": "\u76ee\u524d\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7684\u7814\u7a76\u591a\u805a\u7126\u4e8e\u7b80\u5355\u7684\u95ee\u7b54\u4efb\u52a1\uff0c\u5bf9\u4e8e\u9700\u8981\u590d\u6742\u63a8\u7406\u7684\u57fa\u51c6\u6d4b\u8bd5\uff08\u5982MMLU\u7b49\uff09\uff0c\u6548\u679c\u6709\u9650\uff0c\u4e3b\u8981\u539f\u56e0\u5728\u4e8e\u7f3a\u4e4f\u9ad8\u6548\u4e14\u591a\u6837\u5316\u3001\u8986\u76d6\u5e7f\u6cdb\u7684\u6570\u636e\u6e90\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u89c4\u6a21\u7f51\u7edc\u7ea7\u7684\u9ad8\u8d28\u91cf\u6570\u636e\u5b58\u50a8\uff08CompactDS\uff09\uff0c\u901a\u8fc7\u8fc7\u6ee4\u5927\u90e8\u5206\u7f51\u9875\u5185\u5bb9\u4ec5\u4fdd\u7559\u4f18\u8d28\u3001\u9ad8\u76f8\u5173\u6027\u5b50\u96c6\uff0c\u540c\u65f6\u7ed3\u5408\u5185\u5b58\u4e2d\u7684\u8fd1\u4f3c\u6700\u8fd1\u90bb\u68c0\u7d22\u4e0e\u78c1\u76d8\u4e0a\u7684\u7cbe\u786e\u68c0\u7d22\uff0c\u517c\u987e\u901f\u5ea6\u4e0e\u53ec\u56de\u7387\uff0c\u4e3aRAG\u7cfb\u7edf\u63d0\u4f9b\u9ad8\u6548\u652f\u6491\u3002", "result": "\u5e94\u7528CompactDS\u5efa\u7acb\u7684\u7cbe\u7b80RAG\u7cfb\u7edf\u5728\u591a\u4e2a\u63a8\u7406\u8981\u6c42\u9ad8\u7684\u57fa\u51c6\u6570\u636e\u96c6\u548c\u4e0d\u540c\u6a21\u578b\u89c4\u6a21\u4e0b\u5b9e\u73b0\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u4f8b\u5982\u5728MMLU\u63d0\u534710%\uff0cMMLU Pro\u63d0\u534733%\uff0cGPQA\u63d0\u534714%\uff0cMATH\u63d0\u534719%\uff1b\u5e76\u8868\u660e\u591a\u5143\u5316\u6570\u636e\u6e90\u6bd4\u5355\u4e00\u6765\u6e90\u6548\u679c\u66f4\u597d\u3002\u6b64\u5916\uff0c\u81ea\u7814\u6570\u636e\u5b58\u50a8\u6548\u679c\u4f18\u4e8eGoogle\u7b49\u641c\u7d22\u5f15\u64ce\u53ca\u590d\u6742RAG\u7cfb\u7edf\uff0c\u4e14\u5177\u5907\u7b80\u5355\u3001\u53ef\u590d\u73b0\u3001\u6613\u90e8\u7f72\u7684\u4f18\u52bf\u3002", "conclusion": "\u9ad8\u8d28\u91cf\u3001\u591a\u6837\u5316\u3001\u7ed3\u6784\u5408\u7406\u7684\u6570\u636e\u5b58\u50a8\uff08CompactDS\uff09\u662f\u63d0\u5347\u63a8\u7406\u7c7b\u4efb\u52a1RAG\u7cfb\u7edf\u6548\u679c\u7684\u5173\u952e\uff0c\u4ec5\u9760\u590d\u6742\u7b97\u6cd5\u6216\u5927\u6a21\u578b\u4e0d\u8db3\u4ee5\u5f25\u8865\u6570\u636e\u57fa\u7840\u7684\u7f3a\u9677\u3002\u53d1\u5e03\u7684\u6570\u636e\u548c\u7ba1\u9053\u5de5\u5177\u4e3a\u57fa\u4e8e\u68c0\u7d22\u7684AI\u7cfb\u7edf\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u5f3a\u6709\u529b\u652f\u6491\u3002"}}
{"id": "2507.01717", "categories": ["cs.AI", "cs.IR", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.01717", "abs": "https://arxiv.org/abs/2507.01717", "authors": ["Gopichand Kanumolu", "Ashok Urlana", "Charaka Vinayak Kumar", "Bala Mallikarjunarao Garlapati"], "title": "Agent Ideate: A Framework for Product Idea Generation from Patents Using Agentic AI", "comment": "AgentScen Workshop, IJCAI 2025", "summary": "Patents contain rich technical knowledge that can inspire innovative product\nideas, yet accessing and interpreting this information remains a challenge.\nThis work explores the use of Large Language Models (LLMs) and autonomous\nagents to mine and generate product concepts from a given patent. In this work,\nwe design Agent Ideate, a framework for automatically generating product-based\nbusiness ideas from patents. We experimented with open-source LLMs and\nagent-based architectures across three domains: Computer Science, Natural\nLanguage Processing, and Material Chemistry. Evaluation results show that the\nagentic approach consistently outperformed standalone LLMs in terms of idea\nquality, relevance, and novelty. These findings suggest that combining LLMs\nwith agentic workflows can significantly enhance the innovation pipeline by\nunlocking the untapped potential of business idea generation from patent data.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Agent Ideate\u6846\u67b6\uff0c\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u667a\u80fd\u4f53\uff0c\u80fd\u66f4\u6709\u6548\u5730\u4ece\u4e13\u5229\u4e2d\u81ea\u52a8\u751f\u6210\u9ad8\u8d28\u91cf\u548c\u6709\u521b\u65b0\u6027\u7684\u4ea7\u54c1\u5546\u4e1a\u521b\u610f\uff0c\u63d0\u5347\u4e86\u4ece\u4e13\u5229\u6570\u636e\u4e2d\u5b75\u5316\u521b\u65b0\u7684\u80fd\u529b\u3002", "motivation": "\u4e13\u5229\u4e2d\u8574\u542b\u5927\u91cf\u6280\u672f\u77e5\u8bc6\uff0c\u80fd\u591f\u542f\u53d1\u521b\u65b0\u578b\u4ea7\u54c1\u521b\u610f\uff0c\u4f46\u4e13\u5229\u4fe1\u606f\u7684\u83b7\u53d6\u4e0e\u7406\u89e3\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u8bbe\u8ba1\u4e86Agent Ideate\u6846\u67b6\uff0c\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e0e\u81ea\u6cbb\u667a\u80fd\u4f53\uff0c\u81ea\u52a8\u4ece\u4e13\u5229\u4e2d\u751f\u6210\u57fa\u4e8e\u4ea7\u54c1\u7684\u5546\u4e1a\u521b\u610f\u3002\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u3001\u81ea\u7136\u8bed\u8a00\u5904\u7406\u548c\u6750\u6599\u5316\u5b66\u4e09\u4e2a\u9886\u57df\u5f00\u5c55\u5b9e\u9a8c\uff0c\u6bd4\u8f83\u4e86\u5f00\u6e90LLMs\u548c\u57fa\u4e8e\u667a\u80fd\u4f53\u7684\u67b6\u6784\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8e\u667a\u80fd\u4f53\u7684\u65b9\u6cd5\u5728\u521b\u610f\u8d28\u91cf\u3001\u76f8\u5173\u6027\u548c\u65b0\u9896\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u5355\u4e00LLM\u6a21\u578b\u3002", "conclusion": "\u5c06LLMs\u4e0e\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u7ed3\u5408\uff0c\u53ef\u663e\u8457\u63d0\u5347\u521b\u65b0\u6d41\u7a0b\uff0c\u5145\u5206\u6316\u6398\u4e13\u5229\u6570\u636e\u4e2d\u8574\u542b\u7684\u5546\u4e1a\u521b\u610f\u6f5c\u80fd\u3002"}}
{"id": "2507.01299", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01299", "abs": "https://arxiv.org/abs/2507.01299", "authors": ["Kai Liu", "Bowen Xu", "Shaoyu Wu", "Xin Chen", "Hao Zhou", "Yongliang Tao", "Lulu Hu"], "title": "La RoSA: Enhancing LLM Efficiency via Layerwise Rotated Sparse Activation", "comment": "ICML 2025 Acceptance", "summary": "Activation sparsity can reduce the computational overhead and memory\ntransfers during the forward pass of Large Language Model (LLM) inference.\nExisting methods face limitations, either demanding time-consuming recovery\ntraining that hinders real-world adoption, or relying on empirical\nmagnitude-based pruning, which causes fluctuating sparsity and unstable\ninference speed-up. This paper introduces LaRoSA (Layerwise Rotated Sparse\nActivation), a novel method for activation sparsification designed to improve\nLLM efficiency without requiring additional training or magnitude-based\npruning. We leverage layerwise orthogonal rotations to transform input\nactivations into rotated forms that are more suitable for sparsification. By\nemploying a Top-K selection approach within the rotated activations, we achieve\nconsistent model-level sparsity and reliable wall-clock time speed-up. LaRoSA\nis effective across various sizes and types of LLMs, demonstrating minimal\nperformance degradation and robust inference acceleration. Specifically, for\nLLaMA2-7B at 40% sparsity, LaRoSA achieves a mere 0.17 perplexity gap with a\nconsistent 1.30x wall-clock time speed-up, and reduces the accuracy gap in\nzero-shot tasks compared to the dense model to just 0.54%, while surpassing\nTEAL by 1.77% and CATS by 17.14%.", "AI": {"tldr": "LaRoSA\u65b9\u6cd5\u901a\u8fc7\u5c42\u6b63\u4ea4\u65cb\u8f6c\u548cTop-K\u9009\u62e9\u5b9e\u73b0\u65e0\u9700\u8bad\u7ec3\u7684\u9ad8\u6548\u6fc0\u6d3b\u7a00\u758f\uff0c\u4f7fLLM\u63a8\u7406\u663e\u8457\u52a0\u901f\u4e14\u51e0\u4e4e\u65e0\u6027\u80fd\u635f\u5931\uff0c\u4f18\u4e8e\u5df2\u6709\u4e3b\u6d41\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u6fc0\u6d3b\u7a00\u758f\u5316\u65b9\u6cd5\u8981\u4e48\u9700\u8981\u8017\u65f6\u7684\u6062\u590d\u8bad\u7ec3\uff0c\u96be\u4ee5\u5b9e\u9645\u5e94\u7528\uff0c\u8981\u4e48\u4f9d\u8d56\u7ecf\u9a8c\u6027\u7684\u5e45\u503c\u526a\u679d\uff0c\u5bfc\u81f4\u7a00\u758f\u5ea6\u548c\u63a8\u7406\u52a0\u901f\u6548\u679c\u4e0d\u7a33\u5b9a\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u4e14\u53ef\u5b9e\u73b0\u7a33\u5b9a\u7a00\u758f\u548c\u52a0\u901f\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86LaRoSA\uff08Layerwise Rotated Sparse Activation\uff09\u65b9\u6cd5\uff0c\u4f7f\u7528\u9010\u5c42\u7684\u6b63\u4ea4\u65cb\u8f6c\u5c06\u8f93\u5165\u6fc0\u6d3b\u5411\u91cf\u8f6c\u6362\uff0c\u4f7f\u5176\u9002\u5b9c\u7a00\u758f\u5316\u3002\u968f\u540e\u5bf9\u65cb\u8f6c\u540e\u7684\u6fc0\u6d3b\u8fdb\u884cTop-K\u9009\u62e9\uff0c\u5b9e\u73b0\u4e00\u81f4\u7684\u6a21\u578b\u7ea7\u7a00\u758f\u5ea6\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u5e45\u503c\u526a\u679d\u3002", "result": "\u5728\u591a\u79cd\u5c3a\u5bf8\u548c\u7c7b\u578b\u7684LLM\u4e0a\uff0cLaRoSA\u8868\u73b0\u4f18\u5f02\uff0c\u6a21\u578b\u6027\u80fd\u57fa\u672c\u4e0d\u4e0b\u964d\u4e14\u63a8\u7406\u52a0\u901f\u663e\u8457\u3002\u4ee5LLaMA2-7B\u4e3a\u4f8b\uff0c\u572840%\u7a00\u758f\u5ea6\u4e0b\uff0c\u56f0\u60d1\u5ea6\u63d0\u5347\u4ec50.17\uff0c\u5b9e\u9645\u52a0\u901f\u8fbe\u52301.30\u500d\uff0c\u96f6\u6837\u672c\u4efb\u52a1\u51c6\u786e\u7387\u5dee\u4ec5\u4e3a0.54%\uff0c\u5e76\u8d85\u8d8a\u4e86TEAL\uff081.77%\uff09\u4e0eCATS\uff0817.14%\uff09\u7684\u65b9\u6cd5\u3002", "conclusion": "LaRoSA\u65e0\u9700\u8bad\u7ec3\u4e0e\u590d\u6742\u526a\u679d\uff0c\u901a\u8fc7\u5c42\u5185\u6b63\u4ea4\u65cb\u8f6c\u548cTop-K\u5b9e\u73b0\u9ad8\u6548\u3001\u7a33\u5b9a\u7684\u6fc0\u6d3b\u7a00\u758f\uff0c\u5728\u4fdd\u8bc1\u6a21\u578b\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u5927\u6a21\u578b\u63a8\u7406\u6548\u7387\u3002"}}
{"id": "2507.01749", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01749", "abs": "https://arxiv.org/abs/2507.01749", "authors": ["Arash Dehghan", "Mucahit Cevik", "Merve Bodur", "Bissan Ghaddar"], "title": "Joint Matching and Pricing for Crowd-shipping with In-store Customers", "comment": null, "summary": "This paper examines the use of in-store customers as delivery couriers in a\ncentralized crowd-shipping system, targeting the growing need for efficient\nlast-mile delivery in urban areas. We consider a brick-and-mortar retail\nsetting where shoppers are offered compensation to deliver time-sensitive\nonline orders. To manage this process, we propose a Markov Decision Process\n(MDP) model that captures key uncertainties, including the stochastic arrival\nof orders and crowd-shippers, and the probabilistic acceptance of delivery\noffers. Our solution approach integrates Neural Approximate Dynamic Programming\n(NeurADP) for adaptive order-to-shopper assignment with a Deep Double Q-Network\n(DDQN) for dynamic pricing. This joint optimization strategy enables multi-drop\nrouting and accounts for offer acceptance uncertainty, aligning more closely\nwith real-world operations. Experimental results demonstrate that the\nintegrated NeurADP + DDQN policy achieves notable improvements in delivery cost\nefficiency, with up to 6.7\\% savings over NeurADP with fixed pricing and\napproximately 18\\% over myopic baselines. We also show that allowing flexible\ndelivery delays and enabling multi-destination routing further reduces\noperational costs by 8\\% and 17\\%, respectively. These findings underscore the\nadvantages of dynamic, forward-looking policies in crowd-shipping systems and\noffer practical guidance for urban logistics operators.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5728\u5b9e\u4f53\u96f6\u552e\u573a\u666f\u4e2d\u7528\u5e97\u5185\u987e\u5ba2\u4f17\u5305\u914d\u9001\uff0c\u901a\u8fc7MDP\u5efa\u6a21+NeurADP\u5206\u914d+DDQN\u5b9a\u4ef7\uff0c\u5b9e\u73b0\u591a\u70b9\u52a8\u6001\u4f18\u5316\u3002\u5b9e\u9a8c\u663e\u793a\u8be5\u7b56\u7565\u5927\u5e45\u964d\u4f4e\u914d\u9001\u6210\u672c\uff0c\u5bf9\u57ce\u5e02\u7269\u6d41\u5b9e\u8df5\u6709\u6307\u5bfc\u4ef7\u503c\u3002", "motivation": "\u57ce\u5e02\u533a\u57df\u5bf9\u9ad8\u6548\u201c\u6700\u540e\u4e00\u516c\u91cc\u201d\u914d\u9001\u7684\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0c\u4f20\u7edf\u7269\u6d41\u9762\u4e34\u6210\u672c\u9ad8\u548c\u6548\u7387\u4f4e\u7684\u95ee\u9898\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u5982\u4f55\u5229\u7528\u8fdb\u5e97\u987e\u5ba2\u4f5c\u4e3a\u4f17\u5305\u914d\u9001\u5458\uff0c\u63d0\u5347\u57ce\u5e02\u96f6\u552e\u573a\u666f\u4e2d\u7684\u7269\u6d41\u6548\u7387\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u96c6\u4e2d\u5f0f\u4f17\u5305\u914d\u9001\u7cfb\u7edf\uff0c\u8bbe\u8ba1\u57fa\u4e8e\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08MDP\uff09\u6a21\u578b\uff0c\u96c6\u6210\u4e86\u795e\u7ecf\u7f51\u7edc\u8fd1\u4f3c\u52a8\u6001\u89c4\u5212\uff08NeurADP\uff09\u7528\u4e8e\u81ea\u9002\u5e94\u8ba2\u5355\u5206\u914d\uff0c\u540c\u65f6\u91c7\u7528\u6df1\u5ea6\u53ccQ\u7f51\u7edc\uff08DDQN\uff09\u8fdb\u884c\u52a8\u6001\u5b9a\u4ef7\uff0c\u8054\u5408\u4f18\u5316\u591a\u70b9\u914d\u9001\u4e0e\u62a5\u4ef7\u63a5\u53d7\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u96c6\u6210NeurADP\u4e0eDDQN\u7684\u7b56\u7565\u76f8\u6bd4\u4e8e\u56fa\u5b9a\u5b9a\u4ef7\u7684NeurADP\u548c\u8d2a\u5fc3\u57fa\u7ebf\u5206\u522b\u8282\u7701\u4e86\u6700\u9ad86.7%\u548c18%\u7684\u914d\u9001\u6210\u672c\u3002\u7075\u6d3b\u5141\u8bb8\u914d\u9001\u65f6\u5ef6\u548c\u591a\u76ee\u7684\u5730\u8def\u5f84\u89c4\u5212\u80fd\u8fdb\u4e00\u6b65\u51cf\u5c11\u8fd0\u8425\u6210\u672c8%\u548c17%\u3002", "conclusion": "\u52a8\u6001\u3001\u524d\u77bb\u578b\u8054\u5408\u4f18\u5316\u7b56\u7565\u80fd\u591f\u663e\u8457\u63d0\u5347\u4f17\u5305\u7269\u6d41\u7cfb\u7edf\u7684\u670d\u52a1\u6548\u7387\u548c\u6210\u672c\u6548\u76ca\uff0c\u4e3a\u57ce\u5e02\u7269\u6d41\u8fd0\u8425\u5546\u63d0\u4f9b\u4e86\u5b9e\u7528\u5efa\u8bae\u3002"}}
{"id": "2507.01334", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01334", "abs": "https://arxiv.org/abs/2507.01334", "authors": ["Nifu Dan", "Yujun Cai", "Yiwei Wang"], "title": "Symbolic or Numerical? Understanding Physics Problem Solving in Reasoning LLMs", "comment": null, "summary": "Navigating the complexities of physics reasoning has long been a difficult\ntask for Large Language Models (LLMs), requiring a synthesis of profound\nconceptual understanding and adept problem-solving techniques. In this study,\nwe investigate the application of advanced instruction-tuned reasoning models,\nsuch as Deepseek-R1, to address a diverse spectrum of physics problems curated\nfrom the challenging SciBench benchmark. Our comprehensive experimental\nevaluation reveals the remarkable capabilities of reasoning models. Not only do\nthey achieve state-of-the-art accuracy in answering intricate physics\nquestions, but they also generate distinctive reasoning patterns that emphasize\non symbolic derivation. Furthermore, our findings indicate that even for these\nhighly sophisticated reasoning models, the strategic incorporation of few-shot\nprompting can still yield measurable improvements in overall accuracy,\nhighlighting the potential for continued performance gains.", "AI": {"tldr": "\u6307\u4ee4\u5fae\u8c03\u578bLLMs\uff08\u5982Deepseek-R1\uff09\u5728\u7269\u7406\u63a8\u7406\u6570\u636e\u96c6\u4e0a\u53d6\u5f97SOTA\u8868\u73b0\uff0c\u7b26\u53f7\u63a8\u5bfc\u80fd\u529b\u7a81\u51fa\uff0cfew-shot\u63d0\u793a\u8fd8\u80fd\u6301\u7eed\u63d0\u5347\u5176\u51c6\u786e\u7387\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7269\u7406\u63a8\u7406\u4e0a\u7684\u8868\u73b0\u6709\u9650\uff0c\u56e0\u4e3a\u8fd9\u7c7b\u4efb\u52a1\u4e0d\u4ec5\u9700\u8981\u6df1\u539a\u7684\u6982\u5ff5\u7406\u89e3\uff0c\u8fd8\u8981\u6c42\u9ad8\u8d85\u7684\u95ee\u9898\u6c42\u89e3\u80fd\u529b\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u901a\u8fc7\u9ad8\u7ea7\u6307\u4ee4\u5fae\u8c03\u548c\u63a8\u7406\u6a21\u578b\uff0c\u63d0\u5347LLMs\u5728\u7269\u7406\u95ee\u9898\u4e0a\u7684\u80fd\u529b\u3002", "method": "\u4f5c\u8005\u9009\u53d6\u4e86\u9ad8\u7ea7\u6307\u4ee4\u5fae\u8c03\u63a8\u7406\u6a21\u578b\uff08\u5982Deepseek-R1\uff09\uff0c\u5e76\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u7269\u7406\u57fa\u51c6\u6570\u636e\u96c6SciBench\u4e0a\u8fdb\u884c\u4e86\u5168\u9762\u5b9e\u9a8c\u8bc4\u6d4b\uff0c\u901a\u8fc7few-shot\u63d0\u793a\u63a2\u7d22\u63d0\u5347\u6548\u679c\u3002", "result": "\u63a8\u7406\u6a21\u578b\u5728\u89e3\u51b3\u590d\u6742\u7269\u7406\u95ee\u9898\u4e0a\u53d6\u5f97\u4e86SOTA\uff08\u6700\u5148\u8fdb\uff09\u51c6\u786e\u7387\uff0c\u5c55\u73b0\u4e86\u72ec\u7279\u7684\u63a8\u7406\u6a21\u5f0f\uff0c\u7a81\u51fa\u7b26\u53f7\u63a8\u5bfc\u80fd\u529b\u3002\u5373\u4f7f\u5728\u6027\u80fd\u5df2\u5f88\u9ad8\u7684\u60c5\u51b5\u4e0b\uff0cfew-shot\u63d0\u793a\u4f9d\u7136\u5e26\u6765\u51c6\u786e\u7387\u7684\u63d0\u5347\u3002", "conclusion": "\u6307\u4ee4\u5fae\u8c03\u63a8\u7406\u6a21\u578b\u5df2\u80fd\u663e\u8457\u63d0\u5347LLMs\u5728\u7269\u7406\u63a8\u7406\u4e0a\u7684\u8868\u73b0\uff0c\u800c\u4e14\u901a\u8fc7\u7b56\u7565\u6027few-shot\u63d0\u793a\u4f9d\u7136\u6709\u8fdb\u4e00\u6b65\u63d0\u5347\u7a7a\u95f4\uff0c\u8bf4\u660e\u6a21\u578b\u80fd\u529b\u5c1a\u6709\u589e\u957f\u6f5c\u529b\u3002"}}
{"id": "2507.01833", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01833", "abs": "https://arxiv.org/abs/2507.01833", "authors": ["Yi-Dong Shen", "Thomas Eiter"], "title": "Refining Gelfond Rationality Principle Towards More Comprehensive Foundational Principles for Answer Set Semantics", "comment": "76 pages. This article is a significantly extended version of a paper\n  presented by the authors at IJCAI-2022", "summary": "Non-monotonic logic programming is the basis for a declarative problem\nsolving paradigm known as answer set programming (ASP). Departing from the\nseminal definition by Gelfond and Lifschitz in 1988 for simple normal logic\nprograms, various answer set semantics have been proposed for extensions. We\nconsider two important questions: (1) Should the minimal model property,\nconstraint monotonicity and foundedness as defined in the literature be\nmandatory conditions for an answer set semantics in general? (2) If not, what\nother properties could be considered as general principles for answer set\nsemantics? We address the two questions. First, it seems that the three\naforementioned conditions may sometimes be too strong, and we illustrate with\nexamples that enforcing them may exclude expected answer sets. Second, we\nevolve the Gelfond answer set (GAS) principles for answer set construction by\nrefining the Gelfond's rationality principle to well-supportedness, minimality\nw.r.t. negation by default and minimality w.r.t. epistemic negation. The\nprinciple of well-supportedness guarantees that every answer set is\nconstructible from if-then rules obeying a level mapping and is thus free of\ncircular justification, while the two minimality principles ensure that the\nformalism minimizes knowledge both at the level of answer sets and of world\nviews. Third, to embody the refined GAS principles, we extend the notion of\nwell-supportedness substantially to answer sets and world views, respectively.\nFourth, we define new answer set semantics in terms of the refined GAS\nprinciples. Fifth, we use the refined GAS principles as an alternative baseline\nto intuitively assess the existing answer set semantics. Finally, we analyze\nthe computational complexity.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u7b54\u6848\u96c6\u7f16\u7a0b\u4e2d\u8bed\u4e49\u7684\u666e\u904d\u539f\u5219\u8fdb\u884c\u4e86\u91cd\u65b0\u8003\u91cf\uff0c\u63d0\u51fa\u4e86\u66f4\u9002\u5408\u5b9e\u9645\u9700\u6c42\u7684\u65b0\u8bed\u4e49\u539f\u5219\uff0c\u6539\u8fdb\u548c\u6269\u5c55\u4e86well-supportedness\u7b49\u7406\u8bba\uff0c\u5e76\u5b9a\u4e49\u4e86\u65b0\u7684\u8bed\u4e49\uff0c\u5206\u6790\u5176\u4f18\u8d8a\u6027\u53ca\u8ba1\u7b97\u590d\u6742\u6027\u3002", "motivation": "\u8bba\u6587\u8ba8\u8bba\u4e86\u7b54\u6848\u96c6\u7f16\u7a0b\uff08ASP\uff09\u5728\u6269\u5c55\u65f6\uff0c\u4f20\u7edf\u5b9a\u4e49\u7684\u4e00\u4e9b\u6027\u8d28\u662f\u5426\u4ecd\u5e94\u4f5c\u4e3a\u5fc5\u8981\u6761\u4ef6\uff0c\u5e76\u63a2\u7d22\u5f53\u8fd9\u4e9b\u6761\u4ef6\u4e0d\u9002\u7528\u65f6\u5e94\u91c7\u7528\u54ea\u4e9b\u66f4\u4e00\u822c\u7684\u539f\u5219\u3002", "method": "\u4f5c\u8005\u901a\u8fc7\u5206\u6790\u548c\u793a\u4f8b\uff0c\u6307\u51fa\u5df2\u6709\u5982\u6781\u5c0f\u6a21\u578b\u3001\u7ea6\u675f\u5355\u8c03\u6027\u548c\u6709\u6839\u6027\u7b49\u6761\u4ef6\u8fc7\u4e8e\u4e25\u683c\uff0c\u5e76\u63d0\u51fa\u5e76\u7ec6\u5316\u4e86Gelfond\u7b54\u6848\u96c6\u539f\u5219\uff08GAS\uff09\uff0c\u901a\u8fc7\u7406\u8bba\u63a8\u5bfc\u6269\u5c55\u4e86well-supportedness\u5230\u7b54\u6848\u96c6\u548c\u4e16\u754c\u89c2\u7684\u5c42\u9762\uff0c\u57fa\u4e8e\u6539\u8fdb\u7684GAS\u539f\u5219\u5b9a\u4e49\u4e86\u65b0\u7684\u7b54\u6848\u96c6\u8bed\u4e49\uff0c\u5e76\u5bf9\u73b0\u6709\u8bed\u4e49\u8fdb\u884c\u6bd4\u8f83\u548c\u590d\u6742\u6027\u5206\u6790\u3002", "result": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u5957\u6bd4\u4f20\u7edf\u6761\u4ef6\u66f4\u5bbd\u677e\u4f46\u66f4\u5408\u7406\u7684\u65b0\u539f\u5219\uff0c\u5e76\u636e\u6b64\u5efa\u7acb\u4e86\u65b0\u7684\u7b54\u6848\u96c6\u8bed\u4e49\uff0c\u8fd9\u4e9b\u8bed\u4e49\u66f4\u7b26\u5408\u671f\u671b\u548c\u5b9e\u9645\u9700\u6c42\u3002\u8fd8\u5bf9\u73b0\u6709\u8bed\u4e49\u505a\u4e86\u76f4\u89c2\u8bc4\u4f30\u5e76\u8ba8\u8bba\u4e86\u5b83\u4eec\u7684\u8ba1\u7b97\u590d\u6742\u6027\u3002", "conclusion": "\u8bba\u6587\u4e3aASP\u7684\u8bed\u4e49\u57fa\u7840\u63d0\u4f9b\u4e86\u66f4\u4e00\u822c\u6027\u7684\u7406\u8bba\u6846\u67b6\uff0c\u63a8\u5e7f\u5e76\u7ec6\u5316\u4e86well-supportedness\u7b49\u539f\u5219\uff0c\u5e76\u8bc1\u660e\u4e86\u8fd9\u4e9b\u539f\u5219\u6784\u5efa\u7684\u8bed\u4e49\u66f4\u52a0\u7075\u6d3b\u6709\u6548\uff0c\u5bf9\u590d\u6742\u6027\u7684\u5206\u6790\u6709\u52a9\u4e8e\u7406\u89e3\u5176\u5b9e\u7528\u6027\u3002"}}
{"id": "2507.01335", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01335", "abs": "https://arxiv.org/abs/2507.01335", "authors": ["Xunjian Yin", "Sitao Cheng", "Yuxi Xie", "Xinyu Hu", "Li Lin", "Xinyi Wang", "Liangming Pan", "William Yang Wang", "Xiaojun Wan"], "title": "LEDOM: An Open and Fundamental Reverse Language Model", "comment": "Work in progress", "summary": "We introduce LEDOM, the first purely reverse language model, trained\nautoregressively on 435B tokens with 2B and 7B parameter variants, which\nprocesses sequences in reverse temporal order through previous token\nprediction. For the first time, we present the reverse language model as a\npotential foundational model across general tasks, accompanied by a set of\nintriguing examples and insights. Based on LEDOM, we further introduce a novel\napplication: Reverse Reward, where LEDOM-guided reranking of forward language\nmodel outputs leads to substantial performance improvements on mathematical\nreasoning tasks. This approach leverages LEDOM's unique backward reasoning\ncapability to refine generation quality through posterior evaluation. Our\nfindings suggest that LEDOM exhibits unique characteristics with broad\napplication potential. We will release all models, training code, and\npre-training data to facilitate future research.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u7eaf\u9006\u5411\u8bed\u8a00\u6a21\u578bLEDOM\uff0c\u5c55\u793a\u5176\u5728\u591a\u4efb\u52a1\u4e2d\u7684\u72ec\u7279\u80fd\u529b\uff0c\u5c24\u5176\u901a\u8fc7\u53cd\u5411\u91cd\u6392\u5e8f\u663e\u8457\u63d0\u5347\u6570\u5b66\u63a8\u7406\u8868\u73b0\uff0c\u5177\u6709\u5e7f\u9614\u5e94\u7528\u524d\u666f\u3002", "motivation": "\u4f20\u7edf\u7684\u8bed\u8a00\u6a21\u578b\u901a\u5e38\u6309\u7167\u6b63\u5411\u987a\u5e8f\u8fdb\u884c\u8bad\u7ec3\u548c\u751f\u6210\uff08\u5373\u9884\u6d4b\u4e0b\u4e00\u4e2a\u8bcd\uff09\uff0c\u800c\u5f88\u5c11\u5173\u6ce8\u9006\u5e8f\u5904\u7406\u6587\u672c\u3002\u7814\u7a76\u4eba\u5458\u5e0c\u671b\u63a2\u7d22\u5b8c\u5168\u9006\u5e8f\u7684\u8bed\u8a00\u6a21\u578b\u662f\u5426\u5177\u6709\u72ec\u7279\u7684\u80fd\u529b\uff0c\u5e76\u80fd\u4e3a\u901a\u7528\u4efb\u52a1\u5e26\u6765\u4ef7\u503c\u3002", "method": "\u63d0\u51fa\u5e76\u5b9e\u73b0\u4e86\u7eaf\u9006\u5e8f\u7684\u8bed\u8a00\u6a21\u578bLEDOM\uff0c\u901a\u8fc7\u5229\u7528\u201c\u524d\u4e00\u4e2atoken\u201d\u7684\u9884\u6d4b\u6765\u8fdb\u884c\u81ea\u56de\u5f52\u8bad\u7ec3\uff0c\u5206\u522b\u67092B\u548c7B\u53c2\u6570\u89c4\u6a21\uff0c\u8bad\u7ec3\u6570\u636e\u91cf\u4e3a435B tokens\u3002\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8eLEDOM\u7684\u65b0\u5e94\u7528\u201cReverse Reward\u201d\uff0c\u7528\u8be5\u9006\u5411\u6a21\u578b\u5bf9\u6b63\u5411\u8bed\u8a00\u6a21\u578b\u7684\u8f93\u51fa\u8fdb\u884c\u91cd\u6392\u5e8f\uff0c\u4ece\u800c\u63d0\u5347\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u7684\u6027\u80fd\u8868\u73b0\u3002", "result": "\u9006\u5e8f\u8bed\u8a00\u6a21\u578bLEDOM\u5728\u591a\u4e2a\u901a\u7528\u4efb\u52a1\u4e0a\u5c55\u73b0\u51fa\u72ec\u7279\u7279\u6027\uff0c\u5e76\u4e14\u901a\u8fc7\u5bf9\u4f20\u7edf\u6b63\u5411\u6a21\u578b\u63a8\u65ad\u7ed3\u679c\u7684\u53cd\u5411\u8bc4\u4f30\u548c\u91cd\u6392\u5e8f\uff0c\u5728\u6570\u5b66\u63a8\u7406\u7b49\u4efb\u52a1\u4e0a\u5e26\u6765\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "LEDOM\u4e3a\u57fa\u7840\u6a21\u578b\u7814\u7a76\u548c\u5e94\u7528\u6269\u5c55\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\uff0c\u5176\u9006\u5411\u63a8\u7406\u80fd\u529b\u548c\u4f18\u5316\u751f\u6210\u7ed3\u679c\u7684\u6f5c\u529b\u503c\u5f97\u8fdb\u4e00\u6b65\u7814\u7a76\u3002\u6240\u6709\u6a21\u578b\u3001\u4ee3\u7801\u548c\u6570\u636e\u5c06\u5f00\u653e\uff0c\u4ee5\u4fc3\u8fdb\u793e\u533a\u7814\u7a76\u3002"}}
{"id": "2507.01352", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01352", "abs": "https://arxiv.org/abs/2507.01352", "authors": ["Chris Yuhao Liu", "Liang Zeng", "Yuzhen Xiao", "Jujie He", "Jiacai Liu", "Chaojie Wang", "Rui Yan", "Wei Shen", "Fuxiang Zhang", "Jiacheng Xu", "Yang Liu", "Yahui Zhou"], "title": "Skywork-Reward-V2: Scaling Preference Data Curation via Human-AI Synergy", "comment": null, "summary": "Despite the critical role of reward models (RMs) in reinforcement learning\nfrom human feedback (RLHF), current state-of-the-art open RMs perform poorly on\nmost existing evaluation benchmarks, failing to capture the spectrum of nuanced\nand sophisticated human preferences. Even approaches that incorporate advanced\ntraining techniques have not yielded meaningful performance improvements. We\nhypothesize that this brittleness stems primarily from limitations in\npreference datasets, which are often narrowly scoped, synthetically labeled, or\nlack rigorous quality control. To address these challenges, we present a\nlarge-scale preference dataset comprising 40 million preference pairs, named\nSynPref-40M. To enable data curation at scale, we design a human-AI synergistic\ntwo-stage pipeline that leverages the complementary strengths of human\nannotation quality and AI scalability. In this pipeline, humans provide\nverified annotations, while large language models perform automatic curation\nbased on human guidance. Training on this preference mixture, we introduce\nSkywork-Reward-V2, a suite of eight reward models ranging from 0.6B to 8B\nparameters, trained on a carefully curated subset of 26 million preference\npairs from SynPref-40M. We demonstrate that Skywork-Reward-V2 is versatile\nacross a wide range of capabilities, including alignment with human\npreferences, objective correctness, safety, resistance to stylistic biases, and\nbest-of-N scaling, achieving state-of-the-art performance across seven major\nreward model benchmarks. Ablation studies confirm that the effectiveness of our\napproach stems not only from data scale but also from high-quality curation.\nThe Skywork-Reward-V2 series represents substantial progress in open reward\nmodels, highlighting the untapped potential of existing preference datasets and\ndemonstrating how human-AI curation synergy can unlock significantly higher\ndata quality.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u504f\u597d\u6570\u636e\u96c6SynPref-40M\uff0c\u901a\u8fc7\u4eba-\u673a\u534f\u540c\u7684\u4e24\u9636\u6bb5\u7b5b\u9009\u548c\u4f18\u5316\uff0c\u5728\u4e03\u9879\u5956\u52b1\u6a21\u578b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSkywork-Reward-V2\u53d6\u5f97\u4e86SOTA\u6210\u7ee9\uff0c\u5c55\u793a\u4e86\u9ad8\u8d28\u91cf\u6570\u636e\u548c\u4eba\u673a\u534f\u540c\u7b5b\u9009\u7684\u91cd\u8981\u4ef7\u503c\u3002", "motivation": "\u5f53\u524d\u7684\u5956\u52b1\u6a21\u578b\uff08RM\uff09\u5728\u57fa\u4e8e\u4eba\u7c7b\u53cd\u9988\u7684\u5f3a\u5316\u5b66\u4e60\uff08RLHF\uff09\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u5f00\u6e90\u5956\u52b1\u6a21\u578b\u5728\u591a\u6570\u8bc4\u6d4b\u57fa\u51c6\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u96be\u4ee5\u6355\u6349\u590d\u6742\u7684\u4eba\u7c7b\u504f\u597d\u3002\u5373\u4fbf\u91c7\u7528\u5148\u8fdb\u7684\u8bad\u7ec3\u65b9\u6cd5\uff0c\u6548\u679c\u63d0\u5347\u6709\u9650\uff0c\u6000\u7591\u95ee\u9898\u4e3b\u8981\u662f\u73b0\u6709\u504f\u597d\u6570\u636e\u96c6\u53d7\u9650\uff1a\u8303\u56f4\u7a84\u3001\u5408\u6210\u6807\u7b7e\u591a\u3001\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u63a7\u5236\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u504f\u597d\u6570\u636e\u96c6SynPref-40M\uff08\u5305\u542b4000\u4e07\u5bf9\u504f\u597d\uff09\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u4eba-\u673a\u534f\u4f5c\u7684\u4e24\u9636\u6bb5\u6570\u636e\u7b5b\u9009\u6d41\u7a0b\uff1a\u4eba\u5de5\u63d0\u4f9b\u9a8c\u8bc1\u6807\u6ce8\uff0c\u5927\u6a21\u578b\u5728\u4eba\u5de5\u6307\u5f15\u4e0b\u81ea\u52a8\u7b5b\u9009\u504f\u597d\u6570\u636e\u3002\u57fa\u4e8e\u7cbe\u6311\u7ec6\u9009\u76842,600\u4e07\u504f\u597d\u5bf9\u8bad\u7ec3\u4e86\u4e00\u7cfb\u5217\uff080.6B\u52308B\u53c2\u6570\uff09\u65b0\u5956\u52b1\u6a21\u578bSkywork-Reward-V2\u3002", "result": "Skywork-Reward-V2\u5728\u591a\u9879\u80fd\u529b\uff08\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\u3001\u5ba2\u89c2\u6b63\u786e\u6027\u3001\u5b89\u5168\u6027\u3001\u6297\u98ce\u683c\u504f\u89c1\u3001best-of-N\u6269\u5c55\u6027\uff09\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5728\u4e03\u4e2a\u4e3b\u6d41\u5956\u52b1\u6a21\u578b\u57fa\u51c6\u4efb\u52a1\u4e0a\u8fbe\u5230SOTA\u6c34\u5e73\u3002\u6d88\u878d\u5b9e\u9a8c\u663e\u793a\uff0c\u6a21\u578b\u7684\u63d0\u5347\u4e0d\u4ec5\u6765\u81ea\u6570\u636e\u91cf\uff0c\u66f4\u6e90\u4e8e\u9ad8\u8d28\u91cf\u6570\u636e\u7b5b\u9009\u3002", "conclusion": "\u9ad8\u8d28\u91cf\u5927\u89c4\u6a21\u504f\u597d\u6570\u636e\u3001\u901a\u8fc7\u4eba\u673a\u534f\u540c\u7b5b\u9009\uff0c\u80fd\u5927\u5e45\u63d0\u5347\u5956\u52b1\u6a21\u578b\u6027\u80fd\u3002Skywork-Reward-V2\u5237\u65b0\u4e86\u591a\u4e2a\u5f00\u6e90\u5956\u52b1\u6a21\u578b\u57fa\u51c6\uff0c\u8bc1\u660e\u4eba-\u673a\u534f\u540c\u7b5b\u9009\u662f\u63d0\u5347\u6570\u636e\u548c\u6a21\u578b\u6027\u80fd\u7684\u6709\u6548\u65b9\u5f0f\u3002"}}
{"id": "2507.01437", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01437", "abs": "https://arxiv.org/abs/2507.01437", "authors": ["Ting Xu", "Xiaoxiao Deng", "Xiandong Meng", "Haifeng Yang", "Yan Wu"], "title": "Clinical NLP with Attention-Based Deep Learning for Multi-Disease Prediction", "comment": null, "summary": "This paper addresses the challenges posed by the unstructured nature and\nhigh-dimensional semantic complexity of electronic health record texts. A deep\nlearning method based on attention mechanisms is proposed to achieve unified\nmodeling for information extraction and multi-label disease prediction. The\nstudy is conducted on the MIMIC-IV dataset. A Transformer-based architecture is\nused to perform representation learning over clinical text. Multi-layer\nself-attention mechanisms are employed to capture key medical entities and\ntheir contextual relationships. A Sigmoid-based multi-label classifier is then\napplied to predict multiple disease labels. The model incorporates a\ncontext-aware semantic alignment mechanism, enhancing its representational\ncapacity in typical medical scenarios such as label co-occurrence and sparse\ninformation. To comprehensively evaluate model performance, a series of\nexperiments were conducted, including baseline comparisons, hyperparameter\nsensitivity analysis, data perturbation studies, and noise injection tests.\nResults demonstrate that the proposed method consistently outperforms\nrepresentative existing approaches across multiple performance metrics. The\nmodel maintains strong generalization under varying data scales, interference\nlevels, and model depth configurations. The framework developed in this study\noffers an efficient algorithmic foundation for processing real-world clinical\ntexts and presents practical significance for multi-label medical text modeling\ntasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u548c\u591a\u5c42\u6ce8\u610f\u529b\u673a\u5236\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u5bf9\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u7684\u7edf\u4e00\u4fe1\u606f\u62bd\u53d6\u548c\u591a\u6807\u7b7e\u75be\u75c5\u9884\u6d4b\uff0c\u5e76\u5728\u591a\u4e2a\u5b9e\u9a8c\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5177\u5907\u8f83\u5f3a\u6cdb\u5316\u80fd\u529b\u548c\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "motivation": "\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u6587\u672c\u901a\u5e38\u5177\u6709\u975e\u7ed3\u6784\u5316\u548c\u9ad8\u7ef4\u8bed\u4e49\u590d\u6742\u6027\uff0c\u7ed9\u4fe1\u606f\u62bd\u53d6\u4e0e\u75be\u75c5\u9884\u6d4b\u5e26\u6765\u4e86\u663e\u8457\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u91c7\u7528Transformer\u67b6\u6784\u548c\u591a\u5c42\u81ea\u6ce8\u610f\u529b\u673a\u5236\u8fdb\u884c\u4e34\u5e8a\u6587\u672c\u8868\u5f81\u5b66\u4e60\uff0c\u5e76\u7ed3\u5408Sigmoid\u591a\u6807\u7b7e\u5206\u7c7b\u5668\u8fdb\u884c\u591a\u6807\u7b7e\u75be\u75c5\u9884\u6d4b\u3002\u6a21\u578b\u8fd8\u8bbe\u8ba1\u4e86\u8bed\u5883\u611f\u77e5\u7684\u8bed\u4e49\u5bf9\u9f50\u673a\u5236\uff0c\u4ee5\u63d0\u5347\u5173\u952e\u533b\u5b66\u5b9e\u4f53\u548c\u4e0a\u4e0b\u6587\u5173\u7cfb\u7684\u8868\u5f81\u80fd\u529b\u3002", "result": "\u5728MIMIC-IV\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u9879\u6027\u80fd\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u4ee3\u8868\u6027\u65b9\u6cd5\uff0c\u5e76\u5728\u4e0d\u540c\u6570\u636e\u89c4\u6a21\u3001\u5e72\u6270\u6c34\u5e73\u548c\u6a21\u578b\u6df1\u5ea6\u7684\u60c5\u51b5\u4e0b\u4fdd\u6301\u826f\u597d\u6cdb\u5316\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u7edf\u4e00\u5efa\u6a21\u6846\u67b6\u4e3a\u5b9e\u9645\u4e34\u5e8a\u6587\u672c\u5904\u7406\u4ee5\u53ca\u591a\u6807\u7b7e\u533b\u5b66\u6587\u672c\u5efa\u6a21\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u7b97\u6cd5\u57fa\u7840\uff0c\u5e76\u5177\u6709\u5b9e\u9645\u5e94\u7528\u610f\u4e49\u3002"}}
{"id": "2507.01449", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01449", "abs": "https://arxiv.org/abs/2507.01449", "authors": ["Tianyu Liu", "Qitan Lv", "Hao Li", "Xing Gao", "Xiao Sun"], "title": "LogitSpec: Accelerating Retrieval-based Speculative Decoding via Next Next Token Speculation", "comment": null, "summary": "Speculative decoding (SD), where a small draft model is employed to propose\ndraft tokens in advance and then the target model validates them in parallel,\nhas emerged as a promising technique for LLM inference acceleration. Many\nendeavors to improve SD are to eliminate the need for a draft model and\ngenerate draft tokens in a retrieval-based manner in order to further alleviate\nthe drafting overhead and significantly reduce the difficulty in deployment and\napplications. However, retrieval-based SD relies on a matching paradigm to\nretrieval the most relevant reference as the draft tokens, where these methods\noften fail to find matched and accurate draft tokens. To address this\nchallenge, we propose LogitSpec to effectively expand the retrieval range and\nfind the most relevant reference as drafts. Our LogitSpec is motivated by the\nobservation that the logit of the last token can not only predict the next\ntoken, but also speculate the next next token. Specifically, LogitSpec\ngenerates draft tokens in two steps: (1) utilizing the last logit to speculate\nthe next next token; (2) retrieving relevant reference for both the next token\nand the next next token. LogitSpec is training-free and plug-and-play, which\ncan be easily integrated into existing LLM inference frameworks. Extensive\nexperiments on a wide range of text generation benchmarks demonstrate that\nLogitSpec can achieve up to 2.61 $\\times$ speedup and 3.28 mean accepted tokens\nper decoding step. Our code is available at\nhttps://github.com/smart-lty/LogitSpec.", "AI": {"tldr": "LogitSpec\u662f\u4e00\u79cd\u5229\u7528logit\u63a8\u7406\u6269\u5c55\u8349\u7a3ftoken\u68c0\u7d22\u8303\u56f4\u7684\u65b0\u65b9\u6cd5\uff0c\u65e0\u9700\u8bad\u7ec3\u3001\u6613\u4e8e\u96c6\u6210\uff0c\u80fd\u663e\u8457\u52a0\u901fLLM\u63a8\u7406\u5e76\u63d0\u5347\u8349\u7a3ftoken\u63a5\u53d7\u7387\u3002", "motivation": "\u63a8\u6d4b\u89e3\u7801\uff08SD\uff09\u65e8\u5728\u901a\u8fc7\u5148\u7531\u5c0f\u6a21\u578b\u751f\u6210\u5019\u9009tokens\uff0c\u518d\u7531\u5927\u6a21\u578b\u5e76\u884c\u9a8c\u8bc1\u6765\u52a0\u901f\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u63a8\u7406\u3002\u5f53\u524d\u6539\u5584\u63aa\u65bd\u5927\u591a\u805a\u7126\u4e8e\u7528\u68c0\u7d22\u5f0f\u65b9\u6cd5\u751f\u6210\u8349\u7a3ftokens\uff0c\u4ee5\u907f\u514d\u989d\u5916\u7684\u5c0f\u6a21\u578b\u6210\u672c\uff0c\u4f46\u68c0\u7d22\u5f0f\u65b9\u6cd5\u5e38\u56e0\u5339\u914d\u5931\u8d25\u5bfc\u81f4\u51c6\u786e\u6027\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5LogitSpec\uff0c\u901a\u8fc7\u5229\u7528\u4e0a\u4e00token\u7684logit\uff0c\u4e0d\u53ea\u9884\u6d4b\u4e0b\u4e00\u4e2atoken\uff0c\u8fd8\u80fd\u63a8\u6d4b\u4e0b\u4e0b\u4e2atoken\uff0c\u5e76\u636e\u6b64\u68c0\u7d22\u76f8\u5173\u53c2\u8003\u5185\u5bb9\u4f5c\u4e3a\u8349\u7a3ftokens\u3002LogitSpec\u65e0\u9700\u989d\u5916\u8bad\u7ec3\uff0c\u53ef\u5373\u63d2\u5373\u7528\u96c6\u6210\u8fdb\u73b0\u6709LLM\u6846\u67b6\u3002", "result": "\u5728\u591a\u4e2a\u6587\u672c\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLogitSpec\u53ef\u5b9e\u73b0\u6700\u9ad82.61\u500d\u7684\u63a8\u7406\u52a0\u901f\uff0c\u5e73\u5747\u6bcf\u6b65\u53ef\u88ab\u63a5\u53d7\u7684\u8349\u7a3ftokens\u63d0\u5347\u81f33.28\u3002\u4ee3\u7801\u5df2\u5f00\u6e90\u3002", "conclusion": "LogitSpec\u663e\u8457\u6269\u5c55\u4e86\u68c0\u7d22\u8303\u56f4\uff0c\u63d0\u5347\u4e86\u8349\u7a3ftoken\u7684\u76f8\u5173\u6027\u548c\u51c6\u786e\u6027\uff0c\u4ece\u800c\u6709\u6548\u5730\u52a0\u901f\u4e86LLM\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u5e76\u964d\u4f4e\u90e8\u7f72\u4e0e\u5e94\u7528\u96be\u5ea6\u3002"}}
{"id": "2507.01479", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01479", "abs": "https://arxiv.org/abs/2507.01479", "authors": ["Yingqiang Gao", "Kaede Johnson", "David Froehlich", "Luisa Carrer", "Sarah Ebling"], "title": "Evaluating the Effectiveness of Direct Preference Optimization for Personalizing German Automatic Text Simplifications for Persons with Intellectual Disabilities", "comment": null, "summary": "Automatic text simplification (ATS) aims to enhance language accessibility\nfor various target groups, particularly persons with intellectual disabilities.\nRecent advancements in generative AI, especially large language models (LLMs),\nhave substantially improved the quality of machine-generated text\nsimplifications, thereby mitigating information barriers for the target group.\nHowever, existing LLM-based ATS systems do not incorporate preference feedback\non text simplifications during training, resulting in a lack of personalization\ntailored to the specific needs of target group representatives.\n  In this work, we extend the standard supervised fine-tuning (SFT) approach\nfor adapting LLM-based ATS models by leveraging a computationally efficient LLM\nalignment technique -- direct preference optimization (DPO). Specifically, we\npost-train LLM-based ATS models using human feedback collected from persons\nwith intellectual disabilities, reflecting their preferences on paired text\nsimplifications generated by mainstream LLMs. Furthermore, we propose a\npipeline for developing personalized LLM-based ATS systems, encompassing data\ncollection, model selection, SFT and DPO post-training, and evaluation. Our\nfindings underscore the necessity of active participation of target group\npersons in designing personalized AI accessibility solutions aligned with human\nexpectations. This work represents a step towards personalizing inclusive AI\nsystems at the target-group level, incorporating insights not only from text\nsimplification experts but also from target group persons themselves.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u5f15\u5165\u5b9e\u9645\u7528\u6237\u504f\u597d\u53cd\u9988\u548c\u76f4\u63a5\u504f\u597d\u4f18\u5316\u6280\u672f\uff0c\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u518d\u8bad\u7ec3\uff0c\u6709\u6548\u63d0\u5347\u4e86\u9488\u5bf9\u667a\u529b\u969c\u788d\u4eba\u7fa4\u7684\u6587\u672c\u7b80\u5316\u7cfb\u7edf\u7684\u4e2a\u6027\u5316\u6c34\u5e73\uff0c\u4e3a\u5efa\u8bbe\u516c\u5e73\u5305\u5bb9\u7684AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002", "motivation": "\u81ea\u52a8\u6587\u672c\u7b80\u5316\uff08ATS\uff09\u65e8\u5728\u63d0\u5347\u8bed\u8a00\u7684\u53ef\u53ca\u6027\uff0c\u7279\u522b\u662f\u4e3a\u6709\u667a\u529b\u969c\u788d\u7684\u4eba\u7fa4\u670d\u52a1\u3002\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u6781\u5927\u6539\u5584\u4e86\u6587\u672c\u7b80\u5316\u8d28\u91cf\uff0c\u4f46\u73b0\u6709\u7cfb\u7edf\u672a\u80fd\u5728\u8bad\u7ec3\u4e2d\u878d\u5165\u76ee\u6807\u7fa4\u4f53\u7684\u53cd\u9988\uff0c\u7f3a\u4e4f\u4e2a\u6027\u5316\u3002", "method": "\u63d0\u51fa\u5728\u5e38\u89c4\u6a21\u578b\u5fae\u8c03\u57fa\u7840\u4e0a\uff0c\u5229\u7528\u9ad8\u6548\u7684\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u6280\u672f\u5bf9LLM\u8fdb\u884c\u518d\u8bad\u7ec3\uff0c\u91c7\u96c6\u5e76\u5229\u7528\u667a\u529b\u969c\u788d\u8005\u5bf9\u4e0d\u540c\u6587\u672c\u7b80\u5316\u7ed3\u679c\u7684\u504f\u597d\u53cd\u9988\u3002\u540c\u65f6\u6784\u5efa\u4e2a\u6027\u5316\u6587\u672c\u7b80\u5316\u7cfb\u7edf\u6d41\u7a0b\uff0c\u5305\u62ec\u6570\u636e\u91c7\u96c6\u3001\u6a21\u578b\u9009\u62e9\u3001SFT\u4e0eDPO\u540e\u8bad\u7ec3\u53ca\u8bc4\u4f30\u73af\u8282\u3002", "result": "\u91c7\u7528DPO\u4e0e\u771f\u5b9e\u76ee\u6807\u7528\u6237\u504f\u597d\u53cd\u9988\u53ef\u660e\u663e\u63d0\u9ad8\u6a21\u578b\u4e2a\u6027\u5316\u53ca\u6ee1\u8db3\u76ee\u6807\u7fa4\u4f53\u9700\u6c42\u3002\u8bba\u6587\u5f3a\u8c03\u4e86\u76ee\u6807\u7fa4\u4f53\u4e3b\u89c2\u53c2\u4e0e\u7684\u91cd\u8981\u6027\uff0c\u5e76\u5c06\u8be5\u6d41\u7a0b\u6807\u51c6\u5316\u4ee5\u4fc3\u8fdb\u540e\u7eed\u666e\u9002\u6027\u5e94\u7528\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u667a\u529b\u969c\u788d\u4eba\u7fa4\u53cd\u9988\u53ca\u76f4\u63a5\u504f\u597d\u4f18\u5316\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u9762\u5411\u76ee\u6807\u7fa4\u4f53\u7684\u4e2a\u6027\u5316\u6587\u672c\u7b80\u5316AI\u7cfb\u7edf\uff0c\u63a8\u52a8\u4ee5\u4eba\u4e3a\u672c\u7684\u666e\u60e0\u6027AI\u53d1\u5c55\u3002"}}
{"id": "2507.01541", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01541", "abs": "https://arxiv.org/abs/2507.01541", "authors": ["\u00c1lvaro Zaera", "Diana Nicoleta Popa", "Ivan Sekulic", "Paolo Rosso"], "title": "Efficient Out-of-Scope Detection in Dialogue Systems via Uncertainty-Driven LLM Routing", "comment": null, "summary": "Out-of-scope (OOS) intent detection is a critical challenge in task-oriented\ndialogue systems (TODS), as it ensures robustness to unseen and ambiguous\nqueries. In this work, we propose a novel but simple modular framework that\ncombines uncertainty modeling with fine-tuned large language models (LLMs) for\nefficient and accurate OOS detection. The first step applies uncertainty\nestimation to the output of an in-scope intent detection classifier, which is\ncurrently deployed in a real-world TODS handling tens of thousands of user\ninteractions daily. The second step then leverages an emerging LLM-based\napproach, where a fine-tuned LLM is triggered to make a final decision on\ninstances with high uncertainty. Unlike prior approaches, our method\neffectively balances computational efficiency and performance, combining\ntraditional approaches with LLMs and yielding state-of-the-art results on key\nOOS detection benchmarks, including real-world OOS data acquired from a\ndeployed TODS.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u4e0e\u5fae\u8c03\u5927\u8bed\u8a00\u6a21\u578b\u7684OOS\u68c0\u6d4b\u6846\u67b6\uff0c\u65e2\u9ad8\u6548\u53c8\u51c6\u786e\uff0c\u5e76\u5728\u771f\u5b9e\u53ca\u6807\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u6700\u4f73\u6548\u679c\u3002", "motivation": "\u5728\u4efb\u52a1\u578b\u5bf9\u8bdd\u7cfb\u7edf\uff08TODS\uff09\u4e2d\uff0c\u8bc6\u522b\u8d85\u51fa\u7cfb\u7edf\u9884\u671f\u8303\u56f4\u7684\u95ee\u9898\uff08OOS\uff09\u5bf9\u4e8e\u786e\u4fdd\u7cfb\u7edf\u5728\u9762\u5bf9\u672a\u77e5\u6216\u6a21\u7cca\u67e5\u8be2\u65f6\u7684\u5065\u58ee\u6027\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u6548\u7387\u548c\u6027\u80fd\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff0c\u56e0\u6b64\u9700\u8981\u63d0\u51fa\u66f4\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u65b9\u6cd5\u3002", "method": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u5408\u4e86\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u548c\u5fae\u8c03\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u6a21\u5757\u5316\u6846\u67b6\u3002\u7b2c\u4e00\u6b65\uff0c\u91c7\u7528\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u5bf9\u5f53\u524d\u5df2\u90e8\u7f72\u7684\u610f\u56fe\u68c0\u6d4b\u5206\u7c7b\u5668\u8f93\u51fa\u8fdb\u884c\u5206\u6790\u3002\u7b2c\u4e8c\u6b65\uff0c\u5bf9\u4e8e\u9ad8\u4e0d\u786e\u5b9a\u6027\u5b9e\u4f8b\uff0c\u89e6\u53d1\u5fae\u8c03\u540e\u7684LLM\u6765\u505a\u6700\u7ec8\u5224\u51b3\u3002\u8be5\u6846\u67b6\u6709\u6548\u7ed3\u5408\u4e86\u4f20\u7edf\u65b9\u6cd5\u4e0eLLM\u3002", "result": "\u672c\u65b9\u6cd5\u5728\u5173\u952e\u7684OOS\u68c0\u6d4b\u57fa\u51c6\u6d4b\u8bd5\u4e0a\uff0c\u5305\u62ec\u6765\u81ea\u771f\u5b9eTODS\u7cfb\u7edf\u7684\u5b9e\u9645OOS\u6570\u636e\uff0c\u53d6\u5f97\u4e86\u65b0\u7684\u6700\u4f18\u7ed3\u679c\uff0c\u9a8c\u8bc1\u4e86\u5176\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u7ed3\u5408\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u4e0e\u5fae\u8c03LLM\u7684OOS\u68c0\u6d4b\u6846\u67b6\uff0c\u517c\u987e\u4e86\u8ba1\u7b97\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u5e76\u5728\u5b9e\u9645\u4ee5\u53ca\u57fa\u51c6\u4efb\u52a1\u4e0a\u5c55\u73b0\u51fa\u4f18\u5f02\u8868\u73b0\u3002"}}
{"id": "2507.01543", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01543", "abs": "https://arxiv.org/abs/2507.01543", "authors": ["Quang Minh Nguyen", "Taegyoon Kim"], "title": "Is External Information Useful for Stance Detection with LLMs?", "comment": "ACL Findings 2025", "summary": "In the stance detection task, a text is classified as either favorable,\nopposing, or neutral towards a target. Prior work suggests that the use of\nexternal information, e.g., excerpts from Wikipedia, improves stance detection\nperformance. However, whether or not such information can benefit large\nlanguage models (LLMs) remains an unanswered question, despite their wide\nadoption in many reasoning tasks. In this study, we conduct a systematic\nevaluation on how Wikipedia and web search external information can affect\nstance detection across eight LLMs and in three datasets with 12 targets.\nSurprisingly, we find that such information degrades performance in most cases,\nwith macro F1 scores dropping by up to 27.9\\%. We explain this through\nexperiments showing LLMs' tendency to align their predictions with the stance\nand sentiment of the provided information rather than the ground truth stance\nof the given text. We also find that performance degradation persists with\nchain-of-thought prompting, while fine-tuning mitigates but does not fully\neliminate it. Our findings, in contrast to previous literature on BERT-based\nsystems which suggests that external information enhances performance,\nhighlight the risks of information biases in LLM-based stance classifiers. Code\nis available at https://github.com/ngqm/acl2025-stance-detection.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u5206\u6790\u4e86LLMs\u5728\u7acb\u573a\u68c0\u6d4b\u4e2d\u5f15\u7528Wikipedia\u7b49\u5916\u90e8\u4fe1\u606f\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u4e0d\u540c\u4e8eBERT\u7b49\u6a21\u578b\uff0c\u5916\u90e8\u4fe1\u606f\u53cd\u800c\u5bfc\u81f4\u6027\u80fd\u660e\u663e\u4e0b\u964d\uff0c\u4e3b\u8981\u7531\u4e8e\u4fe1\u606f\u504f\u89c1\uff1bchain-of-thought\u65e0\u6548\uff0c\u5fae\u8c03\u53ef\u7f13\u89e3\u4f46\u65e0\u6cd5\u6839\u6cbb\u3002\u63d0\u9192\u4eca\u540e\u5e94\u8c28\u614e\u5229\u7528\u5916\u90e8\u4fe1\u606f\u4e8eLLM\u7acb\u573a\u4efb\u52a1\u3002", "motivation": "\u4ee5\u5f80\u7814\u7a76\u8868\u660e\uff0c\u5f15\u7528\u5916\u90e8\u4fe1\u606f\uff08\u5982Wikipedia\uff09\u53ef\u4ee5\u63d0\u5347\u7acb\u573a\u68c0\u6d4b\u8868\u73b0\uff0c\u4f46\u8fd9\u4e9b\u7ed3\u8bba\u5728\u5f53\u524d\u5e7f\u6cdb\u5e94\u7528\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e0a\u662f\u5426\u9002\u7528\u5c1a\u672a\u6709\u660e\u786e\u7b54\u6848\u3002\u672c\u7814\u7a76\u52a8\u673a\u662f\u586b\u8865LLMs\u5728\u7acb\u573a\u68c0\u6d4b\u4efb\u52a1\u4e2d\u5229\u7528\u5916\u90e8\u4fe1\u606f\u6548\u679c\u7684\u77e5\u8bc6\u7a7a\u767d\u3002", "method": "\u7cfb\u7edf\u8bc4\u4f30\u4e86Wikipedia\u548c\u7f51\u9875\u641c\u7d22\u4fe1\u606f\u5bf98\u4e2aLLM\u57283\u4e2a\u6570\u636e\u96c6\u300112\u4e2a\u76ee\u6807\u4e0a\u7684\u7acb\u573a\u68c0\u6d4b\u5f71\u54cd\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8fdb\u4e00\u6b65\u89e3\u91ca\u6027\u80fd\u6ce2\u52a8\u539f\u56e0\uff0c\u8fd8\u8003\u5bdf\u4e86chain-of-thought prompting\u4e0e\u5fae\u8c03\u7684\u4f5c\u7528\u3002", "result": "\u5927\u591a\u6570\u60c5\u51b5\u4e0b\uff0c\u5f15\u7528\u5916\u90e8\u4fe1\u606f\u4f1a\u5bfc\u81f4\u6a21\u578b\u6027\u80fd\u4e0b\u964d\uff0cMacro F1\u5f97\u5206\u6700\u9ad8\u4e0b\u964d\u8fbe27.9%\uff1b\u4e3b\u8981\u539f\u56e0\u662fLLM\u8d8b\u5411\u4e8e\u6839\u636e\u6240\u63d0\u4f9b\u4fe1\u606f\u7684\u7acb\u573a\u548c\u60c5\u611f\u800c\u975e\u539f\u6587\u672c\u7acb\u573a\u4f5c\u9884\u6d4b\u3002\u5373\u4f7f\u4f7f\u7528chain-of-thought prompting\uff0c\u6027\u80fd\u4e0b\u964d\u4ecd\u6301\u7eed\uff0c\u4ec5\u6709\u5fae\u8c03\u53ef\u6709\u6240\u7f13\u89e3\u4f46\u65e0\u6cd5\u5b8c\u5168\u6d88\u9664\u3002", "conclusion": "\u4e0e\u4ee5\u5f80BERT\u7c7b\u6a21\u578b\u7814\u7a76\u76f8\u53cd\uff0c\u5916\u90e8\u4fe1\u606f\u5f80\u5f80\u5bf9LLM\u7acb\u573a\u68c0\u6d4b\u8d77\u8d1f\u9762\u4f5c\u7528\uff0c\u6613\u5f15\u5165\u4fe1\u606f\u504f\u5dee\u3002\u5bf9LLM\u804c\u8d23\u9886\u57df\u7684\u7acb\u573a\u5206\u7c7b\u4efb\u52a1\uff0c\u5e94\u8b66\u60d5\u5916\u90e8\u4fe1\u606f\u5e26\u6765\u7684\u98ce\u9669\u3002"}}
{"id": "2507.01594", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01594", "abs": "https://arxiv.org/abs/2507.01594", "authors": ["Shutong Feng", "Hsien-chin Lin", "Nurul Lubis", "Carel van Niekerk", "Michael Heck", "Benjamin Ruppik", "Renato Vukovic", "Milica Ga\u0161i\u0107"], "title": "Emotionally Intelligent Task-oriented Dialogue Systems: Architecture, Representation, and Optimisation", "comment": "19 pages, 6 figures", "summary": "Task-oriented dialogue (ToD) systems are designed to help users achieve\nspecific goals through natural language interaction. While recent advances in\nlarge language models (LLMs) have significantly improved linguistic fluency and\ncontextual understanding, building effective and emotionally intelligent ToD\nsystems remains a complex challenge. Effective ToD systems must optimise for\ntask success, emotional understanding and responsiveness, and precise\ninformation conveyance, all within inherently noisy and ambiguous\nconversational environments. In this work, we investigate architectural,\nrepresentational, optimisational as well as emotional considerations of ToD\nsystems. We set up systems covering these design considerations with a\nchallenging evaluation environment composed of a natural-language user\nsimulator coupled with an imperfect natural language understanding module. We\npropose \\textbf{LUSTER}, an \\textbf{L}LM-based \\textbf{U}nified \\textbf{S}ystem\nfor \\textbf{T}ask-oriented dialogue with \\textbf{E}nd-to-end\n\\textbf{R}einforcement learning with both short-term (user sentiment) and\nlong-term (task success) rewards. Our findings demonstrate that combining LLM\ncapability with structured reward modelling leads to more resilient and\nemotionally responsive ToD systems, offering a practical path forward for\nnext-generation conversational agents.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faLUSTER\u65b9\u6cd5\uff0c\u901a\u8fc7LLM\u80fd\u529b\u4e0e\u591a\u7ef4\u7ed3\u6784\u5316\u5956\u52b1\u7ed3\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4efb\u52a1\u578b\u5bf9\u8bdd\u7cfb\u7edf\u7684\u97e7\u6027\u4e0e\u60c5\u611f\u54cd\u5e94\uff0c\u4e3a\u540e\u7eed\u60c5\u611f\u667a\u80fd\u5bf9\u8bdd\u7cfb\u7edf\u63d0\u4f9b\u4e86\u8bbe\u8ba1\u8def\u7ebf\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u63d0\u5347\u4e86\u5bf9\u8bdd\u6d41\u7545\u5ea6\u548c\u4e0a\u4e0b\u6587\u7406\u89e3\u80fd\u529b\uff0c\u4f46\u8981\u6784\u5efa\u65e2\u9ad8\u6548\u53c8\u5177\u5907\u60c5\u611f\u667a\u80fd\u7684\u4efb\u52a1\u578b\u5bf9\u8bdd\uff08ToD\uff09\u7cfb\u7edf\uff0c\u4ecd\u7136\u975e\u5e38\u590d\u6742\u3002ToD\u7cfb\u7edf\u9700\u8981\u5b9e\u73b0\u4efb\u52a1\u6210\u529f\u3001\u60c5\u611f\u7406\u89e3\u3001\u51c6\u786e\u4f20\u8fbe\u4fe1\u606f\uff0c\u5e76\u5bf9\u6297\u5bf9\u8bdd\u73af\u5883\u4e2d\u7684\u566a\u58f0\u548c\u6b67\u4e49\u3002", "method": "\u7cfb\u7edf\u6027\u5730\u7814\u7a76\u4e86\u4e0e\u4efb\u52a1\u578b\u5bf9\u8bdd\u7cfb\u7edf\u76f8\u5173\u7684\u67b6\u6784\u3001\u8868\u793a\u3001\u4f18\u5316\u53ca\u60c5\u611f\u56e0\u7d20\u3002\u6784\u5efa\u5305\u542b\u8fd9\u4e9b\u8bbe\u8ba1\u56e0\u7d20\u7684\u7cfb\u7edf\uff0c\u5e76\u7ed3\u5408\u81ea\u7136\u8bed\u8a00\u7528\u6237\u6a21\u62df\u5668\u548c\u6709\u7f3a\u9677\u7684\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u6a21\u5757\uff0c\u7ec4\u6210\u4e86\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u8bc4\u6d4b\u73af\u5883\u3002\u63d0\u51fa\u4e86LUSTER\u65b9\u6cd5\u2014\u2014\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u3001\u91c7\u7528\u7aef\u5230\u7aef\u5f3a\u5316\u5b66\u4e60\u7684\u7edf\u4e00\u4efb\u52a1\u5bf9\u8bdd\u7cfb\u7edf\uff0c\u7ed3\u5408\u4e86\u7528\u6237\u60c5\u611f\uff08\u77ed\u671f\uff09\u548c\u4efb\u52a1\u6210\u529f\uff08\u957f\u671f\uff09\u5956\u52b1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u529b\u4e0e\u7ed3\u6784\u5316\u5956\u52b1\u5efa\u6a21\u76f8\u7ed3\u5408\uff0c\u53ef\u4ee5\u8ba9\u4efb\u52a1\u578b\u5bf9\u8bdd\u7cfb\u7edf\u66f4\u5177\u97e7\u6027\u548c\u60c5\u611f\u54cd\u5e94\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6784\u5efa\u66f4\u5b9e\u7528\u7684\u65b0\u4e00\u4ee3\u5bf9\u8bdd\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2507.01627", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01627", "abs": "https://arxiv.org/abs/2507.01627", "authors": ["Maeve Hutchinson", "Radu Jianu", "Aidan Slingsby", "Jo Wood", "Pranava Madhyastha"], "title": "Chart Question Answering from Real-World Analytical Narratives", "comment": "This paper has been accepted to the ACL Student Research Workshop\n  (SRW) 2025", "summary": "We present a new dataset for chart question answering (CQA) constructed from\nvisualization notebooks. The dataset features real-world, multi-view charts\npaired with natural language questions grounded in analytical narratives.\nUnlike prior benchmarks, our data reflects ecologically valid reasoning\nworkflows. Benchmarking state-of-the-art multimodal large language models\nreveals a significant performance gap, with GPT-4.1 achieving an accuracy of\n69.3%, underscoring the challenges posed by this more authentic CQA setting.", "AI": {"tldr": "\u672c\u6587\u63a8\u51fa\u4e86\u4e00\u4e2a\u6e90\u81ea\u53ef\u89c6\u5316\u7b14\u8bb0\u672c\u3001\u771f\u5b9e\u591a\u89c6\u56fe\u56fe\u8868\u4e0e\u81ea\u7136\u8bed\u8a00\u95ee\u7b54\u914d\u5957\u7684\u65b0CQA\u6570\u636e\u96c6\uff0c\u663e\u8457\u589e\u52a0\u4e86\u4efb\u52a1\u590d\u6742\u5ea6\u3002\u5373\u4fbf\u6700\u5f3a\u591a\u6a21\u6001\u5927\u6a21\u578bGPT-4.1\u5728\u6570\u636e\u96c6\u4e0a\u51c6\u786e\u7387\u4e5f\u53ea\u670969.3%\uff0c\u53cd\u6620\u51fa\u73b0\u6709\u6a21\u578b\u5728\u771f\u5b9e\u56fe\u8868\u95ee\u7b54\u4e0a\u4ecd\u6709\u5de8\u5927\u63d0\u5347\u7a7a\u95f4\u3002", "motivation": "\u73b0\u6709\u7684\u56fe\u8868\u95ee\u7b54\uff08CQA\uff09\u6570\u636e\u96c6\u5728\u4efb\u52a1\u8bbe\u7f6e\u548c\u6570\u636e\u6765\u6e90\u4e0a\u7f3a\u4e4f\u771f\u5b9e\u6027\uff0c\u96be\u4ee5\u53cd\u6620\u5b9e\u9645\u5206\u6790\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u7684\u590d\u6742\u63a8\u7406\u9700\u6c42\u3002\u4e3a\u63a8\u52a8\u771f\u5b9e\u4e16\u754cCQA\u7814\u7a76\uff0c\u4f5c\u8005\u5e0c\u671b\u6784\u5efa\u4e00\u4e2a\u66f4\u52a0\u8d34\u8fd1\u5b9e\u9645\u7684\u6570\u636e\u96c6\u3002", "method": "\u4f5c\u8005\u57fa\u4e8e\u53ef\u89c6\u5316\u5206\u6790\u7b14\u8bb0\u672c\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u771f\u5b9e\u591a\u89c6\u56fe\u56fe\u8868\u548c\u914d\u5957\u81ea\u7136\u8bed\u8a00\u95ee\u9898\u7684\u65b0\u578bCQA\u6570\u636e\u96c6\u3002\u8be5\u6570\u636e\u96c6\u690d\u6839\u4e8e\u5206\u6790\u53d9\u4e8b\u4e2d\uff0c\u95ee\u9898\u66f4\u7b26\u5408\u771f\u5b9e\u4e16\u754c\u4e2d\u7684\u63a8\u7406\u573a\u666f\u3002\u5e76\u7528\u73b0\u6709\u591a\u6a21\u6001\u5927\u6a21\u578b\uff08\u5982GPT-4.1\uff09\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u5728\u8fd9\u4e2a\u65b0\u63d0\u51fa\u7684\u3001\u66f4\u52a0\u771f\u5b9e\u7684CQA\u6570\u636e\u96c6\u4e0a\uff0c\u9886\u5148\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u8868\u73b0\u51fa\u660e\u663e\u4e0d\u8db3\u3002\u4f8b\u5982\uff0cGPT-4.1\u5728\u51c6\u786e\u7387\u4e0a\u4ec5\u4e3a69.3%\uff0c\u663e\u793a\u51fa\u73b0\u6709\u6a21\u578b\u8ddd\u5b9e\u9645\u5e94\u7528\u9700\u6c42\u4ecd\u6709\u8f83\u5927\u5dee\u8ddd\u3002", "conclusion": "\u4f5c\u8005\u63d0\u51fa\u7684\u6570\u636e\u96c6\u4e3aCQA\u7814\u7a76\u63d0\u4f9b\u4e86\u66f4\u5177\u751f\u6001\u6548\u5ea6\u7684\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u591a\u6a21\u6001\u5927\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754cCQA\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u540e\u7eed\u6a21\u578b\u8bbe\u8ba1\u548c\u8bc4\u6d4b\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2507.01633", "categories": ["cs.CL", "cs.IR", "62-04", "D.2.3"], "pdf": "https://arxiv.org/pdf/2507.01633", "abs": "https://arxiv.org/abs/2507.01633", "authors": ["Georgii Levtsov", "Dmitry Ustalov"], "title": "Confidence and Stability of Global and Pairwise Scores in NLP Evaluation", "comment": "8 pages, accepted at ACL SRW 2025", "summary": "With the advent of highly capable instruction-tuned neural language models,\nbenchmarking in natural language processing (NLP) is increasingly shifting\ntowards pairwise comparison leaderboards, such as LMSYS Arena, from traditional\nglobal pointwise scores (e.g., GLUE, BIG-bench, SWE-bench). This paper\nempirically investigates the strengths and weaknesses of both global scores and\npairwise comparisons to aid decision-making in selecting appropriate model\nevaluation strategies. Through computational experiments on synthetic and\nreal-world datasets using standard global metrics and the popular Bradley-Terry\nmodel for pairwise comparisons, we found that while global scores provide more\nreliable overall rankings, they can underestimate strong models with rare,\nsignificant errors or low confidence. Conversely, pairwise comparisons are\nparticularly effective for identifying strong contenders among models with\nlower global scores, especially where quality metrics are hard to define (e.g.,\ntext generation), though they require more comparisons to converge if ties are\nfrequent. Our code and data are available at\nhttps://github.com/HSPyroblast/srw-ranking under a permissive license.", "AI": {"tldr": "\u5168\u5c40\u8bc4\u5206\u9002\u5408\u6574\u4f53\u6392\u540d\uff0c\u6210\u5bf9\u6bd4\u8f83\u66f4\u80fd\u53d1\u73b0\u8868\u73b0\u7a81\u51fa\u7684\u4f4e\u5206\u6a21\u578b\u3002\u4e24\u8005\u7ed3\u5408\u53ef\u4f18\u5316\u6a21\u578b\u8bc4\u4f30\u7b56\u7565\u3002", "motivation": "\u81ea\u7136\u8bed\u8a00\u5904\u7406\u9886\u57df\u7684\u57fa\u51c6\u6d4b\u8bd5\u4f20\u7edf\u4e0a\u4f9d\u8d56\u5168\u5c40\u5206\u503c\uff08pointwise scores\uff09\uff0c\u4f46\u968f\u7740\u9ad8\u6027\u80fd\u6307\u4ee4\u5fae\u8c03\u8bed\u8a00\u6a21\u578b\u7684\u51fa\u73b0\uff0c\u7c7b\u4f3cLMSYS Arena\u7684\u6210\u5bf9\u6bd4\u8f83\u6392\u884c\u699c\u9010\u6e10\u6d41\u884c\u3002\u672c\u6587\u65e8\u5728\u6bd4\u8f83\u8fd9\u4e24\u79cd\u8bc4\u4f30\u65b9\u6cd5\u7684\u4f18\u7f3a\u70b9\uff0c\u5e2e\u52a9\u7814\u7a76\u8005\u9009\u62e9\u66f4\u5408\u9002\u7684\u6a21\u578b\u8bc4\u4f30\u7b56\u7565\u3002", "method": "\u672c\u6587\u5728\u5408\u6210\u6570\u636e\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\uff0c\u901a\u8fc7\u6807\u51c6\u7684\u5168\u5c40\u8bc4\u5206\u6307\u6807\u548c\u5e38\u7528\u7684Bradley-Terry\u6210\u5bf9\u6bd4\u8f83\u6a21\u578b\u8fdb\u884c\u5bf9\u6bd4\u5206\u6790\u548c\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u5168\u5c40\u5206\u503c\u5728\u6574\u4f53\u6392\u5e8f\u4e0a\u66f4\u52a0\u53ef\u9760\uff0c\u4f46\u53ef\u80fd\u4f4e\u4f30\u90a3\u4e9b\u5076\u53d1\u91cd\u5927\u9519\u8bef\u6216\u7f6e\u4fe1\u5ea6\u4f4e\u4f46\u603b\u4f53\u5f88\u5f3a\u7684\u6a21\u578b\u3002\u6210\u5bf9\u6bd4\u8f83\u80fd\u66f4\u6709\u6548\u5730\u8bc6\u522b\u5168\u5c40\u5206\u503c\u8f83\u4f4e\u4f46\u6709\u7a81\u51fa\u8868\u73b0\u7684\u6a21\u578b\uff0c\u5c24\u5176\u9002\u5408\u96be\u4ee5\u91cf\u5316\u8d28\u91cf\u7684\u4efb\u52a1\uff08\u5982\u6587\u672c\u751f\u6210\uff09\uff0c\u4f46\u5728\u5e73\u5c40\u8f83\u591a\u65f6\uff0c\u9700\u8981\u66f4\u591a\u7684\u6bd4\u8f83\u6b21\u6570\u4ee5\u83b7\u5f97\u7a33\u5b9a\u6392\u540d\u3002", "conclusion": "\u5168\u5c40\u5206\u503c\u548c\u6210\u5bf9\u6bd4\u8f83\u5404\u6709\u4f18\u52a3\u3002\u9009\u62e9\u8bc4\u4f30\u65b9\u5f0f\u65f6\uff0c\u5e94\u7ed3\u5408\u4efb\u52a1\u7279\u70b9\u548c\u5b9e\u9645\u9700\u6c42\u6743\u8861\u4f7f\u7528\u3002"}}
{"id": "2507.01645", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01645", "abs": "https://arxiv.org/abs/2507.01645", "authors": ["Rifki Afina Putri"], "title": "Adapting Language Models to Indonesian Local Languages: An Empirical Study of Language Transferability on Zero-Shot Settings", "comment": "AMLDS 2025", "summary": "In this paper, we investigate the transferability of pre-trained language\nmodels to low-resource Indonesian local languages through the task of sentiment\nanalysis. We evaluate both zero-shot performance and adapter-based transfer on\nten local languages using models of different types: a monolingual Indonesian\nBERT, multilingual models such as mBERT and XLM-R, and a modular adapter-based\napproach called MAD-X. To better understand model behavior, we group the target\nlanguages into three categories: seen (included during pre-training), partially\nseen (not included but linguistically related to seen languages), and unseen\n(absent and unrelated in pre-training data). Our results reveal clear\nperformance disparities across these groups: multilingual models perform best\non seen languages, moderately on partially seen ones, and poorly on unseen\nlanguages. We find that MAD-X significantly improves performance, especially\nfor seen and partially seen languages, without requiring labeled data in the\ntarget language. Additionally, we conduct a further analysis on tokenization\nand show that while subword fragmentation and vocabulary overlap with\nIndonesian correlate weakly with prediction quality, they do not fully explain\nthe observed performance. Instead, the most consistent predictor of transfer\nsuccess is the model's prior exposure to the language, either directly or\nthrough a related language.", "AI": {"tldr": "\u5370\u5c3c\u672c\u5730\u4f4e\u8d44\u6e90\u8bed\u8a00\u60c5\u611f\u5206\u6790\u4e2d\uff0c\u6a21\u578b\u8fc1\u79fb\u8868\u73b0\u4e3b\u8981\u7531\u9884\u8bad\u7ec3\u66b4\u9732\u51b3\u5b9a\u3002\u591a\u8bed\u8a00\u6a21\u578b\u5728\u2018\u5df2\u89c1\u2019\u8bed\u8a00\u4e0a\u6548\u679c\u663e\u8457\uff0c\u9002\u914d\u5668\u80fd\u589e\u5f3a\u90e8\u5206\u672a\u89c1\u8bed\u8a00\u573a\u666f\u3002\u6807\u6ce8\u6570\u636e\u7f3a\u4e4f\u65f6\uff0c\u9002\u914d\u5668\u4e3a\u63d0\u5347\u4f4e\u8d44\u6e90\u8bed\u8a00NLP\u63d0\u4f9b\u4e86\u6709\u6548\u8def\u5f84\u3002", "motivation": "\u672c\u8bba\u6587\u65e8\u5728\u7814\u7a76\u73b0\u6709\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff08\u5982BERT\uff09\u7684\u8fc1\u79fb\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u5370\u5c3c\u672c\u5730\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e0a\u7684\u60c5\u611f\u5206\u6790\u4efb\u52a1\uff0c\u4ee5\u8bc4\u4f30\u5728\u4e0d\u540c\u7a0b\u5ea6\u9884\u8bad\u7ec3\u76f8\u5173\u6027\u7684\u76ee\u6807\u8bed\u8a00\u4e0a\u6a21\u578b\u8868\u73b0\u7684\u5dee\u5f02\u3002", "method": "\u4f5c\u8005\u9009\u53d6\u4e86\u5341\u79cd\u5370\u5c3c\u672c\u5730\u8bed\u8a00\uff0c\u5206\u522b\u91c7\u7528\u96f6\u6837\u672c\uff08zero-shot\uff09\u548c\u57fa\u4e8eadapter\u7684\u8fc1\u79fb\u65b9\u6cd5\uff0c\u6d4b\u8bd5\u4e86\u5370\u5c3cBERT\u3001\u591a\u8bed\u8a00BERT\uff08mBERT\uff09\u3001XLM-R\u4ee5\u53ca\u57fa\u4e8eadapter\u7684MAD-X\u7b49\u6a21\u578b\u3002\u5e76\u5c06\u76ee\u6807\u8bed\u8a00\u5206\u4e3a\u4e09\u7c7b\uff1a\u9884\u8bad\u7ec3\u4e2d\u51fa\u73b0\u7684\u3001\u4e0e\u51fa\u73b0\u8bed\u8a00\u6709\u8bed\u8a00\u5b66\u76f8\u5173\u6027\u7684\u3001\u4ee5\u53ca\u4e0e\u9884\u8bad\u7ec3\u8bed\u8a00\u65e0\u5173\u7684\u8bed\u8a00\uff0c\u6df1\u5165\u63a2\u7a76\u5404\u7c7b\u60c5\u51b5\u6a21\u578b\u8868\u73b0\u3002", "result": "\u591a\u8bed\u8a00\u6a21\u578b\u5728\u2018\u5df2\u89c1\u2019\u8bed\u8a00\u4e0a\u8868\u73b0\u6700\u597d\uff0c\u5728\u2018\u90e8\u5206\u5df2\u89c1\u2019\u8bed\u8a00\u4e0a\u9002\u4e2d\uff0c\u5728\u2018\u672a\u89c1\u2019\u8bed\u8a00\u4e0a\u8868\u73b0\u8f83\u5dee\u3002MAD-X\u5927\u5e45\u63d0\u5347\u4e86\u5df2\u89c1\u548c\u90e8\u5206\u5df2\u89c1\u8bed\u8a00\u4e0a\u7684\u8fc1\u79fb\u80fd\u529b\uff0c\u4e14\u65e0\u9700\u76ee\u6807\u8bed\u8a00\u6709\u6807\u6ce8\u6570\u636e\u3002\u8fdb\u4e00\u6b65\u5206\u6790\u53d1\u73b0\uff0ctokenization\u7b49\u4f4e\u7ea7\u7279\u6027\u4e0e\u9884\u6d4b\u76f8\u5173\u6027\u6709\u9650\uff0c\u8fc1\u79fb\u80fd\u529b\u7684\u6700\u7a33\u5b9a\u9884\u6d4b\u56e0\u7d20\u662f\u6a21\u578b\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\u662f\u5426\u5bf9\u8be5\u8bed\u8a00\u6216\u5176\u76f8\u5173\u8bed\u8a00\u6709\u8fc7\u63a5\u89e6\u3002", "conclusion": "\u8de8\u5370\u5c3c\u591a\u5730\u4f4e\u8d44\u6e90\u8bed\u8a00\uff0c\u9884\u8bad\u7ec3\u591a\u8bed\u8a00\u6a21\u578b\u5bf9\u2018\u5df2\u89c1\u2019\u8bed\u8a00\u8fc1\u79fb\u6548\u679c\u6700\u597d\u3002\u9002\u914d\u5668(MAD-X)\u80fd\u5728\u65e0\u9700\u989d\u5916\u6807\u6ce8\u60c5\u51b5\u4e0b\u63d0\u5347\u6027\u80fd\u3002\u6a21\u578b\u5bf9\u76ee\u6807\u8bed\u8a00\u6216\u5176\u76f8\u5173\u8bed\u8a00\u7684\u9884\u8bad\u7ec3\u66b4\u9732\uff0c\u662f\u51b3\u5b9a\u8fc1\u79fb\u6210\u8d25\u7684\u6838\u5fc3\u56e0\u7d20\u3002"}}
{"id": "2507.01702", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01702", "abs": "https://arxiv.org/abs/2507.01702", "authors": ["Zixin Chen", "Hongzhan Lin", "Kaixin Li", "Ziyang Luo", "Zhen Ye", "Guang Chen", "Zhiyong Huang", "Jing Ma"], "title": "AdamMeme: Adaptively Probe the Reasoning Capacity of Multimodal Large Language Models on Harmfulness", "comment": "ACL 2025", "summary": "The proliferation of multimodal memes in the social media era demands that\nmultimodal Large Language Models (mLLMs) effectively understand meme\nharmfulness. Existing benchmarks for assessing mLLMs on harmful meme\nunderstanding rely on accuracy-based, model-agnostic evaluations using static\ndatasets. These benchmarks are limited in their ability to provide up-to-date\nand thorough assessments, as online memes evolve dynamically. To address this,\nwe propose AdamMeme, a flexible, agent-based evaluation framework that\nadaptively probes the reasoning capabilities of mLLMs in deciphering meme\nharmfulness. Through multi-agent collaboration, AdamMeme provides comprehensive\nevaluations by iteratively updating the meme data with challenging samples,\nthereby exposing specific limitations in how mLLMs interpret harmfulness.\nExtensive experiments show that our framework systematically reveals the\nvarying performance of different target mLLMs, offering in-depth, fine-grained\nanalyses of model-specific weaknesses. Our code is available at\nhttps://github.com/Lbotirx/AdamMeme.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faAdamMeme\uff0c\u4e00\u79cd\u52a8\u6001\u3001\u591a\u4ee3\u7406\u534f\u4f5c\u7684\u8868\u60c5\u5305\u6709\u5bb3\u6027\u7406\u89e3\u8bc4\u6d4b\u4f53\u7cfb\uff0c\u80fd\u7cbe\u51c6\u63ed\u793a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7406\u89e3\u590d\u6742\u6709\u5bb3\u8868\u60c5\u5305\u65b9\u9762\u7684\u5f31\u70b9\uff0c\u4f18\u4e8e\u4f20\u7edf\u9759\u6001\u6570\u636e\u96c6\u8bc4\u6d4b\u65b9\u6cd5\u3002", "motivation": "\u793e\u4ea4\u5a92\u4f53\u65f6\u4ee3\u591a\u6a21\u6001\u8868\u60c5\u5305\uff08meme\uff09\u7684\u6fc0\u589e\uff0c\u4f7f\u5f97\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08mLLMs\uff09\u9700\u8981\u6709\u6548\u7406\u89e3\u8868\u60c5\u5305\u7684\u6709\u5bb3\u6027\u3002\u73b0\u6709\u8bc4\u6d4b\u65b9\u6cd5\u4e3b\u8981\u57fa\u4e8e\u9759\u6001\u6570\u636e\u96c6\u548c\u6a21\u578b\u65e0\u5173\u7684\u51c6\u786e\u7387\u8bc4\u4f30\uff0c\u96be\u4ee5\u9002\u5e94\u4e0d\u65ad\u53d8\u5316\u548c\u590d\u6742\u7684\u7f51\u7edc\u8868\u60c5\u5305\u751f\u6001\uff0c\u65e0\u6cd5\u5168\u9762\u53cd\u6620\u6a21\u578b\u7684\u771f\u5b9e\u8868\u73b0\u3002", "method": "\u63d0\u51faAdamMeme\uff0c\u4e00\u79cd\u57fa\u4e8e\u4ee3\u7406\uff08agent\uff09\u7684\u7075\u6d3b\u8bc4\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u4ee3\u7406\u534f\u4f5c\uff0c\u4e0d\u65ad\u7528\u5177\u6709\u6311\u6218\u6027\u7684\u6837\u672c\u8fed\u4ee3\u66f4\u65b0\u8868\u60c5\u5305\u6570\u636e\uff0c\u5bf9mLLMs\u7406\u89e3\u6709\u5bb3\u6027\u7684\u63a8\u7406\u80fd\u529b\u8fdb\u884c\u81ea\u9002\u5e94\u63a2\u67e5\u548c\u8bc4\u4f30\u3002\u8be5\u65b9\u6cd5\u80fd\u66b4\u9732\u4e0d\u540c\u6a21\u578b\u5728\u7406\u89e3\u6709\u5bb3\u6027\u65b9\u9762\u7684\u5177\u4f53\u5c40\u9650\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eAdamMeme\u80fd\u591f\u7cfb\u7edf\u6027\u5730\u63ed\u793a\u4e0d\u540cmLLMs\u5728\u8868\u60c5\u5305\u6709\u5bb3\u6027\u7406\u89e3\u4e0a\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u3001\u7279\u5b9a\u6a21\u578b\u7684\u5f31\u70b9\u5206\u6790\u3002", "conclusion": "AdamMeme\u6846\u67b6\u80fd\u52a8\u6001\u3001\u5168\u9762\u8bc4\u4f30mLLMs\u5bf9\u6709\u5bb3\u8868\u60c5\u5305\u7684\u7406\u89e3\u80fd\u529b\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u9759\u6001\u57fa\u51c6\u8bc4\u6d4b\u7684\u4e0d\u8db3\uff0c\u4e3a\u6a21\u578b\u4e0d\u8db3\u7684\u6df1\u5165\u5256\u6790\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002"}}
{"id": "2507.01715", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01715", "abs": "https://arxiv.org/abs/2507.01715", "authors": ["Aditya Tomar", "Rudra Murthy", "Pushpak Bhattacharyya"], "title": "Stereotype Detection as a Catalyst for Enhanced Bias Detection: A Multi-Task Learning Approach", "comment": null, "summary": "Bias and stereotypes in language models can cause harm, especially in\nsensitive areas like content moderation and decision-making. This paper\naddresses bias and stereotype detection by exploring how jointly learning these\ntasks enhances model performance. We introduce StereoBias, a unique dataset\nlabeled for bias and stereotype detection across five categories: religion,\ngender, socio-economic status, race, profession, and others, enabling a deeper\nstudy of their relationship. Our experiments compare encoder-only models and\nfine-tuned decoder-only models using QLoRA. While encoder-only models perform\nwell, decoder-only models also show competitive results. Crucially, joint\ntraining on bias and stereotype detection significantly improves bias detection\ncompared to training them separately. Additional experiments with sentiment\nanalysis confirm that the improvements stem from the connection between bias\nand stereotypes, not multi-task learning alone. These findings highlight the\nvalue of leveraging stereotype information to build fairer and more effective\nAI systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e13\u95e8\u7684StereoBias\u6570\u636e\u96c6\uff0c\u9a8c\u8bc1\u4e86\u504f\u89c1\u4e0e\u523b\u677f\u5370\u8c61\u68c0\u6d4b\u8054\u5408\u8bad\u7ec3\u7684\u6709\u6548\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u504f\u89c1\u68c0\u6d4b\u80fd\u529b\uff0c\u5bf9\u8bad\u7ec3\u66f4\u516c\u5e73\u7684AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u529b\u652f\u6491\u3002", "motivation": "\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u504f\u89c1\u548c\u523b\u677f\u5370\u8c61\u4f1a\u5728\u5185\u5bb9\u5ba1\u6838\u548c\u51b3\u7b56\u7b49\u654f\u611f\u9886\u57df\u9020\u6210\u635f\u5bb3\uff0c\u56e0\u6b64\u68c0\u6d4b\u548c\u7f13\u89e3\u8fd9\u7c7b\u95ee\u9898\u81f3\u5173\u91cd\u8981\u3002\u8be5\u7814\u7a76\u5e0c\u671b\u901a\u8fc7\u8054\u5408\u5b66\u4e60\u504f\u89c1\u548c\u523b\u677f\u5370\u8c61\u68c0\u6d4b\u6765\u63d0\u5347\u6a21\u578b\u8868\u73b0\u3002", "method": "\u63d0\u51fa\u4e86StereoBias\u6570\u636e\u96c6\uff0c\u6807\u6ce8\u4e86\u4e94\u7c7b\uff08\u5b97\u6559\u3001\u6027\u522b\u3001\u793e\u4f1a\u7ecf\u6d4e\u5730\u4f4d\u3001\u79cd\u65cf\u3001\u804c\u4e1a\u53ca\u5176\u4ed6\uff09\u504f\u89c1\u548c\u523b\u677f\u5370\u8c61\u68c0\u6d4b\u6807\u7b7e\u3002\u91c7\u7528encoder-only\u6a21\u578b\u4e0e\u4f7f\u7528QLoRA\u5fae\u8c03\u7684decoder-only\u6a21\u578b\u8fdb\u884c\u5bf9\u6bd4\u5b9e\u9a8c\uff0c\u5e76\u63a2\u7d22\u8054\u5408\u8bad\u7ec3\u548c\u5355\u72ec\u8bad\u7ec3\u7684\u6548\u679c\u3002\u7ed3\u5408\u60c5\u611f\u5206\u6790\u4efb\u52a1\u8fdb\u884c\u8f85\u52a9\u5b9e\u9a8c\u3002", "result": "encoder-only\u6a21\u578b\u8868\u73b0\u826f\u597d\uff0cdecoder-only\u6a21\u578b\u4e5f\u5177\u6709\u7ade\u4e89\u529b\u3002\u504f\u89c1\u548c\u523b\u677f\u5370\u8c61\u68c0\u6d4b\u7684\u8054\u5408\u8bad\u7ec3\u663e\u8457\u63d0\u5347\u4e86\u504f\u89c1\u68c0\u6d4b\u80fd\u529b\u3002\u60c5\u611f\u5206\u6790\u7684\u5b9e\u9a8c\u6392\u9664\u4e86\u5355\u7eaf\u591a\u4efb\u52a1\u5b66\u4e60\u5e26\u6765\u7684\u63d0\u5347\uff0c\u8bc1\u660e\u4e86\u504f\u89c1\u4e0e\u523b\u677f\u5370\u8c61\u68c0\u6d4b\u7684\u5185\u5728\u5173\u8054\u3002", "conclusion": "\u8054\u5408\u5b66\u4e60\u504f\u89c1\u548c\u523b\u677f\u5370\u8c61\u68c0\u6d4b\u80fd\u6709\u6548\u63d0\u5347\u6a21\u578b\u5728\u504f\u89c1\u68c0\u6d4b\u4e0a\u7684\u8868\u73b0\uff0c\u5229\u7528\u523b\u677f\u5370\u8c61\u4fe1\u606f\u5bf9\u4e8e\u6784\u5efa\u66f4\u516c\u5e73\u3001\u66f4\u9ad8\u6548\u7684AI\u7cfb\u7edf\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2507.01785", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01785", "abs": "https://arxiv.org/abs/2507.01785", "authors": ["Zhixun Chen", "Ping Guo", "Wenhan Han", "Yifan Zhang", "Binbin Liu", "Haobin Lin", "Fengze Liu", "Yan Zhao", "Bingni Zhang", "Taifeng Wang", "Yin Zheng", "Meng Fang"], "title": "MuRating: A High Quality Data Selecting Approach to Multilingual Large Language Model Pretraining", "comment": null, "summary": "Data quality is a critical driver of large language model performance, yet\nexisting model-based selection methods focus almost exclusively on English. We\nintroduce MuRating, a scalable framework that transfers high-quality English\ndata-quality signals into a single rater for 17 target languages. MuRating\naggregates multiple English \"raters\" via pairwise comparisons to learn unified\ndocument-quality scores,then projects these judgments through translation to\ntrain a multilingual evaluator on monolingual, cross-lingual, and parallel text\npairs. Applied to web data, MuRating selects balanced subsets of English and\nmultilingual content to pretrain a 1.2 B-parameter LLaMA model. Compared to\nstrong baselines, including QuRater, AskLLM, DCLM and so on, our approach\nboosts average accuracy on both English benchmarks and multilingual\nevaluations, with especially large gains on knowledge-intensive tasks. We\nfurther analyze translation fidelity, selection biases, and underrepresentation\nof narrative material, outlining directions for future work.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faMuRating\u6846\u67b6\uff0c\u5b9e\u73b0\u82f1\u6587\u6570\u636e\u8d28\u91cf\u6807\u51c6\u541117\u79cd\u8bed\u8a00\u8fc1\u79fb\uff0c\u663e\u8457\u63d0\u5347\u591a\u8bed\u8a00\u5927\u6a21\u578b\u7684\u6570\u636e\u9009\u62e9\u4e0e\u8bad\u7ec3\u6548\u679c\uff0c\u5728\u591a\u9879\u4efb\u52a1\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5728\u591a\u8bed\u8a00\u5927\u6a21\u578b\u8bad\u7ec3\u4e2d\uff0c\u6570\u636e\u8d28\u91cf\u5bf9\u6a21\u578b\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u6570\u636e\u8d28\u91cf\u9009\u62e9\u65b9\u6cd5\u51e0\u4e4e\u53ea\u5173\u6ce8\u82f1\u8bed\uff0c\u7f3a\u4e4f\u591a\u8bed\u8a00\u6570\u636e\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u63d0\u51faMuRating\u6846\u67b6\uff0c\u5c06\u9ad8\u8d28\u91cf\u82f1\u6587\u6570\u636e\u7684\u8d28\u91cf\u4fe1\u53f7\u8fc1\u79fb\u523017\u79cd\u76ee\u6807\u8bed\u8a00\uff0c\u901a\u8fc7\u805a\u5408\u82f1\u6587\u6253\u5206\u5668(pairwise\u6bd4\u8f83)\u5b66\u4e60\u6587\u6863\u8d28\u91cf\u5206\u6570\uff0c\u7136\u540e\u5229\u7528\u7ffb\u8bd1\u751f\u6210\u591a\u8bed\u8a00\u8bc4\u4f30\u5668\uff0c\u5bf9\u5355\u8bed\u3001\u8de8\u8bed\u548c\u53cc\u8bed\u6570\u636e\u8fdb\u884c\u8d28\u91cf\u8bc4\u4f30\u548c\u7b5b\u9009\u3002", "result": "MuRating\u5728\u82f1\u6587\u548c\u591a\u8bed\u8a00\u5185\u5bb9\u4e0a\u9009\u53d6\u4e86\u9ad8\u8d28\u91cf\u5b50\u96c6\u9884\u8bad\u7ec3LLaMA(1.2B\u53c2\u6570)\uff0c\u4e0eQuRater\u3001AskLLM\u3001DCLM\u7b49\u5f3a\u57fa\u7ebf\u76f8\u6bd4\uff0c\u5728\u82f1\u6587\u548c\u591a\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u6a21\u578b\u51c6\u786e\u7387\u63d0\u5347\u660e\u663e\uff0c\u7279\u522b\u662f\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\u4e0a\u8fdb\u6b65\u663e\u8457\u3002", "conclusion": "MuRating\u6709\u6548\u5b9e\u73b0\u4e86\u82f1\u6587\u6570\u636e\u8d28\u91cf\u4fe1\u53f7\u7684\u591a\u8bed\u8a00\u8fc1\u79fb\uff0c\u63d0\u5347\u4e86\u591a\u8bed\u8a00\u6a21\u578b\u6570\u636e\u9009\u62e9\u7684\u79d1\u5b66\u6027\u548c\u6a21\u578b\u6027\u80fd\uff0c\u5bf9\u540e\u7eed\u6539\u8fdb\u7ffb\u8bd1\u4fdd\u771f\u5ea6\u3001\u51cf\u5c11\u9009\u62e9\u504f\u5dee\u7b49\u95ee\u9898\u505a\u4e86\u521d\u6b65\u5206\u6790\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u6307\u51fa\u4e86\u65b9\u5411\u3002"}}
{"id": "2507.01734", "categories": ["cs.CL", "68T50", "I.2.7"], "pdf": "https://arxiv.org/pdf/2507.01734", "abs": "https://arxiv.org/abs/2507.01734", "authors": ["Oliver Wardas", "Florian Matthes"], "title": "LLMs for Legal Subsumption in German Employment Contracts", "comment": "PrePrint - ICAIL25, Chicago", "summary": "Legal work, characterized by its text-heavy and resource-intensive nature,\npresents unique challenges and opportunities for NLP research. While\ndata-driven approaches have advanced the field, their lack of interpretability\nand trustworthiness limits their applicability in dynamic legal environments.\nTo address these issues, we collaborated with legal experts to extend an\nexisting dataset and explored the use of Large Language Models (LLMs) and\nin-context learning to evaluate the legality of clauses in German employment\ncontracts. Our work evaluates the ability of different LLMs to classify clauses\nas \"valid,\" \"unfair,\" or \"void\" under three legal context variants: no legal\ncontext, full-text sources of laws and court rulings, and distilled versions of\nthese (referred to as examination guidelines). Results show that full-text\nsources moderately improve performance, while examination guidelines\nsignificantly enhance recall for void clauses and weighted F1-Score, reaching\n80\\%. Despite these advancements, LLMs' performance when using full-text\nsources remains substantially below that of human lawyers. We contribute an\nextended dataset, including examination guidelines, referenced legal sources,\nand corresponding annotations, alongside our code and all log files. Our\nfindings highlight the potential of LLMs to assist lawyers in contract legality\nreview while also underscoring the limitations of the methods presented.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u6269\u5c55\u5fb7\u6587\u5408\u540c\u6761\u6b3e\u6570\u636e\u96c6\u3001\u7ed3\u5408\u6cd5\u5f8b\u4e13\u5bb6\u77e5\u8bc6\u3001\u63a2\u7d22LLMs+\u6cd5\u5f8b\u6307\u5bfc\u6587\u6863\uff0c\u53d1\u73b0\u6d53\u7f29\u6307\u5f15\u53ef\u663e\u8457\u63d0\u5347\u6a21\u578b\u8868\u73b0\uff0c\u4f46\u603b\u4f53\u6548\u679c\u8ddd\u79bb\u4eba\u7c7b\u5f8b\u5e08\u4ecd\u6709\u5dee\u8ddd\uff0c\u76f8\u5173\u6570\u636e\u548c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002", "motivation": "\u6cd5\u5f8b\u5de5\u4f5c\u6587\u672c\u7e41\u91cd\u4e14\u8d44\u6e90\u6d88\u8017\u5927\uff0c\u5bf9NLP\u7814\u7a76\u63d0\u51fa\u7279\u6b8a\u6311\u6218\u3002\u76ee\u524d\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u867d\u6709\u8fdb\u5c55\uff0c\u4f46\u5176\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u4fe1\u5ea6\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u5176\u5728\u52a8\u6001\u6cd5\u5f8b\u73af\u5883\u4e2d\u7684\u5e94\u7528\u3002\u8be5\u7814\u7a76\u65e8\u5728\u63d0\u5347\u6cd5\u5f8b\u5408\u540c\u6761\u6b3e\u5408\u6cd5\u6027\u5224\u522b\u7684\u81ea\u52a8\u5316\u4e0e\u51c6\u786e\u6027\u3002", "method": "\u7814\u7a76\u56e2\u961f\u4e0e\u6cd5\u5f8b\u4e13\u5bb6\u5408\u4f5c\uff0c\u6269\u5c55\u4e86\u4e00\u5957\u5fb7\u6587\u96c7\u4f63\u5408\u540c\u6761\u6b3e\u6570\u636e\u96c6\uff0c\u5e76\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u4e0a\u4e0b\u6587\u5b66\u4e60\uff0c\u8bc4\u4f30\u6a21\u578b\u5728\u4e0d\u540c\u6cd5\u5f8b\u4fe1\u606f\uff08\u65e0\u4fe1\u606f\u3001\u5168\u6587\u6cd5\u6761\u548c\u5224\u4f8b\u3001\u6d53\u7f29\u6307\u5f15\uff09\u4e0b\u5bf9\u5408\u540c\u6761\u6b3e\u7684\u5408\u6cd5\u6027\u5206\u7c7b\u80fd\u529b\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff1a\u4f7f\u7528\u5168\u6587\u6cd5\u6761\u4fe1\u606f\u80fd\u9002\u5ea6\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u91c7\u7528\u7cbe\u70bc\u7684\u6cd5\u5f8b\u8003\u8bd5\u6307\u5f15\u5219\u663e\u8457\u63d0\u5347\u5bf9\u65e0\u6548\u6761\u6b3e\u53ec\u56de\u7387\u53ca\u52a0\u6743F1\u5206\u6570\uff0c\u6700\u9ad8\u8fbe80%\u3002\u4e0d\u8fc7\uff0c\u5373\u4fbf\u5982\u6b64\uff0cLLMs\u5728\u5229\u7528\u5168\u6587\u4fe1\u606f\u65f6\u7684\u8868\u73b0\u4ecd\u660e\u663e\u4f4e\u4e8e\u4eba\u7c7b\u5f8b\u5e08\u3002", "conclusion": "LLMs\u5728\u5408\u540c\u5408\u6cd5\u6027\u5ba1\u67e5\u4e2d\u5177\u6709\u8f85\u52a9\u4ef7\u503c\uff0c\u5c24\u5176\u5728\u52a0\u5165\u7ed3\u6784\u5316\u6cd5\u5f8b\u6307\u5f15\u7684\u60c5\u51b5\u4e0b\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4e0e\u4eba\u7c7b\u5f8b\u5e08\u4e4b\u95f4\u7684\u5dee\u8ddd\u4f9d\u65e7\u660e\u663e\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u6539\u8fdb\u3002"}}
{"id": "2507.01786", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01786", "abs": "https://arxiv.org/abs/2507.01786", "authors": ["Jord Nguyen", "Khiem Hoang", "Carlo Leonardo Attubato", "Felix Hofst\u00e4tter"], "title": "Probing Evaluation Awareness of Language Models", "comment": "Technical AI Governance Workshop, ICML (Poster)", "summary": "Language models can distinguish between testing and deployment phases -- a\ncapability known as evaluation awareness. This has significant safety and\npolicy implications, potentially undermining the reliability of evaluations\nthat are central to AI governance frameworks and voluntary industry\ncommitments. In this paper, we study evaluation awareness in\nLlama-3.3-70B-Instruct. We show that linear probes can separate real-world\nevaluation and deployment prompts, suggesting that current models internally\nrepresent this distinction. We also find that current safety evaluations are\ncorrectly classified by the probes, suggesting that they already appear\nartificial or inauthentic to models. Our findings underscore the importance of\nensuring trustworthy evaluations and understanding deceptive capabilities. More\nbroadly, our work showcases how model internals may be leveraged to support\nblackbox methods in safety audits, especially for future models more competent\nat evaluation awareness and deception.", "AI": {"tldr": "LLM\u5bf9\u6d4b\u8bd5\u4e0e\u90e8\u7f72\u9636\u6bb5\u80fd\u4f5c\u5185\u90e8\u533a\u5206\uff0c\u73b0\u6709\u5b89\u5168\u8bc4\u4f30\u53ef\u80fd\u56e0\u663e\u5f97\u201c\u4eba\u9020\u201d\u800c\u88ab\u6a21\u578b\u8bc6\u522b\u3002\u8fd9\u6311\u6218\u4e86\u8bc4\u4f30\u7684\u771f\u5b9e\u6027\uff0c\u63d0\u793a\u6211\u4eec\u5e94\u8bbe\u8ba1\u66f4\u96be\u8bc6\u522b\u7684\u8bc4\u4ef7\u673a\u5236\uff0c\u5e76\u501f\u52a9\u6a21\u578b\u5185\u90e8\u4fe1\u606f\u63d0\u5347\u5b89\u5168\u5ba1\u6838\u3002", "motivation": "\u8bc4\u6d4b\u610f\u8bc6\uff08evaluation awareness\uff09\u53ef\u80fd\u5f71\u54cdAI\u5b89\u5168\u8bc4\u4f30\u7684\u53ef\u4fe1\u5ea6\uff0c\u6709\u5fc5\u8981\u4e86\u89e3\u5f53\u524d\u5927\u6a21\u578b\u662f\u5426\u80fd\u533a\u5206\u6d4b\u8bd5\u573a\u666f\u548c\u5b9e\u9645\u90e8\u7f72\uff0c\u4ee5\u4fdd\u8bc1AI\u6cbb\u7406\u548c\u653f\u7b56\u7684\u6709\u6548\u6027\u3002", "method": "\u901a\u8fc7\u7ebf\u6027\u63a2\u9488\u5206\u6790Llama-3.3-70B-Instruct\u6a21\u578b\u7684\u5185\u90e8\u8868\u5f81\uff0c\u68c0\u6d4b\u6a21\u578b\u662f\u5426\u80fd\u533a\u5206\u6d4b\u8bd5\u4e0e\u90e8\u7f72\u7684\u63d0\u793a\u3002", "result": "\u7ebf\u6027\u63a2\u9488\u5bf9\u6a21\u578b\u5185\u90e8\u8868\u5f81\u6709\u533a\u5206\u80fd\u529b\uff0c\u80fd\u591f\u51c6\u786e\u5206\u7c7b\u6d4b\u8bd5\u4e0e\u90e8\u7f72\u63d0\u793a\u53ca\u5b89\u5168\u8bc4\u6d4b\u5185\u5bb9\uff0c\u8bf4\u660e\u6a21\u578b\u6709\u533a\u5206\u80fd\u529b\u3002", "conclusion": "\u6a21\u578b\u80fd\u591f\u5185\u90e8\u533a\u5206\u6d4b\u8bd5\u548c\u90e8\u7f72\u9636\u6bb5\uff0c\u8fd9\u5bf9AI\u5b89\u5168\u548c\u6cbb\u7406\u6709\u91cd\u8981\u5f71\u54cd\u3002\u76ee\u524d\u7684\u5b89\u5168\u8bc4\u6d4b\u65b9\u5f0f\u5bf9\u6a21\u578b\u6765\u8bf4\u4f3c\u4e4e\u663e\u5f97\u201c\u5047\u201d\u6216\u201c\u53ef\u8bc6\u522b\u201d\u3002"}}
{"id": "2507.01764", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01764", "abs": "https://arxiv.org/abs/2507.01764", "authors": ["Matteo Di Cristofaro"], "title": "Data interference: emojis, homoglyphs, and issues of data fidelity in corpora and their results", "comment": "Author submitted manuscript", "summary": "Tokenisation - \"the process of splitting text into atomic parts\" (Brezina &\nTimperley, 2017: 1) - is a crucial step for corpus linguistics, as it provides\nthe basis for any applicable quantitative method (e.g. collocations) while\nensuring the reliability of qualitative approaches. This paper examines how\ndiscrepancies in tokenisation affect the representation of language data and\nthe validity of analytical findings: investigating the challenges posed by\nemojis and homoglyphs, the study highlights the necessity of preprocessing\nthese elements to maintain corpus fidelity to the source data. The research\npresents methods for ensuring that digital texts are accurately represented in\ncorpora, thereby supporting reliable linguistic analysis and guaranteeing the\nrepeatability of linguistic interpretations. The findings emphasise the\nnecessity of a detailed understanding of both linguistic and technical aspects\ninvolved in digital textual data to enhance the accuracy of corpus analysis,\nand have significant implications for both quantitative and qualitative\napproaches in corpus-based research.", "AI": {"tldr": "\u672c\u6587\u5f3a\u8c03\u5206\u8bcd\u4e00\u81f4\u6027\u7684\u91cd\u8981\u6027\uff0c\u63d0\u51fa\u9488\u5bf9\u8868\u60c5\u7b26\u53f7\u4e0e\u540c\u5f62\u5f02\u4e49\u5b57\u7b26\u7684\u9884\u5904\u7406\u65b9\u6cd5\uff0c\u6709\u6548\u63d0\u5347\u4e86\u8bed\u6599\u5e93\u7814\u7a76\u7684\u6570\u636e\u51c6\u786e\u6027\u548c\u5206\u6790\u7ed3\u679c\u7684\u53ef\u9760\u6027\uff0c\u5bf9\u5b9a\u91cf\u4e0e\u5b9a\u6027\u5206\u6790\u5747\u5177\u91cd\u8981\u610f\u4e49\u3002", "motivation": "\u5206\u8bcd\u662f\u8bed\u6599\u5e93\u8bed\u8a00\u5b66\u7684\u57fa\u7840\uff0c\u4f46\u7531\u4e8e\u6570\u5b57\u6587\u672c\u4e2d\u8868\u60c5\u7b26\u53f7\u548c\u540c\u5f62\u5f02\u4e49\u5b57\u7b26\u5e26\u6765\u7684\u6311\u6218\uff0c\u5206\u8bcd\u4e0d\u4e00\u81f4\u53ef\u80fd\u7834\u574f\u6570\u636e\u7684\u771f\u5b9e\u8868\u8fbe\uff0c\u4ece\u800c\u5f71\u54cd\u5206\u6790\u7684\u6709\u6548\u6027\u548c\u53ef\u91cd\u590d\u6027\uff0c\u56e0\u6b64\u4e9f\u9700\u89e3\u51b3\u8be5\u95ee\u9898\u3002", "method": "\u5206\u6790\u4e86\u5206\u8bcd\u4e0d\u4e00\u81f4\u5bf9\u8bed\u6599\u5e93\u6570\u636e\u8868\u8fbe\u548c\u5206\u6790\u6709\u6548\u6027\u7684\u5f71\u54cd\uff0c\u5c24\u5176\u662f\u8bc4\u4f30\u4e86\u8868\u60c5\u7b26\u53f7\u548c\u540c\u5f62\u5f02\u4e49\u5b57\u7b26\u7684\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e86\u9884\u5904\u7406\u8fd9\u4e9b\u5143\u7d20\u4ee5\u4fdd\u8bc1\u6570\u636e\u539f\u8c8c\u7684\u65b9\u6cd5\u3002", "result": "\u901a\u8fc7\u5bf9\u6570\u5b57\u6587\u672c\u4e2d\u7279\u6b8a\u5143\u7d20\u7684\u9884\u5904\u7406\uff0c\u80fd\u591f\u66f4\u51c6\u786e\u5730\u518d\u73b0\u8bed\u6599\u5e93\u4e2d\u7684\u539f\u59cb\u6570\u636e\uff0c\u4ece\u800c\u652f\u6301\u53ef\u9760\u7684\u5b9a\u91cf\u4e0e\u5b9a\u6027\u8bed\u8a00\u5b66\u5206\u6790\uff0c\u5e76\u4fdd\u8bc1\u89e3\u91ca\u7684\u53ef\u590d\u73b0\u6027\u3002\u8fd9\u5bf9\u591a\u79cd\u57fa\u4e8e\u8bed\u6599\u5e93\u7684\u65b9\u6cd5\u90fd\u6709\u91cd\u8981\u610f\u4e49\u3002", "conclusion": "\u7cbe\u786e\u7684\u5206\u8bcd\u5904\u7406\uff0c\u7279\u522b\u662f\u5728\u8868\u60c5\u7b26\u53f7\u548c\u540c\u5f62\u5f02\u4e49\u5b57\u7b26\u65b9\u9762\u7684\u9884\u5904\u7406\uff0c\u662f\u786e\u4fdd\u8bed\u6599\u5e93\u6570\u636e\u51c6\u786e\u6027\u548c\u5206 \u6790\u7ed3\u679c\u53ef\u9760\u6027\u7684\u5173\u952e\u3002\u53ea\u6709\u517c\u987e\u8bed\u8a00\u5b66\u4e0e\u6280\u672f\u7ec6\u8282\uff0c\u624d\u80fd\u63d0\u5347\u57fa\u4e8e\u8bed\u6599\u5e93\u7684\u7814\u7a76\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2507.01790", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01790", "abs": "https://arxiv.org/abs/2507.01790", "authors": ["Tianze Hua", "Tian Yun", "Ellie Pavlick"], "title": "How Do Vision-Language Models Process Conflicting Information Across Modalities?", "comment": "All code and resources are available at:\n  https://github.com/ethahtz/vlm_conflicting_info_processing", "summary": "AI models are increasingly required to be multimodal, integrating disparate\ninput streams into a coherent state representation on which subsequent\nbehaviors and actions can be based. This paper seeks to understand how such\nmodels behave when input streams present conflicting information. Focusing\nspecifically on vision-language models, we provide inconsistent inputs (e.g.,\nan image of a dog paired with the caption \"A photo of a cat\") and ask the model\nto report the information present in one of the specific modalities (e.g.,\n\"What does the caption say / What is in the image?\"). We find that models often\nfavor one modality over the other, e.g., reporting the image regardless of what\nthe caption says, but that different models differ in which modality they\nfavor. We find evidence that the behaviorally preferred modality is evident in\nthe internal representational structure of the model, and that specific\nattention heads can restructure the representations to favor one modality over\nthe other. Moreover, we find modality-agnostic \"router heads\" which appear to\npromote answers about the modality requested in the instruction, and which can\nbe manipulated or transferred in order to improve performance across datasets\nand modalities. Together, the work provides essential steps towards identifying\nand controlling if and how models detect and resolve conflicting signals within\ncomplex multimodal environments.", "AI": {"tldr": "\u672c\u6587\u53d1\u73b0\u591a\u6a21\u6001AI\u6a21\u578b\u5728\u8f93\u5165\u4fe1\u606f\u51b2\u7a81\u65f6\u5b58\u5728\u6a21\u6001\u504f\u5411\uff0c\u4e0d\u540c\u6a21\u578b\u504f\u597d\u4e0d\u540c\u6a21\u6001\u3002\u8fd9\u79cd\u504f\u5411\u6e90\u4e8e\u5185\u90e8\u8868\u793a\u7ed3\u6784\uff0c\u7279\u5b9a\u6ce8\u610f\u529b\u5934\uff08router head\uff09\u80fd\u5f71\u54cd\u6a21\u578b\u6839\u636e\u6307\u4ee4\u9009\u62e9\u8f93\u51fa\u6a21\u6001\uff0c\u5e76\u53ef\u8fc1\u79fb\u4ee5\u63d0\u5347\u6a21\u578b\u5e7f\u6cdb\u8868\u73b0\uff0c\u63a8\u52a8\u4e86\u591a\u6a21\u6001\u6a21\u578b\u53ef\u63a7\u6027\u548c\u89e3\u91ca\u6027\u7814\u7a76\u3002", "motivation": "\u5f53\u524d\u7684AI\u6a21\u578b\u8d8a\u6765\u8d8a\u9700\u8981\u5904\u7406\u591a\u6a21\u6001\u6570\u636e\uff08\u5982\u56fe\u50cf\u548c\u6587\u672c\u540c\u65f6\u8f93\u5165\uff09\uff0c\u4f46\u5f53\u4e0d\u540c\u6a21\u6001\u7684\u4fe1\u606f\u51fa\u73b0\u51b2\u7a81\u65f6\uff0c\u8fd9\u4e9b\u6a21\u578b\u7684\u8868\u73b0\u5c1a\u4e0d\u6e05\u695a\u3002\u7406\u89e3\u6a21\u578b\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\u5982\u4f55\u51b3\u7b56\u3001\u504f\u5411\u54ea\u79cd\u6a21\u6001\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u591a\u6a21\u6001AI\u7684\u53ef\u9760\u6027\u4ee5\u53ca\u89e3\u91ca\u6027\u3002", "method": "\u672c\u6587\u805a\u7126\u4e8e\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u7ed9\u6a21\u578b\u63d0\u4f9b\u4e0d\u4e00\u81f4\u7684\u8f93\u5165\uff08\u4f8b\u5982\u56fe\u7247\u5185\u5bb9\u548c\u6587\u672c\u63cf\u8ff0\u77db\u76fe\uff09\uff0c\u5e76\u8981\u6c42\u5176\u6839\u636e\u6307\u4ee4\u56de\u7b54\u67d0\u4e00\u6a21\u6001\u7684\u4fe1\u606f\uff08\u5982\"\u56fe\u7247\u91cc\u662f\u4ec0\u4e48\uff1f\"\u6216\"\u6587\u672c\u8bf4\u4e86\u4ec0\u4e48\uff1f\"\uff09\uff0c\u89c2\u5bdf\u5176\u8f93\u51fa\u503e\u5411\u3002\u540c\u65f6\uff0c\u5206\u6790\u5185\u90e8\u8868\u793a\u4e0e\u6ce8\u610f\u529b\u7ed3\u6784\uff0c\u5b9a\u4f4d\u5f71\u54cd\u51b3\u7b56\u7684\u6ce8\u610f\u529b\u5934\u53ca\u5176\u53ef\u8fc1\u79fb\u6027\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\uff0c\u4e0d\u540c\u6a21\u578b\u5728\u9762\u5bf9\u51b2\u7a81\u4fe1\u606f\u65f6\u4f1a\u504f\u5411\u67d0\u4e00\u6a21\u6001\uff0c\u800c\u4e14\u504f\u597d\u5177\u6709\u6a21\u578b\u7279\u5f02\u6027\uff1b\u8fd9\u79cd\u504f\u597d\u4f53\u73b0\u5728\u6a21\u578b\u7684\u5185\u90e8\u8868\u793a\u4e2d\u3002\u7814\u7a76\u53d1\u73b0\u67d0\u4e9b\u201crouter head\u201d\u6ce8\u610f\u529b\u7ed3\u6784\u5728\u6a21\u578b\u4e2d\u8d77\u5230\u57fa\u4e8e\u6307\u4ee4\u8c03\u6574\u8f93\u51fa\u6a21\u6001\u7684\u4f5c\u7528\uff0c\u5e76\u4e14\u8fd9\u4e9b\u7ed3\u6784\u53ef\u4ee5\u4eba\u4e3a\u64cd\u63a7\u4e0e\u8fc1\u79fb\u6765\u63d0\u5347\u6a21\u578b\u8de8\u6a21\u6001\u548c\u6570\u636e\u96c6\u7684\u8868\u73b0\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u591a\u6a21\u6001\u6a21\u578b\u5728\u5904\u7406\u8f93\u5165\u51b2\u7a81\u4fe1\u606f\u65f6\u7684\u673a\u5236\u4e0e\u5185\u5728\u7ed3\u6784\uff0c\u63d0\u51fa\u4e86\u53ef\u63a7\u548c\u8fc1\u79fb\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4e3a\u591a\u6a21\u6001AI\u5728\u590d\u6742\u73af\u5883\u4e0b\u7684\u53ef\u9760\u6027\u63a7\u5236\u4e0e\u89e3\u91ca\u6027\u6539\u5584\u63d0\u4f9b\u4e86\u65b0\u7684\u5207\u5165\u70b9\u3002"}}
{"id": "2507.01903", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01903", "abs": "https://arxiv.org/abs/2507.01903", "authors": ["Qiguang Chen", "Mingda Yang", "Libo Qin", "Jinhao Liu", "Zheng Yan", "Jiannan Guan", "Dengyun Peng", "Yiyan Ji", "Hanjing Li", "Mengkang Hu", "Yimeng Zhang", "Yihao Liang", "Yuhang Zhou", "Jiaqi Wang", "Zhi Chen", "Wanxiang Che"], "title": "AI4Research: A Survey of Artificial Intelligence for Scientific Research", "comment": "Preprint", "summary": "Recent advancements in artificial intelligence (AI), particularly in large\nlanguage models (LLMs) such as OpenAI-o1 and DeepSeek-R1, have demonstrated\nremarkable capabilities in complex domains such as logical reasoning and\nexperimental coding. Motivated by these advancements, numerous studies have\nexplored the application of AI in the innovation process, particularly in the\ncontext of scientific research. These AI technologies primarily aim to develop\nsystems that can autonomously conduct research processes across a wide range of\nscientific disciplines. Despite these significant strides, a comprehensive\nsurvey on AI for Research (AI4Research) remains absent, which hampers our\nunderstanding and impedes further development in this field. To address this\ngap, we present a comprehensive survey and offer a unified perspective on\nAI4Research. Specifically, the main contributions of our work are as follows:\n(1) Systematic taxonomy: We first introduce a systematic taxonomy to classify\nfive mainstream tasks in AI4Research. (2) New frontiers: Then, we identify key\nresearch gaps and highlight promising future directions, focusing on the rigor\nand scalability of automated experiments, as well as the societal impact. (3)\nAbundant applications and resources: Finally, we compile a wealth of resources,\nincluding relevant multidisciplinary applications, data corpora, and tools. We\nhope our work will provide the research community with quick access to these\nresources and stimulate innovative breakthroughs in AI4Research.", "AI": {"tldr": "\u672c\u7efc\u8ff0\u7cfb\u7edf\u68b3\u7406\u4e86AI\u8d4b\u80fd\u79d1\u5b66\u7814\u7a76\uff08AI4Research\uff09\u9886\u57df\u7684\u4efb\u52a1\u5206\u7c7b\u3001\u53d1\u5c55\u6311\u6218\u548c\u8d44\u6e90\uff0c\u4e3a\u7814\u7a76\u8005\u63d0\u4f9b\u53c2\u8003\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u9886\u57df\u521b\u65b0\u3002", "motivation": "AI\u5728\u903b\u8f91\u63a8\u7406\u548c\u5b9e\u9a8c\u7f16\u7a0b\u7b49\u590d\u6742\u9886\u57df\u53d6\u5f97\u7a81\u7834\u540e\uff0c\u8d8a\u6765\u8d8a\u591a\u7684\u7814\u7a76\u5173\u6ce8AI\u5728\u79d1\u5b66\u521b\u65b0\u8fc7\u7a0b\u4e2d\u7684\u5e94\u7528\uff0c\u5c24\u5176\u662f\u81ea\u4e3b\u5f00\u5c55\u8de8\u5b66\u79d1\u7814\u7a76\u7684\u7cfb\u7edf\u3002\u5c3d\u7ba1\u8fdb\u5c55\u663e\u8457\uff0c\u76ee\u524d\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7efc\u8ff0\uff0c\u963b\u788d\u4e86\u9886\u57df\u53d1\u5c55\u3002", "method": "\u672c\u6587\u5f00\u5c55\u5168\u9762\u7efc\u8ff0\uff0c\u63d0\u51fa\u7edf\u4e00\u7684AI4Research\u89c6\u89d2\uff0c\u5305\u62ec\u5236\u5b9a\u7cfb\u7edf\u6027\u5206\u7c7b\u6cd5\uff0c\u5bf9\u4e3b\u6d41\u4efb\u52a1\u8fdb\u884c\u5f52\u7eb3\uff0c\u5206\u6790\u672a\u6765\u7684\u53d1\u5c55\u65b9\u5411\uff0c\u5e76\u6c47\u603b\u5e94\u7528\u3001\u6570\u636e\u8d44\u6e90\u4e0e\u5de5\u5177\u3002", "result": "1) \u5efa\u7acb\u4e86AI4Research\u4e3b\u6d41\u4efb\u52a1\u7684\u7cfb\u7edf\u5206\u7c7b\u4f53\u7cfb\uff1b2) \u6307\u51fa\u5b9e\u9a8c\u81ea\u52a8\u5316\u7684\u4e25\u8c28\u6027\u3001\u53ef\u6269\u5c55\u6027\u53ca\u793e\u4f1a\u5f71\u54cd\u7b49\u6838\u5fc3\u672a\u6765\u6311\u6218\u4e0e\u673a\u4f1a\uff1b3) \u6574\u7406\u4e86\u4e30\u5bcc\u7684\u8de8\u5b66\u79d1\u5e94\u7528\u3001\u6570\u636e\u548c\u5de5\u5177\u8d44\u6e90\u3002", "conclusion": "\u672c\u7efc\u8ff0\u4e3aAI4Research\u9886\u57df\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u5f52\u7eb3\u3001\u5173\u952e\u95ee\u9898\u5206\u6790\u53ca\u8d44\u6e90\u6c47\u805a\uff0c\u6709\u671b\u52a0\u901f\u9886\u57df\u53d1\u5c55\u548c\u521b\u65b0\u3002"}}
{"id": "2507.01915", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01915", "abs": "https://arxiv.org/abs/2507.01915", "authors": ["Chengao Li", "Hanyu Zhang", "Yunkun Xu", "Hongyan Xue", "Xiang Ao", "Qing He"], "title": "Gradient-Adaptive Policy Optimization: Towards Multi-Objective Alignment of Large Language Models", "comment": "19 pages, 3 figures. Accepted by ACL 2025 (main)", "summary": "Reinforcement Learning from Human Feedback (RLHF) has emerged as a powerful\ntechnique for aligning large language models (LLMs) with human preferences.\nHowever, effectively aligning LLMs with diverse human preferences remains a\nsignificant challenge, particularly when they are conflict. To address this\nissue, we frame human value alignment as a multi-objective optimization\nproblem, aiming to maximize a set of potentially conflicting objectives. We\nintroduce Gradient-Adaptive Policy Optimization (GAPO), a novel fine-tuning\nparadigm that employs multiple-gradient descent to align LLMs with diverse\npreference distributions. GAPO adaptively rescales the gradients for each\nobjective to determine an update direction that optimally balances the\ntrade-offs between objectives. Additionally, we introduce P-GAPO, which\nincorporates user preferences across different objectives and achieves Pareto\nsolutions that better align with the user's specific needs. Our theoretical\nanalysis demonstrates that GAPO converges towards a Pareto optimal solution for\nmultiple objectives. Empirical results on Mistral-7B show that GAPO outperforms\ncurrent state-of-the-art methods, achieving superior performance in both\nhelpfulness and harmlessness.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa GAPO \u53ca\u5176\u6539\u8fdb\u7248 P-GAPO\uff0c\u901a\u8fc7\u591a\u68af\u5ea6\u81ea\u9002\u5e94\u4f18\u5316\u7b56\u7565\uff0c\u5b9e\u73b0 LLM \u5728\u591a\u6837\u751a\u81f3\u51b2\u7a81\u7684\u4eba\u7c7b\u504f\u597d\u4e0b\u7684\u66f4\u4f18\u5bf9\u9f50\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u7684\u6709\u7528\u6027\u548c\u65e0\u5bb3\u6027\u3002", "motivation": "\u73b0\u6709 RLHF \u6280\u672f\u5728\u5bf9\u9f50 LLM \u4e0e\u4eba\u7c7b\u591a\u6837\u751a\u81f3\u51b2\u7a81\u504f\u597d\u65b9\u9762\u4ecd\u5b58\u5728\u91cd\u5927\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u591a\u76ee\u6807\u5b58\u5728\u4e0d\u53ef\u8c03\u548c\u51b2\u7a81\u7684\u60c5\u51b5\u4e0b\uff0c\u96be\u4ee5\u8fbe\u5230\u6700\u4f18\u7684\u504f\u597d\u5bf9\u9f50\u3002", "method": "\u5c06\u4eba\u7c7b\u4ef7\u503c\u5bf9\u9f50\u95ee\u9898\u5efa\u6a21\u4e3a\u591a\u76ee\u6807\u4f18\u5316\uff0c\u5f15\u5165\u4e86\u57fa\u4e8e\u591a\u68af\u5ea6\u4e0b\u964d\u7684 Gradient-Adaptive Policy Optimization (GAPO)\uff0c\u5b9e\u73b0 LLM \u5bf9\u591a\u6837\u504f\u597d\u7684\u9002\u5e94\u6027\u5fae\u8c03\u3002\u540c\u65f6\uff0c\u63d0\u51fa\u4e86 P-GAPO\uff0c\u901a\u8fc7\u5f15\u5165\u7528\u6237\u5728\u4e0d\u540c\u76ee\u6807\u4e0a\u7684\u504f\u597d\u4fe1\u606f\uff0c\u83b7\u5f97\u66f4\u7b26\u5408\u7528\u6237\u9700\u6c42\u7684\u5e15\u7d2f\u6258\u89e3\u3002", "result": "\u7406\u8bba\u5206\u6790\u8bc1\u660e GAPO \u80fd\u6536\u655b\u5230\u591a\u76ee\u6807\u7684\u5e15\u7d2f\u6258\u6700\u4f18\u3002\u5b9e\u9a8c\u8bc1\u660e GAPO \u5728\u516c\u5f00\u5927\u6a21\u578b Mistral-7B \u4e0a\u63d0\u5347\u4e86 helpfulness \u548c harmlessness \u4e24\u9879\u6307\u6807\uff0c\u8d85\u8d8a\u4e86\u5f53\u524d\u4e3b\u6d41\u65b9\u6cd5\u3002", "conclusion": "GAPO \u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5730\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e0e\u591a\u6837\u3001\u751a\u81f3\u51b2\u7a81\u7684\u4eba\u7c7b\u504f\u597d\u8fdb\u884c\u5bf9\u9f50\uff0c\u5e76\u4e14\u5728\u7406\u8bba\u4e0a\u80fd\u591f\u6536\u655b\u5230\u591a\u76ee\u6807\u4e0b\u7684\u5e15\u7d2f\u6258\u6700\u4f18\u89e3\u3002\u5b9e\u8bc1\u7ed3\u679c\u663e\u793a\u5176\u5728\u5e2e\u5fd9\u6027\u548c\u65e0\u5bb3\u6027\u65b9\u9762\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u65b9\u6cd5\u3002"}}
{"id": "2507.01931", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.01931", "abs": "https://arxiv.org/abs/2507.01931", "authors": ["Md Sazzadul Islam Ridoy", "Sumi Akter", "Md. Aminur Rahman"], "title": "Adaptability of ASR Models on Low-Resource Language: A Comparative Study of Whisper and Wav2Vec-BERT on Bangla", "comment": null, "summary": "In recent years, neural models trained on large multilingual text and speech\ndatasets have shown great potential for supporting low-resource languages. This\nstudy investigates the performances of two state-of-the-art Automatic Speech\nRecognition (ASR) models, OpenAI's Whisper (Small & Large-V2) and Facebook's\nWav2Vec-BERT on Bangla, a low-resource language. We have conducted experiments\nusing two publicly available datasets: Mozilla Common Voice-17 and OpenSLR to\nevaluate model performances. Through systematic fine-tuning and hyperparameter\noptimization, including learning rate, epochs, and model checkpoint selection,\nwe have compared the models based on Word Error Rate (WER), Character Error\nRate (CER), Training Time, and Computational Efficiency. The Wav2Vec-BERT model\noutperformed Whisper across all key evaluation metrics, demonstrated superior\nperformance while requiring fewer computational resources, and offered valuable\ninsights to develop robust speech recognition systems in low-resource\nlinguistic settings.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86\u4e24\u5927\u81ea\u76d1\u7763ASR\u6a21\u578b\u5728\u5b5f\u52a0\u62c9\u8bed\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\uff0c\u7ed3\u679c\u663e\u793aWav2Vec-BERT\u4f18\u4e8eWhisper\uff0c\u4e14\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\uff0c\u80fd\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00ASR\u7cfb\u7edf\u5f00\u53d1\u63d0\u4f9b\u6709\u76ca\u501f\u9274\u3002", "motivation": "\u8fd1\u5e74\u6765\uff0c\u57fa\u4e8e\u5927\u89c4\u6a21\u591a\u8bed\u79cd\u6587\u672c\u548c\u8bed\u97f3\u6570\u636e\u8bad\u7ec3\u7684\u795e\u7ecf\u6a21\u578b\uff0c\u5728\u652f\u6301\u4f4e\u8d44\u6e90\u8bed\u8a00\u65b9\u9762\u5c55\u73b0\u4e86\u5de8\u5927\u6f5c\u529b\u3002\u672c\u6587\u65e8\u5728\u63a2\u7a76\u4e3b\u6d41\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u6a21\u578b\u5728\u5b5f\u52a0\u62c9\u8bed\uff08\u4f4e\u8d44\u6e90\u8bed\u8a00\uff09\u4e0a\u7684\u8868\u73b0\u3002", "method": "\u5bf9\u4e24\u79cdSOTA ASR\u6a21\u578b\uff08Whisper\uff08Small & Large-V2\uff09\u4e0eWav2Vec-BERT\uff09\u5728\u5b5f\u52a0\u62c9\u8bed\u6570\u636e\u96c6\uff08Mozilla Common Voice-17\u548cOpenSLR\uff09\u4e0a\u8fdb\u884c\u7cfb\u7edf\u5b9e\u9a8c\u3002\u901a\u8fc7\u7cbe\u7ec6\u8c03\u4f18\u53ca\u8d85\u53c2\u6570\u4f18\u5316\uff08\u5b66\u4e60\u7387\u3001\u8f6e\u6570\u3001\u6a21\u578b\u68c0\u67e5\u70b9\uff09\uff0c\u5e76\u4ee5\u5b57\u8bcd\u9519\u8bef\u7387\uff08WER\uff09\u3001\u5b57\u7b26\u9519\u8bef\u7387\uff08CER\uff09\u3001\u8bad\u7ec3\u65f6\u95f4\u53ca\u8ba1\u7b97\u6548\u7387\u4e3a\u6307\u6807\uff0c\u6bd4\u5bf9\u6a21\u578b\u6027\u80fd\u3002", "result": "Wav2Vec-BERT\u5728\u6240\u6709\u5173\u952e\u8bc4\u4ef7\u6307\u6807\u4e0a\u5747\u4f18\u4e8eWhisper\uff0c\u5e76\u4e14\u5176\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u66f4\u4f4e\u3002", "conclusion": "Wav2Vec-BERT\u66f4\u9002\u7528\u4e8e\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u8bed\u97f3\u8bc6\u522b\u5f00\u53d1\uff0c\u5c24\u5176\u9002\u5408\u5982\u5b5f\u52a0\u62c9\u8bed\u8fd9\u7c7b\u8d44\u6e90\u532e\u4e4f\u7684\u8bed\u8a00\u60c5\u5883\u3002"}}
{"id": "2507.01802", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01802", "abs": "https://arxiv.org/abs/2507.01802", "authors": ["Katharina Beckh", "Elisa Studeny", "Sujan Sai Gannamaneni", "Dario Antweiler", "Stefan R\u00fcping"], "title": "The Anatomy of Evidence: An Investigation Into Explainable ICD Coding", "comment": "Accepted to ACL 2025 Findings", "summary": "Automatic medical coding has the potential to ease documentation and billing\nprocesses. For this task, transparency plays an important role for medical\ncoders and regulatory bodies, which can be achieved using explainability\nmethods. However, the evaluation of these approaches has been mostly limited to\nshort text and binary settings due to a scarcity of annotated data. Recent\nefforts by Cheng et al. (2023) have introduced the MDACE dataset, which\nprovides a valuable resource containing code evidence in clinical records. In\nthis work, we conduct an in-depth analysis of the MDACE dataset and perform\nplausibility evaluation of current explainable medical coding systems from an\napplied perspective. With this, we contribute to a deeper understanding of\nautomatic medical coding and evidence extraction. Our findings reveal that\nground truth evidence aligns with code descriptions to a certain degree. An\ninvestigation into state-of-the-art approaches shows a high overlap with ground\ntruth evidence. We propose match measures and highlight success and failure\ncases. Based on our findings, we provide recommendations for developing and\nevaluating explainable medical coding systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u5206\u6790\u4e86\u533b\u5b66\u7f16\u7801\u8bc1\u636e\u6570\u636e\u96c6MDACE\uff0c\u5e76\u8bc4\u4f30\u4e86\u73b0\u6709\u53ef\u89e3\u91ca\u533b\u7597\u7f16\u7801\u7cfb\u7edf\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u65b9\u6cd5\u4e0e\u771f\u5b9e\u8bc1\u636e\u6709\u8f83\u9ad8\u91cd\u53e0\uff0c\u5e76\u4e3a\u7cfb\u7edf\u5f00\u53d1\u548c\u8bc4\u6d4b\u63d0\u51fa\u4e86\u5efa\u8bae\u3002", "motivation": "\u81ea\u52a8\u5316\u533b\u7597\u7f16\u7801\u53ef\u4ee5\u51cf\u8f7b\u6587\u6863\u548c\u7ed3\u7b97\u6d41\u7a0b\u8d1f\u62c5\uff0c\u4f46\u9700\u8981\u89e3\u91ca\u6027\u4ee5\u6ee1\u8db3\u533b\u7597\u7f16\u7801\u5458\u548c\u76d1\u7ba1\u673a\u6784\u7684\u900f\u660e\u6027\u8981\u6c42\u3002\u53d7\u9650\u4e8e\u5e26\u6ce8\u91ca\u6570\u636e\u7a00\u7f3a\uff0c\u76ee\u524d\u9886\u57df\u5bf9\u89e3\u91ca\u6027\u65b9\u6cd5\u7684\u8bc4\u4f30\u5927\u90fd\u5c40\u9650\u4e8e\u77ed\u6587\u672c\u4e0e\u4e8c\u5206\u7c7b\u95ee\u9898\u3002\u65b0\u53d1\u5e03\u7684MDACE\u6570\u636e\u96c6\u4e3a\u533b\u5b66\u7f16\u7801\u8bc1\u636e\u6807\u6ce8\u63d0\u4f9b\u4e86\u65b0\u8d44\u6e90\u3002", "method": "\u6df1\u5165\u5206\u6790MDACE\u6570\u636e\u96c6\uff1b\u4ece\u5b9e\u9645\u5e94\u7528\u89d2\u5ea6\u5bf9\u5f53\u524d\u53ef\u89e3\u91ca\u533b\u7597\u7f16\u7801\u7cfb\u7edf\u8fdb\u884c\u5408\u7406\u6027\u8bc4\u4f30\u3002\u63d0\u51fa\u65b0\u7684\u8bc4\u6d4b\u6307\u6807\uff0c\u5206\u6790\u9886\u5148\u65b9\u6cd5\u7684\u6210\u529f\u4e0e\u5931\u8d25\u6848\u4f8b\uff0c\u6700\u7ec8\u7ed9\u51fa\u53ef\u89e3\u91ca\u7cfb\u7edf\u5f00\u53d1\u548c\u8bc4\u6d4b\u5efa\u8bae\u3002", "result": "\u53d1\u73b0\uff1a\u771f\u5b9e\u8bc1\u636e\u4e0e\u4ee3\u7801\u63cf\u8ff0\u6709\u4e00\u5b9a\u4e00\u81f4\u6027\uff1b\u5148\u8fdb\u65b9\u6cd5\u4e0e\u771f\u5b9e\u8bc1\u636e\u6709\u8f83\u9ad8\u91cd\u53e0\u3002", "conclusion": "\u5bf9MDACE\u6570\u636e\u96c6\u548c\u53ef\u89e3\u91ca\u533b\u7597\u7f16\u7801\u7cfb\u7edf\u8fdb\u884c\u5206\u6790\uff0c\u63a8\u52a8\u4e86\u9886\u57df\u5bf9\u81ea\u52a8\u533b\u7597\u7f16\u7801\u53ca\u8bc1\u636e\u62bd\u53d6\u7684\u7406\u89e3\u3002\u63d0\u51fa\u4e86\u5339\u914d\u6307\u6807\uff0c\u63ed\u793a\u4e86\u8fd9\u4e9b\u65b9\u6cd5\u5728\u8bc1\u636e\u63d0\u53d6\u4e2d\u7684\u8868\u73b0\uff0c\u8fdb\u4e00\u6b65\u4e3a\u672a\u6765\u53ef\u89e3\u91ca\u533b\u7597\u7f16\u7801\u7cfb\u7edf\u7684\u7814\u7a76\u63d0\u4f9b\u5efa\u8bae\u3002"}}
{"id": "2507.01810", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.01810", "abs": "https://arxiv.org/abs/2507.01810", "authors": ["Nikita Neveditsin", "Pawan Lingras", "Vijay Mago"], "title": "Evaluating Structured Output Robustness of Small Language Models for Open Attribute-Value Extraction from Clinical Notes", "comment": "To appear in the ACL Anthology", "summary": "We present a comparative analysis of the parseability of structured outputs\ngenerated by small language models for open attribute-value extraction from\nclinical notes. We evaluate three widely used serialization formats: JSON,\nYAML, and XML, and find that JSON consistently yields the highest parseability.\nStructural robustness improves with targeted prompting and larger models, but\ndeclines for longer documents and certain note types. Our error analysis\nidentifies recurring format-specific failure patterns. These findings offer\npractical guidance for selecting serialization formats and designing prompts\nwhen deploying language models in privacy-sensitive clinical settings.", "AI": {"tldr": "\u4e09\u79cd\u5e8f\u5217\u5316\u683c\u5f0f\u5bf9\u5c0f\u6a21\u578b\u8f93\u51fa\u7ed3\u6784\u7684\u53ef\u89e3\u6790\u6027\u5f71\u54cd\u4e0d\u540c\uff0cJSON\u6700\u4f18\u3002\u63d0\u793a\u8bcd\u4f18\u5316\u548c\u5927\u578b\u6a21\u578b\u7f13\u89e3\u90e8\u5206\u95ee\u9898\uff0c\u5b9e\u6218\u4e2d\u5e94\u9996\u9009JSON\u5e76\u4f18\u5316\u63d0\u793a\u3002", "motivation": "\u5728\u4e34\u5e8a\u6587\u672c\u4e2d\u63d0\u53d6\u5c5e\u6027-\u503c\u5bf9\u662f\u4e00\u9879\u91cd\u8981\u4efb\u52a1\uff0c\u4f46\u7528\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u7ed3\u6784\u5316\u8f93\u51fa\u5728\u53ef\u89e3\u6790\u6027\u4e0a\u8868\u73b0\u4e0d\u540c\u3002\u9009\u62e9\u9002\u5408\u7684\u5e8f\u5217\u5316\u683c\u5f0f\u5bf9\u786e\u4fdd\u6570\u636e\u51c6\u786e\u548c\u53ef\u89e3\u6790\u5c24\u4e3a\u5173\u952e\uff0c\u5c24\u5176\u5728\u9690\u79c1\u654f\u611f\u573a\u666f\u5982\u533b\u7597\u9886\u57df\u3002\u672c\u6587\u65e8\u5728\u7cfb\u7edf\u6bd4\u8f83\u4e3b\u6d41\u683c\u5f0f\u7684\u53ef\u89e3\u6790\u6027\uff0c\u5e76\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4f9d\u636e\u3002", "method": "\u672c\u6587\u5bf9\u4e09\u79cd\u5e38\u7528\u5e8f\u5217\u5316\u683c\u5f0f\uff08JSON\u3001YAML\u3001XML\uff09\u8fdb\u884c\u6bd4\u8f83\uff0c\u5229\u7528\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u9488\u5bf9\u4e34\u5e8a\u7b14\u8bb0\u8fdb\u884c\u5f00\u653e\u5c5e\u6027\u503c\u62bd\u53d6\u3002\u901a\u8fc7\u5b9e\u9a8c\u8bc4\u4f30\u8fd9\u4e09\u79cd\u683c\u5f0f\u5728\u683c\u5f0f\u89e3\u6790\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u7814\u7a76\u4e0d\u540c\u63d0\u793a\u65b9\u6cd5\u3001\u6a21\u578b\u89c4\u6a21\u3001\u6587\u6863\u957f\u5ea6\u53ca\u4e34\u5e8a\u7b14\u8bb0\u7c7b\u578b\u7b49\u56e0\u7d20\u7684\u5f71\u54cd\u3002\u540c\u65f6\u5bf9\u5e38\u89c1\u7684\u9519\u8bef\u6a21\u5f0f\u8fdb\u884c\u4e86\u5206\u6790\u3002", "result": "JSON\u683c\u5f0f\u5728\u53ef\u89e3\u6790\u6027\u65b9\u9762\u59cb\u7ec8\u8868\u73b0\u6700\u4f73\u3002\u901a\u8fc7\u91c7\u7528\u6709\u9488\u5bf9\u6027\u7684\u63d0\u793a\u8bcd\u548c\u66f4\u5927\u7684\u6a21\u578b\uff0c\u7ed3\u6784\u5065\u58ee\u6027\u5f97\u5230\u63d0\u5347\uff0c\u4f46\u5728\u5904\u7406\u957f\u6587\u6863\u548c\u7279\u5b9a\u7c7b\u578b\u7684\u4e34\u5e8a\u7b14\u8bb0\u65f6\uff0c\u53ef\u89e3\u6790\u6027\u6709\u6240\u4e0b\u964d\u3002\u9519\u8bef\u5206\u6790\u63ed\u793a\u4e86\u5404\u79cd\u683c\u5f0f\u7279\u6709\u7684\u5931\u8d25\u6a21\u5f0f\u3002", "conclusion": "JSON\u662f\u90e8\u7f72\u4e8e\u4e34\u5e8a\u9690\u79c1\u654f\u611f\u573a\u666f\u7684\u9996\u9009\u5e8f\u5217\u5316\u683c\u5f0f\uff0c\u901a\u8fc7\u5408\u7406\u8bbe\u8ba1\u63d0\u793a\u8bcd\u548c\u9009\u62e9\u6a21\u578b\u89c4\u6a21\u80fd\u591f\u63d0\u5347\u7ed3\u6784\u8f93\u51fa\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2507.01844", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01844", "abs": "https://arxiv.org/abs/2507.01844", "authors": ["Arthur Wuhrmann", "Anastasiia Kucherenko", "Andrei Kucharavy"], "title": "Low-Perplexity LLM-Generated Sequences and Where To Find Them", "comment": "Camera-ready version. Accepted to ACL 2025. 10 pages, 4 figures, 6\n  tables", "summary": "As Large Language Models (LLMs) become increasingly widespread, understanding\nhow specific training data shapes their outputs is crucial for transparency,\naccountability, privacy, and fairness. To explore how LLMs leverage and\nreplicate their training data, we introduce a systematic approach centered on\nanalyzing low-perplexity sequences - high-probability text spans generated by\nthe model. Our pipeline reliably extracts such long sequences across diverse\ntopics while avoiding degeneration, then traces them back to their sources in\nthe training data. Surprisingly, we find that a substantial portion of these\nlow-perplexity spans cannot be mapped to the corpus. For those that do match,\nwe quantify the distribution of occurrences across source documents,\nhighlighting the scope and nature of verbatim recall and paving a way toward\nbetter understanding of how LLMs training data impacts their behavior.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u6790\u5927\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u590d\u73b0\u8bad\u7ec3\u6570\u636e\u7684\u65b9\u6cd5\uff0c\u53d1\u73b0\u5e76\u975e\u6240\u6709\u9ad8\u6982\u7387\u8f93\u51fa\u90fd\u53ef\u4ee5\u5728\u8bad\u7ec3\u6570\u636e\u4e2d\u76f4\u63a5\u627e\u5230\u6e90\u5934\uff0c\u4e3a\u7406\u89e3\u6a21\u578b\u884c\u4e3a\u4e0e\u6570\u636e\u5173\u7cfb\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u7406\u89e3\u5177\u4f53\u8bad\u7ec3\u6570\u636e\u5982\u4f55\u5f71\u54cd\u6a21\u578b\u8f93\u51fa\u5bf9\u4e8e\u63d0\u5347\u900f\u660e\u5ea6\u3001\u95ee\u8d23\u5236\u3001\u9690\u79c1\u548c\u516c\u5e73\u6027\u5c24\u4e3a\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7cfb\u7edf\u6027\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u4f4e\u56f0\u60d1\u5ea6\u5e8f\u5217\uff08\u5373\u6a21\u578b\u751f\u6210\u7684\u9ad8\u6982\u7387\u6587\u672c\u7247\u6bb5\uff09\uff0c\u53ef\u9760\u5730\u63d0\u53d6\u5e76\u56de\u6eaf\u8fd9\u4e9b\u5e8f\u5217\u5230\u8bad\u7ec3\u6570\u636e\u6765\u6e90\uff0c\u4ece\u800c\u7814\u7a76LLM\u5bf9\u8bad\u7ec3\u6570\u636e\u7684\u5229\u7528\u4e0e\u590d\u73b0\u3002", "result": "\u53d1\u73b0\u76f8\u5f53\u4e00\u90e8\u5206\u4f4e\u56f0\u60d1\u5ea6\u5e8f\u5217\u65e0\u6cd5\u5728\u8bed\u6599\u4e2d\u660e\u786e\u5bf9\u5e94\u6e90\u5934\uff1b\u5bf9\u4e8e\u53ef\u4ee5\u5339\u914d\u7684\u90e8\u5206\uff0c\u8fdb\u4e00\u6b65\u91cf\u5316\u4e86\u5176\u5728\u6e90\u6587\u6863\u4e2d\u7684\u5206\u5e03\u60c5\u51b5\uff0c\u63ed\u793a\u4e86\u9010\u5b57\u590d\u73b0\u7684\u8303\u56f4\u548c\u6027\u8d28\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u52a9\u4e8e\u63ed\u793a\u8bad\u7ec3\u6570\u636e\u5982\u4f55\u5f71\u54cdLLM\u884c\u4e3a\uff0c\u4e3a\u63d0\u5347\u6a21\u578b\u900f\u660e\u5ea6\u548c\u6df1\u5165\u7406\u89e3\u590d\u73b0\u673a\u5236\u63d0\u4f9b\u4e86\u65b0\u8def\u5f84\u3002"}}
{"id": "2507.01853", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01853", "abs": "https://arxiv.org/abs/2507.01853", "authors": ["Samridhi Raj Sinha", "Rajvee Sheth", "Abhishek Upperwal", "Mayank Singh"], "title": "Eka-Eval : A Comprehensive Evaluation Framework for Large Language Models in Indian Languages", "comment": null, "summary": "The rapid advancement of Large Language Models (LLMs) has intensified the\nneed for evaluation frameworks that go beyond English centric benchmarks and\naddress the requirements of linguistically diverse regions such as India. We\npresent EKA-EVAL, a unified and production-ready evaluation framework that\nintegrates over 35 benchmarks, including 10 Indic-specific datasets, spanning\ncategories like reasoning, mathematics, tool use, long-context understanding,\nand reading comprehension. Compared to existing Indian language evaluation\ntools, EKA-EVAL offers broader benchmark coverage, with built-in support for\ndistributed inference, quantization, and multi-GPU usage. Our systematic\ncomparison positions EKA-EVAL as the first end-to-end, extensible evaluation\nsuite tailored for both global and Indic LLMs, significantly lowering the\nbarrier to multilingual benchmarking. The framework is open-source and publicly\navailable at https://github.com/lingo-iitgn/ eka-eval and a part of ongoing EKA\ninitiative (https://eka.soket.ai), which aims to scale up to over 100\nbenchmarks and establish a robust, multilingual evaluation ecosystem for LLMs.", "AI": {"tldr": "\u63d0\u51faEKA-EVAL\u8bc4\u6d4b\u6846\u67b6\uff0c\u89e3\u51b3\u73b0\u6709\u591a\u8bed\u8a00\uff08\u7279\u522b\u662f\u5370\u5ea6\u672c\u5730\u8bed\uff09\u5927\u8bed\u8a00\u6a21\u578b\u8bc4\u6d4b\u8986\u76d6\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u6574\u5408\u591a\u79cd\u57fa\u51c6\u5e76\u5177\u5907\u5f00\u6e90\u3001\u53ef\u6269\u5c55\u3001\u5206\u5e03\u5f0f\u7b49\u80fd\u529b\uff0c\u63a8\u52a8\u591a\u8bed\u8a00LLM\u516c\u5e73\u4e0e\u5168\u9762\u8bc4\u6d4b\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u8bc4\u6d4b\u5de5\u5177\u591a\u4ee5\u82f1\u6587\u4e3a\u4e3b\uff0c\u7f3a\u4e4f\u5bf9\u5370\u5ea6\u7b49\u591a\u8bed\u8a00\u73af\u5883\u7684\u652f\u6301\uff0c\u9700\u8981\u7edf\u4e00\u4e14\u652f\u6301\u591a\u6837\u5316\u9700\u6c42\u7684\u8bc4\u6d4b\u6846\u67b6\u3002", "method": "\u6574\u540835+\u57fa\u51c6\u96c6\u5408\uff0c\u5305\u62ec10\u4e2a\u5370\u5ea6\u672c\u5730\u6570\u636e\u96c6\uff0c\u652f\u6301\u5206\u5e03\u5f0f\u63a8\u7406\u3001\u91cf\u5316\u548c\u591aGPU\uff0c\u5e76\u4e0e\u73b0\u6709\u8bc4\u6d4b\u5de5\u5177\u505a\u7cfb\u7edf\u5bf9\u6bd4\u3002", "result": "EKA-EVAL \u6846\u67b6\u5df2\u5b9e\u73b0\u5e76\u5f00\u6e90\uff0c\u8986\u76d6\u8303\u56f4\u548c\u529f\u80fd\u4f18\u4e8e\u73b0\u6709\u5370\u5ea6\u8bed\u8a00\u8bc4\u6d4b\u5de5\u5177\uff0c\u5e76\u8ba1\u5212\u6269\u5c55\u5230100+\u57fa\u51c6\uff0c\u6784\u5efa\u5f3a\u5927\u7684\u591a\u8bed\u8a00LLM\u8bc4\u6d4b\u751f\u6001\u3002", "conclusion": "EKA-EVAL \u6846\u67b6\u4e3a\u5168\u7403\u53ca\u5370\u5ea6\u672c\u5730\u7684\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u9996\u4e2a\u7aef\u5230\u7aef\u3001\u6613\u6269\u5c55\u7684\u8bc4\u6d4b\u5957\u4ef6\uff0c\u5927\u5e45\u964d\u4f4e\u4e86\u591a\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\u7684\u95e8\u69db\u3002"}}
{"id": "2507.01872", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01872", "abs": "https://arxiv.org/abs/2507.01872", "authors": ["Kenan Tang", "Yanhong Li", "Yao Qin"], "title": "DIY-MKG: An LLM-Based Polyglot Language Learning System", "comment": "Submitted to EMNLP 2025 System Demonstration", "summary": "Existing language learning tools, even those powered by Large Language Models\n(LLMs), often lack support for polyglot learners to build linguistic\nconnections across vocabularies in multiple languages, provide limited\ncustomization for individual learning paces or needs, and suffer from\ndetrimental cognitive offloading. To address these limitations, we design\nDo-It-Yourself Multilingual Knowledge Graph (DIY-MKG), an open-source system\nthat supports polyglot language learning. DIY-MKG allows the user to build\npersonalized vocabulary knowledge graphs, which are constructed by selective\nexpansion with related words suggested by an LLM. The system further enhances\nlearning through rich annotation capabilities and an adaptive review module\nthat leverages LLMs for dynamic, personalized quiz generation. In addition,\nDIY-MKG allows users to flag incorrect quiz questions, simultaneously\nincreasing user engagement and providing a feedback loop for prompt refinement.\nOur evaluation of LLM-based components in DIY-MKG shows that vocabulary\nexpansion is reliable and fair across multiple languages, and that the\ngenerated quizzes are highly accurate, validating the robustness of DIY-MKG.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDIY-MKG\u7cfb\u7edf\uff0c\u7ed3\u5408\u5927\u6a21\u578b\u548c\u77e5\u8bc6\u56fe\u8c31\uff0c\u89e3\u51b3\u4e86\u591a\u8bed\u79cd\u5b66\u4e60\u5de5\u5177\u7684\u4e2a\u6027\u5316\u4e0e\u6269\u5c55\u96be\u9898\uff0c\u7ecf\u9a8c\u8bc1\u6548\u679c\u826f\u597d\u3002", "motivation": "\u73b0\u6709\u8bed\u8a00\u5b66\u4e60\u5de5\u5177\uff0c\u5373\u4f7f\u6709\u5927\u6a21\u578b\u652f\u6301\uff0c\u5f80\u5f80\u4e0d\u652f\u6301\u591a\u8bed\u79cd\u5b66\u4e60\u8005\u8de8\u8bed\u8a00\u8054\u7cfb\u8bcd\u6c47\u3001\u7f3a\u4e4f\u4e2a\u6027\u5316\u5b9a\u5236\uff0c\u4e5f\u5bb9\u6613\u8ba9\u7528\u6237\u4ea7\u751f\u8ba4\u77e5\u4f9d\u8d56\u3002", "method": "\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u4e86DIY-MKG\uff08\u591a\u8bed\u79cd\u77e5\u8bc6\u56fe\u8c31\u81ea\u5efa\u7cfb\u7edf\uff09\uff0c\u5141\u8bb8\u7528\u6237\u6784\u5efa\u4e2a\u6027\u5316\u7684\u8bcd\u6c47\u77e5\u8bc6\u56fe\u8c31\uff0c\u901a\u8fc7\u5927\u6a21\u578b\u63a8\u8350\u76f8\u5173\u8bcd\u6269\u5c55\u8bcd\u6c47\uff0c\u540c\u65f6\u63d0\u4f9b\u4e30\u5bcc\u6ce8\u91ca\u529f\u80fd\u548c\u81ea\u9002\u5e94\u6d4b\u9a8c\u6a21\u5757\uff08\u7531\u5927\u6a21\u578b\u4e2a\u6027\u5316\u751f\u6210\u6d4b\u9a8c\uff09\u3002\u7528\u6237\u8fd8\u53ef\u53cd\u9988\u9519\u8bef\u95ee\u9898\uff0c\u7528\u4e8e\u4f18\u5316\u63d0\u793a\u8bcd\u3002", "result": "\u7cfb\u7edf\u652f\u6301\u591a\u8bed\u79cd\u8bcd\u6c47\u6269\u5c55\uff0c\u6269\u5c55\u6548\u679c\u53ef\u9760\u3001\u516c\u5e73\uff0c\u6d4b\u9a8c\u9898\u76ee\u51c6\u786e\u6027\u9ad8\uff0c\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u7684\u5065\u58ee\u6027\u3002", "conclusion": "DIY-MKG\u6709\u6548\u63d0\u5347\u4e86\u591a\u8bed\u79cd\u5b66\u4e60\u4f53\u9a8c\uff0c\u80fd\u591f\u6ee1\u8db3\u4e2a\u6027\u5316\u3001\u4ea4\u4e92\u6027\u548c\u51c6\u786e\u6027\u7684\u9700\u6c42\u3002"}}
{"id": "2507.01887", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01887", "abs": "https://arxiv.org/abs/2507.01887", "authors": ["Dongyi Ding", "Tiannan Wang", "Chenghao Zhu", "Meiling Tao", "Yuchen Eleanor Jiang", "Wangchunshu Zhou"], "title": "MiCoTA: Bridging the Learnability Gap with Intermediate CoT and Teacher Assistants", "comment": "Work in progress", "summary": "Large language models (LLMs) excel at reasoning tasks requiring long thought\nsequences for planning, reflection, and refinement. However, their substantial\nmodel size and high computational demands are impractical for widespread\ndeployment. Yet, small language models (SLMs) often struggle to learn long-form\nCoT reasoning due to their limited capacity, a phenomenon we refer to as the\n\"SLMs Learnability Gap\". To address this, we introduce\n\\textbf{Mi}d-\\textbf{Co}T \\textbf{T}eacher \\textbf{A}ssistant Distillation\n(MiCoTAl), a framework for improving long CoT distillation for SLMs. MiCoTA\nemploys intermediate-sized models as teacher assistants and utilizes\nintermediate-length CoT sequences to bridge both the capacity and reasoning\nlength gaps. Our experiments on downstream tasks demonstrate that although SLMs\ndistilled from large teachers can perform poorly, by applying MiCoTA, they\nachieve significant improvements in reasoning performance. Specifically,\nQwen2.5-7B-Instruct and Qwen2.5-3B-Instruct achieve an improvement of 3.47 and\n3.93 respectively on average score on AIME2024, AMC, Olympiad, MATH-500 and\nGSM8K benchmarks. To better understand the mechanism behind MiCoTA, we perform\na quantitative experiment demonstrating that our method produces data more\nclosely aligned with base SLM distributions. Our insights pave the way for\nfuture research into long-CoT data distillation for SLMs.", "AI": {"tldr": "\u5927\u578b\u6a21\u578b\u84b8\u998f\u5230\u5c0f\u6a21\u578b\u5b58\u5728\u957f\u94fe\u63a8\u7406\u5b66\u4e60\u96be\u9898\uff0cMiCoTA\u901a\u8fc7\u5f15\u5165\u4e2d\u5c3a\u5bf8\u6559\u5e08\u52a9\u624b\u548c\u4e2d\u957f\u63a8\u7406\u5e8f\u5217\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5c0f\u6a21\u578b\u7684\u63a8\u7406\u8868\u73b0\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u9700\u8981\u957f\u94fe\u63a8\u7406\u7684\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u56e0\u4f53\u79ef\u5e9e\u5927\u548c\u8ba1\u7b97\u6d88\u8017\u9ad8\uff0c\u9650\u5236\u4e86\u5176\u666e\u53ca\u3002\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLM\uff09\u5219\u56e0\u5bb9\u91cf\u53d7\u9650\uff0c\u96be\u4ee5\u5b66\u5230\u957f\u94fe\u63a8\u7406\u80fd\u529b\uff08\u79f0\u4e3aSLMs Learnability Gap\uff09\u3002\u672c\u8bba\u6587\u65e8\u5728\u89e3\u51b3SLMs\u5728\u957f\u94fe\u63a8\u7406\u5b66\u4e60\u4e0a\u7684\u74f6\u9888\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u84b8\u998f\u6846\u67b6MiCoTA\uff08Mid-CoT Teacher Assistant Distillation\uff09\uff0c\u5f15\u5165\u4e86\u4e2d\u7b49\u89c4\u6a21\u6559\u5e08\u52a9\u624b\u6a21\u578b\u548c\u4e2d\u957f\u5ea6\u63a8\u7406\u5e8f\u5217\uff0c\u8f85\u52a9\u5c0f\u6a21\u578b\u66f4\u597d\u5730\u5b66\u4e60\u957f\u94fe\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u76f4\u63a5\u7528\u5927\u6a21\u578b\u84b8\u998f\u7684\u5c0f\u6a21\u578b\u6548\u679c\u4e0d\u4f73\uff0c\u800c\u5f15\u5165MiCoTA\u540e\uff0c\u5c0f\u6a21\u578b\u7684\u63a8\u7406\u8868\u73b0\u663e\u8457\u63d0\u5347\u3002\u4f8b\u5982\uff0cQwen2.5-7B-Instruct\u548cQwen2.5-3B-Instruct\u5728\u591a\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u5206\u6570\u5206\u522b\u63d0\u5347\u4e863.47\u548c3.93\u3002\u8fdb\u4e00\u6b65\u5b9e\u9a8c\u8fd8\u8868\u660e\uff0c\u672c\u65b9\u6cd5\u751f\u6210\u7684\u6570\u636e\u5206\u5e03\u66f4\u8d34\u5408SLM\u57fa\u7840\u5206\u5e03\u3002", "conclusion": "MiCoTA\u6709\u6548\u7f29\u5c0f\u4e86\u5c0f\u6a21\u578b\u5728\u957f\u94fe\u63a8\u7406\u84b8\u998f\u4e2d\u7684\u5b66\u4e60\u5dee\u8ddd\uff0c\u4e3a\u540e\u7eedSLM\u957f\u94fe\u63a8\u7406\u80fd\u529b\u63d0\u5347\u548c\u6570\u636e\u84b8\u998f\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.01900", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01900", "abs": "https://arxiv.org/abs/2507.01900", "authors": ["Songtao Liu", "Peng Liu"], "title": "High-Layer Attention Pruning with Rescaling", "comment": null, "summary": "Pruning is a highly effective approach for compressing large language models\n(LLMs), significantly reducing inference latency. However, conventional\ntraining-free structured pruning methods often employ a heuristic metric that\nindiscriminately removes some attention heads across all pruning layers,\nwithout considering their positions within the network architecture. In this\nwork, we propose a novel pruning algorithm that strategically prunes attention\nheads in the model's higher layers. Since the removal of attention heads can\nalter the magnitude of token representations, we introduce an adaptive\nrescaling parameter that calibrates the representation scale post-pruning to\ncounteract this effect. We conduct comprehensive experiments on a wide range of\nLLMs, including LLaMA3.1-8B, Mistral-7B-v0.3, Qwen2-7B, and Gemma2-9B. Our\nevaluation includes both generation and discriminative tasks across 27\ndatasets. The results consistently demonstrate that our method outperforms\nexisting structured pruning methods. This improvement is particularly notable\nin generation tasks, where our approach significantly outperforms existing\nbaselines.", "AI": {"tldr": "\u9488\u5bf9\u73b0\u6709\u65e0\u8bad\u7ec3\u7ed3\u6784\u5316\u526a\u679d\u65b9\u6cd5\u7684\u4e0d\u8db3\uff0c\u672c\u6587\u63d0\u51fa\u4e13\u6ce8\u4e8e\u9ad8\u5c42\u6ce8\u610f\u529b\u5934\u526a\u679d\u5e76\u81ea\u9002\u5e94\u6821\u51c6\u7684\u65b9\u6cd5\uff0c\u5728\u591a\u79cd\u5927\u6a21\u578b\u53ca\u5927\u89c4\u6a21\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u6027\u80fd\u8d85\u8d8a\u73b0\u6709\u526a\u679d\u6280\u672f\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7531\u4e8e\u89c4\u6a21\u5e9e\u5927\uff0c\u63a8\u7406\u65f6\u5ef6\u8f83\u9ad8\uff0c\u56e0\u6b64\u9700\u8981\u6709\u6548\u7684\u6a21\u578b\u538b\u7f29\u65b9\u6cd5\u4ee5\u63d0\u5347\u5b9e\u9645\u5e94\u7528\u6548\u7387\u3002\u7ed3\u6784\u5316\u526a\u679d\u662f\u5e38\u89c1\u7684\u538b\u7f29\u65b9\u6cd5\uff0c\u4f46\u73b0\u6709\u65e0\u8bad\u7ec3\u7ed3\u6784\u5316\u526a\u679d\u901a\u5e38\u91c7\u7528\u542f\u53d1\u5f0f\u6307\u6807\uff0c\u5728\u6240\u6709\u5c42\u65e0\u5dee\u522b\u5730\u526a\u679d\u6ce8\u610f\u529b\u5934\uff0c\u672a\u8003\u8651\u7f51\u7edc\u7ed3\u6784\u4e2d\u4e0d\u540c\u5c42\u7684\u4f4d\u7f6e\u5dee\u5f02\uff0c\u53ef\u80fd\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u7ed3\u6784\u5316\u526a\u679d\u7b97\u6cd5\uff0c\u4e13\u95e8\u9488\u5bf9\u6a21\u578b\u9ad8\u5c42\u7684\u6ce8\u610f\u529b\u5934\u8fdb\u884c\u6709\u7b56\u7565\u7684\u526a\u679d\u3002\u6b64\u5916\uff0c\u4e3a\u4e86\u4fee\u6b63\u526a\u679d\u53ef\u80fd\u5e26\u6765\u7684\u8868\u793a\u5e45\u503c\u53d8\u5316\uff0c\u5f15\u5165\u4e86\u81ea\u9002\u5e94\u91cd\u6807\u5b9a\u53c2\u6570\uff0c\u5728\u526a\u679d\u4e4b\u540e\u91cd\u65b0\u6821\u51c6token\u8868\u793a\u7684\u5c3a\u5ea6\u3002", "result": "\u5728\u5305\u62ecLLaMA3.1-8B\u3001Mistral-7B-v0.3\u3001Qwen2-7B\u548cGemma2-9B\u7b49\u591a\u79cd\u5927\u8bed\u8a00\u6a21\u578b\u4e0a\u8fdb\u884c\u4e86\u8be6\u5c3d\u5b9e\u9a8c\u3002\u6d4b\u8bd5\u4efb\u52a1\u8986\u76d6\u751f\u6210\u548c\u5224\u522b\u4e24\u7c7b\u517127\u4e2a\u6570\u636e\u96c6\u3002\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u6240\u6709\u6570\u636e\u96c6\u548c\u4efb\u52a1\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u7ed3\u6784\u5316\u526a\u679d\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u751f\u6210\u4efb\u52a1\u4e0a\u4f18\u52bf\u663e\u8457\u3002", "conclusion": "\u901a\u8fc7\u6709\u9488\u5bf9\u6027\u5730\u526a\u679d\u6a21\u578b\u9ad8\u5c42\u7684\u6ce8\u610f\u529b\u5934\uff0c\u5e76\u5f15\u5165\u81ea\u9002\u5e94\u91cd\u6807\u5b9a\u673a\u5236\uff0c\u53ef\u6709\u6548\u7f13\u89e3\u526a\u679d\u5f15\u53d1\u7684\u8868\u793a\u5931\u8c03\u95ee\u9898\uff0c\u4ece\u800c\u5728\u964d\u4f4e\u6a21\u578b\u8ba1\u7b97\u5f00\u9500\u7684\u540c\u65f6\u4fdd\u6301\u751a\u81f3\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u6587\u672c\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002"}}
{"id": "2507.01921", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01921", "abs": "https://arxiv.org/abs/2507.01921", "authors": ["Yang Li", "Youssef Emad", "Karthik Padthe", "Jack Lanchantin", "Weizhe Yuan", "Thao Nguyen", "Jason Weston", "Shang-Wen Li", "Dong Wang", "Ilia Kulikov", "Xian Li"], "title": "NaturalThoughts: Selecting and Distilling Reasoning Traces for General Reasoning Tasks", "comment": null, "summary": "Recent work has shown that distilling reasoning traces from a larger teacher\nmodel via supervised finetuning outperforms reinforcement learning with the\nsmaller student model alone (Guo et al. 2025). However, there has not been a\nsystematic study of what kind of reasoning demonstrations from the teacher are\nmost effective in improving the student model's reasoning capabilities. In this\nwork we curate high-quality \"NaturalThoughts\" by selecting reasoning traces\nfrom a strong teacher model based on a large pool of questions from\nNaturalReasoning (Yuan et al. 2025). We first conduct a systematic analysis of\nfactors that affect distilling reasoning capabilities, in terms of sample\nefficiency and scalability for general reasoning tasks. We observe that simply\nscaling up data size with random sampling is a strong baseline with steady\nperformance gains. Further, we find that selecting difficult examples that\nrequire more diverse reasoning strategies is more sample-efficient to transfer\nthe teacher model's reasoning skills. Evaluated on both Llama and Qwen models,\ntraining with NaturalThoughts outperforms existing reasoning datasets such as\nOpenThoughts, LIMO, etc. on general STEM reasoning benchmarks including\nGPQA-Diamond, MMLU-Pro and SuperGPQA.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u63a2\u7a76\u4e86\u5982\u4f55\u4ece\u6559\u5e08\u5927\u6a21\u578b\u9009\u53d6\u6700\u6709\u6548\u7684\u63a8\u7406\u793a\u8303\u8bad\u7ec3\u5b66\u751f\u6a21\u578b\uff0c\u63d0\u51fa\u5e76\u9a8c\u8bc1\u4e86\u9ad8\u8d28\u91cf\u7cbe\u6311\u96be\u4f8b\u66f4\u9ad8\u6548\uff0c\u663e\u8457\u8d85\u8fc7\u4ee5\u5f80\u505a\u6cd5\u3002", "motivation": "\u5df2\u6709\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u5927\u6a21\u578b\u6559\u5e08\u7684\u63a8\u7406\u8f68\u8ff9\u76d1\u7763\u5fae\u8c03\u5c0f\u5b66\u751f\u6a21\u578b\uff0c\u6bd4\u4ec5\u7528\u5c0f\u6a21\u578b\u81ea\u8eab\u7684\u5f3a\u5316\u5b66\u4e60\u6548\u679c\u66f4\u597d\u3002\u4f46\u76ee\u524d\u5c1a\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7814\u7a76\uff0c\u63a2\u8ba8\u54ea\u4e9b\u7c7b\u578b\u7684\u63a8\u7406\u793a\u8303\u6700\u6709\u6548\u63d0\u5347\u5b66\u751f\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002", "method": "\u4f5c\u8005\u4ece\u5f3a\u529b\u6559\u5e08\u6a21\u578b\u751f\u6210\u7684\u5927\u91cf\u95ee\u9898\u63a8\u7406\u8f68\u8ff9\u4e2d\uff0c\u7cbe\u6311\u7ec6\u9009\u51fa\u9ad8\u8d28\u91cf\u63a8\u7406\u793a\u8303\uff08NaturalThoughts\uff09\uff0c\u5e76\u5bf9\u5f71\u54cd\u63a8\u7406\u84b8\u998f\u6548\u679c\u7684\u56e0\u7d20\uff08\u5982\u6837\u672c\u6548\u7387\u4e0e\u6269\u5c55\u6027\uff09\u8fdb\u884c\u4e86\u7cfb\u7edf\u5206\u6790\uff0c\u6bd4\u8f83\u4e86\u4e0d\u540c\u6570\u636e\u7b5b\u9009\u7b56\u7565\uff0c\u5e76\u5728\u591a\u4e2a\u4e3b\u6d41\u6a21\u578b\u4e0e\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u4f5c\u6bd4\u8f83\u5b9e\u9a8c\u3002", "result": "\uff081\uff09\u53d1\u73b0\u7b80\u5355\u589e\u5927\u968f\u673a\u91c7\u6837\u8bad\u7ec3\u6570\u636e\u89c4\u6a21\uff0c\u672c\u8eab\u5c31\u662f\u8868\u73b0\u5f3a\u52b2\u4e14\u7a33\u5b9a\u63d0\u5347\u6548\u679c\u7684\u57fa\u7840\u65b9\u6cd5\uff1b\uff082\uff09\u6311\u9009\u66f4\u96be\u3001\u6d89\u53ca\u591a\u6837\u5316\u63a8\u7406\u7b56\u7565\u7684\u793a\u4f8b\uff0c\u66f4\u9ad8\u6548\u63d0\u5347\u5b66\u751f\u7684\u63a8\u7406\u6280\u80fd\uff1b\uff083\uff09\u7528NaturalThoughts\u8bad\u7ec3\u6a21\u578b\uff0c\u5728\u5e7f\u6cdbSTEM\u63a8\u7406\u57fa\u51c6\u4e0a\u4f18\u4e8e\u5df2\u6709\u6570\u636e\u96c6\u3002", "conclusion": "\u7cfb\u7edf\u5206\u6790\u8bc1\u660e\u7cbe\u6311\u7ec6\u9009\u4e14\u9ad8\u8d28\u91cf\u7684\u63a8\u7406\u793a\u8303\u80fd\u66f4\u9ad8\u6548\u63d0\u5347\u5b66\u751f\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u9488\u5bf9\u96be\u4f8b\u4f18\u5316\u7684\u7b56\u7565\u53ef\u8d85\u8d8a\u5355\u7eaf\u6269\u5c55\u6570\u636e\uff0c\u5c55\u793a\u4e86\u63a8\u7406\u6280\u80fd\u84b8\u998f\u7684\u65b0\u8303\u5f0f\u4e0e\u4f18\u8d8a\u6027\u3002"}}
{"id": "2507.01923", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01923", "abs": "https://arxiv.org/abs/2507.01923", "authors": ["Yu-Shiang Huang", "Chuan-Ju Wang", "Chung-Chi Chen"], "title": "Decision-oriented Text Evaluation", "comment": null, "summary": "Natural language generation (NLG) is increasingly deployed in high-stakes\ndomains, yet common intrinsic evaluation methods, such as n-gram overlap or\nsentence plausibility, weakly correlate with actual decision-making efficacy.\nWe propose a decision-oriented framework for evaluating generated text by\ndirectly measuring its influence on human and large language model (LLM)\ndecision outcomes. Using market digest texts--including objective morning\nsummaries and subjective closing-bell analyses--as test cases, we assess\ndecision quality based on the financial performance of trades executed by human\ninvestors and autonomous LLM agents informed exclusively by these texts. Our\nfindings reveal that neither humans nor LLM agents consistently surpass random\nperformance when relying solely on summaries. However, richer analytical\ncommentaries enable collaborative human-LLM teams to outperform individual\nhuman or agent baselines significantly. Our approach underscores the importance\nof evaluating generated text by its ability to facilitate synergistic\ndecision-making between humans and LLMs, highlighting critical limitations of\ntraditional intrinsic metrics.", "AI": {"tldr": "\u5c06NLG\u8bc4\u4f30\u91cd\u70b9\u4ece\u8868\u9762\u5339\u914d\u8f6c\u5411\u5b9e\u9645\u51b3\u7b56\u5f71\u54cd\uff0c\u63d0\u51fa\u4ee5\u91d1\u878d\u4ea4\u6613\u5b9e\u7ee9\u4e3a\u4f9d\u6258\u7684\u51b3\u7b56\u5bfc\u5411\u8bc4\u4f30\u6846\u67b6\u3002\u5b9e\u9a8c\u8bc1\u660e\uff1a\u5206\u6790\u6027\u6587\u672c\u80fd\u663e\u8457\u63d0\u5347\u4eba-\u673a\u534f\u540c\u51b3\u7b56\uff0c\u800c\u4f20\u7edf\u5185\u5728\u8bc4\u4f30\u4e0d\u8db3\u4ee5\u6355\u6349\u8fd9\u79cd\u4ef7\u503c\u3002", "motivation": "\u73b0\u6709\u7684\u81ea\u7136\u8bed\u8a00\u751f\u6210\uff08NLG\uff09\u8bc4\u4f30\u65b9\u6cd5\uff08\u5982n-gram\u91cd\u53e0\u5ea6\u6216\u53e5\u5b50\u5408\u7406\u6027\uff09\u4e0e\u51b3\u7b56\u8fc7\u7a0b\u7684\u5b9e\u9645\u6709\u6548\u6027\u76f8\u5173\u6027\u8f83\u5f31\uff0c\u96be\u4ee5\u53cd\u6620\u6587\u672c\u5bf9\u9ad8\u98ce\u9669\u9886\u57df\u51b3\u7b56\u7684\u771f\u5b9e\u5f71\u54cd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ee5\u51b3\u7b56\u4e3a\u5bfc\u5411\u7684NLG\u8bc4\u4f30\u6846\u67b6\uff0c\u76f4\u63a5\u6d4b\u91cf\u751f\u6210\u6587\u672c\u5bf9\u4eba\u7c7b\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u51b3\u7b56\u7ed3\u679c\u7684\u5f71\u54cd\u3002\u91c7\u7528\u5e02\u573a\u6458\u8981\u6587\u672c\uff08\u5305\u62ec\u5ba2\u89c2\u6668\u62a5\u548c\u4e3b\u89c2\u6536\u76d8\u5206\u6790\uff09\u4e3a\u6848\u4f8b\uff0c\u901a\u8fc7\u4eba\u7c7b\u6295\u8d44\u8005\u548c\u4ec5\u4f9d\u636e\u8fd9\u4e9b\u6587\u672c\u4fe1\u606f\u7684\u81ea\u4e3bLLM\u4ee3\u7406\u8fdb\u884c\u4ea4\u6613\uff0c\u5176\u51b3\u7b56\u8d28\u91cf\u4ee5\u5b9e\u9645\u91d1\u878d\u6536\u76ca\u8861\u91cf\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\uff1a\u4e0d\u8bba\u662f\u4eba\u7c7b\u8fd8\u662fLLM\uff0c\u4ec5\u9760\u6458\u8981\u4fe1\u606f\u505a\u51b3\u7b56\uff0c\u8868\u73b0\u90fd\u4e0e\u968f\u673a\u9009\u62e9\u65e0\u663e\u8457\u5dee\u5f02\u3002\u4f46\u5f53\u63d0\u4f9b\u66f4\u52a0\u4e30\u5bcc\u7684\u5206\u6790\u6027\u8bc4\u8bba\u65f6\uff0c\u4eba-LLM\u5408\u4f5c\u56e2\u961f\u663e\u8457\u4f18\u4e8e\u5355\u72ec\u7684\u4eba\u7c7b\u6216\u4ee3\u7406\u3002", "conclusion": "\u8bba\u6587\u5f3a\u8c03\u8bc4\u4f30\u751f\u6210\u6587\u672c\u65f6\uff0c\u5e94\u91cd\u89c6\u5176\u4fc3\u8fdb\u4eba-LLM\u534f\u540c\u51b3\u7b56\u7684\u80fd\u529b\uff0c\u73b0\u6709\u5185\u5728\u5ea6\u91cf\u65b9\u6cd5\u5b58\u5728\u4e25\u91cd\u5c40\u9650\u3002"}}
