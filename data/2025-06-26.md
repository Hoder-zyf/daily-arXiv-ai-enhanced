<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 15]
- [cs.CL](#cs.CL) [Total: 12]
- [cs.CE](#cs.CE) [Total: 3]
- [cs.CY](#cs.CY) [Total: 5]
- [cs.LG](#cs.LG) [Total: 2]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Prover Agent: An Agent-based Framework for Formal Mathematical Proofs](https://arxiv.org/abs/2506.19923)
*Kaito Baba,Chaoran Liu,Shuhei Kurita,Akiyoshi Sannai*

Main category: cs.AI

TL;DR: Prover Agent结合LLM与Lean证明助理，能自动生成辅助引理，极大提升自动定理证明效果，在主流基准上以小模型、低采样预算取得SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 当前自动定理证明领域受限于纯语言模型的推理能力和外部形式化工具协同有限，难以高效、准确地完成复杂证明任务。

Method: 提出Prover Agent，将大语言模型与Lean形式化证明助理结合，协调非正式推理LLM、形式化证明模型及Lean的反馈，并能自动生成辅助引理，协助推理策略发现。

Result: 在MiniF2F基准上取得了86.1%的成功率，刷新了使用小型语言模型下的最优结果，且采样预算远低于既有方法。通过案例分析展示生成引理对解决难题的作用。

Conclusion: Prover Agent有效联合LLM与形式化系统，实现高效、准确的自动定理证明，在SLM条件下性能大幅提升，展现新颖协作模式。

Abstract: We present Prover Agent, a novel AI agent for automated theorem proving that
integrates large language models (LLMs) with a formal proof assistant, Lean.
Prover Agent coordinates an informal reasoning LLM, a formal prover model, and
feedback from Lean while also generating auxiliary lemmas to assist in
discovering the overall proof strategy. It achieves an 86.1% success rate on
the MiniF2F benchmark, establishing a new state-of-the-art among methods using
small language models (SLMs) with a much lower sample budget than previous
approaches. We also present case studies illustrating how these generated
lemmas contribute to solving challenging problems.

</details>


### [2] [Context Attribution with Multi-Armed Bandit Optimization](https://arxiv.org/abs/2506.19977)
*Deng Pan,Keerthiram Murugesan,Nuno Moniz,Nitesh Chawla*

Main category: cs.AI

TL;DR: 本文提出将上下文归因表述为组合多臂赌博机问题，并用组合型汤普森采样提升效率，比传统方法更省模型查询数，同时归因效果有竞争力。


<details>
  <summary>Details</summary>
Motivation: 理解检索到的上下文中哪些部分对大语言模型生成答案起到关键作用，对于构建可解释和可信赖的生成式问答系统非常重要。现有基于扰动的方法效率低下，因此需要更高效的归因方法。

Method: 提出将上下文归因问题建模为组合多臂赌博机（CMAB）问题。将每个上下文片段视为一个臂，通过组合型汤普森采样（CTS）算法，使用有限的模型调用预算，高效探索片段子集空间，并定义基于归一化分词概率的奖励函数，衡量各片段对原始回复支持度。

Result: 实验证明，在多个数据集和大语言模型上，该方法以更少的模型调用次数实现了与主流归因方法相当甚至更高的归因质量，显著提高了查询效率。

Conclusion: 所提出的基于CMAB的上下文归因方法能在保证归因高保真的同时，大幅提高效率，为解释性问答系统提供了有效工具。

Abstract: Understanding which parts of the retrieved context contribute to a large
language model's generated answer is essential for building interpretable and
trustworthy generative QA systems. We propose a novel framework that formulates
context attribution as a combinatorial multi-armed bandit (CMAB) problem. Each
context segment is treated as a bandit arm, and we employ Combinatorial
Thompson Sampling (CTS) to efficiently explore the exponentially large space of
context subsets under a limited query budget. Our method defines a reward
function based on normalized token likelihoods, capturing how well a subset of
segments supports the original model response. Unlike traditional
perturbation-based attribution methods such as SHAP, which sample subsets
uniformly and incur high computational costs, our approach adaptively balances
exploration and exploitation by leveraging posterior estimates of segment
relevance. This leads to substantially improved query efficiency while
maintaining high attribution fidelity. Extensive experiments on diverse
datasets and LLMs demonstrate that our method achieves competitive attribution
quality with fewer model queries.

</details>


### [3] [QHackBench: Benchmarking Large Language Models for Quantum Code Generation Using PennyLane Hackathon Challenges](https://arxiv.org/abs/2506.20008)
*Abdul Basit,Minghao Shao,Haider Asif,Nouhaila Innan,Muhammad Kashif,Alberto Marchisio,Muhammad Shafique*

Main category: cs.AI

TL;DR: 本文提出针对量子编程代码生成的QHackBench基准，系统评测了LLM与RAG方法，发现RAG在复杂任务与基础提示表现相当，多Agent方法可提升成功率，并将相关资源开源以支持后续研究。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型（LLM）在代码生成上展现了强大潜力，但其在量子计算领域的有效性尚未被充分探索。本文专注于评估LLM在PennyLane量子代码生成任务中的表现，尤其面对真实的量子编程挑战。

Method: 提出了QHackBench基准集，该数据集来源于QHack竞赛的真实挑战。围绕功能正确性、语法有效性及执行成功率三个维度，搭建结构化评测框架，对比了基础提示和检索增强生成（RAG）两种模型方案，并引入多Agent评测流程以迭代修正错误答案。

Result: RAG增强模型结合扩充后的PennyLane数据集，在复杂量子算法场景下，生成效果大致与基础提示一致。多Agent评测流程有效提升了代码执行成功率。

Conclusion: QHackBench工具、完整评测框架和实验结果将全部开源，推动AI辅助量子编程研究的进一步发展。

Abstract: Recent advances in Large Language Models (LLMs) have demonstrated strong
potential in code generation, yet their effectiveness in quantum computing
remains underexplored. This paper benchmarks LLMs for PennyLane-based quantum
code generation using real-world challenges from the Quantum Hackathon (QHack).
We introduce QHackBench, a novel benchmark dataset derived from QHack
competitions, and evaluate model performance under vanilla prompting and
Retrieval-Augmented Generation (RAG). Our structured evaluation framework
assesses functional correctness, syntactic validity, and execution success
across varying challenge difficulties. Results indicate that RAG-enhanced
models, supplemented with an augmented PennyLane dataset, approximately
generate similar results as the standard prompting, particularly in complex
quantum algorithms. Additionally, we introduce a multi-agent evaluation
pipeline that iteratively refines incorrect solutions, further enhancing
execution success rates. To foster further research, we commit to publicly
releasing QHackBench, along with our evaluation framework and experimental
results, enabling continued advancements in AI-assisted quantum programming.

</details>


### [4] [Accurate and Energy Efficient: Local Retrieval-Augmented Generation Models Outperform Commercial Large Language Models in Medical Tasks](https://arxiv.org/abs/2506.20009)
*Konstantinos Vrettos,Michail E. Klontzas*

Main category: cs.AI

TL;DR: 开源本地化医疗RAG能效和准确性均优于主流商业医疗LLM，对环境影响更小，促进了医疗AI的可持续发展。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能（AI）在医疗健康领域的广泛应用，人们对其环境与伦理问题的关注逐渐上升。商用的大语言模型（LLM）如ChatGPT等，在消耗大量资源的同时，也带来了患者隐私和安全等进一步问题。

Method: 作者设计了一个可定制的面向医疗任务的RAG（Retrieval-Augmented Generation）框架，能实时监控能耗和二氧化碳排放。RAG系统基于多种开源LLM（如llama3.1:8b和专门的medgemma-4b-it）构建，并与商业化模型DeepSeekV3-R1及OpenAI的o4-mini做对比。评估采用一组医学问题数据集。

Result: 定制的RAG模型在准确率和能耗方面均优于商业模型。以llama3.1:8B为基础的RAG取得了最高准确率（58.5%），其性能显著高于包括o4-mini和DeepSeekV3-R1在内的其他模型，并且能耗及碳足迹最低（性能/kWh为0.52，总CO2排放473g）。相较o4-mini，其准确率每kWh提升2.7倍，耗电量降低172%，且准确性更高。

Conclusion: 本研究表明，本地化开源LLM可用于开发RAG系统，在医疗任务中表现优于商业在线LLM，并具有更低的环境影响。此外，本文提出的模块化框架有助于推动AI可持续发展，减少能耗，契合联合国可持续发展目标。

Abstract: Background The increasing adoption of Artificial Intelligence (AI) in
healthcare has sparked growing concerns about its environmental and ethical
implications. Commercial Large Language Models (LLMs), such as ChatGPT and
DeepSeek, require substantial resources, while the utilization of these systems
for medical purposes raises critical issues regarding patient privacy and
safety. Methods We developed a customizable Retrieval-Augmented Generation
(RAG) framework for medical tasks, which monitors its energy usage and CO2
emissions. This system was then used to create RAGs based on various
open-source LLMs. The tested models included both general purpose models like
llama3.1:8b and medgemma-4b-it, which is medical-domain specific. The best RAGs
performance and energy consumption was compared to DeepSeekV3-R1 and OpenAIs
o4-mini model. A dataset of medical questions was used for the evaluation.
Results Custom RAG models outperformed commercial models in accuracy and energy
consumption. The RAG model built on llama3.1:8B achieved the highest accuracy
(58.5%) and was significantly better than other models, including o4-mini and
DeepSeekV3-R1. The llama3.1-RAG also exhibited the lowest energy consumption
and CO2 footprint among all models, with a Performance per kWh of 0.52 and a
total CO2 emission of 473g. Compared to o4-mini, the llama3.1-RAG achieved 2.7x
times more accuracy points per kWh and 172% less electricity usage while
maintaining higher accuracy. Conclusion Our study demonstrates that local LLMs
can be leveraged to develop RAGs that outperform commercial, online LLMs in
medical tasks, while having a smaller environmental impact. Our modular
framework promotes sustainable AI development, reducing electricity usage and
aligning with the UNs Sustainable Development Goals.

</details>


### [5] [Achieving Trustworthy Real-Time Decision Support Systems with Low-Latency Interpretable AI Models](https://arxiv.org/abs/2506.20018)
*Zechun Deng,Ziwei Liu,Ziqian Bi,Junhao Song,Chia Xin Liang,Joe Yeong,Junfeng Hao*

Main category: cs.AI

TL;DR: 本文综述了如何在资源有限的环境下，通过低延迟AI模型、与Edge-IoT的融合，以及模型压缩等技术提升实时决策支持系统的效率和灵活性，并展望了未来的发展方向。


<details>
  <summary>Details</summary>
Motivation: 在实时决策支持系统中，如何在资源有限的情况下利用低延迟的AI模型，提高决策效率，是当前科技和工业领域的重要需求。此外，融合AI与Edge-IoT技术及人机协作的新进展为系统带来了新的挑战和机遇。

Method: 通过综述近年来涉及全局AI决策工具、与Edge-IoT的集成以及人机协作方法等前沿进展，理论上探讨了大语言模型在辅助决策和边缘设备上运行的可行性，并分析了如DeLLMa技术、模型压缩、分析优化等技术的具体应用。

Result: 论文梳理并总结了各类技术进展对于提升AI决策支持系统效率和灵活性的积累经验，提出了实际开发中的策略和可应用的领域，同时明确了未来系统优化的潜力方向。

Conclusion: AI与边缘计算的结合正推动实时决策支持系统迈向更高的效率和灵活性。该综述为今后相关领域的创新、系统开发和技术研究提供了理论基础和实践参考。

Abstract: This paper investigates real-time decision support systems that leverage
low-latency AI models, bringing together recent progress in holistic AI-driven
decision tools, integration with Edge-IoT technologies, and approaches for
effective human-AI teamwork. It looks into how large language models can assist
decision-making, especially when resources are limited. The research also
examines the effects of technical developments such as DeLLMa, methods for
compressing models, and improvements for analytics on edge devices, while also
addressing issues like limited resources and the need for adaptable frameworks.
Through a detailed review, the paper offers practical perspectives on
development strategies and areas of application, adding to the field by
pointing out opportunities for more efficient and flexible AI-supported
systems. The conclusions set the stage for future breakthroughs in this
fast-changing area, highlighting how AI can reshape real-time decision support.

</details>


### [6] [Persona-Assigned Large Language Models Exhibit Human-Like Motivated Reasoning](https://arxiv.org/abs/2506.20020)
*Saloni Dash,Amélie Reymond,Emma S. Spiro,Aylin Caliskan*

Main category: cs.AI

TL;DR: 为LLM指定人物身份会导致其推理出现明显的动机性偏见，而且难以通过常规方法有效去偏，值得引起关注。


<details>
  <summary>Details</summary>
Motivation: 人类因身份保护等动机，推理常受偏见影响，LLM已被发现也会有类似的认知偏见。然而，LLM在多大程度上也会因为“身份一致性”而产生动机性推理尚未明确。这一研究试图填补此领域空白。

Method: 为LLM分配8种基于政治和社会人口统计属性的人物设定，并在两个推理任务（虚假信息头条真实性辨别、科学证据评价）中测试8种主流LLM的表现，比较有无人物设定下模型的推理差异。

Result: 设定人物身份的LLM在真实性判断任务中准确率最多下降9%。政治身份设定的模型在其身份立场与事实一致时，对枪支管控科学证据的准确评价概率高达提升90%。传统去偏提示对缓解此影响基本无效。

Conclusion: 指派特定人物设定（persona）的LLM会表现出类似人类的动机性推理，而且传统的去偏提示（debiasing prompts）难以缓解这种现象。

Abstract: Reasoning in humans is prone to biases due to underlying motivations like
identity protection, that undermine rational decision-making and judgment. This
motivated reasoning at a collective level can be detrimental to society when
debating critical issues such as human-driven climate change or vaccine safety,
and can further aggravate political polarization. Prior studies have reported
that large language models (LLMs) are also susceptible to human-like cognitive
biases, however, the extent to which LLMs selectively reason toward
identity-congruent conclusions remains largely unexplored. Here, we investigate
whether assigning 8 personas across 4 political and socio-demographic
attributes induces motivated reasoning in LLMs. Testing 8 LLMs (open source and
proprietary) across two reasoning tasks from human-subject studies -- veracity
discernment of misinformation headlines and evaluation of numeric scientific
evidence -- we find that persona-assigned LLMs have up to 9% reduced veracity
discernment relative to models without personas. Political personas
specifically, are up to 90% more likely to correctly evaluate scientific
evidence on gun control when the ground truth is congruent with their induced
political identity. Prompt-based debiasing methods are largely ineffective at
mitigating these effects. Taken together, our empirical findings are the first
to suggest that persona-assigned LLMs exhibit human-like motivated reasoning
that is hard to mitigate through conventional debiasing prompts -- raising
concerns of exacerbating identity-congruent reasoning in both LLMs and humans.

</details>


### [7] [DiaLLMs: EHR Enhanced Clinical Conversational System for Clinical Test Recommendation and Diagnosis Prediction](https://arxiv.org/abs/2506.20059)
*Weijieying Ren,Tianxiang Zhao,Lei Wang,Tianchun Wang,Vasant Honavar*

Main category: cs.AI

TL;DR: DiaLLM首次将异构EHR信息融入医学大语言模型，实现了更贴合实际流程的多功能医患对话系统，实验证明其在临床检测推荐与诊断预测任务上表现优越。


<details>
  <summary>Details</summary>
Motivation: 现有的医学大模型主要侧重于诊断推荐，忽视了电子健康记录（EHR）在实际医疗中的核心作用，导致应用场景受限，缺乏与临床实践的紧密结合。

Method: 提出DiaLLM，将异构EHR数据整合进医患对话，支持临床检查推荐、结果解读和诊断预测。设计了临床检查引用（CTR）策略，将临床编码映射为描述并区分检查结果是否正常；引入强化学习框架获取证据并进行自动诊断，采用拒绝采样策略优化大动作空间的探索效率，并设计确认奖励与类别敏感诊断奖励提升诊断准确性。

Result: 大量实验证明，DiaLLM在临床检查推荐和诊断预测上优于现有方法。

Conclusion: DiaLLM有效结合EHR数据和对话模型，为临床流程提供全方位支持，提升了医学大模型在真实医疗场景的实用性。

Abstract: Recent advances in Large Language Models (LLMs) have led to remarkable
progresses in medical consultation. However, existing medical LLMs overlook the
essential role of Electronic Health Records (EHR) and focus primarily on
diagnosis recommendation, limiting their clinical applicability. We propose
DiaLLM, the first medical LLM that integrates heterogeneous EHR data into
clinically grounded dialogues, enabling clinical test recommendation, result
interpretation, and diagnosis prediction to better align with real-world
medical practice. To construct clinically grounded dialogues from EHR, we
design a Clinical Test Reference (CTR) strategy that maps each clinical code to
its corresponding description and classifies test results as "normal" or
"abnormal". Additionally, DiaLLM employs a reinforcement learning framework for
evidence acquisition and automated diagnosis. To handle the large action space,
we introduce a reject sampling strategy to reduce redundancy and improve
exploration efficiency. Furthermore, a confirmation reward and a
class-sensitive diagnosis reward are designed to guide accurate diagnosis
prediction. Extensive experimental results demonstrate that DiaLLM outperforms
baselines in clinical test recommendation and diagnosis prediction.

</details>


### [8] [Case-based Reasoning Augmented Large Language Model Framework for Decision Making in Realistic Safety-Critical Driving Scenarios](https://arxiv.org/abs/2506.20531)
*Wenbin Gan,Minh-Son Dao,Koji Zettsu*

Main category: cs.AI

TL;DR: 本文提出CBR-LLM框架，将案例推理与大模型结合，显著增强了自动驾驶决策的准确性、可解释性和与人类行为的一致性，验证了其应对高风险驾驶环境的实际价值。


<details>
  <summary>Details</summary>
Motivation: 现有大模型在自动驾驶领域的应用受限于情景适应能力不足、缺乏经验性知识与可解释性，难以胜任复杂高风险环境中的决策需求。为此，亟需引入基于过往经验的推理方式增强其实际决策效果。

Method: 提出了一种结合案例推理与大语言模型（CBR-LLM）的框架，将行车记录仪视频语义场景理解与检索相似历史驾驶案例结合，通过检索相关案例引导大模型生成更加符合真实场景和人类行为的规避操作建议。

Result: CBR-LLM在多种开源大模型及多样高风险场景下提升了决策准确性和解释质量，并通过相似案例检索和风险感知提示策略有效优于随机采样，展现出在复杂实际场景中的鲁棒性和适应性。

Conclusion: CBR-LLM框架有效提升了LLM在自动驾驶风险情境下的决策能力、解释性与对人类专家行为的对齐度，展示了其作为智能驾驶决策可信工具的潜力。

Abstract: Driving in safety-critical scenarios requires quick, context-aware
decision-making grounded in both situational understanding and experiential
reasoning. Large Language Models (LLMs), with their powerful general-purpose
reasoning capabilities, offer a promising foundation for such decision-making.
However, their direct application to autonomous driving remains limited due to
challenges in domain adaptation, contextual grounding, and the lack of
experiential knowledge needed to make reliable and interpretable decisions in
dynamic, high-risk environments. To address this gap, this paper presents a
Case-Based Reasoning Augmented Large Language Model (CBR-LLM) framework for
evasive maneuver decision-making in complex risk scenarios. Our approach
integrates semantic scene understanding from dashcam video inputs with the
retrieval of relevant past driving cases, enabling LLMs to generate maneuver
recommendations that are both context-sensitive and human-aligned. Experiments
across multiple open-source LLMs show that our framework improves decision
accuracy, justification quality, and alignment with human expert behavior.
Risk-aware prompting strategies further enhance performance across diverse risk
types, while similarity-based case retrieval consistently outperforms random
sampling in guiding in-context learning. Case studies further demonstrate the
framework's robustness in challenging real-world conditions, underscoring its
potential as an adaptive and trustworthy decision-support tool for intelligent
driving systems.

</details>


### [9] [AI Copilots for Reproducibility in Science: A Case Study](https://arxiv.org/abs/2506.20130)
*Adrien Bibal,Steven N. Minton,Deborah Khider,Yolanda Gil*

Main category: cs.AI

TL;DR: 论文提出的AI平台OpenPub通过自动化工具大幅提升科研成果的可复现性效率，显著节省时间并发现常见障碍，为开放科学提供了有效支持。


<details>
  <summary>Details</summary>
Motivation: 开放科学虽推动科研成果的透明、可访问与可复用性，但实现研究结果独立可复现性仍具较大挑战。该领域需要新工具提高复现效率和准确性。

Method: 提出了OpenPub，一个AI驱动的平台，特别开发了Reproducibility Copilot模块，自动分析论文、代码和补充材料，生成结构化Jupyter Notebooks和可复现性建议。通过已知复现基准的论文进行可行性测试。

Result: OpenPub显著降低了复现所需时间，从30小时缩短至约1小时，并能较高覆盖论文中的图表及结果，检测常见可复现性障碍如缺失超参数、数据集不完整等。

Conclusion: AI工具可大幅减轻科研复现负担，提升研究透明度和可信度，并可扩展到开放科学的更多目标。

Abstract: Open science initiatives seek to make research outputs more transparent,
accessible, and reusable, but ensuring that published findings can be
independently reproduced remains a persistent challenge. This paper introduces
OpenPub, an AI-powered platform that supports researchers, reviewers, and
readers through a suite of modular copilots focused on key open science tasks.
In this work, we present the Reproducibility Copilot, which analyzes
manuscripts, code, and supplementary materials to generate structured Jupyter
Notebooks and recommendations aimed at facilitating computational, or "rote",
reproducibility. We conducted feasibility tests using previously studied
research papers with known reproducibility benchmarks. Results indicate that
OpenPub can substantially reduce reproduction time - from over 30 hours to
about 1 hour - while achieving high coverage of figures, tables, and results
suitable for computational reproduction. The system systematically detects
barriers to reproducibility, including missing hyperparameters, undocumented
preprocessing steps, and incomplete or inaccessible datasets. These findings
suggest that AI-driven tools can meaningfully reduce the burden of
reproducibility efforts and contribute to more transparent and verifiable
scientific communication. The modular copilot architecture also provides a
foundation for extending AI assistance to additional open science objectives
beyond reproducibility.

</details>


### [10] [Language Modeling by Language Models](https://arxiv.org/abs/2506.20249)
*Junyan Cheng,Peter Clark,Kyle Richardson*

Main category: cs.AI

TL;DR: 本文提出了Genesys，一个基于LLM的多智能体系统，通过自动化模拟科研流程和采用遗传编程，有效提升了新语言模型架构的发现和优化，并取得了超越现有模型的性能。


<details>
  <summary>Details</summary>
Motivation: 探索是否能利用大语言模型（LLM）来自动化发现和改进新的语言模型（LM）架构的过程，提升架构创新效率，模拟真实科研流程。

Method: 提出一个多智能体LLM系统Genesys，模拟从创意生成、文献检索、设计实现、预训练到效果验证等科研各阶段。采用“Ladder of Scales”方法，在不同参数规模（14M~350M）下对模型架构进行递进式验证。系统内核为创新的遗传编程方式，替代直接prompt生成方法。

Result: 系统成功发现并验证了1,062个新设计，总共提案1,162个。其中最佳设计在6/9个常用基准上优于现有架构（如GPT2、Mamba2）。遗传编程方法在设计生成上的成功率提升约86个百分点，并展现出其他主流方法不具备的优势。

Conclusion: Genesys多智能体自动化系统显著提升了语言模型架构创新的效率和质量，并为构建更高效的自主发现系统提供了新方法和实证依据。

Abstract: Can we leverage LLMs to model the process of discovering novel language model
(LM) architectures? Inspired by real research, we propose a multi-agent LLM
approach that simulates the conventional stages of research, from ideation and
literature search (proposal stage) to design implementation (code generation),
generative pre-training, and downstream evaluation (verification). Using ideas
from scaling laws, our system, Genesys, employs a Ladder of Scales approach;
new designs are proposed, adversarially reviewed, implemented, and selectively
verified at increasingly larger model scales (14M$\sim$350M parameters) with a
narrowing budget (the number of models we can train at each scale). To help
make discovery efficient and factorizable, Genesys uses a novel genetic
programming backbone, which we show has empirical advantages over commonly used
direct prompt generation workflows (e.g., $\sim$86\% percentage point
improvement in successful design generation, a key bottleneck). We report
experiments involving 1,162 newly discovered designs (1,062 fully verified
through pre-training) and find the best designs to be highly competitive with
known architectures (e.g., outperform GPT2, Mamba2, etc., on 6/9 common
benchmarks). We couple these results with comprehensive system-level ablations
and formal results, which give broader insights into the design of effective
autonomous discovery systems.

</details>


### [11] [Enterprise Large Language Model Evaluation Benchmark](https://arxiv.org/abs/2506.20274)
*Liya Wang,David Yi,Damien Jose,John Passarelli,James Gao,Jordan Leventis,Kang Li*

Main category: cs.AI

TL;DR: 本研究提出并实现了一个更适合企业场景的LLM能力评测体系，不仅构建了大规模高质量基准，还发现了开源与专有模型在不同任务上的优劣和性能短板，为企业LLM部署和优化提供具体参考。


<details>
  <summary>Details</summary>
Motivation: 现有通用基准（如MMLU）无法充分反映企业级任务的复杂性和实际需求，企业需要更贴近实际的评测方法去优化和选型LLM模型。

Method: 结合布鲁姆认知分类学，设计14类企业相关任务，通过引入LLM标注、LLM评判以及纠错检索增强生成(CRAG)，构建9700个样本的强健基准。对6个主流大模型进行横向评测。

Result: 开源模型在推理类任务上表现接近甚至媲美专有模型，但在判断型任务上表现不足，存在“过度思考”问题。所提出基准揭示了模型在企业应用中的关键性能差距。

Conclusion: 该研究提出的企业级任务评测框架可以更全面地识别和分析大语言模型在实际企业场景中的表现短板，为企业落地应用和模型优化提供了可操作性建议。

Abstract: Large Language Models (LLMs) ) have demonstrated promise in boosting
productivity across AI-powered tools, yet existing benchmarks like Massive
Multitask Language Understanding (MMLU) inadequately assess enterprise-specific
task complexities. We propose a 14-task framework grounded in Bloom's Taxonomy
to holistically evaluate LLM capabilities in enterprise contexts. To address
challenges of noisy data and costly annotation, we develop a scalable pipeline
combining LLM-as-a-Labeler, LLM-as-a-Judge, and corrective retrieval-augmented
generation (CRAG), curating a robust 9,700-sample benchmark. Evaluation of six
leading models shows open-source contenders like DeepSeek R1 rival proprietary
models in reasoning tasks but lag in judgment-based scenarios, likely due to
overthinking. Our benchmark reveals critical enterprise performance gaps and
offers actionable insights for model optimization. This work provides
enterprises a blueprint for tailored evaluations and advances practical LLM
deployment.

</details>


### [12] [Mobile-R1: Towards Interactive Reinforcement Learning for VLM-Based Mobile Agent via Task-Level Rewards](https://arxiv.org/abs/2506.20332)
*Jihao Gu,Qihang Ai,Yingyao Wang,Pi Bu,Jingxuan Xing,Zekun Zhu,Wei Jiang,Ziming Wang,Yingxiu Zhao,Ming-Liang Zhang,Jun Song,Yuning Jiang,Bo Zheng*

Main category: cs.AI

TL;DR: Mobile-R1通过多阶段训练和任务级奖励机制，大幅提升了基于视觉-语言模型的移动智能体的探索和错误修正能力，并发布了大规模中文应用数据集和新基准。


<details>
  <summary>Details</summary>
Motivation: 当前基于视觉-语言模型的移动智能体在理解复杂指令和移动端截图、动作输出方面虽然取得进展，但现有方法多采用离线强化学习或仅基于动作级奖励的在线优化。这导致智能体缺乏动态交互能力，容易陷入局部最优，探索与错误修正能力不足。

Method: 提出Mobile-R1方法，通过多轮交互式强化学习和基于任务级奖励的训练框架，流程分为三阶段：初始格式微调、基于动作级奖励的单步在线训练，随后基于多轮轨迹的任务级奖励在线训练。

Result: Mobile-R1显著提升了探索能力与错误动作修正能力，性能上取得了较大提升。此外，作者还收集了涵盖28个中文App、共24,521条高质量人工标注的数据集，并建立了包含500条轨迹的新基准。

Conclusion: Mobile-R1通过多轮互动与任务级奖励机制，有效克服了传统方法的局部最优困境，加强了移动智能体探索与自我修正能力。相关资源已开源。

Abstract: Vision-language model-based mobile agents have gained the ability to not only
understand complex instructions and mobile screenshots, but also optimize their
action outputs via thinking and reasoning, benefiting from reinforcement
learning, such as Group Relative Policy Optimization (GRPO). However, existing
research centers on offline reinforcement learning training or online
optimization using action-level rewards, which limits the agent's dynamic
interaction with the environment. This often results in agents settling into
local optima, thereby weakening their ability for exploration and error action
correction. To address these challenges, we introduce an approach called
Mobile-R1, which employs interactive multi-turn reinforcement learning with
task-level rewards for mobile agents. Our training framework consists of three
stages: initial format finetuning, single-step online training via action-level
reward, followed by online training via task-level reward based on multi-turn
trajectories. This strategy is designed to enhance the exploration and error
correction capabilities of Mobile-R1, leading to significant performance
improvements. Moreover, we have collected a dataset covering 28 Chinese
applications with 24,521 high-quality manual annotations and established a new
benchmark with 500 trajectories. We will open source all resources, including
the dataset, benchmark, model weight, and codes:
https://mobile-r1.github.io/Mobile-R1/.

</details>


### [13] [Tabular Feature Discovery With Reasoning Type Exploration](https://arxiv.org/abs/2506.20357)
*Sungwon Han,Sungkyu Park,Seungeon Lee*

Main category: cs.AI

TL;DR: REFeat方法通过引入多类型推理指导LLM生成特征，提升了表格数据预测准确性和特征多样性，验证了推理范式整合在特征工程中的潜力。


<details>
  <summary>Details</summary>
Motivation: 表格数据的特征工程是机器学习中的关键难题，当前主流的基于大语言模型（LLM）的方法生成的特征过于简单或重复，缺乏结构化推理的指导。

Method: 提出了一种新方法REFeat，通过引入多种推理方式，指导LLM生成多样且有信息量的特征，从而优化特征生成流程。

Result: 在59个基准数据集上的实验显示，REFeat不仅平均预测准确率更高，还能发现更多样且有意义的特征。

Conclusion: 结合丰富的推理范式和自适应策略，能显著提升LLM驱动的表格数据特征发现的效果。

Abstract: Feature engineering for tabular data remains a critical yet challenging step
in machine learning. Recently, large language models (LLMs) have been used to
automatically generate new features by leveraging their vast knowledge.
However, existing LLM-based approaches often produce overly simple or
repetitive features, partly due to inherent biases in the transformations the
LLM chooses and the lack of structured reasoning guidance during generation. In
this paper, we propose a novel method REFeat, which guides an LLM to discover
diverse and informative features by leveraging multiple types of reasoning to
steer the feature generation process. Experiments on 59 benchmark datasets
demonstrate that our approach not only achieves higher predictive accuracy on
average, but also discovers more diverse and meaningful features. These results
highlight the promise of incorporating rich reasoning paradigms and adaptive
strategy selection into LLM-driven feature discovery for tabular data.

</details>


### [14] [Paladin-mini: A Compact and Efficient Grounding Model Excelling in Real-World Scenarios](https://arxiv.org/abs/2506.20384)
*Dror Ivry,Oran Nahum*

Main category: cs.AI

TL;DR: 提出了Paladin-mini模型和grounding-benchmark新数据集，提升了论点有据性判断的效果，为相关任务提供了先进工具和评价标准。


<details>
  <summary>Details</summary>
Motivation: 当前在给定上下文中验证论点是否有证据支持（grounding）的问题尚未得到充分解决，缺乏高效的分类模型与评测集。

Method: 作者提出了Paladin-mini——一个3.8B参数的紧凑型开源模型，用于对数据进行“有证据支持”或“无证据支持”两类标注，并构建了新的grounding-benchmark评测数据集，针对关键推理任务设计。

Result: Paladin-mini模型在grounding-benchmark上取得了优异表现，与现有最先进方法进行了对比，并提供了结果的可复现性。

Conclusion: Paladin-mini模型和grounding-benchmark提升了论点grounding任务的效果与评测标准，为实际关键推理任务提供了强有力的工具和新基准。

Abstract: This paper introduces two significant contributions to address the issue of
grounding claims in a given context. Grounding means that given a context
(document) and a claim, there's at least one supportive evidence for the claim
in the document. We will introduce Paladin-mini, a compact (3.8B parameters)
open-source classifier model (used for labeling data as grounded or ungrounded)
engineered for robust performance in real-world scenarios, and the
grounding-benchmark, a new evaluation dataset designed to assess performance on
critical reasoning tasks. We'll also demonstrate the results of Paladin-mini
with benchmarks against the current State-of-the-art and share clear and
reproducible results.

</details>


### [15] [Smart Ride and Delivery Services with Electric Vehicles: Leveraging Bidirectional Charging for Profit Optimisation](https://arxiv.org/abs/2506.20401)
*Jinchun Du,Bojie Shen,Muhammad Aamir Cheema,Adel N. Toosi*

Main category: cs.AI

TL;DR: 本文提出并求解了考虑车网互动的电动车路径与充/放电优化问题（EVOP-V2G），利用混合整数规划和两种元启发算法提升了司机利润，并在实验中展示了方案的效益和可扩展性，为未来智能化、盈利性更强的电动车移动服务提供了新思路。


<details>
  <summary>Details</summary>
Motivation: 随着电动汽车（EVs）普及，现代服务系统（如网约车、配送服务）越来越多地整合电动汽车。然而，电动车续航较短，需要合理安排充电。随着车网互动（V2G）技术的发展，电动车还能向电网回馈能量，带来新的机会和复杂性。

Method: 将问题建模为电动车路径选择与车辆-电网互动问题（EVOP-V2G），该问题旨在利润最大化，涉及订单选择、充/放电计划、动态电价、充电站选择和路径约束。作者提出混合整数规划模型（MIP），并设计了两种近似最优的元启发算法：进化算法（EA）和大邻域搜索（LNS）。

Result: 在真实数据集上的实验结果显示，所提算法相比基线方法能将司机利润提升一倍，在小规模实例上接近最优，在大规模实例上表现出优良的可扩展性。

Conclusion: 本研究证明，通过有效地结合订单分派、充/放电策略和路径优化及车网互动，可以显著提升电动车移动服务系统的经济效益，并对支持电网具有积极作用。

Abstract: With the rising popularity of electric vehicles (EVs), modern service
systems, such as ride-hailing delivery services, are increasingly integrating
EVs into their operations. Unlike conventional vehicles, EVs often have a
shorter driving range, necessitating careful consideration of charging when
fulfilling requests. With recent advances in Vehicle-to-Grid (V2G) technology -
allowing EVs to also discharge energy back to the grid - new opportunities and
complexities emerge. We introduce the Electric Vehicle Orienteering Problem
with V2G (EVOP-V2G): a profit-maximization problem where EV drivers must select
customer requests or orders while managing when and where to charge or
discharge. This involves navigating dynamic electricity prices, charging
station selection, and route constraints. We formulate the problem as a Mixed
Integer Programming (MIP) model and propose two near-optimal metaheuristic
algorithms: one evolutionary (EA) and the other based on large neighborhood
search (LNS). Experiments on real-world data show our methods can double driver
profits compared to baselines, while maintaining near-optimal performance on
small instances and excellent scalability on larger ones. Our work highlights a
promising path toward smarter, more profitable EV-based mobility systems that
actively support the energy grid.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [16] [CycleDistill: Bootstrapping Machine Translation using LLMs with Cyclical Distillation](https://arxiv.org/abs/2506.19952)
*Deepon Halder,Thanmay Jayakumar,Raj Dabre*

Main category: cs.CL

TL;DR: 本文针对低资源机器翻译提出了CycleDistill方法，通过LLMs和少量样本自举生成伪平行语料，显著提升了翻译质量，无需大量平行语料即可取得优异效果，在三种印度语言上获得超过20-30 chrF的提升，具有良好实用前景。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在少样本机器翻译方面虽然有潜力，但与基于大规模平行语料专门训练的传统机器翻译系统相比仍有差距。而对于低资源语言，高质量的平行语料往往稀缺甚至不存在，因此如何利用有限的平行语料或无监督数据提高低资源机器翻译表现成为一个关键问题。

Method: 提出了CycleDistill，一种迭代式自举训练方法。其核心是：利用LLMs进行零样本或少样本翻译，从单语语料中合成伪平行语料，然后用这些数据对原有模型进行微调。整个过程重复进行，无需大量平行语料，仅需1到4个少样本例子即可启动。此外，研究还考察了在蒸馏过程中引入softmax激活对翻译质量的影响。

Result: 在三种印度语言上的实验表明，CycleDistill仅依赖单语语料，第一轮就能将基线模型的chrF分数平均提升20-30分。此外，通过分析softmax激活引入带来轻微的翻译质量提升。

Conclusion: CycleDistill是一种有效的、几乎不依赖平行语料的新型低资源机器翻译训练方法，在实测中获得了显著的性能提升，对低资源语言机器翻译有重要参考价值。

Abstract: Large language models (LLMs), despite their ability to perform few-shot
machine translation (MT), often lag behind dedicated MT systems trained on
parallel corpora, which are crucial for high quality machine translation (MT).
However, parallel corpora are often scarce or non-existent for low-resource
languages. In this paper, we propose CycleDistill, a bootstrapping approach
leveraging LLMs and few-shot translation to obtain high-quality MT systems.
CycleDistill involves iteratively generating synthetic parallel corpora from
monolingual corpora via zero- or few-shot MT, which is then used to fine-tune
the model that was used for generating said data for MT. CycleDistill does not
need parallel corpora beyond 1 to 4 few-shot examples, and in our experiments
focusing on three Indian languages, by relying solely on monolingual corpora,
it can achieve high-quality machine translation, improving upon a few-shot
baseline model by over 20-30 chrF points on average in the first iteration. We
also study the effect of leveraging softmax activations during the distillation
process and observe mild improvements in translation quality.

</details>


### [17] [Inference Scaled GraphRAG: Improving Multi Hop Question Answering on Knowledge Graphs](https://arxiv.org/abs/2506.19967)
*Travis Thompson,Seung-Hwan Lim,Paul Liu,Ruoying He,Dongkuan Xu*

Main category: cs.CL

TL;DR: 本文提出一种能够在推理阶段可扩展的图结构增强RAG方法，大幅提升LLMs多步复杂知识问答能力，实验验证其优越性，对结构化推理有重要意义。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在理解和生成自然语言方面表现出色，但在需要多步推理和结构化知识的任务中仍有不足，现有RAG和GraphRAG方法难以充分利用知识图谱中的关系结构。

Method: 提出了一种新型推理阶段可扩展图结构RAG（Inference-Scaled GraphRAG）框架。该方法在推理时采用序列扩展与深度chain-of-thought图遍历，以及并行扩展与多数采样轨迹投票，实现在交错推理-执行循环下的更优图推理能力。

Result: 在GRBench基准集上的实验结果表明，本文方法相比于传统GraphRAG及现有图遍历基线，在多步问答性能上具有显著提升。

Conclusion: 推理阶段可扩展是一种高效且与模型结构无关的解决方案，可提升大型语言模型在结构化知识推理场景下的效果。

Abstract: Large Language Models (LLMs) have achieved impressive capabilities in
language understanding and generation, yet they continue to underperform on
knowledge-intensive reasoning tasks due to limited access to structured context
and multi-hop information. Retrieval-Augmented Generation (RAG) partially
mitigates this by grounding generation in retrieved context, but conventional
RAG and GraphRAG methods often fail to capture relational structure across
nodes in knowledge graphs. We introduce Inference-Scaled GraphRAG, a novel
framework that enhances LLM-based graph reasoning by applying inference-time
compute scaling. Our method combines sequential scaling with deep
chain-of-thought graph traversal, and parallel scaling with majority voting
over sampled trajectories within an interleaved reasoning-execution loop.
Experiments on the GRBench benchmark demonstrate that our approach
significantly improves multi-hop question answering performance, achieving
substantial gains over both traditional GraphRAG and prior graph traversal
baselines. These findings suggest that inference-time scaling is a practical
and architecture-agnostic solution for structured knowledge reasoning with LLMs

</details>


### [18] [Doc2Agent: Scalable Generation of Tool-Using Agents from API Documentation](https://arxiv.org/abs/2506.19998)
*Xinyi Ni,Haonan Jian,Qiuyang Wang,Vedanshi Chetan Shah,Pengyu Hong*

Main category: cs.CL

TL;DR: Doc2Agent自动从API文档生成可用Python工具，并反复优化，能实现高效、低成本地为任意领域构建API工具型智能体，提升系统性能且适应复杂领域任务。


<details>
  <summary>Details</summary>
Motivation: 现有依赖API的智能体通常基于人工挑选且统一的工具集，这与真实世界API的复杂性并不匹配。要在任意领域构建能够灵活调用工具的智能体，需面对理解非结构化API文档、测试API及推断正确参数等重大挑战。

Method: 提出了Doc2Agent，一种可扩展的流程，能够从API文档生成可执行的Python工具，并通过代码代理进行迭代优化。这一流程自动解析API文档，生成工具，并反复改进，最终产出经验证可用的工具。

Result: 在真实世界API、WebArena API及科研API上进行了测试，能稳定生成可用工具。相比WebArena基准直接API调用，性能提升55%，成本降低90%。上述流程还成功适配到糖材料科学等知识密集型复杂领域，表现出很强的通用性。

Conclusion: Doc2Agent为从非结构化API文档大规模自动构建工具型智能体提供了一种通用型解决方案，既提升了性能又降低了成本。

Abstract: REST APIs play important roles in enriching the action space of web agents,
yet most API-based agents rely on curated and uniform toolsets that do not
reflect the complexity of real-world APIs. Building tool-using agents for
arbitrary domains remains a major challenge, as it requires reading
unstructured API documentation, testing APIs and inferring correct parameters.
We propose Doc2Agent, a scalable pipeline to build agents that can call
Python-based tools generated from API documentation. Doc2Agent generates
executable tools from API documentations and iteratively refines them using a
code agent. We evaluate our approach on real-world APIs, WebArena APIs, and
research APIs, producing validated tools. We achieved a 55\% relative
performance improvement with 90\% lower cost compared to direct API calling on
WebArena benchmark. A domain-specific agent built for glycomaterial science
further demonstrates the pipeline's adaptability to complex, knowledge-rich
tasks. Doc2Agent offers a generalizable solution for building tool agents from
unstructured API documentation at scale.

</details>


### [19] [A Modular Multitask Reasoning Framework Integrating Spatio-temporal Models and LLMs](https://arxiv.org/abs/2506.20073)
*Kethmi Hirushini Hettige,Jiahao Ji,Cheng Long,Shili Xiang,Gao Cong,Jingyuan Wang*

Main category: cs.CL

TL;DR: 本文提出的STReason框架结合了大模型与时空数据模型优势，无需微调地实现多任务推理与详细解释输出，取得了优异的实验和人工评测结果，具备实际应用前景。


<details>
  <summary>Details</summary>
Motivation: 现有的时空数据挖掘模型只适合窄任务，缺乏多任务推理和复杂深度推理的能力，难以生成解释性强的输出，限制了其在实际多面向决策中的应用。

Method: 提出了名为STReason的框架，将大语言模型（LLM）的推理能力与时空建模分析能力结合，无需特定任务微调，利用In-context learning自动将复杂自然语言问题拆解为可解释的模块化程序并执行，生成解答和详细推理。同时，构建了新数据集并提出统一评价指标体系用于长文本时空推理任务的评测。

Result: 实验表明，STReason在所有评测指标上显著优于先进的大语言模型基线，在复杂、推理型强的时空推理任务上表现尤为突出。人工评测也表明STReason具备较高的可信度和实用性，有助于减少专家工作量，拓展实际应用场景。

Conclusion: STReason为发展更通用、更强时空推理能力系统提供了新方向，拓宽了其实际应用范围。

Abstract: Spatio-temporal data mining plays a pivotal role in informed decision making
across diverse domains. However, existing models are often restricted to narrow
tasks, lacking the capacity for multi-task inference and complex long-form
reasoning that require generation of in-depth, explanatory outputs. These
limitations restrict their applicability to real-world, multi-faceted decision
scenarios. In this work, we introduce STReason, a novel framework that
integrates the reasoning strengths of large language models (LLMs) with the
analytical capabilities of spatio-temporal models for multi-task inference and
execution. Without requiring task-specific finetuning, STReason leverages
in-context learning to decompose complex natural language queries into modular,
interpretable programs, which are then systematically executed to generate both
solutions and detailed rationales. To facilitate rigorous evaluation, we
construct a new benchmark dataset and propose a unified evaluation framework
with metrics specifically designed for long-form spatio-temporal reasoning.
Experimental results show that STReason significantly outperforms advanced LLM
baselines across all metrics, particularly excelling in complex,
reasoning-intensive spatio-temporal scenarios. Human evaluations further
validate STReason's credibility and practical utility, demonstrating its
potential to reduce expert workload and broaden the applicability to real-world
spatio-temporal tasks. We believe STReason provides a promising direction for
developing more capable and generalizable spatio-temporal reasoning systems.

</details>


### [20] [SACL: Understanding and Combating Textual Bias in Code Retrieval with Semantic-Augmented Reranking and Localization](https://arxiv.org/abs/2506.20081)
*Dhruv Gupta,Gayathri Ganesh Lakshmy,Yiqing Xie*

Main category: cs.CL

TL;DR: 论文通过分析代码检索模型表层特征依赖，提出SACL框架丰富语义信息，显著提升多任务下的代码检索和生成效果。


<details>
  <summary>Details</summary>
Motivation: 现有的代码检索方法虽然训练于代码本身，但主要依赖于表层的文本特征（如文档字符串和标识符名称），且对文档完善但内容无关的代码有强烈偏好，这影响了检索和代码生成的效果。为了解决表层特征依赖和文档偏见问题，作者希望改进代码检索和生成能力。

Method: 系统性地对代码进行特征遮蔽实验，分析当前代码检索模型依赖哪些信息。基于实验，提出了SACL框架，通过增加语义信息丰富文本内容，减少模型对无关文档的偏见，增强结构化和语义知识的表达。

Result: SACL在多个代码检索基准（HumanEval/MBPP/SWE-Bench-Lite）上，Recall@1提升了12.8%/9.4%/7.0%；同时推动了代码生成性能提升，在HumanEval数据集上Pass@1提高4.88%。

Conclusion: 当前代码检索模型过度依赖表层文本特征且对文档偏见明显。SACL通过增强语义信息，有效提升了代码检索和生成性能，降低了对表层特征的依赖。

Abstract: Retrieval-Augmented Code Generation (RACG) is a critical technique for
enhancing code generation by retrieving relevant information. In this work, we
conduct an in-depth analysis of code retrieval by systematically masking
specific features while preserving code functionality. Our discoveries include:
(1) although trained on code, current retrievers heavily rely on surface-level
textual features (e.g., docstrings, identifier names), and (2) they exhibit a
strong bias towards well-documented code, even if the documentation is
irrelevant.Based on our discoveries, we propose SACL, a framework that enriches
textual information and reduces bias by augmenting code or structural knowledge
with semantic information. Extensive experiments show that SACL substantially
improves code retrieval (e.g., by 12.8% / 9.4% / 7.0% Recall@1 on HumanEval /
MBPP / SWE-Bench-Lite), which also leads to better code generation performance
(e.g., by 4.88% Pass@1 on HumanEval).

</details>


### [21] [Bridging Compositional and Distributional Semantics: A Survey on Latent Semantic Geometry via AutoEncoder](https://arxiv.org/abs/2506.20083)
*Yingji Zhang,Danilo S. Carvalho,André Freitas*

Main category: cs.CL

TL;DR: 本文系统性评测了VAE、VQVAE和SAE三种自编码器架构在语义表征学习中的潜力，为提升语言模型的可解释性和泛化能力提供了新思路。


<details>
  <summary>Details</summary>
Motivation: 当前分布式语义空间在解释性、可控性和泛化方面存在不足，期望通过融合组合性和符号属性来弥合分布式与符号语义之间的差距。

Method: 综述和比较三种主流自编码器架构（VAE、VQVAE、SAE），分析它们在提升语义结构和可解释性方面的表现。

Result: 展示三种自编码器诱导的不同潜在语义空间结构，对每种方法提升语义可解释性的效果进行了比较，提供了理论支持以增强现有语言模型性能。

Conclusion: 通过引入组合性和符号属性，可以提升基于Transformer的自回归语言模型的可解释性、可控性、组合性和泛化能力。本文通过语义空间几何的视角，展现了语义表示学习的新方向，并对主流自编码器架构及其在语义结构和可解释性上的表现进行了比较。

Abstract: Integrating compositional and symbolic properties into current distributional
semantic spaces can enhance the interpretability, controllability,
compositionality, and generalisation capabilities of Transformer-based
auto-regressive language models (LMs). In this survey, we offer a novel
perspective on latent space geometry through the lens of compositional
semantics, a direction we refer to as \textit{semantic representation
learning}. This direction enables a bridge between symbolic and distributional
semantics, helping to mitigate the gap between them. We review and compare
three mainstream autoencoder architectures-Variational AutoEncoder (VAE),
Vector Quantised VAE (VQVAE), and Sparse AutoEncoder (SAE)-and examine the
distinctive latent geometries they induce in relation to semantic structure and
interpretability.

</details>


### [22] [ITFormer: Bridging Time Series and Natural Language for Multi-Modal QA with Large-Scale Multitask Dataset](https://arxiv.org/abs/2506.20093)
*Yilin Wang,Peixuan Lei,Jie Song,Yuzhe Hao,Tao Chen,Yuxuan Zhang,Lei Jia,Yuanxiang Li,Zhongyu Wei*

Main category: cs.CL

TL;DR: 本文针对时序数据与自然语言的整合难题，引入了时序问答新任务，并发布了对应大规模数据集；提出的ITFormer框架在大幅提升准确率的同时实现高效、低参数融合，为多模态AI开辟了新方向。


<details>
  <summary>Details</summary>
Motivation: 时序数据与自然语言的高效集成对于动态、交互式任务极具挑战性，目前在多模态AI领域尚未有很好的解决方案。

Method: 提出了一种新任务——时序问答（Time-Series QA），并发布了首个大规模多任务时序-文本QA数据集EngineMT-QA。基于该数据集，作者提出了ITFormer框架，将时序编码器与冻结的大型语言模型对接，并通过特征抽取、对齐和融合，实现时序与文本信息的整合。

Result: ITFormer在问答准确率上，相较于强基线模型，有显著提升，且新增可训练参数少于1%。方法兼具计算高效性与跨模态建模能力。

Conclusion: 该研究提出的ITFormer为时序数据与自然语言融合提供了新的高效范式，为多模态AI领域带来了新的研究和应用前景。

Abstract: Time-series data are critical in diverse applications, such as industrial
monitoring, medical diagnostics, and climate research. However, effectively
integrating these high-dimensional temporal signals with natural language for
dynamic, interactive tasks remains a significant challenge. To address this, we
introduce the Time-Series Question Answering (Time-Series QA) task and release
EngineMT-QA, the first large-scale, multi-task, temporal-textual QA dataset
designed to capture complex interactions between time-series signals and
natural language. Building on this resource, we propose the Instruct Time
Transformer (ITFormer), a novel framework that bridges time-series encoders
with frozen large language models (LLMs). ITFormer effectively extracts,
aligns, and fuses temporal and textual features, achieving a strong improvement
in QA accuracy over strong baselines with fewer than 1\% additional trainable
parameters. By combining computational efficiency with robust cross-modal
modeling, our work establishes a adaptable paradigm for integrating temporal
data with natural language, paving the way for new research and applications in
multi-modal AI. More details about the project, including datasets and code,
are available at: https://pandalin98.github.io/itformer_site/

</details>


### [23] [A Multi-Pass Large Language Model Framework for Precise and Efficient Radiology Report Error Detection](https://arxiv.org/abs/2506.20112)
*Songsoo Kim,Seungtae Lee,See Young Lee,Joonho Kim,Keechan Kan,Dukyong Yoon*

Main category: cs.CL

TL;DR: 三通道大语言模型校对框架显著提升放射学报告校准确率、降低人工及模型成本，性能优于传统单/双通道方法，且检测能力不打折。


<details>
  <summary>Details</summary>
Motivation: 目前基于大语言模型（LLM）对放射学报告进行校对的正预测值（PPV）较低，主要受限于真实错误率低。需要探索新方法提升PPV并降低人工审核等运营成本。

Method: 对MIMIC-III数据库中1,000份连续放射学报告（X光、超声、CT、MRI各250份）进行回顾性分析，使用三种LLM框架：（1）单提示检测器；（2）提取器+检测器；（3）提取器+检测器+假阳性验证器。以CheXpert和Open-i外部数据集作验证。用PPV和绝对真阳性率（aTPR）评价性能，计算模型推理和人工审核成本，并用自助法、McNemar检验和Holm-Bonferroni校正做统计分析。

Result: 三通道框架（Framework 3）PPV从0.063提升到0.159，显著优于前两种方法，aTPR基本持平。每千份报告运营成本从9.72美元降至5.58美元，人工审核报告数从192降到88。外部数据集验证三通道PPV优势，aTPR依然稳定。

Conclusion: 三通道LLM框架显著提升了放射学报告校对的PPV并降低运营成本，同时检出率未降低，为AI辅助报告质量控制提供有效方案。

Abstract: Background: The positive predictive value (PPV) of large language model
(LLM)-based proofreading for radiology reports is limited due to the low error
prevalence. Purpose: To assess whether a three-pass LLM framework enhances PPV
and reduces operational costs compared with baseline approaches. Materials and
Methods: A retrospective analysis was performed on 1,000 consecutive radiology
reports (250 each: radiography, ultrasonography, CT, MRI) from the MIMIC-III
database. Two external datasets (CheXpert and Open-i) were validation sets.
Three LLM frameworks were tested: (1) single-prompt detector; (2) extractor
plus detector; and (3) extractor, detector, and false-positive verifier.
Precision was measured by PPV and absolute true positive rate (aTPR).
Efficiency was calculated from model inference charges and reviewer
remuneration. Statistical significance was tested using cluster bootstrap,
exact McNemar tests, and Holm-Bonferroni correction. Results: Framework PPV
increased from 0.063 (95% CI, 0.036-0.101, Framework 1) to 0.079 (0.049-0.118,
Framework 2), and significantly to 0.159 (0.090-0.252, Framework 3; P<.001 vs.
baselines). aTPR remained stable (0.012-0.014; P>=.84). Operational costs per
1,000 reports dropped to USD 5.58 (Framework 3) from USD 9.72 (Framework 1) and
USD 6.85 (Framework 2), reflecting reductions of 42.6% and 18.5%, respectively.
Human-reviewed reports decreased from 192 to 88. External validation supported
Framework 3's superior PPV (CheXpert 0.133, Open-i 0.105) and stable aTPR
(0.007). Conclusion: A three-pass LLM framework significantly enhanced PPV and
reduced operational costs, maintaining detection performance, providing an
effective strategy for AI-assisted radiology report quality assurance.

</details>


### [24] [Leveraging AI Graders for Missing Score Imputation to Achieve Accurate Ability Estimation in Constructed-Response Tests](https://arxiv.org/abs/2506.20119)
*Masaki Uto,Yuma Ito*

Main category: cs.CL

TL;DR: 本文提出用自动评分技术补全缺失答题分数，从而提升基于IRT模型的能力测量准确性，并大幅降低人工阅卷负担。


<details>
  <summary>Details</summary>
Motivation: 随着教育领域对评估高阶能力（如表达能力和逻辑思维）的需求增长，开放性问答如简答题和作文题被广泛使用，但人工阅卷既耗时又成本高。IRT（项目反应理论）在部分评分缺失情况下可以推断能力，但缺失比例增加时效果变差，因此亟需提升IRT在缺失数据条件下的测量准确性。

Method: 提出了一种创新方法，利用自动评分技术对缺失分数进行补全，以便于更准确地用IRT模型进行能力估计，从而显著减少人工评分工作量。该方法与现有的数据增强技术相比，能更好处理稀疏或异质化数据。

Result: 该方法能在大幅减少人工评分的基础上，实现高精度的能力估计，有效克服了现有数据增强方法在稀疏或异质化数据下的准确性不足问题。

Conclusion: 结合自动评分技术与IRT理论，能在减轻人工负担的同时，显著提升高阶能力评估的准确性。

Abstract: Evaluating the abilities of learners is a fundamental objective in the field
of education. In particular, there is an increasing need to assess higher-order
abilities such as expressive skills and logical thinking. Constructed-response
tests such as short-answer and essay-based questions have become widely used as
a method to meet this demand. Although these tests are effective, they require
substantial manual grading, making them both labor-intensive and costly. Item
response theory (IRT) provides a promising solution by enabling the estimation
of ability from incomplete score data, where human raters grade only a subset
of answers provided by learners across multiple test items. However, the
accuracy of ability estimation declines as the proportion of missing scores
increases. Although data augmentation techniques for imputing missing scores
have been explored in order to address this limitation, they often struggle
with inaccuracy for sparse or heterogeneous data. To overcome these challenges,
this study proposes a novel method for imputing missing scores by leveraging
automated scoring technologies for accurate IRT-based ability estimation. The
proposed method achieves high accuracy in ability estimation while markedly
reducing manual grading workload.

</details>


### [25] [CCRS: A Zero-Shot LLM-as-a-Judge Framework for Comprehensive RAG Evaluation](https://arxiv.org/abs/2506.20128)
*Aashiq Muhamed*

Main category: cs.CL

TL;DR: 本文提出CCRS评价体系，用单一大模型零样本端到端评估RAG系统，对比现有方法更高效且判别力更强，能为实际的RAG系统优化提供有力工具。


<details>
  <summary>Details</summary>
Motivation: RAG系统通过引入外部知识提升大模型的表现，尤其在需要事实准确性和最新信息的领域非常关键。但现有RAG系统输出的评估方法要么只依赖简单的词汇重叠指标，无法细致反映多维度质量；要么流程复杂、需要专门的模型微调，实用性较差。

Method: 提出CCRS评价体系，基于单个强大的预训练大模型，零样本自动评判RAG输出，从五个维度（上下文连贯性、问题相关性、信息密度、答案正确性、信息召回）全面打分，并在BioASQ数据集下对六种RAG配置系统进行分析比较。

Result: CCRS能有效区分不同RAG系统表现，如Mistral-7B在部分指标优于Llama系列，并且CCRS与现有复杂评测如RAGChecker相比，在关键指标（如召回率和事实性）表现出可比甚至更优判别力，同时计算效率大幅提升。

Conclusion: CCRS是一个实用、高效、全面的RAG系统自动评测框架，无需复杂中间流程或模型微调，提升了实际评估和优化RAG系统的便捷性和效果。

Abstract: RAG systems enhance LLMs by incorporating external knowledge, which is
crucial for domains that demand factual accuracy and up-to-date information.
However, evaluating the multifaceted quality of RAG outputs, spanning aspects
such as contextual coherence, query relevance, factual correctness, and
informational completeness, poses significant challenges. Existing evaluation
methods often rely on simple lexical overlap metrics, which are inadequate for
capturing these nuances, or involve complex multi-stage pipelines with
intermediate steps like claim extraction or require finetuning specialized
judge models, hindering practical efficiency. To address these limitations, we
propose CCRS (Contextual Coherence and Relevance Score), a novel suite of five
metrics that utilizes a single, powerful, pretrained LLM as a zero-shot,
end-to-end judge. CCRS evaluates: Contextual Coherence (CC), Question Relevance
(QR), Information Density (ID), Answer Correctness (AC), and Information Recall
(IR). We apply CCRS to evaluate six diverse RAG system configurations on the
challenging BioASQ dataset. Our analysis demonstrates that CCRS effectively
discriminates between system performances, confirming, for instance, that the
Mistral-7B reader outperforms Llama variants. We provide a detailed analysis of
CCRS metric properties, including score distributions, convergent/discriminant
validity, tie rates, population statistics, and discriminative power. Compared
to the complex RAGChecker framework, CCRS offers comparable or superior
discriminative power for key aspects like recall and faithfulness, while being
significantly more computationally efficient. CCRS thus provides a practical,
comprehensive, and efficient framework for evaluating and iteratively improving
RAG systems.

</details>


### [26] [AALC: Large Language Model Efficient Reasoning via Adaptive Accuracy-Length Control](https://arxiv.org/abs/2506.20160)
*Ruosen Li,Ziming Luo,Quan Zhang,Ruochen Li,Ben Zhou,Ali Payani,Xinya Du*

Main category: cs.CL

TL;DR: AALC是一种结合准确率奖励和动态长度惩罚的强化学习方法，可在维持甚至提升准确率的同时让模型输出大幅缩短，提升推理效率，并减少冗余推理步骤，但可解释性略有降低。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）通过生成冗长的推理链，获得了强大的推理能力，但这种“过度思考”带来了高延迟和高成本，同时并未带来相应的准确率提升。因此，提升推理模型效率成为了亟需解决的问题。

Method: 提出AALC方法——一种轻量级、基于准确率的响应长度奖励机制，并集成到强化学习训练中。AALC结合了验证准确率和一个平滑、动态调整的长度惩罚，在模型达到目标性能前推迟长度惩罚的应用，从而动态平衡正确性与简洁性。

Result: 在标准及分布外数学基准测试中，AALC将模型响应长度减少了50%以上，同时保持或提升了模型原有的准确率。输出结构更精炼，去除了冗余推理模式如过多子目标设定和验证。

Conclusion: 奖励机制可有效引导大型推理模型生成更高效、可泛化的推理路径，但也会减少输出的解释性和叙述上下文。

Abstract: Large reasoning models (LRMs) achieve impressive reasoning capabilities by
generating lengthy chain-of-thoughts, but this "overthinking" incurs high
latency and cost without commensurate accuracy gains. In this work, we
introduce AALC, a lightweight, accuracy-aware length reward integrated into
reinforcement learning that dynamically balances correctness and brevity during
training. By incorporating validation accuracy into the reward and employing a
smooth, dynamically scheduled length penalty, AALC delays length penalty until
target performance is met. Through extensive experiments across standard and
out-of-distribution math benchmarks, we show that our approach reduces response
length by over 50% while maintaining or even improving the original accuracy.
Furthermore, qualitative analysis reveals that our method curbs redundant
reasoning patterns such as excessive subgoal setting and verification, leading
to structurally refined outputs rather than naive truncation. We also identify
that efficiency gains are accompanied by reduced interpretability: models
trained with AALC omit some narrative framing and explanatory context. These
findings highlight the potential of reward-based strategies to guide LRMs
toward more efficient, generalizable reasoning paths.

</details>


### [27] [SEED: A Structural Encoder for Embedding-Driven Decoding in Time Series Prediction with LLMs](https://arxiv.org/abs/2506.20167)
*Fengze Li,Yue Wang,Yangle Liu,Ming Huang,Dou Hong,Jieming Ma*

Main category: cs.CL

TL;DR: SEED是一种新颖的结构+语义解耦多变量时序预测模型，通过模块化架构将结构依赖和语义推理结合，实验证实模型优于现有方法，推进了泛化和统一预测系统的发展。


<details>
  <summary>Details</summary>
Motivation: 多变量时间序列预测需要同时建模变量间的结构性依赖并具备跨多任务泛化能力。当前结构编码器虽能有效建模特征交互，但缺乏语义推理和任务自适应能力；而大语言模型（LLMs）虽具有强泛化能力，却难以直接处理原始时间序列数据，造成结构和语义建模的断层。

Method: 提出SEED模型（structural encoder for embedding-driven decoding），包括四个阶段：1）token-aware编码器进行patch提取；2）投影模块对齐patch与语言模型embedding；3）语义重编程机制将patch映射到任务感知prototype；4）冻结的大语言模型进行预测。通过模块化解耦表示学习和推断，促进数值模式与语义推理之间的高效对齐。

Result: 该方法在多个强基线之上均取得稳定提升，各类数据集实验证明SEED能有效弥合结构与语义建模的鸿沟。

Conclusion: SEED通过结合结构编码和大语言模型优势，实现了跨任务、可泛化和结构-语义一体化的多变量时间序列预测。

Abstract: Multivariate time series forecasting requires models to simultaneously
capture variable-wise structural dependencies and generalize across diverse
tasks. While structural encoders are effective in modeling feature
interactions, they lack the capacity to support semantic-level reasoning or
task adaptation. Conversely, large language models (LLMs) possess strong
generalization capabilities but remain incompatible with raw time series
inputs. This gap limits the development of unified, transferable prediction
systems. Therefore, we introduce SEED, a structural encoder for
embedding-driven decoding, which integrates four stages: a token-aware encoder
for patch extraction, a projection module that aligns patches with language
model embeddings, a semantic reprogramming mechanism that maps patches to
task-aware prototypes, and a frozen language model for prediction. This modular
architecture decouples representation learning from inference, enabling
efficient alignment between numerical patterns and semantic reasoning.
Empirical results demonstrate that the proposed method achieves consistent
improvements over strong baselines, and comparative studies on various datasets
confirm SEED's role in addressing the structural-semantic modeling gap.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [28] [DiT-SGCR: Directed Temporal Structural Representation with Global-Cluster Awareness for Ethereum Malicious Account Detection](https://arxiv.org/abs/2506.20123)
*Ye Tian,Liangliang Song,Peng Qian,Yanbin Wang,Jianguo Sun,Yifan Jia*

Main category: cs.CE

TL;DR: 该论文提出了一种新型无监督图编码器DiT-SGCR，有效综合了时间、方向和聚类特征，大幅提升了以太坊恶意账户的检测性能且具备良好扩展性，在多个基准上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 以太坊上恶意账户的检测对于保护数字资产和维护去中心化金融的信任至关重要。现有方法无法有效建模连续的交易动态，且高昂的计算成本限制了其在大规模网络上的适用性。此外，现有方法忽略了两个高级行为特征：交易流向（资金流动轨迹）和账户聚类（揭示恶意集体的协作行为）。

Method: 提出了一种无监督图编码器DiT-SGCR用于恶意账户检测。该方法用方向性时序聚合捕获账户交互动态，结合可微聚类与图拉普拉斯正则生成高质量低维嵌入。同时编码交易方向时序、全局结构和聚类行为，提升了账户表示的区分与鲁棒性，并规避了传统图传播机制以实现高可扩展性。

Result: 在三个数据集上，DiT-SGCR在所有基准上均优于最先进方法，F1分数提升3.62%到10.83%。

Conclusion: DiT-SGCR能够有效且高效地检测以太坊上的恶意账户，在区分性、鲁棒性及大规模网络可扩展性方面优于现有方法。

Abstract: The detection of malicious accounts on Ethereum - the preeminent DeFi
platform - is critical for protecting digital assets and maintaining trust in
decentralized finance. Recent advances highlight that temporal transaction
evolution reveals more attack signatures than static graphs. However, current
methods either fail to model continuous transaction dynamics or incur high
computational costs that limit scalability to large-scale transaction networks.
Furthermore, current methods fail to consider two higher-order behavioral
fingerprints: (1) direction in temporal transaction flows, which encodes money
movement trajectories, and (2) account clustering, which reveals coordinated
behavior of organized malicious collectives. To address these challenges, we
propose DiT-SGCR, an unsupervised graph encoder for malicious account
detection. Specifically, DiT-SGCR employs directional temporal aggregation to
capture dynamic account interactions, then coupled with differentiable
clustering and graph Laplacian regularization to generate high-quality,
low-dimensional embeddings. Our approach simultaneously encodes directional
temporal dynamics, global topology, and cluster-specific behavioral patterns,
thereby enhancing the discriminability and robustness of account
representations. Furthermore, DiT-SGCR bypasses conventional graph propagation
mechanisms, yielding significant scalability advantages. Extensive experiments
on three datasets demonstrate that DiT-SGCR consistently outperforms
state-of-the-art methods across all benchmarks, achieving F1-score improvements
ranging from 3.62% to 10.83%.

</details>


### [29] [Developing Artificial Mechanics Intuitions from Extremely Small Data](https://arxiv.org/abs/2506.20148)
*Jingruo Peng,Shuze Zhu*

Main category: cs.CE

TL;DR: 提出了一种极少样本训练AI理解复杂力学问题的方法，实现了人脑类似的直觉学习，有望推动AI在物理建模领域的应用。


<details>
  <summary>Details</summary>
Motivation: 人类可以通过少量实例获得良好的力学直觉，因此研究如何让人工智能以小样本数据也能获得类似的力学直觉具有重要意义。

Method: 提出了一种“样本可切换训练方法”（sample-switchable training method），通过极少的训练样本（不超过三个），训练模型解决不同类型的力学问题。

Result: 该方法成功训练出了能解决布拉基斯托克隆（最速降）问题、悬链线问题，以及弹性板的大非线性变形等复杂问题的模型，其预测能力随样本数量非线性提升。

Conclusion: 本研究提出的新方法能让AI通过极有限样本学习获得强大力学直觉，为理解人脑如何形成直觉和人工智能在科学建模中的应用提供了新视角。

Abstract: Humans can possess good mechanics intuitions by learning from a few examples,
which leads to the question of how to develop artificial mechanics intuitions
that can be learned from small data, as we are eagerly entering the era of
artificial intelligence. We propose in this Letter the sample-switchable
training method, which successfully develops highly-accurate artificial
mechanics intuitions that can master brachistochrone problem, catenary problem,
and large nonlinear deformation problem of elastic plate by learning from no
more than three samples. The model's intuitive prediction ability increases
nonlinearly with respect to the number of training samples, suggesting that
superb mechanics intuitions can be in-principle achieved based on a finite
number of samples, reflecting how human brains form good mechanics intuitions
just by learning a few cases. Our current work presents an alternative
perspective for educating artificial intelligence capable of intuitively
understand and predict how materials deform and move, a scenario that has been
frequently seen in Science-Fiction movies.

</details>


### [30] [Opinion Dynamics with Highly Oscillating Opinions](https://arxiv.org/abs/2506.20472)
*Víctor A. Vargas-Pérez,Jesús Giráldez-Cru,Oscar Cordón*

Main category: cs.CE

TL;DR: 本文研究了OD模型在再现高度波动舆论动态上的表现，通过实际数据与进化算法发现ATBCR模型最适合解释这类现象。


<details>
  <summary>Details</summary>
Motivation: 现有舆论动力学（OD）模型主要聚焦于意见演化的共识、分裂或两极化过程，但在解释现实中高度波动的舆论现象时存在不足。

Method: 本文通过构建优化问题，并利用进化算法对多个OD模型的参数进行优化，检验这些模型在再现舆论高度波动动态方面的能力。实验在西班牙社会学调查中心关于移民的真实意见数据集上进行。

Result: 实验表明，基于理性与情感双机制的ATBCR模型在捕捉高度波动的舆论方面最为准确。

Conclusion: ATBCR模型优于其他OD模型，能够更好地再现和解释现实世界中高度动态、波动的群众舆论。

Abstract: Opinion Dynamics (OD) models are a particular case of Agent-Based Models in
which the evolution of opinions within a population is studied. In most OD
models, opinions evolve as a consequence of interactions between agents, and
the opinion fusion rule defines how those opinions are updated. In consequence,
despite being simplistic, OD models provide an explainable and interpretable
mechanism for understanding the underlying dynamics of opinion evolution.
Unfortunately, existing OD models mainly focus on explaining the evolution of
(usually synthetic) opinions towards consensus, fragmentation, or polarization,
but they usually fail to analyze scenarios of (real-world) highly oscillating
opinions. This work overcomes this limitation by studying the ability of
several OD models to reproduce highly oscillating dynamics. To this end, we
formulate an optimization problem which is further solved using Evolutionary
Algorithms, providing both quantitative results on the performance of the
optimization and qualitative interpretations on the obtained results. Our
experiments on a real-world opinion dataset about immigration from the monthly
barometer of the Spanish Sociological Research Center show that the ATBCR,
based on both rational and emotional mechanisms of opinion update, is the most
accurate OD model for capturing highly oscillating opinions.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [31] [The Impact of the Russia-Ukraine Conflict on the Cloud Computing Risk Landscape](https://arxiv.org/abs/2506.20104)
*Malikussaid,Sutiyo*

Main category: cs.CY

TL;DR: 俄乌冲突重塑了全球IT风险与云安全形势，常规做法难以应对国家级网络威胁。该文提倡多层方法，过渡到更有弹性和以地缘政治为导向的云安全治理，强调技术联动与人因因素，共同提升数字防御力。


<details>
  <summary>Details</summary>
Motivation: 俄乌战争导致国际IT风险环境，尤其是云计算环境发生重大变化。地缘政治冲突加剧了对数据主权、网络安全和云基础设施策略的关注，需要重新审视现有IT风险管理框架的适用性。

Method: 通过分析2022年至2025年初的已记录网络行动、监管应对和组织适应，综合整理并战略性调整当前最佳实践，提出多层次方法。

Result: 发现传统IT风险框架在应对国家级威胁和复杂数据治理方面存在不足。论文提出多层次方案：融合弹性的云架构（主权云、混合云）、以数据为中心的增强安全策略（高端加密、隐私增强技术）及地缘政治驱动的治理，强调将人因（人员漏洞与专业能力）纳入技术设计和管理核心。

Conclusion: 仅凭传统IT风险框架难以应对当代地缘政治冲突下多元且复杂的数字威胁。结合地缘政治洞察、强化技术及管理措施的综合多层方法，能更有效提升数字弹性，应对新时期的云计算与网络安全挑战。

Abstract: The Russian invasion of Ukraine has fundamentally altered the information
technology (IT) risk landscape, particularly in cloud computing environments.
This paper examines how this geopolitical conflict has accelerated data
sovereignty concerns, transformed cybersecurity paradigms, and reshaped cloud
infrastructure strategies worldwide. Through an analysis of documented cyber
operations, regulatory responses, and organizational adaptations between 2022
and early 2025, this research demonstrates how the conflict has served as a
catalyst for a broader reassessment of IT risk. The research reveals that while
traditional IT risk frameworks offer foundational guidance, their standard
application may inadequately address the nuances of state-sponsored threats,
conflicting data governance regimes, and the weaponization of digital
dependencies without specific geopolitical augmentation. The contribution of
this paper lies in its focused synthesis and strategic adaptation of existing
best practices into a multi-layered approach. This approach uniquely synergizes
resilient cloud architectures (including sovereign and hybrid models), enhanced
data-centric security strategies (such as advanced encryption and
privacy-enhancing technologies), and geopolitically-informed governance to
build digital resilience. The interplay between these layers, emphasizing how
geopolitical insights directly shape architectural and security choices beyond
standard best practices-particularly by integrating the human element,
including personnel vulnerabilities and expertise, as a core consideration in
technical design and operational management-offers a more robust defense
against the specific, multifaceted risks arising from geopolitical conflict in
increasingly fractured digital territories.

</details>


### [32] [Enhancing Programming Pair Workshops: The Case of Teacher Pre-Prompting](https://arxiv.org/abs/2506.20299)
*Johan Petersson*

Main category: cs.CY

TL;DR: 教师在学生开始编程合作前提出简短的问题，有助于引导其协作过程，明确任务和加强学习互动，研究归纳出了五种有效的提示模式。


<details>
  <summary>Details</summary>
Motivation: 在编程教育中，教师如何通过干预促进学生协作效果，是提升学习成果的重要议题。本文试图探索“教师提前提示（pre-prompting）”这一教学策略的作用机制。

Method: 通过对一门大学系统开发课程中配对编程工作坊的视频数据进行定性分析，识别并分析教师提前提问的模式与影响。

Result: 研究归纳出五种不同的提前提示模式，并发现这些提示能够促进结构化讨论、澄清任务要求，以及创造共享学习机会。

Conclusion: 教师的提前预设问题对指导学生的编程合作具有积极作用，可改善问题理解与劳动分工，进而提升学习互动和成效。

Abstract: This paper explores the pedagogical potential of "teacher pre-prompting" as a
means of guiding student collaboration in programming education. In particular,
we investigate how brief teacher-initiated questions posed before students
engage in pair programming workshops can help shape problem interpretation and
division of labor. Based on qualitative analysis of video data from a
university course in systems development, we identify five distinct
pre-prompting patterns. Our findings suggest that such prompts can foster
structured discussions, clarify task requirements, and create opportunities for
shared learning experiences.

</details>


### [33] [That's Not the Feedback I Need! -- Student Engagement with GenAI Feedback in the Tutor Kai](https://arxiv.org/abs/2506.20433)
*Sven Jacobs,Maurice Kempf,Natalie Kiesler*

Main category: cs.CY

TL;DR: 开发了一款集成GenAI反馈和编译器反馈的Python编程学习平台，通过行为与眼动追踪，发现经验不足的学生更关注且依赖GenAI反馈但理解存在障碍。理解反馈利用方式有助于优化计算教育工具。


<details>
  <summary>Details</summary>
Motivation: 生成式AI（GenAI）已被多项研究应用于计算教育中的反馈生成，但关于学生如何利用及其对解决问题的支持程度的研究仍有限。该论文旨在填补这一空白。

Method: 作者开发了一个集成Python编程任务、代码编辑器、GenAI反馈和编译器反馈的自定义网页版应用。采用思维表达法（think-aloud）、眼动追踪和访谈，对11名本科生进行交互研究，并对有无编程经验的学生群体做比较分析。

Result: 结果显示：GenAI反馈普遍获得了较多视觉关注，尤其是编程经验不足的学生，其注视时间为有经验者的两倍。有经验的学生较少调用GenAI反馈，但利用其解决问题的能力更强；经验不足的学生理解GenAI反馈有困难，且更依赖GenAI反馈而忽视编译器反馈。

Conclusion: 理解学生对GenAI反馈的关注及感受，对开发更好支持学习的教育工具至关重要。GenAI反馈对不同经验水平的学生作用不同，需针对性优化反馈内容和呈现方式。

Abstract: The potential of Generative AI (GenAI) for generating feedback in computing
education has been the subject of numerous studies. However, there is still
limited research on how computing students engage with this feedback and to
what extent it supports their problem-solving. For this reason, we built a
custom web application providing students with Python programming tasks, a code
editor, GenAI feedback, and compiler feedback. Via a think-aloud protocol
including eye-tracking and a post-interview with 11 undergraduate students, we
investigate (1) how much attention the generated feedback received from
learners and (2) to what extent the generated feedback is helpful (or not). In
addition, students' attention to GenAI feedback is compared with that towards
the compiler feedback. We further investigate differences between students with
and without prior programming experience. The findings indicate that GenAI
feedback generally receives a lot of visual attention, with inexperienced
students spending twice as much fixation time. More experienced students
requested GenAI less frequently, and could utilize it better to solve the given
problem. It was more challenging for inexperienced students to do so, as they
could not always comprehend the GenAI feedback. They often relied solely on the
GenAI feedback, while compiler feedback was not read. Understanding students'
attention and perception toward GenAI feedback is crucial for developing
educational tools that support student learning.

</details>


### [34] [When Servers Meet Species: A Fab-to-Grave Lens on Computing's Biodiversity Impact](https://arxiv.org/abs/2506.20442)
*Tianyao Shi,Ritbik Kumar,Inez Hua,Yi Ding*

Main category: cs.CY

TL;DR: 本文提出新指标和建模框架，首次量化计算系统对生物多样性的影响，呼吁将生物多样性纳入可持续计算的评估与设计。


<details>
  <summary>Details</summary>
Motivation: 尽管生物多样性丧失是关键的地球界限，但其与计算机领域的关系却鲜有研究。以往的可持续计算关注于碳排放和水资源，却因为缺乏相关的评估指标和建模框架而忽略了生物多样性。

Method: 本文首次对计算系统生命周期的生物多样性影响进行了端到端的分析。提出了两个新的指标：体现生物多样性指数（EBI）和运行生物多样性指数（OBI），并开发了FABRIC建模框架，将计算工作负载与生物多样性影响相连接。

Result: 通过新指标和FABRIC框架的评估，作者发现可持续计算设计和优化中除了碳和水之外，生物多样性也值得被重视和衡量。

Conclusion: 生物多样性应作为可持续计算的重要维度进行考虑。指标和开源工具为未来相关研究和实际优化提供了基础。

Abstract: Biodiversity loss is a critical planetary boundary, yet its connection to
computing remains largely unexamined. Prior sustainability efforts in computing
have focused on carbon and water, overlooking biodiversity due to the lack of
appropriate metrics and modeling frameworks. This paper presents the first
end-to-end analysis of biodiversity impact from computing systems. We introduce
two new metrics--Embodied Biodiversity Index (EBI) and Operational Biodiversity
Index (OBI)--to quantify biodiversity impact across the lifecycle, and present
FABRIC, a modeling framework that links computing workloads to biodiversity
impacts. Our evaluation highlights the need to consider biodiversity alongside
carbon and water in sustainable computing design and optimization. The code is
available at https://github.com/TianyaoShi/FABRIC.

</details>


### [35] [Toward a Global Regime for Compute Governance: Building the Pause Button](https://arxiv.org/abs/2506.20530)
*Ananthi Al Ramiah,Raymond Koopmanschap,Josh Thorsteinson,Sadruddin Khan,Jim Zhou,Shafira Noh,Joep Meindertsma,Farhan Shafiq*

Main category: cs.CY

TL;DR: 本文提出以“算力暂停按钮”为核心的全球AI治理新框架，主张通过多层措施限制危险AI系统训练，明确规则、可追溯并可验证，有现实可行性，呼吁尽早建立相应架构以防未来失控。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能能力的迅速提升，大规模训练有可能带来灾难性风险，而目前相关的计算基础设施几乎没有监管，因此需要提出有效的治理方案，以防止强大AI系统被滥用。

Method: 文章提出了一个全球性的“算力暂停按钮”治理系统，该体系通过限制计算资源使用，防止危险AI系统训练。具体方法包括技术措施（如防篡改FLOP上限、模型锁定、离线许可）、可追溯性工具（追踪芯片、组件和用户）、监管机制（出口管控、产能上限和许可计划），并将这些手段组织在治理-执法-验证（GEV）框架中。

Result: 通过提供明确规则、可检测违规行为、独立可验证的合规体系，该系统为全球范围内防控高危AI训练提供了可行的路径。该框架参考了核不扩散和疫苗合作等全球治理经验，表明算力约束是实现国际合作的实际抓手。

Conclusion: 文章认为虽然面临技术和政治挑战，但实现全球算力管控的机制已经存在，现在是搭建这套治理体系、以防失去有效干预时机的关键时期。

Abstract: As AI capabilities rapidly advance, the risk of catastrophic harm from
large-scale training runs is growing. Yet the compute infrastructure that
enables such development remains largely unregulated. This paper proposes a
concrete framework for a global "Compute Pause Button": a governance system
designed to prevent dangerously powerful AI systems from being trained by
restricting access to computational resources. We identify three key
intervention points -- technical, traceability, and regulatory -- and organize
them within a Governance--Enforcement--Verification (GEV) framework to ensure
rules are clear, violations are detectable, and compliance is independently
verifiable. Technical mechanisms include tamper-proof FLOP caps, model locking,
and offline licensing. Traceability tools track chips, components, and users
across the compute supply chain. Regulatory mechanisms establish constraints
through export controls, production caps, and licensing schemes. Unlike
post-deployment oversight, this approach targets the material foundations of
advanced AI development. Drawing from analogues ranging from nuclear
non-proliferation to pandemic-era vaccine coordination, we demonstrate how
compute can serve as a practical lever for global cooperation. While technical
and political challenges remain, we argue that credible mechanisms already
exist, and that the time to build this architecture is now, before the window
for effective intervention closes.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [36] [A Spatio-Temporal Point Process for Fine-Grained Modeling of Reading Behavior](https://arxiv.org/abs/2506.19999)
*Francesco Ignazio Re,Andreas Opedal,Glib Manaiev,Mario Giulianelli,Ryan Cotterell*

Main category: cs.LG

TL;DR: 本文提出了一种更具表达力的空间-时间点过程概率模型，用于建模阅读时的注视与扫视，能更准确捕捉阅读时空动态。模型在扫视预测上表现出色，但语境意外性对注视时长的解释贡献有限，暗示意外性理论的适用范围有待进一步探讨。


<details>
  <summary>Details</summary>
Motivation: 以往的读者眼动建模方法大多采用汇总数据并依赖强假设，未能充分利用阅读过程中的时空动态信息。作者希望更精确地刻画和理解阅读中的注视与扫视行为过程。

Method: 提出了一种基于有标空间-时间点过程的概率模型，其中扫视行为用 Hawkes 过程建模，注视持续时间采用与注视相关的预测因子卷积的方式建模，能更全面地捕捉阅读时空动态。

Result: 所提 Hawkes 过程模型在拟合扫视（saccade）行为上优于传统基线模型；而在预测注视持续时间上，加入语境意外性指标带来提升甚微。

Conclusion: 将上下文意外性因素（surprisal）纳入模型对眼动持续时间的预测提升有限，提示意外性理论难以解释精细的眼动行为。

Abstract: Reading is a process that unfolds across space and time, alternating between
fixations where a reader focuses on a specific point in space, and saccades
where a reader rapidly shifts their focus to a new point. An ansatz of
psycholinguistics is that modeling a reader's fixations and saccades yields
insight into their online sentence processing. However, standard approaches to
such modeling rely on aggregated eye-tracking measurements and models that
impose strong assumptions, ignoring much of the spatio-temporal dynamics that
occur during reading. In this paper, we propose a more general probabilistic
model of reading behavior, based on a marked spatio-temporal point process,
that captures not only how long fixations last, but also where they land in
space and when they take place in time. The saccades are modeled using a Hawkes
process, which captures how each fixation excites the probability of a new
fixation occurring near it in time and space. The duration time of fixation
events is modeled as a function of fixation-specific predictors convolved
across time, thus capturing spillover effects. Empirically, our Hawkes process
model exhibits a better fit to human saccades than baselines. With respect to
fixation durations, we observe that incorporating contextual surprisal as a
predictor results in only a marginal improvement in the model's predictive
accuracy. This finding suggests that surprisal theory struggles to explain
fine-grained eye movements.

</details>


### [37] [MIRAGE: A Benchmark for Multimodal Information-Seeking and Reasoning in Agricultural Expert-Guided Conversations](https://arxiv.org/abs/2506.20100)
*Vardhan Dongre,Chi Gui,Shubham Garg,Hooshang Nayyeri,Gokhan Tur,Dilek Hakkani-Tür,Vikram S. Adve*

Main category: cs.LG

TL;DR: MIRAGE是一个基于真实专家互动、多模态输入、开放世界的农业领域推理与决策新基准，极大提升了对视觉-语言模型在复杂现实场景下能力的评测覆盖度和挑战。


<details>
  <summary>Details</summary>
Motivation: 现有多模态推理基准往往场景简单、类别封闭，难以覆盖现实中知识密集、互动复杂的专家咨询需求，尤其是在农业等多样性高的领域。

Method: 提出了MIRAGE基准，基于35000+真实用户与专家的交互，并通过多步流程精心筛选，整合自然用户问题、专家解答与图像上下文，形成一个高保真、多样性的专家级推理与决策任务集，覆盖7000+独特生物实体。

Result: MIRAGE成为了视觉语言模型领域中，生物物种多样性最高且最贴近真实场景的基准，可以测评模型在不完全信息、开放世界下的推理能力、澄清策略和长文本生成能力。

Conclusion: MIRAGE为农业与其他知识密集领域的多模态推理与决策研究提供了高标准、高挑战性的公开评测基准，弥补了现有数据集的不足，推动了视觉-语言模型在实际专家咨询任务中的发展。

Abstract: We introduce MIRAGE, a new benchmark for multimodal expert-level reasoning
and decision-making in consultative interaction settings. Designed for the
agriculture domain, MIRAGE captures the full complexity of expert consultations
by combining natural user queries, expert-authored responses, and image-based
context, offering a high-fidelity benchmark for evaluating models on grounded
reasoning, clarification strategies, and long-form generation in a real-world,
knowledge-intensive domain. Grounded in over 35,000 real user-expert
interactions and curated through a carefully designed multi-step pipeline,
MIRAGE spans diverse crop health, pest diagnosis, and crop management
scenarios. The benchmark includes more than 7,000 unique biological entities,
covering plant species, pests, and diseases, making it one of the most
taxonomically diverse benchmarks available for vision-language models, grounded
in the real world. Unlike existing benchmarks that rely on well-specified user
inputs and closed-set taxonomies, MIRAGE features underspecified, context-rich
scenarios with open-world settings, requiring models to infer latent knowledge
gaps, handle rare entities, and either proactively guide the interaction or
respond. Project Page: https://mirage-benchmark.github.io

</details>
