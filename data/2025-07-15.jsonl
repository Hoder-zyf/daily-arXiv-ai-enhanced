{"id": "2507.09734", "categories": ["q-fin.TR"], "pdf": "https://arxiv.org/pdf/2507.09734", "abs": "https://arxiv.org/abs/2507.09734", "authors": ["Przemys≈Çaw Rola"], "title": "Boltzmann Price: Toward Understanding the Fair Price in High-Frequency Markets", "comment": null, "summary": "In this paper, we introduce a parametrized family of prices derived from the\nMaximum Entropy Principle. The price is obtained from the distribution that\nminimizes bias, given the bid and ask volume imbalance at the top of the order\nbook. Under specific parameter choices, it closely approximates the mid-price\nor the weighted mid-price. Using probabilities of bid and ask states, we\npropose a model of price dynamics in which both drift and volatility are driven\nby volume imbalance. Compared to standard models like Bachelier or Geometric\nBrownian Motion with constant volatility, our model can generate higher\nkurtosis and heavy-tailed distributions. Additionally, the drift term naturally\nemerges as a consequence of the order book imbalance. We validate the model\nthrough simulation and demonstrate its fit to historical equity data. The model\nprovides a theoretical framework, integrating price, volume imbalance, and\nspread."}
{"id": "2507.09739", "categories": ["q-fin.CP", "q-fin.TR"], "pdf": "https://arxiv.org/pdf/2507.09739", "abs": "https://arxiv.org/abs/2507.09739", "authors": ["Haojie Liu", "Zihan Lin", "Randall R. Rojas"], "title": "Enhancing Trading Performance Through Sentiment Analysis with Large Language Models: Evidence from the S&P 500", "comment": null, "summary": "This study integrates real-time sentiment analysis from financial news, GPT-2\nand FinBERT, with technical indicators and time-series models like ARIMA and\nETS to optimize S&P 500 trading strategies. By merging sentiment data with\nmomentum and trend-based metrics, including a benchmark buy-and-hold and\nsentiment-based approach, is evaluated through assets values and returns.\nResults show that combining sentiment-driven insights with traditional models\nimproves trading performance, offering a more dynamic approach to stock trading\nthat adapts to market changes in volatile environments."}
{"id": "2507.09255", "categories": ["cs.CE", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.09255", "abs": "https://arxiv.org/abs/2507.09255", "authors": ["Charidimos Papadakis", "Giorgos Filandrianos", "Angeliki Dimitriou", "Maria Lymperaiou", "Konstantinos Thomas", "Giorgos Stamou"], "title": "StockSim: A Dual-Mode Order-Level Simulator for Evaluating Multi-Agent LLMs in Financial Markets", "comment": null, "summary": "We present StockSim, an open-source simulation platform for systematic\nevaluation of large language models (LLMs) in realistic financial\ndecision-making scenarios. Unlike previous toolkits that offer limited scope,\nStockSim delivers a comprehensive system that fully models market dynamics and\nsupports diverse simulation modes of varying granularity. It incorporates\ncritical real-world factors, such as latency, slippage, and order-book\nmicrostructure, that were previously neglected, enabling more faithful and\ninsightful assessment of LLM-based trading agents. An extensible, role-based\nagent framework supports heterogeneous trading strategies and multi-agent\ncoordination, making StockSim a uniquely capable testbed for NLP research on\nreasoning under uncertainty and sequential decision-making. We open-source all\nour code at https: //github.com/harrypapa2002/StockSim."}
{"id": "2507.09435", "categories": ["cs.CE", "cs.MS", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2507.09435", "abs": "https://arxiv.org/abs/2507.09435", "authors": ["Yidong Zhao", "Xuan Li", "Chenfanfu Jiang", "Jinhyun Choo"], "title": "GeoWarp: An automatically differentiable and GPU-accelerated implicit MPM framework for geomechanics based on NVIDIA Warp", "comment": null, "summary": "The material point method (MPM), a hybrid Lagrangian-Eulerian particle\nmethod, is increasingly used to simulate large-deformation and\nhistory-dependent behavior of geomaterials. While explicit time integration\ndominates current MPM implementations due to its algorithmic simplicity, such\nschemes are unsuitable for quasi-static and long-term processes typical in\ngeomechanics. Implicit MPM formulations are free of these limitations but\nremain less adopted, largely due to the difficulty of computing the Jacobian\nmatrix required for Newton-type solvers, especially when consistent tangent\noperators should be derived for complex constitutive models. In this paper, we\nintroduce GeoWarp -- an implicit MPM framework for geomechanics built on NVIDIA\nWarp -- that exploits GPU parallelism and reverse-mode automatic\ndifferentiation to compute Jacobians without manual derivation. To enhance\nefficiency, we develop a sparse Jacobian construction algorithm that leverages\nthe localized particle-grid interactions intrinsic to MPM. The framework is\nverified through forward and inverse examples in large-deformation\nelastoplasticity and coupled poromechanics. Results demonstrate that GeoWarp\nprovides a robust, scalable, and extensible platform for differentiable\nimplicit MPM simulation in computational geomechanics."}
{"id": "2507.08865", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.08865", "abs": "https://arxiv.org/abs/2507.08865", "authors": ["Javis AI Team", "Amrendra Singh", "Maulik Shah", "Dharshan Sampath"], "title": "Spatial ModernBERT: Spatial-Aware Transformer for Table and Key-Value Extraction in Financial Documents at Scale", "comment": null, "summary": "Extracting tables and key-value pairs from financial documents is essential\nfor business workflows such as auditing, data analytics, and automated invoice\nprocessing. In this work, we introduce Spatial ModernBERT-a transformer-based\nmodel augmented with spatial embeddings-to accurately detect and extract\ntabular data and key-value fields from complex financial documents. We cast the\nextraction task as token classification across three heads: (1) Label Head,\nclassifying each token as a label (e.g., PO Number, PO Date, Item Description,\nQuantity, Base Cost, MRP, etc.); (2) Column Head, predicting column indices;\n(3) Row Head, distinguishing the start of item rows and header rows. The model\nis pretrained on the PubTables-1M dataset, then fine-tuned on a financial\ndocument dataset, achieving robust performance through cross-entropy loss on\neach classification head. We propose a post-processing method to merge tokens\nusing B-I-IB tagging, reconstruct the tabular layout, and extract key-value\npairs. Empirical evaluation shows that Spatial ModernBERT effectively leverages\nboth textual and spatial cues, facilitating highly accurate table and key-value\nextraction in real-world financial documents."}
{"id": "2507.08806", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.08806", "abs": "https://arxiv.org/abs/2507.08806", "authors": ["Daewon Choi", "Jimin Lee", "Jihoon Tack", "Woomin Song", "Saket Dingliwal", "Sai Muralidhar Jayanthi", "Bhavana Ganesh", "Jinwoo Shin", "Aram Galstyan", "Sravan Babu Bodapati"], "title": "Think Clearly: Improving Reasoning via Redundant Token Pruning", "comment": null, "summary": "Recent large language models have shown promising capabilities in long-form\nreasoning, following structured chains of thought before arriving at a final\nanswer. However, we observe that these reasoning paths tend to include\nsubstantial redundancy; analyzing attention patterns reveals that attention\nscores are widely scattered, particularly incorrect answers exhibit greater\nattention sparsity. In this paper, we demonstrate that deliberately removing\nthis redundancy in the reasoning process significantly improves performance\nthrough clear thinking, i.e., removing distraction. Specifically, we\nsystematically identify reasoning redundancy by measuring token-level attention\nscores to a special end-of-thinking token, which is appended to an explicit\ninstruction inserted to conclude each intermediate reasoning step. Furthermore,\nwe propose structure-aware pruning that prioritizes removing tokens in\nlow-contributing reasoning chunks over individual tokens. After evicting\nredundant tokens, we remove the injected end-of-thinking instruction, then\nresume the reasoning generation. We demonstrate that our method significantly\nimproves overall accuracy across reasoning-intensive benchmarks without any\ntraining involved. In particular, our method shows strong performance on\nchallenging mathematical competition benchmarks such as AIME and AMC, where\nreasoning redundancy is more prevalent."}
{"id": "2507.08869", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2507.08869", "abs": "https://arxiv.org/abs/2507.08869", "authors": ["Tatiana Deslouches", "Doreen Kobelo Regalado", "Mohamed Khalafalla", "Tejal Mulay"], "title": "Preliminary Analysis of Construction Work Zone on Roadways in Florida by Crash Severity", "comment": null, "summary": "Construction zones are inherently hazardous, posing significant risks to\nconstruction workers and motorists. Despite existing safety measures,\nconstruction zones continue to witness fatalities and serious injuries,\nimposing economic burdens. Addressing these issues requires understanding root\ncauses and implementing preventive strategies centered around the 4Es\n(Engineering, Education, Enforcement, Emergency Response) and 4Is (Information\nIntelligence, Innovation, Insight into communities, Investment, and Policies).\nProper safety management, integrating these strategic initiatives, aims to\nreduce and potentially eliminate fatalities and serious injuries in work zones.\nIn Florida, road construction work zone fatalities and serious injuries remain\na critical concern, especially in urban counties. Despite a 12 billion dollars\ninfrastructure investment in 2022, Florida ranks eighth nationally for fatal\nwork zone crashes involving commercial motor vehicles (CMVs). Analysis from\n2019 to 2023 shows an average of 71 fatalities and 309 serious injuries\nannually in Florida work zones, reflecting a persistent safety challenge.\nHigh-risk counties include Orange, Broward, Duval, Hillsborough, Pasco,\nMiami-Dade, Seminole, Manatee, Palm Beach, and Lake. This study presents a\npreliminary analysis of work zone crashes in Broward, Duval, Hillsborough, and\nOrange counties. A multilogit model assessed attributes contributing to\nfatalities and serious injuries, such as crash type, weather and light\nconditions, work zone type, type of shoulder, presence of workers, and law\nenforcement. Results indicate significant contributing factors, highlighting\nopportunities to use machine learning for alerting drivers and construction\nmanagers, ultimately enhancing safety protocols and reducing fatalities."}
{"id": "2507.09004", "categories": ["q-fin.CP"], "pdf": "https://arxiv.org/pdf/2507.09004", "abs": "https://arxiv.org/abs/2507.09004", "authors": ["Domagoj Demeterfi", "Kathrin Glau", "Linus Wunderlich"], "title": "Function approximations for counterparty credit exposure calculations", "comment": null, "summary": "The challenge to measure exposures regularly forces financial institutions\ninto a choice between an overwhelming computational burden or\noversimplification of risk. To resolve this unsettling dilemma, we\nsystematically investigate replacing frequently called derivative pricers by\nfunction approximations covering all practically relevant exposure measures,\nincluding quantiles. We prove error bounds for exposure measures in terms of\nthe $L^p$ norm, $1 \\leq p < \\infty$, and for the uniform norm. To fully exploit\nthese results, we employ the Chebyshev interpolation and show exponential\nconvergence of the resulting exposure calculations. As our main result we\nderive probabilistic and finite sample error bounds under mild conditions\nincluding the natural case of unbounded risk factors. We derive an asymptotic\nefficiency gain scaling with $n^{1/2-\\varepsilon}$ for any $\\varepsilon>0$ with\n$n$ the number of simulations. Our numerical experiments cover callable,\nbarrier, stochastic volatility and jump features. Using 10\\,000 simulations, we\nconsistently observe significant run-time reductions in all cases with speed-up\nfactors up to 230, and an average speed-up of 87. We also present an adaptive\nchoice of the interpolation degree. Finally, numerical examples relying on the\napproximation of Greeks highlight the merit of the method beyond the presented\ntheory."}
{"id": "2507.09527", "categories": ["cs.CE"], "pdf": "https://arxiv.org/pdf/2507.09527", "abs": "https://arxiv.org/abs/2507.09527", "authors": ["Hang Fan", "Yunze Chai", "Chenxi Liu", "Weican Liu", "Zuhan Zhang", "Wencai Run", "Dunnan Liu"], "title": "EV-STLLM: Electric vehicle charging forecasting based on spatio-temporal large language models with multi-frequency and multi-scale information fusion", "comment": null, "summary": "With the proliferation of electric vehicles (EVs), accurate charging demand\nand station occupancy forecasting are critical for optimizing urban energy and\nthe profit of EVs aggregator. Existing approaches in this field usually\nstruggle to capture the complex spatio-temporal dependencies in EV charging\nbehaviors, and their limited model parameters hinder their ability to learn\ncomplex data distribution representations from large datasets. To this end, we\npropose a novel EV spatio-temporal large language model (EV-STLLM) for accurate\nprediction. Our proposed framework is divided into two modules. In the data\nprocessing module, we utilize variational mode decomposition (VMD) for data\ndenoising, and improved complete ensemble empirical mode decomposition with\nadaptive noise (ICEEMDAN) for data multi-frequency decomposition. Fuzzy\ninformation granulation (FIG) for extracting multi-scale information.\nAdditionally, ReliefF is used for feature selection to mitigate redundancy. In\nthe forecasting module, the EV-STLLM is used to directly achieve EV charging\nand occupancy forecasting. Firstly, we fully capture the intrinsic\nspatio-temporal characteristics of the data by integrating adjacency matrices\nderived from the regional stations network and spatio-temporal-frequency\nembedding information. Then, the partially frozen graph attention (PFGA) module\nis utilized to maintain the sequential feature modeling capabilities of the\npre-trained large model while incorporating EV domain knowledge. Extensive\nexperiments using real-world data from Shenzhen, China, demonstrate that our\nproposed framework can achieve superior accuracy and robustness compared to the\nstate-of-the-art benchmarks."}
{"id": "2507.08898", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08898", "abs": "https://arxiv.org/abs/2507.08898", "authors": ["Wenliang Shan", "Michael Fu", "Rui Yang", "Chakkrit", "Tantithamthavorn"], "title": "SEALGuard: Safeguarding the Multilingual Conversations in Southeast Asian Languages for LLM Software Systems", "comment": "Under Review at Information and Software Technology", "summary": "Safety alignment is critical for LLM-powered systems. While recent\nLLM-powered guardrail approaches such as LlamaGuard achieve high detection\naccuracy of unsafe inputs written in English (e.g., ``How to create a bomb?''),\nthey struggle with multilingual unsafe inputs. This limitation leaves LLM\nsystems vulnerable to unsafe and jailbreak prompts written in low-resource\nlanguages such as those in Southeast Asia. This paper introduces SEALGuard, a\nmultilingual guardrail designed to improve the safety alignment across diverse\nlanguages. It aims to address the multilingual safety alignment gap of existing\nguardrails and ensure effective filtering of unsafe and jailbreak prompts in\nLLM-powered systems. We adapt a general-purpose multilingual language model\ninto a multilingual guardrail using low-rank adaptation (LoRA). We construct\nSEALSBench, a large-scale multilingual safety alignment dataset containing over\n260,000 prompts in ten languages, including safe, unsafe, and jailbreak cases.\nWe evaluate SEALGuard against state-of-the-art guardrails such as LlamaGuard on\nthis benchmark. Our findings show that multilingual unsafe and jailbreak\nprompts substantially degrade the performance of the state-of-the-art\nLlamaGuard, which experiences a drop in Defense Success Rate (DSR) by 9% and\n18%, respectively, compared to its performance on English-only prompts. In\ncontrast, SEALGuard outperforms existing guardrails in detecting multilingual\nunsafe and jailbreak prompts, improving DSR by 48% over LlamaGuard and\nachieving the best DSR, precision, and F1-score. Our ablation study further\nreveals the contributions of adaptation strategies and model size to the\noverall performance of SEALGuard. SEALGuard advances the safety alignment of\nLLM systems by introducing an effective multilingual guardrail."}
{"id": "2507.08875", "categories": ["cs.AI", "90B50, 90C29, 90C08, 91A80, 91B06"], "pdf": "https://arxiv.org/pdf/2507.08875", "abs": "https://arxiv.org/abs/2507.08875", "authors": ["Fuh-Hwa Franklin Liu", "Su-Chuan Shih"], "title": "A New Approach for Multicriteria Assessment in the Ranking of Alternatives Using Cardinal and Ordinal Data", "comment": "38 pages, 6 figures, 5 table. A practice applicable method for\n  multi-criteria assessments using cardinal and ordinal data", "summary": "Modern methods for multi-criteria assessment (MCA), such as Data Envelopment\nAnalysis (DEA), Stochastic Frontier Analysis (SFA), and Multiple Criteria\nDecision-Making (MCDM), are utilized to appraise a collection of\nDecision-Making Units (DMUs), also known as alternatives, based on several\ncriteria. These methodologies inherently rely on assumptions and can be\ninfluenced by subjective judgment to effectively tackle the complex evaluation\nchallenges in various fields. In real-world scenarios, it is essential to\nincorporate both quantitative and qualitative criteria as they consist of\ncardinal and ordinal data. Despite the inherent variability in the criterion\nvalues of different alternatives, the homogeneity assumption is often employed,\nsignificantly affecting evaluations. To tackle these challenges and determine\nthe most appropriate alternative, we propose a novel MCA approach that combines\ntwo Virtual Gap Analysis (VGA) models. The VGA framework, rooted in linear\nprogramming, is pivotal in the MCA methodology. This approach improves\nefficiency and fairness, ensuring that evaluations are both comprehensive and\ndependable, thus offering a strong and adaptive solution. Two comprehensive\nnumerical examples demonstrate the accuracy and transparency of our proposed\nmethod. The goal is to encourage continued advancement and stimulate progress\nin automated decision systems and decision support systems."}
{"id": "2507.08879", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08879", "abs": "https://arxiv.org/abs/2507.08879", "authors": ["Max-Paul F√∂rster", "Luca Deck", "Raimund Weidlich", "Niklas K√ºhl"], "title": "A Multi-Level Strategy for Deepfake Content Moderation under EU Regulation", "comment": null, "summary": "The growing availability and use of deepfake technologies increases risks for\ndemocratic societies, e.g., for political communication on online platforms.\nThe EU has responded with transparency obligations for providers and deployers\nof Artificial Intelligence (AI) systems and online platforms. This includes\nmarking deepfakes during generation and labeling deepfakes when they are\nshared. However, the lack of industry and enforcement standards poses an\nongoing challenge. Through a multivocal literature review, we summarize methods\nfor marking, detecting, and labeling deepfakes and assess their effectiveness\nunder EU regulation. Our results indicate that individual methods fail to meet\nregulatory and practical requirements. Therefore, we propose a multi-level\nstrategy combining the strengths of existing methods. To account for the masses\nof content on online platforms, our multi-level strategy provides scalability\nand practicality via a simple scoring mechanism. At the same time, it is\nagnostic to types of deepfake technology and allows for context-specific risk\nweighting."}
{"id": "2507.09412", "categories": ["q-fin.CP"], "pdf": "https://arxiv.org/pdf/2507.09412", "abs": "https://arxiv.org/abs/2507.09412", "authors": ["Fabio Baschetti", "Giacomo Bormetti", "Pietro Rossi"], "title": "Joint deep calibration of the 4-factor PDV model", "comment": null, "summary": "Joint calibration to SPX and VIX market data is a delicate task that requires\nsophisticated modeling and incurs significant computational costs. The latter\nis especially true when pricing of volatility derivatives hinges on nested\nMonte Carlo simulation. One such example is the 4-factor Markov Path-Dependent\nVolatility (PDV) model of Guyon and Lekeufack (2023). Nonetheless, its realism\nhas earned it considerable attention in recent years. Gazzani and Guyon (2025)\nmarked a relevant contribution by learning the VIX as a random variable, i.e.,\na measurable function of the model parameters and the Markovian factors. A\nneural network replaces the inner simulation and makes the joint calibration\nproblem accessible. However, the minimization loop remains slow due to\nexpensive outer simulation. The present paper overcomes this limitation by\nlearning SPX implied volatilities, VIX futures, and VIX call option prices. The\npricing functions reduce to simple matrix-vector products that can be evaluated\non the fly, shrinking calibration times to just a few seconds."}
{"id": "2507.09591", "categories": ["cs.CE"], "pdf": "https://arxiv.org/pdf/2507.09591", "abs": "https://arxiv.org/abs/2507.09591", "authors": ["Michael Ryan", "Mohammad Hassan Baqershahi", "Hessamoddin Moshayedi", "Elyas Ghafoori"], "title": "Physics-informed machine learning surrogate for scalable simulation of thermal histories during wire-arc directed energy deposition", "comment": "18 pages, 12 figures", "summary": "Wire-arc directed energy deposition (DED) has emerged as a promising additive\nmanufacturing (AM) technology for large-scale structural engineering\napplications. However, the complex thermal dynamics inherent to the process\npresent challenges in ensuring structural integrity and mechanical properties\nof fabricated thick walls and plates. While finite element method (FEM)\nsimulations have been conventionally employed to predict thermal history during\ndeposition, their computational demand remains prohibitively high for actual\nlarge-scale applications. Given the necessity of multiple repetitive\nsimulations for heat management and the determination of an optimal printing\nstrategy, FEM simulation quickly becomes entirely infeasible. Instead,\nadvancements have been made in using trained neural networks as surrogate\nmodels for rapid prediction. However, traditional data-driven approaches\nnecessitate large amounts of relevant and verifiable external data, during the\ntraining and validation of the neural network. Regarding large-scale wire-arc\nDED, none of these data sources are readily available in quantities sufficient\nfor an accurate surrogate. The introduction of physics-informed neural networks\n(PINNs) has opened up an alternative simulation strategy by leveraging the\nexisting physical knowledge of the phenomena with advanced machine learning\nmethods. Despite their theoretical advantages, PINNs have seen limited\napplication in the context of large-scale wire-arc DED for structural\nengineering. This study investigates the scalability of PINNs, focusing on\nefficient collocation points sampling, a critical factor controlling both the\ntraining time and model performance. Results show PINNs can reduce\ncomputational time and effort by up to 98.6%, while maintaining the desired\naccuracy and offering \"super-resolution\". Future directions for enhancing PINN\nperformance in metal AM are discussed."}
{"id": "2507.08916", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.08916", "abs": "https://arxiv.org/abs/2507.08916", "authors": ["Mahmoud Alwakeel", "Aditya Nagori", "Vijay Krishnamoorthy", "Rishikesan Kamaleswaran"], "title": "Evaluating LLMs in Medicine: A Call for Rigor, Transparency", "comment": null, "summary": "Objectives: To evaluate the current limitations of large language models\n(LLMs) in medical question answering, focusing on the quality of datasets used\nfor their evaluation. Materials and Methods: Widely-used benchmark datasets,\nincluding MedQA, MedMCQA, PubMedQA, and MMLU, were reviewed for their rigor,\ntransparency, and relevance to clinical scenarios. Alternatives, such as\nchallenge questions in medical journals, were also analyzed to identify their\npotential as unbiased evaluation tools. Results: Most existing datasets lack\nclinical realism, transparency, and robust validation processes. Publicly\navailable challenge questions offer some benefits but are limited by their\nsmall size, narrow scope, and exposure to LLM training. These gaps highlight\nthe need for secure, comprehensive, and representative datasets. Conclusion: A\nstandardized framework is critical for evaluating LLMs in medicine.\nCollaborative efforts among institutions and policymakers are needed to ensure\ndatasets and methodologies are rigorous, unbiased, and reflective of clinical\ncomplexities."}
{"id": "2507.08892", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.08892", "abs": "https://arxiv.org/abs/2507.08892", "authors": ["Alexander Sasha Vezhnevets", "Jayd Matyas", "Logan Cross", "Davide Paglieri", "Minsuk Chang", "William A. Cunningham", "Simon Osindero", "William S. Isaac", "Joel Z. Leibo"], "title": "Multi-Actor Generative Artificial Intelligence as a Game Engine", "comment": "13 pages", "summary": "Generative AI can be used in multi-actor environments with purposes ranging\nfrom social science modeling to interactive narrative and AI evaluation.\nSupporting this diversity of use cases -- which we classify as Simulationist,\nDramatist, and Evaluationist -- demands a flexible scenario definition\nframework. We argue here that a good approach is to take inspiration from\ntabletop role-playing games (TTRPGs), where a Game Master (GM) is responsible\nfor the environment and generates all parts of the story not directly\ndetermined by the voluntary actions of player characters. We argue that the\nEntity-Component architectural pattern is useful here. In such a system, the GM\nis not a hardcoded computer game but is itself a configurable entity, composed\nof components just like any other actor. By design, the approach allows for a\nseparation between the underlying implementation details handled by an\nengineer, the creation of reusable components, and their composition and\nconfiguration managed by a designer who constructs entities from the\ncomponents. This separation of concerns is instrumental for achieving rapid\niteration, maintaining modularity, and ultimately to ensure scalability. We\ndescribe the ongoing evolution of the Concordia library in terms of this\nphilosophy, demonstrating how it allows users to effectively configure\nscenarios that align with their specific goals."}
{"id": "2507.08881", "categories": ["cs.CY", "cs.AI", "cs.SI"], "pdf": "https://arxiv.org/pdf/2507.08881", "abs": "https://arxiv.org/abs/2507.08881", "authors": ["Zhang MingDa", "Xu Qing"], "title": "The Consistency-Acceptability Divergence of LLMs in Judicial Decision-Making: Task and Stakeholder Dimensions", "comment": "12 pages,2 figures", "summary": "The integration of large language model (LLM) technology into judicial\nsystems is fundamentally transforming legal practice worldwide. However, this\nglobal transformation has revealed an urgent paradox requiring immediate\nattention. This study introduces the concept of ``consistency-acceptability\ndivergence'' for the first time, referring to the gap between technical\nconsistency and social acceptance. While LLMs achieve high consistency at the\ntechnical level, this consistency demonstrates both positive and negative\neffects. Through comprehensive analysis of recent data on LLM judicial\napplications from 2023--2025, this study finds that addressing this challenge\nrequires understanding both task and stakeholder dimensions. This study\nproposes the Dual-Track Deliberative Multi-Role LLM Judicial Governance\nFramework (DTDMR-LJGF), which enables intelligent task classification and\nmeaningful interaction among diverse stakeholders. This framework offers both\ntheoretical insights and practical guidance for building an LLM judicial\necosystem that balances technical efficiency with social legitimacy."}
{"id": "2507.09739", "categories": ["q-fin.CP", "q-fin.TR"], "pdf": "https://arxiv.org/pdf/2507.09739", "abs": "https://arxiv.org/abs/2507.09739", "authors": ["Haojie Liu", "Zihan Lin", "Randall R. Rojas"], "title": "Enhancing Trading Performance Through Sentiment Analysis with Large Language Models: Evidence from the S&P 500", "comment": null, "summary": "This study integrates real-time sentiment analysis from financial news, GPT-2\nand FinBERT, with technical indicators and time-series models like ARIMA and\nETS to optimize S&P 500 trading strategies. By merging sentiment data with\nmomentum and trend-based metrics, including a benchmark buy-and-hold and\nsentiment-based approach, is evaluated through assets values and returns.\nResults show that combining sentiment-driven insights with traditional models\nimproves trading performance, offering a more dynamic approach to stock trading\nthat adapts to market changes in volatile environments."}
{"id": "2507.09675", "categories": ["cs.CE", "econ.GN", "q-fin.EC"], "pdf": "https://arxiv.org/pdf/2507.09675", "abs": "https://arxiv.org/abs/2507.09675", "authors": ["Seyed Mohammad Ali Jafari", "Ali Mobini Dehkordi", "Ehsan Chitsaz", "Yadollah Yaghoobzadeh"], "title": "What Matters Most? A Quantitative Meta-Analysis of AI-Based Predictors for Startup Success", "comment": null, "summary": "Background: Predicting startup success with machine learning is a rapidly\ngrowing field, yet findings on key predictors are often fragmented and\ncontext-specific. This makes it difficult to discern robust patterns and\nhighlights a need for a systematic synthesis of the evidence.\n  Methods: This study conducts a quantitative meta-analysis to synthesize the\nliterature on predictor importance in AI-based startup evaluation. We performed\na systematic review to identify a final sample of 13 empirical studies that\nreport rankable feature importance. From these papers, we extracted and\ncategorized 58 unique predictors, synthesizing their importance using a\nWeighted Importance Score (WIS) that balances a feature's average rank with its\nfrequency of appearance. We also conducted a moderator analysis to investigate\nhow predictor importance changes with context (e.g., success definition).\n  Results: Our aggregate analysis reveals that the most consistently powerful\npredictors are a quartet of foundational attributes: Firm Characteristics\n(e.g., age, location), Investor Structure (e.g., investor quality), Digital and\nSocial Traction (e.g., online momentum), and Funding History. The moderator\nanalysis further reveals that this hierarchy is highly context-dependent. For\ninstance, predicting near-term funding milestones elevates the importance of\nthe deal's immediate context, while predicting long-term exits prioritizes\nfundamental firm and investor characteristics.\n  Conclusion: The factors that best predict startup success are not universal\nbut are contingent on the startup's goals, stage, and the data used for\nevaluation. Our findings point to a potential \"convenience bias\" in the\nliterature, where predictor importance may be tied to data accessibility. We\nconclude by underscoring the need for standardized reporting practices to\nenable more robust, cumulative knowledge building in the field."}
{"id": "2507.08924", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08924", "abs": "https://arxiv.org/abs/2507.08924", "authors": ["Seokhee Hong", "Sunkyoung Kim", "Guijin Son", "Soyeon Kim", "Yeonjung Hong", "Jinsik Lee"], "title": "From KMMLU-Redux to KMMLU-Pro: A Professional Korean Benchmark Suite for LLM Evaluation", "comment": null, "summary": "The development of Large Language Models (LLMs) requires robust benchmarks\nthat encompass not only academic domains but also industrial fields to\neffectively evaluate their applicability in real-world scenarios. In this\npaper, we introduce two Korean expert-level benchmarks. KMMLU-Redux,\nreconstructed from the existing KMMLU, consists of questions from the Korean\nNational Technical Qualification exams, with critical errors removed to enhance\nreliability. KMMLU-Pro is based on Korean National Professional Licensure exams\nto reflect professional knowledge in Korea. Our experiments demonstrate that\nthese benchmarks comprehensively represent industrial knowledge in Korea. We\nrelease our dataset publicly available."}
{"id": "2507.09080", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09080", "abs": "https://arxiv.org/abs/2507.09080", "authors": ["Athanasios Trantas", "Martino Mensio", "Stylianos Stasinos", "Sebastian Gribincea", "Taimur Khan", "Damian Podareanu", "Aliene van der Veen"], "title": "BioAnalyst: A Foundation Model for Biodiversity", "comment": null, "summary": "The accelerating loss of biodiversity presents critical challenges for\necological research and conservation strategies. The preservation of\nbiodiversity is paramount for maintaining ecological balance and ensuring the\nsustainability of ecosystems. However, biodiversity faces numerous threats,\nincluding habitat loss, climate change, and the proliferation of invasive\nspecies. Addressing these and other ecology-related challenges, both at local\nand global scales, requires comprehensive monitoring, predictive and\nconservation planning capabilities. Artificial Intelligence (AI) Foundation\nModels (FMs) have gained significant momentum in numerous scientific domains by\nleveraging vast datasets to learn general-purpose representations adaptable to\nvarious downstream tasks. This paradigm holds immense promise for biodiversity\nconservation. In response, we introduce BioAnalyst, the first Foundation Model\ntailored for biodiversity analysis and conservation planning. BioAnalyst\nemploys a transformer-based architecture, pre-trained on extensive multi-modal\ndatasets encompassing species occurrence records, remote sensing indicators,\nclimate and environmental variables. BioAnalyst is designed for adaptability,\nallowing for fine-tuning of a range of downstream tasks, such as species\ndistribution modelling, habitat suitability assessments, invasive species\ndetection, and population trend forecasting. We evaluate the model's\nperformance on two downstream use cases, demonstrating its generalisability\ncompared to existing methods, particularly in data-scarce scenarios for two\ndistinct use-cases, establishing a new accuracy baseline for ecological\nforecasting. By openly releasing BioAnalyst and its fine-tuning workflows to\nthe scientific community, we aim to foster collaborative efforts in\nbiodiversity modelling and advance AI-driven solutions to pressing ecological\nchallenges."}
{"id": "2507.08908", "categories": ["cs.CY", "cs.ET", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.08908", "abs": "https://arxiv.org/abs/2507.08908", "authors": ["M. Z. Naser"], "title": "The Engineer's Dilemma: A Review of Establishing a Legal Framework for Integrating Machine Learning in Construction by Navigating Precedents and Industry Expectations", "comment": null, "summary": "Despite the widespread interest in machine learning (ML), the engineering\nindustry has not yet fully adopted ML-based methods, which has left engineers\nand stakeholders uncertain about the legal and regulatory frameworks that\ngovern their decisions. This gap remains unaddressed as an engineer's\ndecision-making process, typically governed by professional ethics and\npractical guidelines, now intersects with complex algorithmic outputs. To\nbridge this gap, this paper explores how engineers can navigate legal\nprinciples and legislative justifications that support and/or contest the\ndeployment of ML technologies. Drawing on recent precedents and experiences\ngained from other fields, this paper argues that analogical reasoning can\nprovide a basis for embedding ML within existing engineering codes while\nmaintaining professional accountability and meeting safety requirements. In\nexploring these issues, the discussion focuses on established liability\ndoctrines, such as negligence and product liability, and highlights how courts\nhave evaluated the use of predictive models. We further analyze how legislative\nbodies and standard-setting organizations can furnish explicit guidance\nequivalent to prior endorsements of emergent technologies. This exploration\nstresses the vitality of understanding the interplay between technical\njustifications and legal precedents for shaping an informed stance on ML's\nlegitimacy in engineering practice. Finally, our analysis catalyzes a legal\nframework for integrating ML through which stakeholders can critically assess\nthe responsibilities, liabilities, and benefits inherent in ML-driven\nengineering solutions."}
{"id": "2507.09863", "categories": ["q-fin.CP"], "pdf": "https://arxiv.org/pdf/2507.09863", "abs": "https://arxiv.org/abs/2507.09863", "authors": ["Ryuji Hashimoto", "Kiyoshi Izumi"], "title": "Towards Realistic and Interpretable Market Simulations: Factorizing Financial Power Law using Optimal Transport", "comment": null, "summary": "We investigate the mechanisms behind the power-law distribution of stock\nreturns using artificial market simulations. While traditional financial theory\nassumes Gaussian price fluctuations, empirical studies consistently show that\nthe tails of return distributions follow a power law. Previous research has\nproposed hypotheses for this phenomenon -- some attributing it to investor\nbehavior, others to institutional demand imbalances. However, these factors\nhave rarely been modeled together to assess their individual and joint\ncontributions. The complexity of real financial markets complicates the\nisolation of the contribution of a single component using existing data. To\naddress this, we construct artificial markets and conduct controlled\nexperiments using optimal transport (OT) as a quantitative similarity measure.\nOur proposed framework incrementally introduces behavioral components into the\nagent models, allowing us to compare each simulation output with empirical data\nvia OT distances. The results highlight that informational effect of prices\nplays a dominant role in reproducing power-law behavior and that multiple\ncomponents interact synergistically to amplify this effect."}
{"id": "2507.09825", "categories": ["cs.CE", "math.PR", "65C05, 86-08, 82-08, 65C60, 60-08"], "pdf": "https://arxiv.org/pdf/2507.09825", "abs": "https://arxiv.org/abs/2507.09825", "authors": ["Michal B√©re≈°"], "title": "Legendre Polynomials and Their Use for Karhunen-Lo√®ve Expansion", "comment": null, "summary": "This paper makes two main contributions. First, we present a pedagogical\nreview of the derivation of the three-term recurrence relation for Legendre\npolynomials, without relying on the classical Legendre differential equation,\nRodrigues' formula, or generating functions. This exposition is designed to be\naccessible to undergraduate students.\n  Second, we develop a computational framework for Karhunen-Lo\\`eve expansions\nof isotropic Gaussian random fields on hyper-rectangular domains. The framework\nleverages Legendre polynomials and their associated Gaussian quadrature, and it\nremains efficient even in higher spatial dimensions.\n  A covariance kernel is first approximated by a non-negative mixture of\nsquared-exponentials, obtained via a Newton-optimized fit with a theoretically\ninformed initialization. The resulting separable kernel enables a\nLegendre-Galerkin discretization in the form of a Kronecker product over single\ndimensions, with submatrices that exhibit even/odd parity structure. For\nassembly, we introduce a Duffy-type transformation followed by quadrature.\nThese structural properties significantly reduce both memory usage and\narithmetic cost compared to naive approaches. All algorithms and numerical\nexperiments are provided in an open-source repository that reproduces every\nfigure and table in this work."}
{"id": "2507.08967", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.08967", "abs": "https://arxiv.org/abs/2507.08967", "authors": ["Rongyi Zhu", "Yuhui Wang", "Tanqiu Jiang", "Jiacheng Liang", "Ting Wang"], "title": "Self-Improving Model Steering", "comment": "16 pages, 9 figures", "summary": "Model steering represents a powerful technique that dynamically aligns large\nlanguage models (LLMs) with human preferences during inference. However,\nconventional model-steering methods rely heavily on externally annotated data,\nnot only limiting their adaptability to varying contexts but also tethering\ntheir effectiveness to annotation quality. In this paper, we present SIMS, the\nfirst self-improving model-steering framework that operates without relying on\nexternal supervision. At its core, SIMS autonomously generates and refines\ncontrastive samples through iterative self-improvement cycles, enabling\nadaptive, context-specific steering. Additionally, SIMS employs novel\nstrategies, including prompt ranking and contrast sampling, to further enhance\nsteering efficacy. Extensive evaluation across diverse LLMs and benchmarks\ndemonstrates that SIMS substantially outperforms existing methods in steering\neffectiveness and adaptability, highlighting self-improving model steering as a\npromising direction for future research on inference-time LLM alignment."}
{"id": "2507.09089", "categories": ["cs.AI", "cs.HC", "cs.SE", "I.2"], "pdf": "https://arxiv.org/pdf/2507.09089", "abs": "https://arxiv.org/abs/2507.09089", "authors": ["Joel Becker", "Nate Rush", "Elizabeth Barnes", "David Rein"], "title": "Measuring the Impact of Early-2025 AI on Experienced Open-Source Developer Productivity", "comment": "50 pages, 8 tables, 22 figures", "summary": "Despite widespread adoption, the impact of AI tools on software development\nin the wild remains understudied. We conduct a randomized controlled trial\n(RCT) to understand how AI tools at the February-June 2025 frontier affect the\nproductivity of experienced open-source developers. 16 developers with moderate\nAI experience complete 246 tasks in mature projects on which they have an\naverage of 5 years of prior experience. Each task is randomly assigned to allow\nor disallow usage of early 2025 AI tools. When AI tools are allowed, developers\nprimarily use Cursor Pro, a popular code editor, and Claude 3.5/3.7 Sonnet.\nBefore starting tasks, developers forecast that allowing AI will reduce\ncompletion time by 24%. After completing the study, developers estimate that\nallowing AI reduced completion time by 20%. Surprisingly, we find that allowing\nAI actually increases completion time by 19%--AI tooling slowed developers\ndown. This slowdown also contradicts predictions from experts in economics (39%\nshorter) and ML (38% shorter). To understand this result, we collect and\nevaluate evidence for 20 properties of our setting that a priori could\ncontribute to the observed slowdown effect--for example, the size and quality\nstandards of projects, or prior developer experience with AI tooling. Although\nthe influence of experimental artifacts cannot be entirely ruled out, the\nrobustness of the slowdown effect across our analyses suggests it is unlikely\nto primarily be a function of our experimental design."}
{"id": "2507.09020", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2507.09020", "abs": "https://arxiv.org/abs/2507.09020", "authors": ["Ebenezer Asem", "Ruijie Fan", "Gloria Y. Tian"], "title": "ESG and the Cost of Capital: Insights from an AI-Assisted Systematic Literature Review", "comment": null, "summary": "This paper explores how AI-powered tools could be leveraged to streamline the\nprocess of identifying, screening, and analyzing relevant literature in\nacademic research. More specifically, we examine the documented relationship\nbetween environmental, social, and governance (ESG) factors and the cost of\ncapital (CoC). By applying an AI-assisted workflow, we identified 36 published\nstudies, synthesized their key findings, and highlighted relevant theories,\nmoderators, and methodological challenges. Our analyses demonstrate the value\nof AI tools in enhancing business research processes and also contribute to the\ngrowing literature on the importance of ESG in the field of corporate finance."}
{"id": "2507.10196", "categories": ["cs.CE", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2507.10196", "abs": "https://arxiv.org/abs/2507.10196", "authors": ["Moritz Flaschel", "Trevor Hastie", "Ellen Kuhl"], "title": "Non-smooth optimization meets automated material model discovery", "comment": null, "summary": "Automated material model discovery disrupts the tedious and time-consuming\ncycle of iteratively calibrating and modifying manually designed models.\nNon-smooth L1-norm regularization is the backbone of automated model discovery;\nhowever, the current literature on automated material model discovery offers\nlimited insights into the robust and efficient minimization of non-smooth\nobjective functions. In this work, we examine the minimization of functions of\nthe form f(w) + a ||w||_1, where w are the material model parameters, f is a\nmetric that quantifies the mismatch between the material model and the observed\ndata, and a is a regularization parameter that determines the sparsity of the\nsolution. We investigate both the straightforward case where f is quadratic and\nthe more complex scenario where it is non-quadratic or even non-convex.\nImportantly, we do not only focus on methods that solve the sparse regression\nproblem for a given value of the regularization parameter a, but propose\nmethods to efficiently compute the entire regularization path, facilitating the\nselection of a suitable a. Specifically, we present four algorithms and discuss\ntheir roles for automated material model discovery in mechanics: First, we\nrecapitulate a well-known coordinate descent algorithm that solves the\nminimization problem assuming that f is quadratic for a given value of a, also\nknown as the LASSO. Second, we discuss the algorithm LARS, which automatically\ndetermines the critical values of a, at which material parameters in w are set\nto zero. Third, we propose to use the proximal gradient method ISTA for\nautomated material model discovery if f is not quadratic, and fourth, we\nsuggest a pathwise extension of ISTA for computing the regularization path. We\ndemonstrate the applicability of all algorithms for the discovery of\nhyperelastic material models from uniaxial tension and simple shear data."}
{"id": "2507.08969", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.08969", "abs": "https://arxiv.org/abs/2507.08969", "authors": ["Drew Walker", "Jennifer Love", "Swati Rajwal", "Isabel C Walker", "Hannah LF Cooper", "Abeed Sarker", "Melvin Livingston III"], "title": "Application of CARE-SD text classifier tools to assess distribution of stigmatizing and doubt-marking language features in EHR", "comment": "3 Tables", "summary": "Introduction: Electronic health records (EHR) are a critical medium through\nwhich patient stigmatization is perpetuated among healthcare teams. Methods: We\nidentified linguistic features of doubt markers and stigmatizing labels in\nMIMIC-III EHR via expanded lexicon matching and supervised learning\nclassifiers. Predictors of rates of linguistic features were assessed using\nPoisson regression models. Results: We found higher rates of stigmatizing\nlabels per chart among patients who were Black or African American (RR: 1.16),\npatients with Medicare/Medicaid or government-run insurance (RR: 2.46),\nself-pay (RR: 2.12), and patients with a variety of stigmatizing disease and\nmental health conditions. Patterns among doubt markers were similar, though\nmale patients had higher rates of doubt markers (RR: 1.25). We found increased\nstigmatizing labels used by nurses (RR: 1.40), and social workers (RR: 2.25),\nwith similar patterns of doubt markers. Discussion: Stigmatizing language\noccurred at higher rates among historically stigmatized patients, perpetuated\nby multiple provider types."}
{"id": "2507.09179", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09179", "abs": "https://arxiv.org/abs/2507.09179", "authors": ["Ronghua Shi", "Yiou Liu", "Xinyu Ying", "Yang Tan", "Yuchun Feng", "Lynn Ai", "Bill Shi", "Xuhui Wang", "Zhuang Liu"], "title": "Hide-and-Shill: A Reinforcement Learning Framework for Market Manipulation Detection in Symphony-a Decentralized Multi-Agent System", "comment": null, "summary": "Decentralized finance (DeFi) has introduced a new era of permissionless\nfinancial innovation but also led to unprecedented market manipulation. Without\ncentralized oversight, malicious actors coordinate shilling campaigns and\npump-and-dump schemes across various platforms. We propose a Multi-Agent\nReinforcement Learning (MARL) framework for decentralized manipulation\ndetection, modeling the interaction between manipulators and detectors as a\ndynamic adversarial game. This framework identifies suspicious patterns using\ndelayed token price reactions as financial indicators.Our method introduces\nthree innovations: (1) Group Relative Policy Optimization (GRPO) to enhance\nlearning stability in sparse-reward and partially observable settings; (2) a\ntheory-based reward function inspired by rational expectations and information\nasymmetry, differentiating price discovery from manipulation noise; and (3) a\nmulti-modal agent pipeline that integrates LLM-based semantic features, social\ngraph signals, and on-chain market data for informed decision-making.The\nframework is integrated within the Symphony system, a decentralized multi-agent\narchitecture enabling peer-to-peer agent execution and trust-aware learning\nthrough distributed logs, supporting chain-verifiable evaluation. Symphony\npromotes adversarial co-evolution among strategic actors and maintains robust\nmanipulation detection without centralized oracles, enabling real-time\nsurveillance across global DeFi ecosystems.Trained on 100,000 real-world\ndiscourse episodes and validated in adversarial simulations, Hide-and-Shill\nachieves top performance in detection accuracy and causal attribution. This\nwork bridges multi-agent systems with financial surveillance, advancing a new\nparadigm for decentralized market intelligence. All resources are available at\nthe Hide-and-Shill GitHub repository to promote open research and\nreproducibility."}
{"id": "2507.09060", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2507.09060", "abs": "https://arxiv.org/abs/2507.09060", "authors": ["Prajna Soni", "Deepika Raman", "Dylan Hadfield-Menell"], "title": "CALMA: A Process for Deriving Context-aligned Axes for Language Model Alignment", "comment": null, "summary": "Datasets play a central role in AI governance by enabling both evaluation\n(measuring capabilities) and alignment (enforcing values) along axes such as\nhelpfulness, harmlessness, toxicity, quality, and more. However, most alignment\nand evaluation datasets depend on researcher-defined or developer-defined axes\ncurated from non-representative samples. As a result, developers typically\nbenchmark models against broad (often Western-centric) values that overlook the\nvaried contexts of their real-world deployment. Consequently, models trained on\nsuch proxies can fail to meet the needs and expectations of diverse user\ncommunities within these deployment contexts. To bridge this gap, we introduce\nCALMA (Context-aligned Axes for Language Model Alignment), a grounded,\nparticipatory methodology for eliciting context-relevant axes for evaluation\nand alignment. In a pilot with two distinct communities, CALMA surfaced novel\npriorities that are absent from standard benchmarks. Our findings demonstrate\nthe value of evaluation practices based on open-ended and use-case-driven\nprocesses. Our work advances the development of pluralistic, transparent, and\ncontext-sensitive alignment pipelines."}
{"id": "2507.10448", "categories": ["cs.CE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10448", "abs": "https://arxiv.org/abs/2507.10448", "authors": ["Yingqian Wu", "Qiushi Wang", "Zefei Long", "Rong Ye", "Zhongtian Lu", "Xianyin Zhang", "Bingxuan Li", "Wei Chen", "Liwen Zhang", "Zhongyu Wei"], "title": "FinTeam: A Multi-Agent Collaborative Intelligence System for Comprehensive Financial Scenarios", "comment": "NLPCC 2025 Oral", "summary": "Financial report generation tasks range from macro- to micro-economics\nanalysis, also requiring extensive data analysis. Existing LLM models are\nusually fine-tuned on simple QA tasks and cannot comprehensively analyze real\nfinancial scenarios. Given the complexity, financial companies often distribute\ntasks among departments. Inspired by this, we propose FinTeam, a financial\nmulti-agent collaborative system, with a workflow with four LLM agents:\ndocument analyzer, analyst, accountant, and consultant. We train these agents\nwith specific financial expertise using constructed datasets. We evaluate\nFinTeam on comprehensive financial tasks constructed from real online\ninvestment forums, including macroeconomic, industry, and company analysis. The\nhuman evaluation shows that by combining agents, the financial reports generate\nfrom FinTeam achieved a 62.00% acceptance rate, outperforming baseline models\nlike GPT-4o and Xuanyuan. Additionally, FinTeam's agents demonstrate a 7.43%\naverage improvement on FinCUGE and a 2.06% accuracy boost on FinEval. Project\nis available at https://github.com/FudanDISC/DISC-FinLLM/."}
{"id": "2507.09011", "categories": ["cs.CL", "q-bio.NC", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2507.09011", "abs": "https://arxiv.org/abs/2507.09011", "authors": ["Ana Chkhaidze", "Reshanne R. Reeder", "Connor Gag", "Anastasia Kiyonaga", "Seana Coulson"], "title": "Beyond vividness: Content analysis of induced hallucinations reveals the hidden structure of individual differences in visual imagery", "comment": null, "summary": "A rapidly alternating red and black display known as Ganzflicker induces\nvisual hallucinations that reflect the generative capacity of the visual\nsystem. Recent proposals regarding the imagery spectrum, that is, differences\nin the visual system of individuals with absent imagery, typical imagery, and\nvivid imagery, suggest these differences should impact the complexity of other\ninternally generated visual experiences. Here, we used tools from natural\nlanguage processing to analyze free-text descriptions of hallucinations from\nover 4,000 participants, asking whether people with different imagery\nphenotypes see different things in their mind's eye during Ganzflicker-induced\nhallucinations. Strong imagers described complex, naturalistic content, while\nweak imagers reported simple geometric patterns. Embeddings from vision\nlanguage models better captured these differences than text-only language\nmodels, and participants with stronger imagery used language with richer\nsensorimotor associations. These findings may reflect individual variation in\ncoordination between early visual areas and higher-order regions relevant for\nthe imagery spectrum."}
{"id": "2507.09329", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.09329", "abs": "https://arxiv.org/abs/2507.09329", "authors": ["Matous Kozak", "Roshanak Zilouchian Moghaddam", "Siva Sivaraman"], "title": "When Developer Aid Becomes Security Debt: A Systematic Analysis of Insecure Behaviors in LLM Coding Agents", "comment": "15 pages", "summary": "LLM-based coding agents are rapidly being deployed in software development,\nyet their security implications remain poorly understood. These agents, while\ncapable of accelerating software development, may inadvertently introduce\ninsecure practices. We conducted the first systematic security evaluation of\nautonomous coding agents, analyzing over 12,000 actions across five\nstate-of-the-art models (GPT-4o, GPT-4.1, Claude variants) on 93 real-world\nsoftware setup tasks. Our findings reveal significant security concerns: 21% of\nagent trajectories contained insecure actions, with models showing substantial\nvariation in security behavior. We developed a high-precision detection system\nthat identified four major vulnerability categories, with information exposure\n(CWE-200) being the most prevalent one. We also evaluated mitigation strategies\nincluding feedback mechanisms and security reminders with various effectiveness\nbetween models. GPT-4.1 demonstrated exceptional security awareness with 96.8%\nmitigation success. Our work provides the first comprehensive framework for\nevaluating coding agent security and highlights the need for security-aware\ndesign of next generation LLM-based coding agents."}
{"id": "2507.09233", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2507.09233", "abs": "https://arxiv.org/abs/2507.09233", "authors": ["Jia Xiao"], "title": "Secondary Bounded Rationality: A Theory of How Algorithms Reproduce Structural Inequality in AI Hiring", "comment": null, "summary": "AI-driven recruitment systems, while promising efficiency and objectivity,\noften perpetuate systemic inequalities by encoding cultural and social capital\ndisparities into algorithmic decision making. This article develops and defends\na novel theory of secondary bounded rationality, arguing that AI systems,\ndespite their computational power, inherit and amplify human cognitive and\nstructural biases through technical and sociopolitical constraints. Analyzing\nmultimodal recruitment frameworks, we demonstrate how algorithmic processes\ntransform historical inequalities, such as elite credential privileging and\nnetwork homophily, into ostensibly meritocratic outcomes. Using Bourdieusian\ncapital theory and Simon's bounded rationality, we reveal a recursive cycle\nwhere AI entrenches exclusion by optimizing for legible yet biased proxies of\ncompetence. We propose mitigation strategies, including counterfactual fairness\ntesting, capital-aware auditing, and regulatory interventions, to disrupt this\nself-reinforcing inequality."}
{"id": "2507.09025", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09025", "abs": "https://arxiv.org/abs/2507.09025", "authors": ["Chien Van Nguyen", "Ruiyi Zhang", "Hanieh Deilamsalehy", "Puneet Mathur", "Viet Dac Lai", "Haoliang Wang", "Jayakumar Subramanian", "Ryan A. Rossi", "Trung Bui", "Nikos Vlassis", "Franck Dernoncourt", "Thien Huu Nguyen"], "title": "Lizard: An Efficient Linearization Framework for Large Language Models", "comment": "15 pages", "summary": "We propose Lizard, a linearization framework that transforms pretrained\nTransformer-based Large Language Models (LLMs) into flexible, subquadratic\narchitectures for infinite-context generation. Transformer-based LLMs face\nsignificant memory and computational bottlenecks as context lengths increase,\ndue to the quadratic complexity of softmax attention and the growing key-value\n(KV) cache. Lizard addresses these limitations by introducing a subquadratic\nattention mechanism that closely approximates softmax attention while\npreserving the output quality. Unlike previous linearization methods, which are\noften limited by fixed model structures and therefore exclude gating\nmechanisms, Lizard incorporates a gating module inspired by recent\nstate-of-the-art linear models. This enables adaptive memory control, supports\nconstant-memory inference, offers strong length generalization, and allows more\nflexible model design. Lizard combines gated linear attention for global\ncontext compression with sliding window attention enhanced by meta memory,\nforming a hybrid mechanism that captures both long-range dependencies and\nfine-grained local interactions. Moreover, we introduce a hardware-aware\nalgorithm that accelerates the training speed of our models. Extensive\nexperiments show that Lizard achieves near-lossless recovery of the teacher\nmodel's performance across standard language modeling tasks, while\nsignificantly outperforming previous linearization methods. On the 5-shot MMLU\nbenchmark, Lizard improves over prior models by 18 points and shows significant\nimprovements on associative recall tasks."}
{"id": "2507.09369", "categories": ["cs.AI", "68T01", "I.2.0"], "pdf": "https://arxiv.org/pdf/2507.09369", "abs": "https://arxiv.org/abs/2507.09369", "authors": ["Andrew Critch", "Jacob Tsimerman"], "title": "A Taxonomy of Omnicidal Futures Involving Artificial Intelligence", "comment": null, "summary": "This report presents a taxonomy and examples of potential omnicidal events\nresulting from AI: scenarios where all or almost all humans are killed. These\nevents are not presented as inevitable, but as possibilities that we can work\nto avoid. Insofar as large institutions require a degree of public support in\norder to take certain actions, we hope that by presenting these possibilities\nin public, we can help to support preventive measures against catastrophic\nrisks from AI."}
{"id": "2507.09239", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2507.09239", "abs": "https://arxiv.org/abs/2507.09239", "authors": ["Yinan Sun", "Ali Unlu", "Aditya Johri"], "title": "The Narrative Construction of Generative AI Efficacy by the Media: A Case Study of the Role of ChatGPT in Higher Education", "comment": "Final draft of article under review at a journal", "summary": "The societal role of technology, including artificial intelligence (AI), is\noften shaped by sociocultural narratives. This study examines how U.S. news\nmedia construct narratives about the efficacy of generative AI (GenAI), using\nChatGPT in higher education as a case study. Grounded in Agenda Setting Theory,\nwe analyzed 198 articles published between November 2022 and October 2024,\nemploying LDA topic modeling and sentiment analysis. Our findings identify six\nkey topics in the media discourse, with sentiment analysis revealing generally\npositive portrayals of ChatGPT's integration into higher education through\npolicy, curriculum, teaching practices, collaborative decision-making, skill\ndevelopment, and human-centered learning. In contrast, media narratives express\nmore negative sentiment regarding their impact on entry-level jobs and college\nadmissions. This research highlights how media coverage can influence public\nperceptions of GenAI in education and provides actionable insights for\npolicymakers, educators, and AI developers navigating its adoption and\nrepresentation in public discourse."}
{"id": "2507.09037", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09037", "abs": "https://arxiv.org/abs/2507.09037", "authors": ["Bharadwaj Ravichandran", "David Joy", "Paul Elliott", "Brian Hu", "Jadie Adams", "Christopher Funk", "Emily Veenhuis", "Anthony Hoogs", "Arslan Basharat"], "title": "ALIGN: Prompt-based Attribute Alignment for Reliable, Responsible, and Personalized LLM-based Decision-Making", "comment": "10 pages total (including appendix), ICML 2025 Workshop on Reliable\n  and Responsible Foundation Models", "summary": "Large language models (LLMs) are increasingly being used as decision aids.\nHowever, users have diverse values and preferences that can affect their\ndecision-making, which requires novel methods for LLM alignment and\npersonalization. Existing LLM comparison tools largely focus on benchmarking\ntasks, such as knowledge-based question answering. In contrast, our proposed\nALIGN system focuses on dynamic personalization of LLM-based decision-makers\nthrough prompt-based alignment to a set of fine-grained attributes. Key\nfeatures of our system include robust configuration management, structured\noutput generation with reasoning, and several algorithm implementations with\nswappable LLM backbones, enabling different types of analyses. Our user\ninterface enables a qualitative, side-by-side comparison of LLMs and their\nalignment to various attributes, with a modular backend for easy algorithm\nintegration. Additionally, we perform a quantitative analysis comparing\nalignment approaches in two different domains: demographic alignment for public\nopinion surveys and value alignment for medical triage decision-making. The\nentire ALIGN framework is open source and will enable new research on reliable,\nresponsible, and personalized LLM-based decision-makers."}
{"id": "2507.09374", "categories": ["cs.AI", "I.2.6; I.2.10"], "pdf": "https://arxiv.org/pdf/2507.09374", "abs": "https://arxiv.org/abs/2507.09374", "authors": ["Chenglin Zhu", "Tao Zhang", "Chong Li", "Mingan Lin", "Zenan Zhou", "Jian Xie"], "title": "EduFlow: Advancing MLLMs' Problem-Solving Proficiency through Multi-Stage, Multi-Perspective Critique", "comment": "14 pages,4 figures", "summary": "Multimodal large language models (MLLMs) still perform poorly on scientific\ntasks, particularly those requiring multi-step and interpretable reasoning.\nTheir limitations include insufficient scientific reasoning patterns, lack of\nglobal coherence in multi-step inference, and the absence of reflective\nself-correction, making them unreliable in structured scientific contexts. We\nintroduce EduFlow, the first end-to-end framework that covers the full pipeline\nof educational scientific reasoning, including data selection, MCTS-based\ntrajectory construction, model training, and output optimization. At its core\nis EduPRM, a process-aware reward model that critiques reasoning steps with\ntags and justifications. EduPRM is trained via curriculum learning on three\ncomplementary supervision sources: MCTS-guided trajectories, error-injected\ncritiques, and teacher-student dialogues, enabling dynamic adaptation to\nmulti-stage problem solving and iterative refinement during inference. We\nfurther propose EduMCTS, a domain-adapted search framework that introduces\nbootstrapping actions specifically designed for educational reasoning, such as\na self-reflection mechanism that promotes reflective error correction. It\nfurther leverages EduPRM's fine-grained feedback to guide the search toward\nhigher-quality reasoning trajectories. By applying self-consistency and\nrejection sampling, we constructed EduMCTS-160K, a large-scale dataset of\neducational reasoning trajectories. Extensive experiments demonstrate that\nEduFlow enhances reasoning consistency and coherence. Code, data, and models\nwill be released."}
{"id": "2507.09296", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2507.09296", "abs": "https://arxiv.org/abs/2507.09296", "authors": ["Joshua Tan", "Nicholas Vincent", "Katherine Elkins", "Magnus Sahlgren"], "title": "If open source is to win, it must go public", "comment": "CodeML @ ICML 2025", "summary": "Open source projects have made incredible progress in producing transparent\nand widely usable machine learning models and systems, but open source alone\nwill face challenges in fully democratizing access to AI. Unlike software, AI\nmodels require substantial resources for activation -- compute, post-training,\ndeployment, and oversight -- which only a few actors can currently provide.\nThis paper argues that open source AI must be complemented by public AI:\ninfrastructure and institutions that ensure models are accessible, sustainable,\nand governed in the public interest. To achieve the full promise of AI models\nas prosocial public goods, we need to build public infrastructure to power and\ndeliver open source software and models."}
{"id": "2507.09075", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09075", "abs": "https://arxiv.org/abs/2507.09075", "authors": ["Wasi Uddin Ahmad", "Somshubra Majumdar", "Aleksander Ficek", "Sean Narenthiran", "Mehrzad Samadi", "Jocelyn Huang", "Siddhartha Jain", "Vahid Noroozi", "Boris Ginsburg"], "title": "OpenCodeReasoning-II: A Simple Test Time Scaling Approach via Self-Critique", "comment": "work in progress", "summary": "Recent advancements in reasoning-based Large Language Models (LLMs),\nparticularly their potential through test-time scaling, have created\nsignificant opportunities for distillation in code generation and critique.\nHowever, progress in both areas fundamentally depends on large-scale,\nhigh-quality datasets. In this work, we introduce OpenCodeReasoning-II, a\ndataset consists of 2.5M question-solution-critique triples (approx. 35K unique\nprogramming questions), making it nearly twice the size of the previous largest\npublicly available code reasoning dataset. In this work, we employ a two-stage\nsupervised fine-tuning strategy. The first stage focuses on fine-tuning for\ncode generation, while the second stage involves the joint training of models\nfor both code generation and critique. Our resulting finetuned Qwen2.5-Instruct\nmodels achieve performance in code generation that either exceeds or equals the\nbest prior open-weight distilled models. Notably, the integration of our code\ngeneration and critique models leads to significant improvements in competitive\ncoding performance. Furthermore, we present an extension of the LiveCodeBench\nbenchmark to specifically support the C++ programming language, thereby\nfacilitating more comprehensive LLM evaluation using this benchmark."}
{"id": "2507.09389", "categories": ["cs.AI", "cs.CY", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.09389", "abs": "https://arxiv.org/abs/2507.09389", "authors": ["Chris Davis Jaldi", "Anmol Saini", "Elham Ghiasi", "O. Divine Eziolise", "Cogan Shimizu"], "title": "Knowledge Conceptualization Impacts RAG Efficacy", "comment": null, "summary": "Explainability and interpretability are cornerstones of frontier and\nnext-generation artificial intelligence (AI) systems. This is especially true\nin recent systems, such as large language models (LLMs), and more broadly,\ngenerative AI. On the other hand, adaptability to new domains, contexts, or\nscenarios is also an important aspect for a successful system. As such, we are\nparticularly interested in how we can merge these two efforts, that is,\ninvestigating the design of transferable and interpretable neurosymbolic AI\nsystems. Specifically, we focus on a class of systems referred to as ''Agentic\nRetrieval-Augmented Generation'' systems, which actively select, interpret, and\nquery knowledge sources in response to natural language prompts. In this paper,\nwe systematically evaluate how different conceptualizations and representations\nof knowledge, particularly the structure and complexity, impact an AI agent (in\nthis case, an LLM) in effectively querying a triplestore. We report our\nresults, which show that there are impacts from both approaches, and we discuss\ntheir impact and implications."}
{"id": "2507.09676", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2507.09676", "abs": "https://arxiv.org/abs/2507.09676", "authors": ["Matthieu Queloz"], "title": "Can AI Rely on the Systematicity of Truth? The Challenge of Modelling Normative Domains", "comment": null, "summary": "A key assumption fuelling optimism about the progress of large language\nmodels (LLMs) in accurately and comprehensively modelling the world is that the\ntruth is systematic: true statements about the world form a whole that is not\njust consistent, in that it contains no contradictions, but coherent, in that\nthe truths are inferentially interlinked. This holds out the prospect that LLMs\nmight in principle rely on that systematicity to fill in gaps and correct\ninaccuracies in the training data: consistency and coherence promise to\nfacilitate progress towards comprehensiveness in an LLM's representation of the\nworld. However, philosophers have identified compelling reasons to doubt that\nthe truth is systematic across all domains of thought, arguing that in\nnormative domains, in particular, the truth is largely asystematic. I argue\nthat insofar as the truth in normative domains is asystematic, this renders it\ncorrespondingly harder for LLMs to make progress, because they cannot then\nleverage the systematicity of truth. And the less LLMs can rely on the\nsystematicity of truth, the less we can rely on them to do our practical\ndeliberation for us, because the very asystematicity of normative domains\nrequires human agency to play a greater role in practical thought."}
{"id": "2507.09076", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7; H.5.2"], "pdf": "https://arxiv.org/pdf/2507.09076", "abs": "https://arxiv.org/abs/2507.09076", "authors": ["Jialong Mai", "Xiaofen Xing", "Yawei Li", "Zhipeng Li", "Jingyuan Xing", "Xiangmin Xu"], "title": "Dynamic Parameter Memory: Temporary LoRA-Enhanced LLM for Long-Sequence Emotion Recognition in Conversation", "comment": "submitted to EMNLP 2025", "summary": "Recent research has focused on applying speech large language model (SLLM) to\nimprove speech emotion recognition (SER). However, the inherently high frame\nrate in speech modality severely limits the signal processing and understanding\ncapabilities of SLLM. For example, a SLLM with a 4K context window can only\nprocess 80 seconds of audio at 50Hz feature sampling rate before reaching its\ncapacity limit. Input token compression methods used in SLLM overlook the\ncontinuity and inertia of emotions across multiple conversation turns. This\npaper proposes a Dynamic Parameter Memory (DPM) mechanism with contextual\nsemantics and sentence-level emotion encoding, enabling processing of\nunlimited-length audio with limited context windows in SLLM. Specifically, DPM\nprogressively encodes sentence-level information and emotions into a temporary\nLoRA module during inference to effectively \"memorize\" the contextual\ninformation. We trained an emotion SLLM as a backbone and incorporated our DPM\ninto inference for emotion recognition in conversation (ERC). Experimental\nresults on the IEMOCAP dataset show that DPM significantly improves the emotion\nrecognition capabilities of SLLM when processing long audio sequences,\nachieving state-of-the-art performance."}
{"id": "2507.09407", "categories": ["cs.AI", "cs.CR", "cs.GT"], "pdf": "https://arxiv.org/pdf/2507.09407", "abs": "https://arxiv.org/abs/2507.09407", "authors": ["Quanyan Zhu"], "title": "LLM-Stackelberg Games: Conjectural Reasoning Equilibria and Their Applications to Spearphishing", "comment": null, "summary": "We introduce the framework of LLM-Stackelberg games, a class of sequential\ndecision-making models that integrate large language models (LLMs) into\nstrategic interactions between a leader and a follower. Departing from\nclassical Stackelberg assumptions of complete information and rational agents,\nour formulation allows each agent to reason through structured prompts,\ngenerate probabilistic behaviors via LLMs, and adapt their strategies through\ninternal cognition and belief updates. We define two equilibrium concepts:\nreasoning and behavioral equilibrium, which aligns an agent's internal\nprompt-based reasoning with observable behavior, and conjectural reasoning\nequilibrium, which accounts for epistemic uncertainty through parameterized\nmodels over an opponent's response. These layered constructs capture bounded\nrationality, asymmetric information, and meta-cognitive adaptation. We\nillustrate the framework through a spearphishing case study, where a sender and\na recipient engage in a deception game using structured reasoning prompts. This\nexample highlights the cognitive richness and adversarial potential of\nLLM-mediated interactions. Our results show that LLM-Stackelberg games provide\na powerful paradigm for modeling decision-making in domains such as\ncybersecurity, misinformation, and recommendation systems."}
{"id": "2507.09225", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.09225", "abs": "https://arxiv.org/abs/2507.09225", "authors": ["Biagio Scalingi", "Chiara Barattieri di San Pietro", "Paolo Canal", "Valentina Bambini"], "title": "MetaClimage: A novel database of visual metaphors related to Climate Change, with costs and benefits analysis", "comment": "27 pages, 5 figures", "summary": "Visual metaphors of climate change (e.g., melting glaciers depicted as a\nmelting ice grenade) are regarded as valuable tools for addressing the\ncomplexity of environmental challenges. However, few studies have examined\ntheir impact on communication, also due to scattered availability of material.\nHere, we present a novel database of Metaphors of Climate Change in Images\n(MetaClimage) https://doi.org/10.5281/zenodo.15861012, paired with literal\nimages and enriched with human ratings. For each image, we collected values of\ndifficulty, efficacy, artistic quality, and emotional arousal from human\nrating, as well as number of tags generated by participants to summarize the\nmessage. Semantic and emotion variables were further derived from the tags via\nNatural Language Processing. Visual metaphors were rated as more difficult to\nunderstand, yet more aesthetically pleasant than literal images, but did not\ndiffer in efficacy and arousal. The latter for visual metaphors, however, was\nhigher in participants with higher Need For Cognition. Furthermore, visual\nmetaphors received more tags, often referring to entities not depicted in the\nimage, and elicited words with more positive valence and greater dominance than\nliteral images. These results evidence the greater cognitive load of visual\nmetaphors, which nevertheless might induce positive effects such as deeper\ncognitive elaboration and abstraction compared to literal stimuli. Furthermore,\nwhile they are not deemed as more effective and arousing, visual metaphors seem\nto generate superior aesthetic appreciation and a more positively valenced\nexperience. Overall, this study contributes to understanding the impact of\nvisual metaphors of climate change both by offering a database for future\nresearch and by elucidating a cost-benefit trade-off to take into account when\nshaping environmental communication."}
{"id": "2507.09104", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09104", "abs": "https://arxiv.org/abs/2507.09104", "authors": ["Taolin Zhang", "Maosong Cao", "Alexander Lam", "Songyang Zhang", "Kai Chen"], "title": "CompassJudger-2: Towards Generalist Judge Model via Verifiable Rewards", "comment": null, "summary": "Recently, the role of LLM-as-judge in evaluating large language models has\ngained prominence. However, current judge models suffer from narrow\nspecialization and limited robustness, undermining their capacity for\ncomprehensive evaluations. In this work, we present CompassJudger-2, a novel\ngeneralist judge model that overcomes these limitations via a task-driven,\nmulti-domain data curation strategy. Central to our approach is supervising\njudgment tasks with verifiable rewards, guiding intrinsic critical reasoning\nthrough rejection sampling to foster robust, generalizable judgment\ncapabilities. We introduce a refined learning objective with margin policy\ngradient loss to enhance performance. Empirically, CompassJudger-2 achieves\nsuperior results across multiple judge and reward benchmarks, and our 7B model\ndemonstrates competitive judgment accuracy with significantly larger models\nlike DeepSeek-V3 and Qwen3-235B-A22B. Additionally, we propose JudgerBenchV2, a\ncomprehensive benchmark evaluating cross-domain judgment accuracy and rank\nconsistency to standardize judge model evaluation. These contributions advance\nrobust, scalable LLM judgment and establish new performance and evaluation\nstandards."}
{"id": "2507.09495", "categories": ["cs.AI", "cs.ET", "cs.HC", "cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.09495", "abs": "https://arxiv.org/abs/2507.09495", "authors": ["Hang Wang", "Junshan Zhang"], "title": "GenAI-based Multi-Agent Reinforcement Learning towards Distributed Agent Intelligence: A Generative-RL Agent Perspective", "comment": "Position paper", "summary": "Multi-agent reinforcement learning faces fundamental challenges that\nconventional approaches have failed to overcome: exponentially growing joint\naction spaces, non-stationary environments where simultaneous learning creates\nmoving targets, and partial observability that constrains coordination. Current\nmethods remain reactive, employing stimulus-response mechanisms that fail when\nfacing novel scenarios. We argue for a transformative paradigm shift from\nreactive to proactive multi-agent intelligence through generative AI-based\nreinforcement learning. This position advocates reconceptualizing agents not as\nisolated policy optimizers, but as sophisticated generative models capable of\nsynthesizing complex multi-agent dynamics and making anticipatory decisions\nbased on predictive understanding of future interactions. Rather than\nresponding to immediate observations, generative-RL agents can model\nenvironment evolution, predict other agents' behaviors, generate coordinated\naction sequences, and engage in strategic reasoning accounting for long-term\ndynamics. This approach leverages pattern recognition and generation\ncapabilities of generative AI to enable proactive decision-making, seamless\ncoordination through enhanced communication, and dynamic adaptation to evolving\nscenarios. We envision this paradigm shift will unlock unprecedented\npossibilities for distributed intelligence, moving beyond individual\noptimization toward emergent collective behaviors representing genuine\ncollaborative intelligence. The implications extend across autonomous systems,\nrobotics, and human-AI collaboration, promising solutions to coordination\nchallenges intractable under traditional reactive frameworks."}
{"id": "2507.09389", "categories": ["cs.AI", "cs.CY", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.09389", "abs": "https://arxiv.org/abs/2507.09389", "authors": ["Chris Davis Jaldi", "Anmol Saini", "Elham Ghiasi", "O. Divine Eziolise", "Cogan Shimizu"], "title": "Knowledge Conceptualization Impacts RAG Efficacy", "comment": null, "summary": "Explainability and interpretability are cornerstones of frontier and\nnext-generation artificial intelligence (AI) systems. This is especially true\nin recent systems, such as large language models (LLMs), and more broadly,\ngenerative AI. On the other hand, adaptability to new domains, contexts, or\nscenarios is also an important aspect for a successful system. As such, we are\nparticularly interested in how we can merge these two efforts, that is,\ninvestigating the design of transferable and interpretable neurosymbolic AI\nsystems. Specifically, we focus on a class of systems referred to as ''Agentic\nRetrieval-Augmented Generation'' systems, which actively select, interpret, and\nquery knowledge sources in response to natural language prompts. In this paper,\nwe systematically evaluate how different conceptualizations and representations\nof knowledge, particularly the structure and complexity, impact an AI agent (in\nthis case, an LLM) in effectively querying a triplestore. We report our\nresults, which show that there are impacts from both approaches, and we discuss\ntheir impact and implications."}
{"id": "2507.09155", "categories": ["cs.CL", "cs.AI", "68T50, 68T07"], "pdf": "https://arxiv.org/pdf/2507.09155", "abs": "https://arxiv.org/abs/2507.09155", "authors": ["Ali Vosoughi", "Ayoub Shahnazari", "Yufeng Xi", "Zeliang Zhang", "Griffin Hess", "Chenliang Xu", "Niaz Abdolrahim"], "title": "OPENXRD: A Comprehensive Benchmark and Enhancement Framework for LLM/MLLM XRD Question Answering", "comment": "10 pages, 6 figures, 5 tables. Code and dataset available at\n  https://github.com/niaz60/OpenXRD. Project webpage:\n  https://niaz60.github.io/OpenXRD/", "summary": "This work presents OPENXRD, an open-book pipeline designed for\ncrystallography question answering, which integrates textual prompts with\nconcise supporting content generated by GPT-4.5. Instead of using scanned\ntextbooks, which may lead to copyright issues, OPENXRD generates compact,\ndomain-specific references that help smaller models understand key concepts in\nX-ray diffraction (XRD). We evaluate OPENXRD on a well-defined set of 217\nexpert-level XRD questions by comparing different vision-language models,\nincluding GPT-4 and LLaVA-based frameworks such as Mistral, LLaMA, and QWEN,\nunder both closed-book (without supporting material) and open-book (with\nsupporting material) conditions. Our experimental results show significant\naccuracy improvements in models that use the GPT-4.5-generated summaries,\nparticularly those with limited prior training in crystallography. OPENXRD uses\nknowledge from larger models to fill knowledge gaps in crystallography and\nshows that AI-generated texts can help smaller models reason more effectively\nin scientific tasks. While the current version of OPENXRD focuses on text-based\ninputs, we also explore future extensions such as adding real crystal diagrams\nor diffraction patterns to improve interpretation in specialized materials\nscience contexts. Overall, OPENXRD shows that specialized open-book systems can\nbe useful in materials science and provides a foundation for broader natural\nlanguage processing (NLP) tools in critical scientific fields."}
{"id": "2507.09534", "categories": ["cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.09534", "abs": "https://arxiv.org/abs/2507.09534", "authors": ["Guanquan Wang", "Takuya Hiraoka", "Yoshimasa Tsuruoka"], "title": "Consistency Trajectory Planning: High-Quality and Efficient Trajectory Optimization for Offline Model-Based Reinforcement Learning", "comment": null, "summary": "This paper introduces Consistency Trajectory Planning (CTP), a novel offline\nmodel-based reinforcement learning method that leverages the recently proposed\nConsistency Trajectory Model (CTM) for efficient trajectory optimization. While\nprior work applying diffusion models to planning has demonstrated strong\nperformance, it often suffers from high computational costs due to iterative\nsampling procedures. CTP supports fast, single-step trajectory generation\nwithout significant degradation in policy quality. We evaluate CTP on the D4RL\nbenchmark and show that it consistently outperforms existing diffusion-based\nplanning methods in long-horizon, goal-conditioned tasks. Notably, CTP achieves\nhigher normalized returns while using significantly fewer denoising steps. In\nparticular, CTP achieves comparable performance with over $120\\times$ speedup\nin inference time, demonstrating its practicality and effectiveness for\nhigh-performance, low-latency offline planning."}
{"id": "2507.09611", "categories": ["cs.AI", "cs.CY", "68T01"], "pdf": "https://arxiv.org/pdf/2507.09611", "abs": "https://arxiv.org/abs/2507.09611", "authors": ["Jenis Winsta"], "title": "The Hidden Costs of AI: A Review of Energy, E-Waste, and Inequality in Model Development", "comment": "5 pages, 3 figures", "summary": "Artificial intelligence (AI) has made remarkable progress in recent years,\nyet its rapid expansion brings overlooked environmental and ethical challenges.\nThis review explores four critical areas where AI's impact extends beyond\nperformance: energy consumption, electronic waste (e-waste), inequality in\ncompute access, and the hidden energy burden of cybersecurity systems. Drawing\nfrom recent studies and institutional reports, the paper highlights systemic\nissues such as high emissions from model training, rising hardware turnover,\nglobal infrastructure disparities, and the energy demands of securing AI. By\nconnecting these concerns, the review contributes to Responsible AI discourse\nby identifying key research gaps and advocating for sustainable, transparent,\nand equitable development practices. Ultimately, it argues that AI's progress\nmust align with ethical responsibility and environmental stewardship to ensure\na more inclusive and sustainable technological future."}
{"id": "2507.09157", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09157", "abs": "https://arxiv.org/abs/2507.09157", "authors": ["Bhavinkumar Vinodbhai Kuwar", "Bikrant Bikram Pratap Maurya", "Priyanshu Gupta", "Nitin Choudhury"], "title": "PU-Lie: Lightweight Deception Detection in Imbalanced Diplomatic Dialogues via Positive-Unlabeled Learning", "comment": null, "summary": "Detecting deception in strategic dialogues is a complex and high-stakes task\ndue to the subtlety of language and extreme class imbalance between deceptive\nand truthful communications. In this work, we revisit deception detection in\nthe Diplomacy dataset, where less than 5% of messages are labeled deceptive. We\nintroduce a lightweight yet effective model combining frozen BERT embeddings,\ninterpretable linguistic and game-specific features, and a Positive-Unlabeled\n(PU) learning objective. Unlike traditional binary classifiers, PU-Lie is\ntailored for situations where only a small portion of deceptive messages are\nlabeled, and the majority are unlabeled. Our model achieves a new best macro F1\nof 0.60 while reducing trainable parameters by over 650x. Through comprehensive\nevaluations and ablation studies across seven models, we demonstrate the value\nof PU learning, linguistic interpretability, and speaker-aware representations.\nNotably, we emphasize that in this problem setting, accurately detecting\ndeception is more critical than identifying truthful messages. This priority\nguides our choice of PU learning, which explicitly models the rare but vital\ndeceptive class."}
{"id": "2507.09540", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.09540", "abs": "https://arxiv.org/abs/2507.09540", "authors": ["Ali Safa", "Farida Mohsen", "Ali Al-Zawqari"], "title": "Learning to Control Dynamical Agents via Spiking Neural Networks and Metropolis-Hastings Sampling", "comment": null, "summary": "Spiking Neural Networks (SNNs) offer biologically inspired, energy-efficient\nalternatives to traditional Deep Neural Networks (DNNs) for real-time control\nsystems. However, their training presents several challenges, particularly for\nreinforcement learning (RL) tasks, due to the non-differentiable nature of\nspike-based communication. In this work, we introduce what is, to our\nknowledge, the first framework that employs Metropolis-Hastings (MH) sampling,\na Bayesian inference technique, to train SNNs for dynamical agent control in RL\nenvironments without relying on gradient-based methods. Our approach\niteratively proposes and probabilistically accepts network parameter updates\nbased on accumulated reward signals, effectively circumventing the limitations\nof backpropagation while enabling direct optimization on neuromorphic\nplatforms. We evaluated this framework on two standard control benchmarks:\nAcroBot and CartPole. The results demonstrate that our MH-based approach\noutperforms conventional Deep Q-Learning (DQL) baselines and prior SNN-based RL\napproaches in terms of maximizing the accumulated reward while minimizing\nnetwork resources and training episodes."}
{"id": "2507.09801", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.09801", "abs": "https://arxiv.org/abs/2507.09801", "authors": ["Peter Barnett", "Aaron Scher", "David Abecassis"], "title": "Technical Requirements for Halting Dangerous AI Activities", "comment": null, "summary": "The rapid development of AI systems poses unprecedented risks, including loss\nof control, misuse, geopolitical instability, and concentration of power. To\nnavigate these risks and avoid worst-case outcomes, governments may proactively\nestablish the capability for a coordinated halt on dangerous AI development and\ndeployment. In this paper, we outline key technical interventions that could\nallow for a coordinated halt on dangerous AI activities. We discuss how these\ninterventions may contribute to restricting various dangerous AI activities,\nand show how these interventions can form the technical foundation for\npotential AI governance plans."}
{"id": "2507.09174", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09174", "abs": "https://arxiv.org/abs/2507.09174", "authors": ["Shuo Yang", "Zijian Yu", "Zhenzhe Ying", "Yuqin Dai", "Guoqing Wang", "Jun Lan", "Jinfeng Xu", "Jinze Li", "Edith C. H. Ngai"], "title": "RAMA: Retrieval-Augmented Multi-Agent Framework for Misinformation Detection in Multimodal Fact-Checking", "comment": null, "summary": "The rapid proliferation of multimodal misinformation presents significant\nchallenges for automated fact-checking systems, especially when claims are\nambiguous or lack sufficient context. We introduce RAMA, a novel\nretrieval-augmented multi-agent framework designed for verifying multimedia\nmisinformation. RAMA incorporates three core innovations: (1) strategic query\nformulation that transforms multimodal claims into precise web search queries;\n(2) cross-verification evidence aggregation from diverse, authoritative\nsources; and (3) a multi-agent ensemble architecture that leverages the\ncomplementary strengths of multiple multimodal large language models and prompt\nvariants. Extensive experiments demonstrate that RAMA achieves superior\nperformance on benchmark datasets, particularly excelling in resolving\nambiguous or improbable claims by grounding verification in retrieved factual\nevidence. Our findings underscore the necessity of integrating web-based\nevidence and multi-agent reasoning for trustworthy multimedia verification,\npaving the way for more reliable and scalable fact-checking solutions. RAMA\nwill be publicly available at https://github.com/kalendsyang/RAMA.git."}
{"id": "2507.09588", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09588", "abs": "https://arxiv.org/abs/2507.09588", "authors": ["Isaac Shi", "Zeyuan Li", "Fan Liu", "Wenli Wang", "Lewei He", "Yang Yang", "Tianyu Shi"], "title": "eSapiens: A Platform for Secure and Auditable Retrieval-Augmented Generation", "comment": null, "summary": "We present eSapiens, an AI-as-a-Service (AIaaS) platform engineered around a\nbusiness-oriented trifecta: proprietary data, operational workflows, and any\nmajor agnostic Large Language Model (LLM). eSapiens gives businesses full\ncontrol over their AI assets, keeping everything in-house for AI knowledge\nretention and data security. eSapiens AI Agents (Sapiens) empower your team by\nproviding valuable insights and automating repetitive tasks, enabling them to\nfocus on high-impact work and drive better business outcomes.\n  The system integrates structured document ingestion, hybrid vector retrieval,\nand no-code orchestration via LangChain, and supports top LLMs including\nOpenAI, Claude, Gemini, and DeepSeek. A key component is the THOR Agent, which\nhandles structured SQL-style queries and generates actionable insights over\nenterprise databases.\n  To evaluate the system, we conduct two experiments. First, a retrieval\nbenchmark on legal corpora reveals that a chunk size of 512 tokens yields the\nhighest retrieval precision (Top-3 accuracy: 91.3%). Second, a generation\nquality test using TRACe metrics across five LLMs shows that eSapiens delivers\nmore context-consistent outputs with up to a 23% improvement in factual\nalignment.\n  These results demonstrate the effectiveness of eSapiens in enabling\ntrustworthy, auditable AI workflows for high-stakes domains like legal and\nfinance."}
{"id": "2507.09185", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09185", "abs": "https://arxiv.org/abs/2507.09185", "authors": ["Ameen Ali", "Shahar Katz", "Lior Wolf", "Ivan Titov"], "title": "Detecting and Pruning Prominent but Detrimental Neurons in Large Language Models", "comment": null, "summary": "Large language models (LLMs) often develop learned mechanisms specialized to\nspecific datasets, such as reliance on domain-specific correlations, which\nyield high-confidence predictions without generalizable reasoning. While\nbeneficial in one setting, these dataset-specific mechanisms typically degrade\nperformance when models encounter novel tasks or distributions. In this work,\nwe introduce a fine-tuning approach designed to enhance generalization by\nidentifying and pruning neurons associated with dataset-specific mechanisms in\ntransformer-based LLMs. Our method employs Integrated Gradients to quantify\neach neuron's influence on high-confidence predictions, pinpointing those that\ndisproportionately contribute to dataset-specific performance without\nsupporting robust, transferable reasoning. Selectively pruning these neurons\ncompels the model to depend on generalizable representations. Evaluated across\nmultiple-choice benchmarks, our pruning-based fine-tuning significantly\nenhances performance, surpassing prior (non-pruning) adaptation methods."}
{"id": "2507.09611", "categories": ["cs.AI", "cs.CY", "68T01"], "pdf": "https://arxiv.org/pdf/2507.09611", "abs": "https://arxiv.org/abs/2507.09611", "authors": ["Jenis Winsta"], "title": "The Hidden Costs of AI: A Review of Energy, E-Waste, and Inequality in Model Development", "comment": "5 pages, 3 figures", "summary": "Artificial intelligence (AI) has made remarkable progress in recent years,\nyet its rapid expansion brings overlooked environmental and ethical challenges.\nThis review explores four critical areas where AI's impact extends beyond\nperformance: energy consumption, electronic waste (e-waste), inequality in\ncompute access, and the hidden energy burden of cybersecurity systems. Drawing\nfrom recent studies and institutional reports, the paper highlights systemic\nissues such as high emissions from model training, rising hardware turnover,\nglobal infrastructure disparities, and the energy demands of securing AI. By\nconnecting these concerns, the review contributes to Responsible AI discourse\nby identifying key research gaps and advocating for sustainable, transparent,\nand equitable development practices. Ultimately, it argues that AI's progress\nmust align with ethical responsibility and environmental stewardship to ensure\na more inclusive and sustainable technological future."}
{"id": "2507.09205", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09205", "abs": "https://arxiv.org/abs/2507.09205", "authors": ["Leiyu Pan", "Bojian Xiong", "Lei Yang", "Renren Jin", "Shaowei Zhang", "Yue Chen", "Ling Shi", "Jiang Zhou", "Junru Wu", "Zhen Wang", "Jianxiang Peng", "Juesi Xiao", "Tianyu Dong", "Zhuowen Han", "Zhuo Chen", "Sangjee Dondrub", "Caizang Tai", "Haixing Zhao", "Huaque Cairang", "Suonan Cairang", "Rou Te", "Lengben Zhaxi", "Gazang Zhaxi", "Zhonglin Ye", "Yuhui Zheng", "Chunyan Peng", "Secha Jia", "Pema Tashi", "Cizhen Jiacuo", "Pema Dorjee", "Hongkai Liu", "Pema Yanggon", "Tsehang Dorjee", "Jiaxin Han", "Qiongying Hu", "Jilin Man", "Huanke You", "Yuqi Ren", "Duo La", "Deyi Xiong"], "title": "Banzhida: Advancing Large Language Models for Tibetan with Curated Data and Continual Pre-Training", "comment": null, "summary": "Large language models have achieved remarkable progress across many\nlanguages. However, Tibetan, as a representative low-resource language, is\nparticularly underrepresented in existing models due to the scarcity of\nhigh-quality training corpora. To address this gap, we curate the largest\nTibetan pre-training corpus to date, aggregating data from diverse sources and\napplying a dedicated data cleaning and processing pipeline tailored for\nTibetan. With the curated data, we continue pre/post-training a multilingual\nbase model into Banzhida, a multilingual large language model that advances\ngenerative AI for Tibetan. To evaluate the Tibetan capabilities of the model,\nwe create new high-quality Tibetan benchmarks, and complement them with\nexisting public benchmarks. Experimental results demonstrate that Banzhida\nconsistently and significantly outperforms both open-source models of similar\nscale and Tibetan-tailored models across a wide range of tasks."}
{"id": "2507.09617", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.09617", "abs": "https://arxiv.org/abs/2507.09617", "authors": ["Margherita Martorana", "Francesca Urgese", "Mark Adamik", "Ilaria Tiddi"], "title": "Bridging Bots: from Perception to Action via Multimodal-LMs and Knowledge Graphs", "comment": null, "summary": "Personal service robots are deployed to support daily living in domestic\nenvironments, particularly for elderly and individuals requiring assistance.\nThese robots must perceive complex and dynamic surroundings, understand tasks,\nand execute context-appropriate actions. However, current systems rely on\nproprietary, hard-coded solutions tied to specific hardware and software,\nresulting in siloed implementations that are difficult to adapt and scale\nacross platforms. Ontologies and Knowledge Graphs (KGs) offer a solution to\nenable interoperability across systems, through structured and standardized\nrepresentations of knowledge and reasoning. However, symbolic systems such as\nKGs and ontologies struggle with raw and noisy sensory input. In contrast,\nmultimodal language models are well suited for interpreting input such as\nimages and natural language, but often lack transparency, consistency, and\nknowledge grounding. In this work, we propose a neurosymbolic framework that\ncombines the perceptual strengths of multimodal language models with the\nstructured representations provided by KGs and ontologies, with the aim of\nsupporting interoperability in robotic applications. Our approach generates\nontology-compliant KGs that can inform robot behavior in a platform-independent\nmanner. We evaluated this framework by integrating robot perception data,\nontologies, and five multimodal models (three LLaMA and two GPT models), using\ndifferent modes of neural-symbolic interaction. We assess the consistency and\neffectiveness of the generated KGs across multiple runs and configurations, and\nperform statistical analyzes to evaluate performance. Results show that GPT-o1\nand LLaMA 4 Maverick consistently outperform other models. However, our\nfindings also indicate that newer models do not guarantee better results,\nhighlighting the critical role of the integration strategy in generating\nontology-compliant KGs."}
{"id": "2507.09225", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.09225", "abs": "https://arxiv.org/abs/2507.09225", "authors": ["Biagio Scalingi", "Chiara Barattieri di San Pietro", "Paolo Canal", "Valentina Bambini"], "title": "MetaClimage: A novel database of visual metaphors related to Climate Change, with costs and benefits analysis", "comment": "27 pages, 5 figures", "summary": "Visual metaphors of climate change (e.g., melting glaciers depicted as a\nmelting ice grenade) are regarded as valuable tools for addressing the\ncomplexity of environmental challenges. However, few studies have examined\ntheir impact on communication, also due to scattered availability of material.\nHere, we present a novel database of Metaphors of Climate Change in Images\n(MetaClimage) https://doi.org/10.5281/zenodo.15861012, paired with literal\nimages and enriched with human ratings. For each image, we collected values of\ndifficulty, efficacy, artistic quality, and emotional arousal from human\nrating, as well as number of tags generated by participants to summarize the\nmessage. Semantic and emotion variables were further derived from the tags via\nNatural Language Processing. Visual metaphors were rated as more difficult to\nunderstand, yet more aesthetically pleasant than literal images, but did not\ndiffer in efficacy and arousal. The latter for visual metaphors, however, was\nhigher in participants with higher Need For Cognition. Furthermore, visual\nmetaphors received more tags, often referring to entities not depicted in the\nimage, and elicited words with more positive valence and greater dominance than\nliteral images. These results evidence the greater cognitive load of visual\nmetaphors, which nevertheless might induce positive effects such as deeper\ncognitive elaboration and abstraction compared to literal stimuli. Furthermore,\nwhile they are not deemed as more effective and arousing, visual metaphors seem\nto generate superior aesthetic appreciation and a more positively valenced\nexperience. Overall, this study contributes to understanding the impact of\nvisual metaphors of climate change both by offering a database for future\nresearch and by elucidating a cost-benefit trade-off to take into account when\nshaping environmental communication."}
{"id": "2507.09626", "categories": ["cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.09626", "abs": "https://arxiv.org/abs/2507.09626", "authors": ["Rodion Nazarov", "Anthony Quinn", "Robert Shorten", "Jakub Marecek"], "title": "humancompatible.interconnect: Testing Properties of Repeated Uses of Interconnections of AI Systems", "comment": null, "summary": "Artificial intelligence (AI) systems often interact with multiple agents. The\nregulation of such AI systems often requires that {\\em a priori\\/} guarantees\nof fairness and robustness be satisfied. With stochastic models of agents'\nresponses to the outputs of AI systems, such {\\em a priori\\/} guarantees\nrequire non-trivial reasoning about the corresponding stochastic systems. Here,\nwe present an open-source PyTorch-based toolkit for the use of stochastic\ncontrol techniques in modelling interconnections of AI systems and properties\nof their repeated uses. It models robustness and fairness desiderata in a\nclosed-loop fashion, and provides {\\em a priori\\/} guarantees for these\ninterconnections. The PyTorch-based toolkit removes much of the complexity\nassociated with the provision of fairness guarantees for closed-loop models of\nmulti-agent systems."}
{"id": "2507.09245", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09245", "abs": "https://arxiv.org/abs/2507.09245", "authors": ["Deshan Sumanathilaka", "Sameera Perera", "Sachithya Dharmasiri", "Maneesha Athukorala", "Anuja Dilrukshi Herath", "Rukshan Dias", "Pasindu Gamage", "Ruvan Weerasinghe", "Y. H. P. P. Priyadarshana"], "title": "Swa-bhasha Resource Hub: Romanized Sinhala to Sinhala Transliteration Systems and Data Resources", "comment": "13 pages, 3 Tables, 3 figures", "summary": "The Swa-bhasha Resource Hub provides a comprehensive collection of data\nresources and algorithms developed for Romanized Sinhala to Sinhala\ntransliteration between 2020 and 2025. These resources have played a\nsignificant role in advancing research in Sinhala Natural Language Processing\n(NLP), particularly in training transliteration models and developing\napplications involving Romanized Sinhala. The current openly accessible data\nsets and corresponding tools are made publicly available through this hub. This\npaper presents a detailed overview of the resources contributed by the authors\nand includes a comparative analysis of existing transliteration applications in\nthe domain."}
{"id": "2507.09662", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09662", "abs": "https://arxiv.org/abs/2507.09662", "authors": ["Jason Zhu", "Hongyu Li"], "title": "Towards Concise and Adaptive Thinking in Large Reasoning Models: A Survey", "comment": null, "summary": "Large reasoning models (LRMs) like OpenAI o1 and DeepSeek R1 have\ndemonstrated impressive performance on complex reasoning tasks like mathematics\nand programming with long Chain-of-Thought (CoT) reasoning sequences\n(slow-thinking), compared with traditional large language models\n(fast-thinking). However, these reasoning models also face a huge challenge\nthat generating unnecessarily lengthy and redundant reasoning chains even for\ntrivial questions. This phenomenon leads to a significant waste of inference\nresources, increases the response time for simple queries, and hinders the\npractical application of LRMs in real-world products. To this end, it is\ncrucial to shorten lengthy reasoning chains and learn adaptive reasoning\nbetween fast and slow thinking based on input difficulty. In this survey, we\nprovide a comprehensive overview of recent progress in concise and adaptive\nthinking for efficient reasoning of LRMs, including methodologies, benchmarks,\nand challenges for future exploration. We hope this survey can help researchers\nquickly understand the landscape of this field and inspire novel adaptive\nthinking ideas to facilitate better usage of LRMs."}
{"id": "2507.09259", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09259", "abs": "https://arxiv.org/abs/2507.09259", "authors": ["Yuchen Su", "Yonghua Zhu", "Yang Chen", "Diana Benavides-Prado", "Michael Witbrock"], "title": "Psychology-Driven Enhancement of Humour Translation", "comment": null, "summary": "Humour translation plays a vital role as a bridge between different cultures,\nfostering understanding and communication. Although most existing Large\nLanguage Models (LLMs) are capable of general translation tasks, these models\nstill struggle with humour translation, which is especially reflected through\nlinguistic interference and lacking humour in translated text. In this paper,\nwe propose a psychology-inspired Humour Decomposition Mechanism (HDM) that\nutilises Chain-of-Thought (CoT) to imitate the ability of the human thought\nprocess, stimulating LLMs to optimise the readability of translated humorous\ntexts. Moreover, we integrate humour theory in HDM to further enhance the\nhumorous elements in the translated text. Our automatic evaluation experiments\non open-source humour datasets demonstrate that our method significantly\nimproves the quality of humour translation, yielding average gains of 7.75\\% in\nhumour, 2.81\\% in fluency, and 6.13\\% in coherence of the generated text."}
{"id": "2507.09742", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09742", "abs": "https://arxiv.org/abs/2507.09742", "authors": ["Xiaofeng Xiao", "Bo Shen", "Xubo Yue"], "title": "Causality-informed Anomaly Detection in Partially Observable Sensor Networks: Moving beyond Correlations", "comment": null, "summary": "Nowadays, as AI-driven manufacturing becomes increasingly popular, the volume\nof data streams requiring real-time monitoring continues to grow. However, due\nto limited resources, it is impractical to place sensors at every location to\ndetect unexpected shifts. Therefore, it is necessary to develop an optimal\nsensor placement strategy that enables partial observability of the system\nwhile detecting anomalies as quickly as possible. Numerous approaches have been\nproposed to address this challenge; however, most existing methods consider\nonly variable correlations and neglect a crucial factor: Causality. Moreover,\nalthough a few techniques incorporate causal analysis, they rely on\ninterventions-artificially creating anomalies-to identify causal effects, which\nis impractical and might lead to catastrophic losses. In this paper, we\nintroduce a causality-informed deep Q-network (Causal DQ) approach for\npartially observable sensor placement in anomaly detection. By integrating\ncausal information at each stage of Q-network training, our method achieves\nfaster convergence and tighter theoretical error bounds. Furthermore, the\ntrained causal-informed Q-network significantly reduces the detection time for\nanomalies under various settings, demonstrating its effectiveness for sensor\nplacement in large-scale, real-world data streams. Beyond the current\nimplementation, our technique's fundamental insights can be applied to various\nreinforcement learning problems, opening up new possibilities for real-world\ncausality-informed machine learning methods in engineering applications."}
{"id": "2507.09282", "categories": ["cs.CL", "cs.CR", "cs.LG", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.09282", "abs": "https://arxiv.org/abs/2507.09282", "authors": ["Dominika Woszczyk", "Ranya Aloufi", "Soteris Demetriou"], "title": "ClaritySpeech: Dementia Obfuscation in Speech", "comment": "Accepted at Interspeech 2025", "summary": "Dementia, a neurodegenerative disease, alters speech patterns, creating\ncommunication barriers and raising privacy concerns. Current speech\ntechnologies, such as automatic speech transcription (ASR), struggle with\ndementia and atypical speech, further challenging accessibility. This paper\npresents a novel dementia obfuscation in speech framework, ClaritySpeech,\nintegrating ASR, text obfuscation, and zero-shot text-to-speech (TTS) to\ncorrect dementia-affected speech while preserving speaker identity in low-data\nenvironments without fine-tuning. Results show a 16% and 10% drop in mean F1\nscore across various adversarial settings and modalities (audio, text, fusion)\nfor ADReSS and ADReSSo, respectively, maintaining 50% speaker similarity. We\nalso find that our system improves WER (from 0.73 to 0.08 for ADReSS and 0.15\nfor ADReSSo) and speech quality from 1.65 to ~2.15, enhancing privacy and\naccessibility."}
{"id": "2507.09751", "categories": ["cs.AI", "cs.CL", "cs.LO"], "pdf": "https://arxiv.org/pdf/2507.09751", "abs": "https://arxiv.org/abs/2507.09751", "authors": ["Bradley P. Allen", "Prateek Chhikara", "Thomas Macaulay Ferguson", "Filip Ilievski", "Paul Groth"], "title": "Sound and Complete Neuro-symbolic Reasoning with LLM-Grounded Interpretations", "comment": "29 pages, 9 tables, 3 figures. Accepted to the 19th Conference on\n  Neurosymbolic Learning and Reasoning (NeSy 2025)", "summary": "Large language models (LLMs) have demonstrated impressive capabilities in\nnatural language understanding and generation, but they exhibit problems with\nlogical consistency in the output they generate. How can we harness LLMs'\nbroad-coverage parametric knowledge in formal reasoning despite their\ninconsistency? We present a method for directly integrating an LLM into the\ninterpretation function of the formal semantics for a paraconsistent logic. We\nprovide experimental evidence for the feasibility of the method by evaluating\nthe function using datasets created from several short-form factuality\nbenchmarks. Unlike prior work, our method offers a theoretical framework for\nneuro-symbolic reasoning that leverages an LLM's knowledge while preserving the\nunderlying logic's soundness and completeness properties."}
{"id": "2507.09424", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09424", "abs": "https://arxiv.org/abs/2507.09424", "authors": ["Cathy Jiao", "Yijun Pan", "Emily Xiao", "Daisy Sheng", "Niket Jain", "Hanzhang Zhao", "Ishita Dasgupta", "Jiaqi W. Ma", "Chenyan Xiong"], "title": "DATE-LM: Benchmarking Data Attribution Evaluation for Large Language Models", "comment": null, "summary": "Data attribution methods quantify the influence of training data on model\noutputs and are becoming increasingly relevant for a wide range of LLM research\nand applications, including dataset curation, model interpretability, data\nvaluation. However, there remain critical gaps in systematic LLM-centric\nevaluation of data attribution methods. To this end, we introduce DATE-LM (Data\nAttribution Evaluation in Language Models), a unified benchmark for evaluating\ndata attribution methods through real-world LLM applications. DATE-LM measures\nattribution quality through three key tasks -- training data selection,\ntoxicity/bias filtering, and factual attribution. Our benchmark is designed for\nease of use, enabling researchers to configure and run large-scale evaluations\nacross diverse tasks and LLM architectures. Furthermore, we use DATE-LM to\nconduct a large-scale evaluation of existing data attribution methods. Our\nfindings show that no single method dominates across all tasks, data\nattribution methods have trade-offs with simpler baselines, and method\nperformance is sensitive to task-specific evaluation design. Finally, we\nrelease a public leaderboard for quick comparison of methods and to facilitate\ncommunity engagement. We hope DATE-LM serves as a foundation for future data\nattribution research in LLMs."}
{"id": "2507.09801", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.09801", "abs": "https://arxiv.org/abs/2507.09801", "authors": ["Peter Barnett", "Aaron Scher", "David Abecassis"], "title": "Technical Requirements for Halting Dangerous AI Activities", "comment": null, "summary": "The rapid development of AI systems poses unprecedented risks, including loss\nof control, misuse, geopolitical instability, and concentration of power. To\nnavigate these risks and avoid worst-case outcomes, governments may proactively\nestablish the capability for a coordinated halt on dangerous AI development and\ndeployment. In this paper, we outline key technical interventions that could\nallow for a coordinated halt on dangerous AI activities. We discuss how these\ninterventions may contribute to restricting various dangerous AI activities,\nand show how these interventions can form the technical foundation for\npotential AI governance plans."}
{"id": "2507.09470", "categories": ["cs.CL", "cs.AI", "68T07"], "pdf": "https://arxiv.org/pdf/2507.09470", "abs": "https://arxiv.org/abs/2507.09470", "authors": ["Mingchuan Yang", "Ziyuan Huang"], "title": "Enhancing Clinical Text Classification via Fine-Tuned DRAGON Longformer Models", "comment": "29 pages, 5 tables", "summary": "This study explores the optimization of the DRAGON Longformer base model for\nclinical text classification, specifically targeting the binary classification\nof medical case descriptions. A dataset of 500 clinical cases containing\nstructured medical observations was used, with 400 cases for training and 100\nfor validation. Enhancements to the pre-trained\njoeranbosma/dragon-longformer-base-mixed-domain model included hyperparameter\ntuning, domain-specific preprocessing, and architectural adjustments. Key\nmodifications involved increasing sequence length from 512 to 1024 tokens,\nadjusting learning rates from 1e-05 to 5e-06, extending training epochs from 5\nto 8, and incorporating specialized medical terminology. The optimized model\nachieved notable performance gains: accuracy improved from 72.0% to 85.2%,\nprecision from 68.0% to 84.1%, recall from 75.0% to 86.3%, and F1-score from\n71.0% to 85.2%. Statistical analysis confirmed the significance of these\nimprovements (p < .001). The model demonstrated enhanced capability in\ninterpreting medical terminology, anatomical measurements, and clinical\nobservations. These findings contribute to domain-specific language model\nresearch and offer practical implications for clinical natural language\nprocessing applications. The optimized model's strong performance across\ndiverse medical conditions underscores its potential for broad use in\nhealthcare settings."}
{"id": "2507.09850", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09850", "abs": "https://arxiv.org/abs/2507.09850", "authors": ["Wei Du", "Branislav Kisacanin", "George Armstrong", "Shubham Toshniwal", "Ivan Moshkov", "Alexan Ayrapetyan", "Sadegh Mahdavi", "Dan Zhao", "Shizhe Diao", "Dragan Masulovic", "Marius Stanean", "Advaith Avadhanam", "Max Wang", "Ashmit Dutta", "Shitij Govil", "Sri Yanamandara", "Mihir Tandon", "Sriram Ananthakrishnan", "Vedant Rathi", "David Zhang", "Joonseok Kang", "Leon Luo", "Titu Andreescu", "Boris Ginsburg", "Igor Gitman"], "title": "Is Human-Written Data Enough? The Challenge of Teaching Reasoning to LLMs Without RL or Distillation", "comment": "Accepted at the Second AI for Math Workshop at the 42nd International\n  Conference on Machine Learning (ICML 2025)", "summary": "Reasoning-capable language models achieve state-of-the-art performance in\ndiverse complex tasks by generating long, explicit Chain-of-Thought (CoT)\ntraces. While recent works show that base models can acquire such reasoning\ntraces via reinforcement learning or distillation from stronger models like\nDeepSeek-R1, previous works demonstrate that even short CoT prompting without\nfine-tuning is able to improve reasoning. We ask whether long CoT can be\ninduced in a base model using only prompting or minimal tuning. Using just 20\nlong CoT examples from the reasoning model \\texttt{QwQ-32B-Preview}, we lightly\nfine-tune the base model \\texttt{Qwen2.5-32B}. The resulting model outperforms\nthe much larger \\texttt{Qwen2.5-Math-72B-Instruct}, showing that a handful of\nhigh-quality examples can unlock strong reasoning capabilities. We further\nexplore using CoT data from non-reasoning models and human annotators, enhanced\nwith prompt engineering, multi-pass editing, and structural guidance. However,\nneither matches the performance of reasoning model traces, suggesting that\ncertain latent qualities of expert CoT are difficult to replicate. We analyze\nkey properties of reasoning data, such as problem difficulty, diversity, and\nanswer length, that influence reasoning distillation. While challenges remain,\nwe are optimistic that carefully curated human-written CoT, even in small\nquantities, can activate reasoning behaviors in base models. We release our\nhuman-authored dataset across refinement stages and invite further\ninvestigation into what makes small-scale reasoning supervision so effective."}
{"id": "2507.09474", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09474", "abs": "https://arxiv.org/abs/2507.09474", "authors": ["Hwee Tou Ng", "Siew Mei Wu", "Yuanbin Wu", "Christian Hadiwinoto", "Joel Tetreault"], "title": "The CoNLL-2013 Shared Task on Grammatical Error Correction", "comment": "12 pages", "summary": "The CoNLL-2013 shared task was devoted to grammatical error correction. In\nthis paper, we give the task definition, present the data sets, and describe\nthe evaluation metric and scorer used in the shared task. We also give an\noverview of the various approaches adopted by the participating teams, and\npresent the evaluation results."}
{"id": "2507.09854", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09854", "abs": "https://arxiv.org/abs/2507.09854", "authors": ["Aniruddha Chattopadhyay", "Raj Dandekar", "Kaushik Roy"], "title": "Model-Grounded Symbolic Artificial Intelligence Systems Learning and Reasoning with Model-Grounded Symbolic Artificial Intelligence Systems", "comment": "Accepted as paper in 19th International Conference on Neurosymbolic\n  Learning and Reasoning,NeSy 2025", "summary": "Neurosymbolic artificial intelligence (AI) systems combine neural network and\nclassical symbolic AI mechanisms to exploit the complementary strengths of\nlarge scale, generalizable learning and robust, verifiable reasoning. Numerous\nclassifications of neurosymbolic AI illustrate how these two components can be\nintegrated in distinctly different ways. In this work, we propose\nreinterpreting instruction tuned large language models as model grounded\nsymbolic AI systems where natural language serves as the symbolic layer and\ngrounding is achieved through the models internal representation space. Within\nthis framework, we investigate and develop novel learning and reasoning\napproaches that preserve structural similarities to traditional learning and\nreasoning paradigms. Preliminary evaluations across axiomatic deductive\nreasoning procedures of varying complexity provide insights into the\neffectiveness of our approach in improving learning efficiency and reasoning\nreliability."}
{"id": "2507.09477", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09477", "abs": "https://arxiv.org/abs/2507.09477", "authors": ["Yangning Li", "Weizhi Zhang", "Yuyao Yang", "Wei-Chieh Huang", "Yaozu Wu", "Junyu Luo", "Yuanchen Bei", "Henry Peng Zou", "Xiao Luo", "Yusheng Zhao", "Chunkit Chan", "Yankai Chen", "Zhongfen Deng", "Yinghui Li", "Hai-Tao Zheng", "Dongyuan Li", "Renhe Jiang", "Ming Zhang", "Yangqiu Song", "Philip S. Yu"], "title": "Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs", "comment": "submitted to ARR May", "summary": "Retrieval-Augmented Generation (RAG) lifts the factuality of Large Language\nModels (LLMs) by injecting external knowledge, yet it falls short on problems\nthat demand multi-step inference; conversely, purely reasoning-oriented\napproaches often hallucinate or mis-ground facts. This survey synthesizes both\nstrands under a unified reasoning-retrieval perspective. We first map how\nadvanced reasoning optimizes each stage of RAG (Reasoning-Enhanced RAG). Then,\nwe show how retrieved knowledge of different type supply missing premises and\nexpand context for complex inference (RAG-Enhanced Reasoning). Finally, we\nspotlight emerging Synergized RAG-Reasoning frameworks, where (agentic) LLMs\niteratively interleave search and reasoning to achieve state-of-the-art\nperformance across knowledge-intensive benchmarks. We categorize methods,\ndatasets, and open challenges, and outline research avenues toward deeper\nRAG-Reasoning systems that are more effective, multimodally-adaptive,\ntrustworthy, and human-centric. The collection is available at\nhttps://github.com/DavidZWZ/Awesome-RAG-Reasoning."}
{"id": "2507.09884", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09884", "abs": "https://arxiv.org/abs/2507.09884", "authors": ["Xuzhao Li", "Xuchen Li", "Shiyu Hu", "Yongzhen Guo", "Wentao Zhang"], "title": "VerifyBench: A Systematic Benchmark for Evaluating Reasoning Verifiers Across Domains", "comment": "Preprint, Under review", "summary": "Large language models (LLMs) increasingly rely on reinforcement learning (RL)\nto enhance their reasoning capabilities through feedback. A critical challenge\nis verifying the consistency of model-generated responses and reference\nanswers, since these responses are often lengthy, diverse, and nuanced.\nRule-based verifiers struggle with complexity, prompting the use of model-based\nverifiers. However, specialized verifiers lack flexibility, while general LLM\njudges can be inconsistent. Existing research primarily focuses on building\nbetter verifiers, yet a systematic evaluation of different types of verifiers'\nperformance across domains remains lacking, severely constraining the reliable\ndevelopment of Reinforcement Learning with Verifiable Reward (RLVR). To address\nthis, we propose VerifyBench--a cross-domain comprehensive benchmark for\nsystematically evaluating verifiers. We construct 4,000 expert-level questions\ncovering mathematics, physics, chemistry, and biology. Each question is\nequipped with reference answers and diverse responses. The reliability of the\nevaluation is ensured through a rigorous annotation process conducted by a\nmultidisciplinary expert team. We design a four-dimensional experimental\nframework to comprehensively compare the performance boundaries of specialized\nverifiers and general LLMs under combined conditions of extracted answers vs.\ncomplete responses, and short vs. long outputs. Our evaluation uncovers\nfundamental trade-offs in verifiers: while specialized verifiers achieve\nleading accuracy, they exhibit deficiencies in recall; general models show\nstronger inclusivity but unstable precision. More importantly, we discover\nverifiers' high sensitivity to input structure and inherent limitations in\ncross-domain generalization, providing critical insights into the bottlenecks\nof current verifier technology."}
{"id": "2507.09482", "categories": ["cs.CL", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.09482", "abs": "https://arxiv.org/abs/2507.09482", "authors": ["Changli Wang", "Rui Wu", "Fang Yin"], "title": "ViSP: A PPO-Driven Framework for Sarcasm Generation with Contrastive Learning", "comment": null, "summary": "Human emotions are complex, with sarcasm being a subtle and distinctive form.\nDespite progress in sarcasm research, sarcasm generation remains underexplored,\nprimarily due to the overreliance on textual modalities and the neglect of\nvisual cues, as well as the mismatch between image content and sarcastic intent\nin existing datasets. In this paper, we introduce M2SaG, a multimodal sarcasm\ngeneration dataset with 4,970 samples, each containing an image, a sarcastic\ntext, and a sarcasm target. To benchmark M2SaG, we propose ViSP, a generation\nframework that integrates Proximal Policy Optimization (PPO) and contrastive\nlearning. PPO utilizes reward scores from DIP to steer the generation of\nsarcastic texts, while contrastive learning encourages the model to favor\noutputs with higher reward scores. These strategies improve overall generation\nquality and produce texts with more pronounced sarcastic intent. We evaluate\nViSP across five metric sets and find it surpasses all baselines, including\nlarge language models, underscoring their limitations in sarcasm generation.\nFurthermore, we analyze the distributions of Sarcasm Scores and Factual\nIncongruity for both M2SaG and the texts generated by ViSP. The generated texts\nexhibit higher mean Sarcasm Scores (0.898 vs. 0.770) and Factual Incongruity\n(0.768 vs. 0.739), demonstrating that ViSP produces higher-quality sarcastic\ncontent than the original dataset. % The dataset and code will be publicly\navailable. Our dataset and code will be released at\n\\textit{https://github.com/wclapply/ViSP}."}
{"id": "2507.09955", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09955", "abs": "https://arxiv.org/abs/2507.09955", "authors": ["Luolin Xiong", "Haofen Wang", "Xi Chen", "Lu Sheng", "Yun Xiong", "Jingping Liu", "Yanghua Xiao", "Huajun Chen", "Qing-Long Han", "Yang Tang"], "title": "DeepSeek: Paradigm Shifts and Technical Evolution in Large AI Models", "comment": null, "summary": "DeepSeek, a Chinese Artificial Intelligence (AI) startup, has released their\nV3 and R1 series models, which attracted global attention due to their low\ncost, high performance, and open-source advantages. This paper begins by\nreviewing the evolution of large AI models focusing on paradigm shifts, the\nmainstream Large Language Model (LLM) paradigm, and the DeepSeek paradigm.\nSubsequently, the paper highlights novel algorithms introduced by DeepSeek,\nincluding Multi-head Latent Attention (MLA), Mixture-of-Experts (MoE),\nMulti-Token Prediction (MTP), and Group Relative Policy Optimization (GRPO).\nThe paper then explores DeepSeek engineering breakthroughs in LLM scaling,\ntraining, inference, and system-level optimization architecture. Moreover, the\nimpact of DeepSeek models on the competitive AI landscape is analyzed,\ncomparing them to mainstream LLMs across various fields. Finally, the paper\nreflects on the insights gained from DeepSeek innovations and discusses future\ntrends in the technical and engineering development of large AI models,\nparticularly in data, training, and reasoning."}
{"id": "2507.09485", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09485", "abs": "https://arxiv.org/abs/2507.09485", "authors": ["Junjie Liu", "Yuanhe Tian", "Yan Song"], "title": "Balanced Training Data Augmentation for Aspect-Based Sentiment Analysis", "comment": null, "summary": "Aspect-based sentiment analysis (ABSA) is a crucial fine-grained task in\nsocial media scenarios to identify the sentiment polarity of specific aspect\nterms in a sentence. Although many existing studies leverage large language\nmodels (LLMs) to perform ABSA due to their strong context understanding\ncapabilities, they still face challenges to learn the context information in\nthe running text because of the short text, as well as the small and unbalanced\nlabeled training data, where most data are labeled with positive sentiment.\nData augmentation (DA) is a feasible strategy for providing richer contextual\ninformation, especially when using LLMs to create synthetic training data, but\nfaces challenges in ensuring a high quality of the augmented data.In this\npaper, we propose an LLM-based ABSA approach with training data\naugmentation.Specifically, an LLM is prompted to generate augmented training\ndata based on the original training data, so as to construct a new training\ndata with larger size and balanced label distributions to better train an ABSA\nmodel. Meanwhile, in order to improve the quality of the augmented data, we\npropose a reinforcement learning approach to optimize the data augmentation.\nLLM.Experiment results and further analyses on English benchmark datasets for\nABSA demonstrate the effectiveness of our approach, where superior performance\nis observed over strong baselines and most existing studies."}
{"id": "2507.09989", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.09989", "abs": "https://arxiv.org/abs/2507.09989", "authors": ["Xiaoyang Yu", "Youfang Lin", "Shuo Wang", "Sheng Han"], "title": "Improving monotonic optimization in heterogeneous multi-agent reinforcement learning with optimal marginal deterministic policy gradient", "comment": null, "summary": "In heterogeneous multi-agent reinforcement learning (MARL), achieving\nmonotonic improvement plays a pivotal role in enhancing performance. The HAPPO\nalgorithm proposes a feasible solution by introducing a sequential update\nscheme, which requires independent learning with No Parameter-sharing (NoPS).\nHowever, heterogeneous MARL generally requires Partial Parameter-sharing\n(ParPS) based on agent grouping to achieve high cooperative performance. Our\nexperiments prove that directly combining ParPS with the sequential update\nscheme leads to the policy updating baseline drift problem, thereby failing to\nachieve improvement. To solve the conflict between monotonic improvement and\nParPS, we propose the Optimal Marginal Deterministic Policy Gradient (OMDPG)\nalgorithm. First, we replace the sequentially computed $Q_{\\psi}^s(s,a_{1:i})$\nwith the Optimal Marginal Q (OMQ) function $\\phi_{\\psi}^*(s,a_{1:i})$ derived\nfrom Q-functions. This maintains MAAD's monotonic improvement while eliminating\nthe conflict through optimal joint action sequences instead of sequential\npolicy ratio calculations. Second, we introduce the Generalized Q Critic (GQC)\nas the critic function, employing pessimistic uncertainty-constrained loss to\noptimize different Q-value estimations. This provides the required Q-values for\nOMQ computation and stable baselines for actor updates. Finally, we implement a\nCentralized Critic Grouped Actor (CCGA) architecture that simultaneously\nachieves ParPS in local policy networks and accurate global Q-function\ncomputation. Experimental results in SMAC and MAMuJoCo environments demonstrate\nthat OMDPG outperforms various state-of-the-art MARL baselines."}
{"id": "2507.09497", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09497", "abs": "https://arxiv.org/abs/2507.09497", "authors": ["Siyi Wu", "Zeyu Wang", "Xinyuan Song", "Zhengpeng Zhou", "Lifan Sun", "Tianyu Shi"], "title": "GoalfyMax: A Protocol-Driven Multi-Agent System for Intelligent Experience Entities", "comment": null, "summary": "Modern enterprise environments demand intelligent systems capable of handling\ncomplex, dynamic, and multi-faceted tasks with high levels of autonomy and\nadaptability. However, traditional single-purpose AI systems often lack\nsufficient coordination, memory reuse, and task decomposition capabilities,\nlimiting their scalability in realistic settings. To address these challenges,\nwe present \\textbf{GoalfyMax}, a protocol-driven framework for end-to-end\nmulti-agent collaboration. GoalfyMax introduces a standardized Agent-to-Agent\n(A2A) communication layer built on the Model Context Protocol (MCP), allowing\nindependent agents to coordinate through asynchronous, protocol-compliant\ninteractions. It incorporates the Experience Pack (XP) architecture, a layered\nmemory system that preserves both task rationales and execution traces,\nenabling structured knowledge retention and continual learning. Moreover, our\nsystem integrates advanced features including multi-turn contextual dialogue,\nlong-short term memory modules, and dynamic safety validation, supporting\nrobust, real-time strategy adaptation. Empirical results on complex task\norchestration benchmarks and case study demonstrate that GoalfyMax achieves\nsuperior adaptability, coordination, and experience reuse compared to baseline\nframeworks. These findings highlight its potential as a scalable, future-ready\nfoundation for multi-agent intelligent systems."}
{"id": "2507.10000", "categories": ["cs.AI", "cs.CL", "I.2.11; F.4.1; I.2.4; G.2.2"], "pdf": "https://arxiv.org/pdf/2507.10000", "abs": "https://arxiv.org/abs/2507.10000", "authors": ["Mark Burgess"], "title": "On The Role of Intentionality in Knowledge Representation: Analyzing Scene Context for Cognitive Agents with a Tiny Language Model", "comment": null, "summary": "Since Searle's work deconstructing intent and intentionality in the realm of\nphilosophy, the practical meaning of intent has received little attention in\nscience and technology. Intentionality and context are both central to the\nscope of Promise Theory's model of Semantic Spacetime, used as an effective\nTiny Language Model. One can identify themes and concepts from a text, on a low\nlevel (without knowledge of the specific language) by using process coherence\nas a guide. Any agent process can assess superficially a degree of latent\n`intentionality' in data by looking for anomalous multi-scale anomalies and\nassessing the work done to form them. Scale separation can be used to sort\nparts into `intended' content and `ambient context', using the spacetime\ncoherence as a measure. This offers an elementary but pragmatic interpretation\nof latent intentionality for very low computational cost, and without reference\nto extensive training or reasoning capabilities. The process is well within the\nreach of basic organisms as it does not require large scale artificial\nprobabilistic batch processing. The level of concept formation depends,\nhowever, on the memory capacity of the agent."}
{"id": "2507.09506", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09506", "abs": "https://arxiv.org/abs/2507.09506", "authors": ["Junjie Wu", "Gefei Gu", "Yanan Zheng", "Dit-Yan Yeung", "Arman Cohan"], "title": "Ref-Long: Benchmarking the Long-context Referencing Capability of Long-context Language Models", "comment": "ACL 2025 Main Conference. First 2 authors contributed equally", "summary": "Long-context language models (LCLMs) have exhibited impressive capabilities\nin long-context understanding tasks. Among these, long-context referencing -- a\ncrucial task that requires LCLMs to attribute items of interest to specific\nparts of long-context data -- remains underexplored. To bridge this gap, this\npaper proposes Referencing Evaluation for Long-context Language Models\n(Ref-Long), a novel benchmark designed to assess the long-context referencing\ncapability of LCLMs. Specifically, Ref-Long requires LCLMs to identify the\nindexes of documents that reference a specific key, emphasizing contextual\nrelationships between the key and the documents over simple retrieval. Based on\nthe task design, we construct three subsets ranging from synthetic to realistic\nscenarios to form the Ref-Long benchmark. Experimental results of 13 LCLMs\nreveal significant shortcomings in long-context referencing, even among\nadvanced models like GPT-4o. To further investigate these challenges, we\nconduct comprehensive analyses, including human evaluations, task format\nadjustments, fine-tuning experiments, and error analyses, leading to several\nkey insights. Our data and code can be found in https://github.\ncom/wujunjie1998/Ref-Long."}
{"id": "2507.10007", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10007", "abs": "https://arxiv.org/abs/2507.10007", "authors": ["Zijun Chen", "Wenbo Hu", "Richang Hong"], "title": "Deep Hidden Cognition Facilitates Reliable Chain-of-Thought Reasoning", "comment": null, "summary": "Chain of Thought (CoT) reasoning has demonstrated remarkable deep reasoning\ncapabilities in both large language models (LLMs) and multimodal large language\nmodels (MLLMs). However, its reliability is often undermined by the\naccumulation of errors in intermediate steps. This paper introduces an novel\napproach to calibrate the CoT reasoning accuracy by leveraging the model's\nintrinsic veracity encoding. We discover that specific attention head\nactivations reliably reflect the truthfulness of reasoning steps in CoT. Based\non this insight, we train a confidence predictor to evaluate the correctness of\neach reasoning step using these truthfulness-sensitive activations, dynamically\nselecting the most plausible reasoning path via beam search. Experimental\nresults demonstrate that our method significantly outperforms the\nstate-of-the-art baselines (e.g., Few-Shot CoT, Self-Consistency, and\nSelf-Evaluation Guided Beam Search) across the mathematical, symbolic, and\ncommonsense reasoning tasks, exhibiting superior accuracy and reliability in\nboth unimodal and multimodal settings. We further validate the approach on\nlarge reasoning models, confirming its applicability to specialized reasoning\nmodels. Additionally, we explore the role of the model's self-correction\nability in CoT reasoning. This work provides a novel reliability improvement\npath for CoT reasoning with broad application potential."}
{"id": "2507.09509", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09509", "abs": "https://arxiv.org/abs/2507.09509", "authors": ["Patr√≠cia Schmidtov√°", "Niyati Bafna", "Seth Aycock", "Gianluca Vico", "Wiktor Kamzela", "Katharina H√§mmerl", "Vil√©m Zouhar"], "title": "How Important is `Perfect' English for Machine Translation Prompts?", "comment": null, "summary": "Large language models (LLMs) have achieved top results in recent machine\ntranslation evaluations, but they are also known to be sensitive to errors and\nperturbations in their prompts. We systematically evaluate how both humanly\nplausible and synthetic errors in user prompts affect LLMs' performance on two\nrelated tasks: Machine translation and machine translation evaluation. We\nprovide both a quantitative analysis and qualitative insights into how the\nmodels respond to increasing noise in the user prompt.\n  The prompt quality strongly affects the translation performance: With many\nerrors, even a good prompt can underperform a minimal or poor prompt without\nerrors. However, different noise types impact translation quality differently,\nwith character-level and combined noisers degrading performance more than\nphrasal perturbations. Qualitative analysis reveals that lower prompt quality\nlargely leads to poorer instruction following, rather than directly affecting\ntranslation quality itself. Further, LLMs can still translate in scenarios with\noverwhelming random noise that would make the prompt illegible to humans."}
{"id": "2507.10045", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.10045", "abs": "https://arxiv.org/abs/2507.10045", "authors": ["Malte Christian Bartels", "Debayan Banerjee", "Ricardo Usbeck"], "title": "Automating SPARQL Query Translations between DBpedia and Wikidata", "comment": "18 pages, 2 figues. Paper accepted at SEMANTiCS 2025 conference\n  happening on September 2025", "summary": "This paper investigates whether state-of-the-art Large Language Models (LLMs)\ncan automatically translate SPARQL between popular Knowledge Graph (KG)\nschemas. We focus on translations between the DBpedia and Wikidata KG, and\nlater on DBLP and OpenAlex KG. This study addresses a notable gap in KG\ninteroperability research by rigorously evaluating LLM performance on\nSPARQL-to-SPARQL translation. Two benchmarks are assembled, where the first\nalign 100 DBpedia-Wikidata queries from QALD-9-Plus; the second contains 100\nDBLP queries aligned to OpenAlex, testing generalizability beyond encyclopaedic\nKGs. Three open LLMs: Llama-3-8B, DeepSeek-R1-Distill-Llama-70B, and\nMistral-Large-Instruct-2407 are selected based on their sizes and architectures\nand tested with zero-shot, few-shot, and two chain-of-thought variants. Outputs\nwere compared with gold answers, and resulting errors were categorized. We find\nthat the performance varies markedly across models and prompting strategies,\nand that translations for Wikidata to DBpedia work far better than translations\nfor DBpedia to Wikidata."}
{"id": "2507.09536", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09536", "abs": "https://arxiv.org/abs/2507.09536", "authors": ["Daniela Kazakouskaya", "Timothee Mickus", "Janine Siewert"], "title": "Adapting Definition Modeling for New Languages: A Case Study on Belarusian", "comment": "To appear at SlavicNLP 2025", "summary": "Definition modeling, the task of generating new definitions for words in\ncontext, holds great prospect as a means to assist the work of lexicographers\nin documenting a broader variety of lects and languages, yet much remains to be\ndone in order to assess how we can leverage pre-existing models for as-of-yet\nunsupported languages. In this work, we focus on adapting existing models to\nBelarusian, for which we propose a novel dataset of 43,150 definitions. Our\nexperiments demonstrate that adapting a definition modeling systems requires\nminimal amounts of data, but that there currently are gaps in what automatic\nmetrics do capture."}
{"id": "2507.10076", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10076", "abs": "https://arxiv.org/abs/2507.10076", "authors": ["Anna Rapberger", "Fabrizio Russo", "Antonio Rago", "Francesca Toni"], "title": "On Gradual Semantics for Assumption-Based Argumentation", "comment": null, "summary": "In computational argumentation, gradual semantics are fine-grained\nalternatives to extension-based and labelling-based semantics . They ascribe a\ndialectical strength to (components of) arguments sanctioning their degree of\nacceptability. Several gradual semantics have been studied for abstract,\nbipolar and quantitative bipolar argumentation frameworks (QBAFs), as well as,\nto a lesser extent, for some forms of structured argumentation. However, this\nhas not been the case for assumption-based argumentation (ABA), despite it\nbeing a popular form of structured argumentation with several applications\nwhere gradual semantics could be useful. In this paper, we fill this gap and\npropose a family of novel gradual semantics for equipping assumptions, which\nare the core components in ABA frameworks, with dialectical strengths. To do\nso, we use bipolar set-based argumentation frameworks as an abstraction of\n(potentially non-flat) ABA frameworks and generalise state-of-the-art modular\ngradual semantics for QBAFs. We show that our gradual ABA semantics satisfy\nsuitable adaptations of desirable properties of gradual QBAF semantics, such as\nbalance and monotonicity. We also explore an argument-based approach that\nleverages established QBAF modular semantics directly, and use it as baseline.\nFinally, we conduct experiments with synthetic ABA frameworks to compare our\ngradual ABA semantics with its argument-based counterpart and assess\nconvergence."}
{"id": "2507.09601", "categories": ["cs.CL", "cs.AI", "q-fin.CP"], "pdf": "https://arxiv.org/pdf/2507.09601", "abs": "https://arxiv.org/abs/2507.09601", "authors": ["Hanwool Lee", "Sara Yu", "Yewon Hwang", "Jonghyun Choi", "Heejae Ahn", "Sungbum Jung", "Youngjae Yu"], "title": "NMIXX: Domain-Adapted Neural Embeddings for Cross-Lingual eXploration of Finance", "comment": "Under Review", "summary": "General-purpose sentence embedding models often struggle to capture\nspecialized financial semantics, especially in low-resource languages like\nKorean, due to domain-specific jargon, temporal meaning shifts, and misaligned\nbilingual vocabularies. To address these gaps, we introduce NMIXX (Neural\neMbeddings for Cross-lingual eXploration of Finance), a suite of cross-lingual\nembedding models fine-tuned with 18.8K high-confidence triplets that pair\nin-domain paraphrases, hard negatives derived from a semantic-shift typology,\nand exact Korean-English translations. Concurrently, we release KorFinSTS, a\n1,921-pair Korean financial STS benchmark spanning news, disclosures, research\nreports, and regulations, designed to expose nuances that general benchmarks\nmiss.\n  When evaluated against seven open-license baselines, NMIXX's multilingual\nbge-m3 variant achieves Spearman's rho gains of +0.10 on English FinSTS and\n+0.22 on KorFinSTS, outperforming its pre-adaptation checkpoint and surpassing\nother models by the largest margin, while revealing a modest trade-off in\ngeneral STS performance. Our analysis further shows that models with richer\nKorean token coverage adapt more effectively, underscoring the importance of\ntokenizer design in low-resource, cross-lingual settings. By making both models\nand the benchmark publicly available, we provide the community with robust\ntools for domain-adapted, multilingual representation learning in finance."}
{"id": "2507.10106", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10106", "abs": "https://arxiv.org/abs/2507.10106", "authors": ["Harshal Nandigramwar", "Syed Qutub", "Kay-Ulrich Scholl"], "title": "BlueGlass: A Framework for Composite AI Safety", "comment": "Accepted at ICML 2025 [Actionable Interpretability Workshop]", "summary": "As AI systems become increasingly capable and ubiquitous, ensuring the safety\nof these systems is critical. However, existing safety tools often target\ndifferent aspects of model safety and cannot provide full assurance in\nisolation, highlighting a need for integrated and composite methodologies. This\npaper introduces BlueGlass, a framework designed to facilitate composite AI\nsafety workflows by providing a unified infrastructure enabling the integration\nand composition of diverse safety tools that operate across model internals and\noutputs. Furthermore, to demonstrate the utility of this framework, we present\nthree safety-oriented analyses on vision-language models for the task of object\ndetection: (1) distributional evaluation, revealing performance trade-offs and\npotential failure modes across distributions; (2) probe-based analysis of layer\ndynamics highlighting shared hierarchical learning via phase transition; and\n(3) sparse autoencoders identifying interpretable concepts. More broadly, this\nwork contributes foundational infrastructure and findings for building more\nrobust and reliable AI systems."}
{"id": "2507.09628", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09628", "abs": "https://arxiv.org/abs/2507.09628", "authors": ["Salvatore Citraro", "Edith Haim", "Alessandra Carini", "Cynthia S. Q. Siew", "Giulio Rossetti", "Massimo Stella"], "title": "SpreadPy: A Python tool for modelling spreading activation and superdiffusion in cognitive multiplex networks", "comment": null, "summary": "We introduce SpreadPy as a Python library for simulating spreading activation\nin cognitive single-layer and multiplex networks. Our tool is designed to\nperform numerical simulations testing structure-function relationships in\ncognitive processes. By comparing simulation results with grounded theories in\nknowledge modelling, SpreadPy enables systematic investigations of how\nactivation dynamics reflect cognitive, psychological and clinical phenomena. We\ndemonstrate the library's utility through three case studies: (1) Spreading\nactivation on associative knowledge networks distinguishes students with high\nversus low math anxiety, revealing anxiety-related structural differences in\nconceptual organization; (2) Simulations of a creativity task show that\nactivation trajectories vary with task difficulty, exposing how cognitive load\nmodulates lexical access; (3) In individuals with aphasia, simulated activation\npatterns on lexical networks correlate with empirical error types (semantic vs.\nphonological) during picture-naming tasks, linking network structure to\nclinical impairments. SpreadPy's flexible framework allows researchers to model\nthese processes using empirically derived or theoretical networks, providing\nmechanistic insights into individual differences and cognitive impairments. The\nlibrary is openly available, supporting reproducible research in psychology,\nneuroscience, and education research."}
{"id": "2507.10119", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10119", "abs": "https://arxiv.org/abs/2507.10119", "authors": ["Sadig Gojayev", "Ahmad Anaqreh", "Carolina Fortuna"], "title": "Analysis of AI Techniques for Orchestrating Edge-Cloud Application Migration", "comment": null, "summary": "Application migration in edge-cloud system enables high QoS and cost\neffective service delivery. However, automatically orchestrating such migration\nis typically solved with heuristic approaches. Starting from the Markov\nDecision Process (MDP), in this paper, we identify, analyze and compare\nselected state-of-the-art Artificial Intelligence (AI) planning and\nReinforcement Learning (RL) approaches for solving the class of edge-cloud\napplication migration problems that can be modeled as Towers of Hanoi (ToH)\nproblems. We introduce a new classification based on state space definition and\nanalyze the compared models also through this lense. The aim is to understand\navailable techniques capable of orchestrating such application migration in\nemerging computing continuum environments."}
{"id": "2507.09629", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09629", "abs": "https://arxiv.org/abs/2507.09629", "authors": ["Basel Mousi", "Nadir Durrani", "Fahim Dalvi"], "title": "An Exploration of Knowledge Editing for Arabic", "comment": null, "summary": "While Knowledge Editing (KE) has been widely explored in English, its\nbehavior in morphologically rich languages like Arabic remains underexamined.\nIn this work, we present the first study of Arabic KE. We evaluate four methods\n(ROME, MEMIT, ICE, and LTE) on Arabic translations of the ZsRE and Counterfact\nbenchmarks, analyzing both multilingual and cross-lingual settings. Our\nexperiments on Llama-2-7B-chat show show that parameter-based methods struggle\nwith cross-lingual generalization, while instruction-tuned methods perform more\nrobustly. We extend Learning-To-Edit (LTE) to a multilingual setting and show\nthat joint Arabic-English training improves both editability and transfer. We\nrelease Arabic KE benchmarks and multilingual training for LTE data to support\nfuture research."}
{"id": "2507.10124", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10124", "abs": "https://arxiv.org/abs/2507.10124", "authors": ["Thomas T. Hills"], "title": "Could you be wrong: Debiasing LLMs using a metacognitive prompt for improving human decision making", "comment": "12 pages, 3 figures", "summary": "Identifying bias in LLMs is ongoing. Because they are still in development,\nwhat is true today may be false tomorrow. We therefore need general strategies\nfor debiasing that will outlive current models. Strategies developed for\ndebiasing human decision making offer one promising approach as they\nincorporate an LLM-style prompt intervention designed to bring latent knowledge\ninto awareness during decision making. LLMs trained on vast amounts of\ninformation contain information about potential biases, counter-arguments, and\ncontradictory evidence, but that information may only be brought to bear if\nprompted. Metacognitive prompts developed in the human decision making\nliterature are designed to achieve this, and as I demonstrate here, they show\npromise with LLMs. The prompt I focus on here is \"could you be wrong?\"\nFollowing an LLM response, this prompt leads LLMs to produce additional\ninformation, including why they answered as they did, errors, biases,\ncontradictory evidence, and alternatives, none of which were apparent in their\ninitial response. Indeed, this metaknowledge often reveals that how LLMs and\nusers interpret prompts are not aligned. Here I demonstrate this prompt using a\nset of questions taken from recent articles about LLM biases, including\nimplicit discriminatory biases and failures of metacognition. \"Could you be\nwrong\" prompts the LLM to identify its own biases and produce cogent\nmetacognitive reflection. I also present another example involving convincing\nbut incomplete information, which is readily corrected by the metacognitive\nprompt. In sum, this work argues that human psychology offers a new avenue for\nprompt engineering, leveraging a long history of effective prompt-based\nimprovements to human decision making."}
{"id": "2507.09638", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09638", "abs": "https://arxiv.org/abs/2507.09638", "authors": ["Pawitsapak Akarajaradwong", "Chompakorn Chaksangchaichot", "Pirat Pothavorn", "Attapol Thamrongrattanarit-Rutherford", "Ekapol Chuangsuwanich", "Sarana Nutanong"], "title": "Can Group Relative Policy Optimization Improve Thai Legal Reasoning and Question Answering?", "comment": null, "summary": "The Retrieval-Augmented Generation (RAG) systems' performance on Thai legal\nquestion answering is still limited, especially for questions requiring\nextensive, complex legal reasoning. To address these limitations, we introduce\nan approach aligning LLMs toward improved law citation accuracy and better\nresponse quality using Group-Relative Policy Optimization (GRPO). Our approach\nleverages BGE-M3 embeddings as a cost-efficient semantic-similarity reward,\nsignificantly reducing computational expenses up to 2.5x compared to large\nlanguage model judges. Experiments on the NitiBench benchmark demonstrate\nsubstantial improvements: GRPO achieves up to 90% citation-F1 gains from the\nbase model and a 31% increase in joint quality metrics over instruction tuning.\nCrucially, our method shows enhanced robustness on complex legal reasoning\ntasks compared to instruction tuning, providing an effective and\nresource-efficient solution for enhancing Thai legal LLMs."}
{"id": "2507.10134", "categories": ["cs.AI", "53-01", "C.2"], "pdf": "https://arxiv.org/pdf/2507.10134", "abs": "https://arxiv.org/abs/2507.10134", "authors": ["Yousef Emami", "Hao Zhou", "Miguel Gutierrez Gaitan", "Kai Li", "Luis Almeida"], "title": "FRSICL: LLM-Enabled In-Context Learning Flight Resource Allocation for Fresh Data Collection in UAV-Assisted Wildfire Monitoring", "comment": "8 pages, 8 figures", "summary": "Unmanned Aerial Vehicles (UAVs) are vital for public safety, particularly in\nwildfire monitoring, where early detection minimizes environmental impact. In\nUAV-Assisted Wildfire Monitoring (UAWM) systems, joint optimization of sensor\ntransmission scheduling and velocity is critical for minimizing Age of\nInformation (AoI) from stale sensor data. Deep Reinforcement Learning (DRL) has\nbeen used for such optimization; however, its limitations such as low sampling\nefficiency, simulation-to-reality gaps, and complex training render it\nunsuitable for time-critical applications like wildfire monitoring. This paper\nintroduces a new online Flight Resource Allocation scheme based on LLM-Enabled\nIn-Context Learning (FRSICL) to jointly optimize the UAV's flight control and\ndata collection schedule along the trajectory in real time, thereby\nasymptotically minimizing the average AoI across ground sensors. In contrast to\nDRL, FRSICL generates data collection schedules and controls velocity using\nnatural language task descriptions and feedback from the environment, enabling\ndynamic decision-making without extensive retraining. Simulation results\nconfirm the effectiveness of the proposed FRSICL compared to Proximal Policy\nOptimization (PPO) and Nearest-Neighbor baselines."}
{"id": "2507.09701", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09701", "abs": "https://arxiv.org/abs/2507.09701", "authors": ["Shulin Huang", "Linyi Yang", "Yue Zhang"], "title": "MCEval: A Dynamic Framework for Fair Multilingual Cultural Evaluation of LLMs", "comment": null, "summary": "Large language models exhibit cultural biases and limited cross-cultural\nunderstanding capabilities, particularly when serving diverse global user\npopulations. We propose MCEval, a novel multilingual evaluation framework that\nemploys dynamic cultural question construction and enables causal analysis\nthrough Counterfactual Rephrasing and Confounder Rephrasing. Our comprehensive\nevaluation spans 13 cultures and 13 languages, systematically assessing both\ncultural awareness and cultural bias across different linguistic scenarios. The\nframework provides 39,897 cultural awareness instances and 17,940 cultural bias\ninstances. Experimental results reveal performance disparities across different\nlinguistic scenarios, demonstrating that optimal cultural performance is not\nonly linked to training data distribution, but also is related to\nlanguage-culture alignment. The evaluation results also expose the fairness\nissue, where approaches appearing successful in the English scenario create\nsubstantial disadvantages. MCEval represents the first comprehensive\nmultilingual cultural evaluation framework that provides deeper insights into\nLLMs' cultural understanding."}
{"id": "2507.10142", "categories": ["cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.10142", "abs": "https://arxiv.org/abs/2507.10142", "authors": ["Siyi Hu", "Mohamad A Hady", "Jianglin Qiao", "Jimmy Cao", "Mahardhika Pratama", "Ryszard Kowalczyk"], "title": "Adaptability in Multi-Agent Reinforcement Learning: A Framework and Unified Review", "comment": null, "summary": "Multi-Agent Reinforcement Learning (MARL) has shown clear effectiveness in\ncoordinating multiple agents across simulated benchmarks and constrained\nscenarios. However, its deployment in real-world multi-agent systems (MAS)\nremains limited, primarily due to the complex and dynamic nature of such\nenvironments. These challenges arise from multiple interacting sources of\nvariability, including fluctuating agent populations, evolving task goals, and\ninconsistent execution conditions. Together, these factors demand that MARL\nalgorithms remain effective under continuously changing system configurations\nand operational demands. To better capture and assess this capacity for\nadjustment, we introduce the concept of \\textit{adaptability} as a unified and\npractically grounded lens through which to evaluate the reliability of MARL\nalgorithms under shifting conditions, broadly referring to any changes in the\nenvironment dynamics that may occur during learning or execution. Centred on\nthe notion of adaptability, we propose a structured framework comprising three\nkey dimensions: learning adaptability, policy adaptability, and scenario-driven\nadaptability. By adopting this adaptability perspective, we aim to support more\nprincipled assessments of MARL performance beyond narrowly defined benchmarks.\nUltimately, this survey contributes to the development of algorithms that are\nbetter suited for deployment in dynamic, real-world multi-agent systems."}
{"id": "2507.09709", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09709", "abs": "https://arxiv.org/abs/2507.09709", "authors": ["Baturay Saglam", "Paul Kassianik", "Blaine Nelson", "Sajana Weerawardhena", "Yaron Singer", "Amin Karbasi"], "title": "Large Language Models Encode Semantics in Low-Dimensional Linear Subspaces", "comment": null, "summary": "Understanding the latent space geometry of large language models (LLMs) is\nkey to interpreting their behavior and improving alignment. \\baturay{However,\nit remains unclear to what extent LLMs internally organize representations\nrelated to semantic understanding. To investigate this, we conduct a\nlarge-scale empirical study of hidden states in transformer-based LLMs,\nanalyzing 11 decoder-only models across 6 scientific topics and 12 layers each.\nWe find that high-level semantic information consistently lies in\nlow-dimensional subspaces that form linearly separable representations across\ndistinct domains. This separability becomes more pronounced in deeper layers\nand under prompts that trigger structured reasoning or alignment\nbehaviors$\\unicode{x2013}$even when surface content is unchanged. This geometry\nenables simple yet effective causal interventions in hidden space; for example,\nreasoning patterns like chain-of-thought can be captured by a single vector\ndirection. Together, these findings support the development of geometry-aware\ntools that operate directly on latent representations to detect and mitigate\nharmful or adversarial content, using methods such as transport-based defenses\nthat leverage this separability. As a proof of concept, we demonstrate this\npotential by training a simple MLP classifier as a lightweight latent-space\nguardrail, which detects adversarial and malicious prompts with high precision."}
{"id": "2507.10156", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10156", "abs": "https://arxiv.org/abs/2507.10156", "authors": ["Lubnaa Abdur Rahman", "Ioannis Papathanail", "Stavroula Mougiakakou"], "title": "Introducing the Swiss Food Knowledge Graph: AI for Context-Aware Nutrition Recommendation", "comment": "10 pages, 2 Figures, 7 tables", "summary": "AI has driven significant progress in the nutrition field, especially through\nmultimedia-based automatic dietary assessment. However, existing automatic\ndietary assessment systems often overlook critical non-visual factors, such as\nrecipe-specific ingredient substitutions that can significantly alter\nnutritional content, and rarely account for individual dietary needs, including\nallergies, restrictions, cultural practices, and personal preferences. In\nSwitzerland, while food-related information is available, it remains\nfragmented, and no centralized repository currently integrates all relevant\nnutrition-related aspects within a Swiss context. To bridge this divide, we\nintroduce the Swiss Food Knowledge Graph (SwissFKG), the first resource, to our\nbest knowledge, to unite recipes, ingredients, and their substitutions with\nnutrient data, dietary restrictions, allergen information, and national\nnutrition guidelines under one graph. We establish a LLM-powered enrichment\npipeline for populating the graph, whereby we further present the first\nbenchmark of four off-the-shelf (<70 B parameter) LLMs for food knowledge\naugmentation. Our results demonstrate that LLMs can effectively enrich the\ngraph with relevant nutritional information. Our SwissFKG goes beyond recipe\nrecommendations by offering ingredient-level information such as allergen and\ndietary restriction information, and guidance aligned with nutritional\nguidelines. Moreover, we implement a Graph-RAG application to showcase how the\nSwissFKG's rich natural-language data structure can help LLM answer\nuser-specific nutrition queries, and we evaluate LLM-embedding pairings by\ncomparing user-query responses against predefined expected answers. As such,\nour work lays the foundation for the next generation of dietary assessment\ntools that blend visual, contextual, and cultural dimensions of eating."}
{"id": "2507.09758", "categories": ["cs.CL", "cs.LG", "I.2.7; I.2.6"], "pdf": "https://arxiv.org/pdf/2507.09758", "abs": "https://arxiv.org/abs/2507.09758", "authors": ["Qi Feng", "Yihong Liu", "Hinrich Sch√ºtze"], "title": "Your Pretrained Model Tells the Difficulty Itself: A Self-Adaptive Curriculum Learning Paradigm for Natural Language Understanding", "comment": "18 pages, 23 figures. To appear in ACL 2025 Student Research Workshop\n  (SRW)", "summary": "Curriculum learning is a widely adopted training strategy in natural language\nprocessing (NLP), where models are exposed to examples organized by increasing\ndifficulty to enhance learning efficiency and performance. However, most\nexisting approaches rely on manually defined difficulty metrics -- such as text\nlength -- which may not accurately reflect the model's own perspective. To\novercome this limitation, we present a self-adaptive curriculum learning\nparadigm that prioritizes fine-tuning examples based on difficulty scores\npredicted by pre-trained language models (PLMs) themselves. Building on these\nscores, we explore various training strategies that differ in the ordering of\nexamples for the fine-tuning: from easy-to-hard, hard-to-easy, to mixed\nsampling. We evaluate our method on four natural language understanding (NLU)\ndatasets covering both binary and multi-class classification tasks.\nExperimental results show that our approach leads to faster convergence and\nimproved performance compared to standard random sampling."}
{"id": "2507.10174", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10174", "abs": "https://arxiv.org/abs/2507.10174", "authors": ["Yumi Omori", "Zixuan Dong", "Keith Ross"], "title": "Should We Ever Prefer Decision Transformer for Offline Reinforcement Learning?", "comment": "Accepted by RLBrew: Ingredients for Developing Generalist Agents\n  workshop (RLC 2025)", "summary": "In recent years, extensive work has explored the application of the\nTransformer architecture to reinforcement learning problems. Among these,\nDecision Transformer (DT) has gained particular attention in the context of\noffline reinforcement learning due to its ability to frame return-conditioned\npolicy learning as a sequence modeling task. Most recently, Bhargava et al.\n(2024) provided a systematic comparison of DT with more conventional MLP-based\noffline RL algorithms, including Behavior Cloning (BC) and Conservative\nQ-Learning (CQL), and claimed that DT exhibits superior performance in\nsparse-reward and low-quality data settings.\n  In this paper, through experimentation on robotic manipulation tasks\n(Robomimic) and locomotion benchmarks (D4RL), we show that MLP-based Filtered\nBehavior Cloning (FBC) achieves competitive or superior performance compared to\nDT in sparse-reward environments. FBC simply filters out low-performing\ntrajectories from the dataset and then performs ordinary behavior cloning on\nthe filtered dataset. FBC is not only very straightforward, but it also\nrequires less training data and is computationally more efficient. The results\ntherefore suggest that DT is not preferable for sparse-reward environments.\nFrom prior work, arguably, DT is also not preferable for dense-reward\nenvironments. Thus, we pose the question: Is DT ever preferable?"}
{"id": "2507.09777", "categories": ["cs.CL", "cs.SI"], "pdf": "https://arxiv.org/pdf/2507.09777", "abs": "https://arxiv.org/abs/2507.09777", "authors": ["Gabriel Mordecki", "Guillermo Moncecchi", "Javier Couto"], "title": "Te Ahorr√© Un Click: A Revised Definition of Clickbait and Detection in Spanish News", "comment": null, "summary": "We revise the definition of clickbait, which lacks current consensus, and\nargue that the creation of a curiosity gap is the key concept that\ndistinguishes clickbait from other related phenomena such as sensationalism and\nheadlines that do not deliver what they promise or diverge from the article.\nTherefore, we propose a new definition: clickbait is a technique for generating\nheadlines and teasers that deliberately omit part of the information with the\ngoal of raising the readers' curiosity, capturing their attention and enticing\nthem to click. We introduce a new approach to clickbait detection datasets\ncreation, by refining the concept limits and annotations criteria, minimizing\nthe subjectivity in the decision as much as possible. Following it, we created\nand release TA1C (for Te Ahorr\\'e Un Click, Spanish for Saved You A Click), the\nfirst open source dataset for clickbait detection in Spanish. It consists of\n3,500 tweets coming from 18 well known media sources, manually annotated and\nreaching a 0.825 Fleiss' K inter annotator agreement. We implement strong\nbaselines that achieve 0.84 in F1-score."}
{"id": "2507.10208", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.10208", "abs": "https://arxiv.org/abs/2507.10208", "authors": ["Hamzah Ziadeh", "Hendrik Knoche"], "title": "Survey for Categorising Explainable AI Studies Using Data Analysis Task Frameworks", "comment": null, "summary": "Research into explainable artificial intelligence (XAI) for data analysis\ntasks suffer from a large number of contradictions and lack of concrete design\nrecommendations stemming from gaps in understanding the tasks that require AI\nassistance. In this paper, we drew on multiple fields such as visual analytics,\ncognition, and dashboard design to propose a method for categorising and\ncomparing XAI studies under three dimensions: what, why, and who. We identified\nthe main problems as: inadequate descriptions of tasks, context-free studies,\nand insufficient testing with target users. We propose that studies should\nspecifically report on their users' domain, AI, and data analysis expertise to\nillustrate the generalisability of their findings. We also propose study\nguidelines for designing and reporting XAI tasks to improve the XAI community's\nability to parse the rapidly growing field. We hope that our contribution can\nhelp researchers and designers better identify which studies are most relevant\nto their work, what gaps exist in the research, and how to handle contradictory\nresults regarding XAI design."}
{"id": "2507.09875", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09875", "abs": "https://arxiv.org/abs/2507.09875", "authors": ["Qinyuan Ye", "Robin Jia", "Xiang Ren"], "title": "Function Induction and Task Generalization: An Interpretability Study with Off-by-One Addition", "comment": "Code: https://github.com/INK-USC/function-induction", "summary": "Large language models demonstrate the intriguing ability to perform unseen\ntasks via in-context learning. However, it remains unclear what mechanisms\ninside the model drive such task-level generalization. In this work, we\napproach this question through the lens of off-by-one addition (i.e., 1+1=3,\n2+2=5, 3+3=?), a two-step, counterfactual task with an unexpected +1 function\nas a second step. Leveraging circuit-style interpretability techniques such as\npath patching, we analyze the models' internal computations behind their\nnotable performance and present three key findings. First, we uncover a\nfunction induction mechanism that explains the model's generalization from\nstandard addition to off-by-one addition. This mechanism resembles the\nstructure of the induction head mechanism found in prior work and elevates it\nto a higher level of abstraction. Second, we show that the induction of the +1\nfunction is governed by multiple attention heads in parallel, each of which\nemits a distinct piece of the +1 function. Finally, we find that this function\ninduction mechanism is reused in a broader range of tasks, including synthetic\ntasks such as shifted multiple-choice QA and algorithmic tasks such as base-8\naddition. Overall, our findings offer deeper insights into how reusable and\ncomposable structures within language models enable task-level generalization."}
{"id": "2507.10281", "categories": ["cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2507.10281", "abs": "https://arxiv.org/abs/2507.10281", "authors": ["Jiaming Tian", "Liyao Li", "Wentao Ye", "Haobo Wang", "Lingxin Wang", "Lihua Yu", "Zujie Ren", "Gang Chen", "Junbo Zhao"], "title": "Toward Real-World Table Agents: Capabilities, Workflows, and Design Principles for LLM-based Table Intelligence", "comment": null, "summary": "Tables are fundamental in domains such as finance, healthcare, and public\nadministration, yet real-world table tasks often involve noise, structural\nheterogeneity, and semantic complexity--issues underexplored in existing\nresearch that primarily targets clean academic datasets. This survey focuses on\nLLM-based Table Agents, which aim to automate table-centric workflows by\nintegrating preprocessing, reasoning, and domain adaptation. We define five\ncore competencies--C1: Table Structure Understanding, C2: Table and Query\nSemantic Understanding, C3: Table Retrieval and Compression, C4: Executable\nReasoning with Traceability, and C5: Cross-Domain Generalization--to analyze\nand compare current approaches. In addition, a detailed examination of the\nText-to-SQL Agent reveals a performance gap between academic benchmarks and\nreal-world scenarios, especially for open-source models. Finally, we provide\nactionable insights to improve the robustness, generalization, and efficiency\nof LLM-based Table Agents in practical settings."}
{"id": "2507.09935", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09935", "abs": "https://arxiv.org/abs/2507.09935", "authors": ["Hai Toan Nguyen", "Tien Dat Nguyen", "Viet Ha Nguyen"], "title": "Enhancing Retrieval Augmented Generation with Hierarchical Text Segmentation Chunking", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) systems commonly use chunking strategies\nfor retrieval, which enhance large language models (LLMs) by enabling them to\naccess external knowledge, ensuring that the retrieved information is\nup-to-date and domain-specific. However, traditional methods often fail to\ncreate chunks that capture sufficient semantic meaning, as they do not account\nfor the underlying textual structure. This paper proposes a novel framework\nthat enhances RAG by integrating hierarchical text segmentation and clustering\nto generate more meaningful and semantically coherent chunks. During inference,\nthe framework retrieves information by leveraging both segment-level and\ncluster-level vector representations, thereby increasing the likelihood of\nretrieving more precise and contextually relevant information. Evaluations on\nthe NarrativeQA, QuALITY, and QASPER datasets indicate that the proposed method\nachieved improved results compared to traditional chunking techniques."}
{"id": "2507.10397", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10397", "abs": "https://arxiv.org/abs/2507.10397", "authors": ["Alessandra M. M. M. Gouv√™a", "Nuno Paulos", "Eduardo Uchoa e Mari√° C. V. Nascimento"], "title": "Instance space analysis of the capacitated vehicle routing problem", "comment": null, "summary": "This paper seeks to advance CVRP research by addressing the challenge of\nunderstanding the nuanced relationships between instance characteristics and\nmetaheuristic (MH) performance. We present Instance Space Analysis (ISA) as a\nvaluable tool that allows for a new perspective on the field. By combining the\nISA methodology with a dataset from the DIMACS 12th Implementation Challenge on\nVehicle Routing, our research enabled the identification of 23 relevant\ninstance characteristics. Our use of the PRELIM, SIFTED, and PILOT stages,\nwhich employ dimensionality reduction and machine learning methods, allowed us\nto create a two-dimensional projection of the instance space to understand how\nthe structure of instances affect the behavior of MHs. A key contribution of\nour work is that we provide a projection matrix, which makes it straightforward\nto incorporate new instances into this analysis and allows for a new method for\ninstance analysis in the CVRP field."}
{"id": "2507.09973", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09973", "abs": "https://arxiv.org/abs/2507.09973", "authors": ["Sarah Pan"], "title": "Tiny Reward Models", "comment": "2025 ICML Efficient Systems for Foundation Models Workshop", "summary": "Large decoder-based language models have become the dominant architecture for\nreward modeling in reinforcement learning from human feedback (RLHF). However,\nas reward models are increasingly deployed in test-time strategies, their\ninference costs become a growing concern. We present TinyRM, a family of small,\nbidirectional masked language models (MLMs) with as few as 400 million\nparameters, that rival the capabilities of models over 175 times larger on\nreasoning and safety preference modeling tasks. TinyRM combines FLAN-style\nprompting, Directional Low-Rank Adaptation (DoRA), and layer freezing to\nachieve strong performance on RewardBench, despite using significantly fewer\nresources. Our experiments suggest that small models benefit from\ndomain-specific tuning strategies, particularly in reasoning, where lightweight\nfinetuning methods are especially effective. While challenges remain in\nbuilding generalist models and conversational preference modeling, our\npreliminary results highlight the promise of lightweight bidirectional\narchitectures as efficient, scalable alternatives for preference modeling."}
{"id": "2507.10421", "categories": ["cs.AI", "cs.ET", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10421", "abs": "https://arxiv.org/abs/2507.10421", "authors": ["Meriem Zerkouk", "Miloud Mihoubi", "Belkacem Chikhaoui"], "title": "SentiDrop: A Multi Modal Machine Learning model for Predicting Dropout in Distance Learning", "comment": "International Conference on Education and New Learning Technologies\n  (2025)", "summary": "School dropout is a serious problem in distance learning, where early\ndetection is crucial for effective intervention and student perseverance.\nPredicting student dropout using available educational data is a widely\nresearched topic in learning analytics. Our partner's distance learning\nplatform highlights the importance of integrating diverse data sources,\nincluding socio-demographic data, behavioral data, and sentiment analysis, to\naccurately predict dropout risks. In this paper, we introduce a novel model\nthat combines sentiment analysis of student comments using the Bidirectional\nEncoder Representations from Transformers (BERT) model with socio-demographic\nand behavioral data analyzed through Extreme Gradient Boosting (XGBoost). We\nfine-tuned BERT on student comments to capture nuanced sentiments, which were\nthen merged with key features selected using feature importance techniques in\nXGBoost. Our model was tested on unseen data from the next academic year,\nachieving an accuracy of 84\\%, compared to 82\\% for the baseline model.\nAdditionally, the model demonstrated superior performance in other metrics,\nsuch as precision and F1-score. The proposed method could be a vital tool in\ndeveloping personalized strategies to reduce dropout rates and encourage\nstudent perseverance"}
{"id": "2507.09982", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09982", "abs": "https://arxiv.org/abs/2507.09982", "authors": ["Hang Yuan", "Chen Li", "Wenjun Ma", "Yuncheng Jiang"], "title": "TextOmics-Guided Diffusion for Hit-like Molecular Generation", "comment": null, "summary": "Hit-like molecular generation with therapeutic potential is essential for\ntarget-specific drug discovery. However, the field lacks heterogeneous data and\nunified frameworks for integrating diverse molecular representations. To bridge\nthis gap, we introduce TextOmics, a pioneering benchmark that establishes\none-to-one correspondences between omics expressions and molecular textual\ndescriptions. TextOmics provides a heterogeneous dataset that facilitates\nmolecular generation through representations alignment. Built upon this\nfoundation, we propose ToDi, a generative framework that jointly conditions on\nomics expressions and molecular textual descriptions to produce biologically\nrelevant, chemically valid, hit-like molecules. ToDi leverages two encoders\n(OmicsEn and TextEn) to capture multi-level biological and semantic\nassociations, and develops conditional diffusion (DiffGen) for controllable\ngeneration. Extensive experiments confirm the effectiveness of TextOmics and\ndemonstrate ToDi outperforms existing state-of-the-art approaches, while also\nshowcasing remarkable potential in zero-shot therapeutic molecular generation.\nSources are available at: https://github.com/hala-ToDi."}
{"id": "2507.10446", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10446", "abs": "https://arxiv.org/abs/2507.10446", "authors": ["Sudarshan Babu"], "title": "Acquiring and Adapting Priors for Novel Tasks via Neural Meta-Architectures", "comment": "arXiv admin note: text overlap with arXiv:2310.17075", "summary": "The ability to transfer knowledge from prior experiences to novel tasks\nstands as a pivotal capability of intelligent agents, including both humans and\ncomputational models. This principle forms the basis of transfer learning,\nwhere large pre-trained neural networks are fine-tuned to adapt to downstream\ntasks. Transfer learning has demonstrated tremendous success, both in terms of\ntask adaptation speed and performance. However there are several domains where,\ndue to lack of data, training such large pre-trained models or foundational\nmodels is not a possibility - computational chemistry, computational\nimmunology, and medical imaging are examples. To address these challenges, our\nwork focuses on designing architectures to enable efficient acquisition of\npriors when large amounts of data are unavailable. In particular, we\ndemonstrate that we can use neural memory to enable adaptation on\nnon-stationary distributions with only a few samples. Then we demonstrate that\nour hypernetwork designs (a network that generates another network) can acquire\nmore generalizable priors than standard networks when trained with Model\nAgnostic Meta-Learning (MAML). Subsequently, we apply hypernetworks to 3D scene\ngeneration, demonstrating that they can acquire priors efficiently on just a\nhandful of training scenes, thereby leading to faster text-to-3D generation. We\nthen extend our hypernetwork framework to perform 3D segmentation on novel\nscenes with limited data by efficiently transferring priors from earlier viewed\nscenes. Finally, we repurpose an existing molecular generative method as a\npre-training framework that facilitates improved molecular property prediction,\naddressing critical challenges in computational immunology"}
{"id": "2507.10008", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.10008", "abs": "https://arxiv.org/abs/2507.10008", "authors": ["Jun Li", "Xiangmeng Wang", "Haoyang Li", "Yifei Yan", "Hong Va Leong", "Ling Feng", "Nancy Xiaonan Yu", "Qing Li"], "title": "Protective Factor-Aware Dynamic Influence Learning for Suicide Risk Prediction on Social Media", "comment": null, "summary": "Suicide is a critical global health issue that requires urgent attention.\nEven though prior work has revealed valuable insights into detecting current\nsuicide risk on social media, little attention has been paid to developing\nmodels that can predict subsequent suicide risk over time, limiting their\nability to capture rapid fluctuations in individuals' mental state transitions.\nIn addition, existing work ignores protective factors that play a crucial role\nin suicide risk prediction, focusing predominantly on risk factors alone.\nProtective factors such as social support and coping strategies can mitigate\nsuicide risk by moderating the impact of risk factors. Therefore, this study\nproposes a novel framework for predicting subsequent suicide risk by jointly\nlearning the dynamic influence of both risk factors and protective factors on\nusers' suicide risk transitions. We propose a novel Protective Factor-Aware\nDataset, which is built from 12 years of Reddit posts along with comprehensive\nannotations of suicide risk and both risk and protective factors. We also\nintroduce a Dynamic Factors Influence Learning approach that captures the\nvarying impact of risk and protective factors on suicide risk transitions,\nrecognizing that suicide risk fluctuates over time according to established\npsychological theories. Our thorough experiments demonstrate that the proposed\nmodel significantly outperforms state-of-the-art models and large language\nmodels across three datasets. In addition, the proposed Dynamic Factors\nInfluence Learning provides interpretable weights, helping clinicians better\nunderstand suicidal patterns and enabling more targeted intervention\nstrategies."}
{"id": "2507.10522", "categories": ["cs.AI", "cs.CL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.10522", "abs": "https://arxiv.org/abs/2507.10522", "authors": ["Jennifer D'Souza", "Endres Keno Sander", "Andrei Aioanei"], "title": "DeepResearch$^{\\text{Eco}}$: A Recursive Agentic Workflow for Complex Scientific Question Answering in Ecology", "comment": "12 pages, 3 figures", "summary": "We introduce DeepResearch$^{\\text{Eco}}$, a novel agentic LLM-based system\nfor automated scientific synthesis that supports recursive, depth- and\nbreadth-controlled exploration of original research questions -- enhancing\nsearch diversity and nuance in the retrieval of relevant scientific literature.\nUnlike conventional retrieval-augmented generation pipelines, DeepResearch\nenables user-controllable synthesis with transparent reasoning and\nparameter-driven configurability, facilitating high-throughput integration of\ndomain-specific evidence while maintaining analytical rigor. Applied to 49\necological research questions, DeepResearch achieves up to a 21-fold increase\nin source integration and a 14.9-fold rise in sources integrated per 1,000\nwords. High-parameter settings yield expert-level analytical depth and\ncontextual diversity.\n  Source code available at: https://github.com/sciknoworg/deep-research."}
{"id": "2507.10059", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.10059", "abs": "https://arxiv.org/abs/2507.10059", "authors": ["David Ponce", "Thierry Etchegoyhen", "Javier Del Ser"], "title": "GeLaCo: An Evolutionary Approach to Layer Compression", "comment": null, "summary": "Large Language Models (LLM) have achieved remarkable performance across a\nlarge number of tasks, but face critical deployment and usage barriers due to\nsubstantial computational requirements. Model compression methods, which aim to\nreduce model size while preserving its capacity, are an important means to\nmitigate these issues. Promising approaches along these lines, such as\nstructured pruning, typically require costly empirical search for optimal\nvariants and may run the risk of ignoring better solutions. In this work we\nintroduce GeLaCo, an evolutionary approach to LLM compression via layer\ncollapse. Our approach supports an efficient exploration of the compression\nsolution space via population-based search and a module-wise similarity fitness\nfunction capturing attention, feed-forward, and hidden state representations.\nGeLaCo also supports both single and multi-objective evolutionary compression\nsearch, establishing the first Pareto frontier along compression and quality\naxes. We evaluate GeLaCo solutions via both perplexity-based and generative\nevaluations over foundational and instruction-tuned models, outperforming\nstate-of-the-art alternatives."}
{"id": "2507.08879", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08879", "abs": "https://arxiv.org/abs/2507.08879", "authors": ["Max-Paul F√∂rster", "Luca Deck", "Raimund Weidlich", "Niklas K√ºhl"], "title": "A Multi-Level Strategy for Deepfake Content Moderation under EU Regulation", "comment": null, "summary": "The growing availability and use of deepfake technologies increases risks for\ndemocratic societies, e.g., for political communication on online platforms.\nThe EU has responded with transparency obligations for providers and deployers\nof Artificial Intelligence (AI) systems and online platforms. This includes\nmarking deepfakes during generation and labeling deepfakes when they are\nshared. However, the lack of industry and enforcement standards poses an\nongoing challenge. Through a multivocal literature review, we summarize methods\nfor marking, detecting, and labeling deepfakes and assess their effectiveness\nunder EU regulation. Our results indicate that individual methods fail to meet\nregulatory and practical requirements. Therefore, we propose a multi-level\nstrategy combining the strengths of existing methods. To account for the masses\nof content on online platforms, our multi-level strategy provides scalability\nand practicality via a simple scoring mechanism. At the same time, it is\nagnostic to types of deepfake technology and allows for context-specific risk\nweighting."}
{"id": "2507.10073", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10073", "abs": "https://arxiv.org/abs/2507.10073", "authors": ["Simon M√ºnker"], "title": "Cultural Bias in Large Language Models: Evaluating AI Agents through Moral Questionnaires", "comment": "15pages, 1 figure, 2 tables", "summary": "Are AI systems truly representing human values, or merely averaging across\nthem? Our study suggests a concerning reality: Large Language Models (LLMs)\nfail to represent diverse cultural moral frameworks despite their linguistic\ncapabilities. We expose significant gaps between AI-generated and human moral\nintuitions by applying the Moral Foundations Questionnaire across 19 cultural\ncontexts. Comparing multiple state-of-the-art LLMs' origins against human\nbaseline data, we find these models systematically homogenize moral diversity.\nSurprisingly, increased model size doesn't consistently improve cultural\nrepresentation fidelity. Our findings challenge the growing use of LLMs as\nsynthetic populations in social science research and highlight a fundamental\nlimitation in current AI alignment approaches. Without data-driven alignment\nbeyond prompting, these systems cannot capture the nuanced, culturally-specific\nmoral intuitions. Our results call for more grounded alignment objectives and\nevaluation metrics to ensure AI systems represent diverse human values rather\nthan flattening the moral landscape."}
{"id": "2507.08881", "categories": ["cs.CY", "cs.AI", "cs.SI"], "pdf": "https://arxiv.org/pdf/2507.08881", "abs": "https://arxiv.org/abs/2507.08881", "authors": ["Zhang MingDa", "Xu Qing"], "title": "The Consistency-Acceptability Divergence of LLMs in Judicial Decision-Making: Task and Stakeholder Dimensions", "comment": "12 pages,2 figures", "summary": "The integration of large language model (LLM) technology into judicial\nsystems is fundamentally transforming legal practice worldwide. However, this\nglobal transformation has revealed an urgent paradox requiring immediate\nattention. This study introduces the concept of ``consistency-acceptability\ndivergence'' for the first time, referring to the gap between technical\nconsistency and social acceptance. While LLMs achieve high consistency at the\ntechnical level, this consistency demonstrates both positive and negative\neffects. Through comprehensive analysis of recent data on LLM judicial\napplications from 2023--2025, this study finds that addressing this challenge\nrequires understanding both task and stakeholder dimensions. This study\nproposes the Dual-Track Deliberative Multi-Role LLM Judicial Governance\nFramework (DTDMR-LJGF), which enables intelligent task classification and\nmeaningful interaction among diverse stakeholders. This framework offers both\ntheoretical insights and practical guidance for building an LLM judicial\necosystem that balances technical efficiency with social legitimacy."}
{"id": "2507.10085", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10085", "abs": "https://arxiv.org/abs/2507.10085", "authors": ["Chenxi Huang", "Shaotian Yan", "Liang Xie", "Binbin Lin", "Sinan Fan", "Yue Xin", "Deng Cai", "Chen Shen", "Jieping Ye"], "title": "Enhancing Chain-of-Thought Reasoning with Critical Representation Fine-tuning", "comment": "Accepted by ACL 2025", "summary": "Representation Fine-tuning (ReFT), a recently proposed Parameter-Efficient\nFine-Tuning (PEFT) method, has attracted widespread attention for significantly\nimproving parameter efficiency by editing representation space alone. In this\nwork, we investigate applying ReFT to complex reasoning tasks. However,\ndirectly using the native ReFT method, which modifies fixed representations at\nthe beginning and end of each layer, yields suboptimal performance, as these\nfixed-position representations have uncertain impact on the outputs. We observe\nthat, in complex reasoning tasks, there often exist certain critical\nrepresentations. These representations either integrate significant information\nfrom preceding layers or regulate subsequent layer representations. Through\nlayer-by-layer propagation, they exert a substantial influence on the final\noutput. Naturally, fine-tuning these critical representations has the potential\nto greatly enhance reasoning performance. Building upon these insights, we\npropose Critical Representation Fine-Tuning (CRFT), a novel method that\nidentifies and optimizes these critical representations through information\nflow analysis. CRFT operates within a supervised learning framework,\ndynamically optimizing critical representations in a low-rank linear subspace\nwhile freezing the base model. The effectiveness and efficiency of our method\nare validated across eight benchmarks for arithmetic and commonsense reasoning,\nusing LLaMA and Mistral model families. Furthermore, our method also adapts\neffectively to few-shot settings, boosting one-shot accuracy by 16.4%. Our work\nhighlights the untapped potential of representation-level optimization for CoT\nreasoning, offering a lightweight yet powerful alternative to traditional PEFT\nmethods."}
{"id": "2507.08898", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08898", "abs": "https://arxiv.org/abs/2507.08898", "authors": ["Wenliang Shan", "Michael Fu", "Rui Yang", "Chakkrit", "Tantithamthavorn"], "title": "SEALGuard: Safeguarding the Multilingual Conversations in Southeast Asian Languages for LLM Software Systems", "comment": "Under Review at Information and Software Technology", "summary": "Safety alignment is critical for LLM-powered systems. While recent\nLLM-powered guardrail approaches such as LlamaGuard achieve high detection\naccuracy of unsafe inputs written in English (e.g., ``How to create a bomb?''),\nthey struggle with multilingual unsafe inputs. This limitation leaves LLM\nsystems vulnerable to unsafe and jailbreak prompts written in low-resource\nlanguages such as those in Southeast Asia. This paper introduces SEALGuard, a\nmultilingual guardrail designed to improve the safety alignment across diverse\nlanguages. It aims to address the multilingual safety alignment gap of existing\nguardrails and ensure effective filtering of unsafe and jailbreak prompts in\nLLM-powered systems. We adapt a general-purpose multilingual language model\ninto a multilingual guardrail using low-rank adaptation (LoRA). We construct\nSEALSBench, a large-scale multilingual safety alignment dataset containing over\n260,000 prompts in ten languages, including safe, unsafe, and jailbreak cases.\nWe evaluate SEALGuard against state-of-the-art guardrails such as LlamaGuard on\nthis benchmark. Our findings show that multilingual unsafe and jailbreak\nprompts substantially degrade the performance of the state-of-the-art\nLlamaGuard, which experiences a drop in Defense Success Rate (DSR) by 9% and\n18%, respectively, compared to its performance on English-only prompts. In\ncontrast, SEALGuard outperforms existing guardrails in detecting multilingual\nunsafe and jailbreak prompts, improving DSR by 48% over LlamaGuard and\nachieving the best DSR, precision, and F1-score. Our ablation study further\nreveals the contributions of adaptation strategies and model size to the\noverall performance of SEALGuard. SEALGuard advances the safety alignment of\nLLM systems by introducing an effective multilingual guardrail."}
{"id": "2507.10098", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.10098", "abs": "https://arxiv.org/abs/2507.10098", "authors": ["Chen Su", "Yuanhe Tian", "Qinyu Liu", "Jun Zhang", "Yan Song"], "title": "Fusing Large Language Models with Temporal Transformers for Time Series Forecasting", "comment": null, "summary": "Recently, large language models (LLMs) have demonstrated powerful\ncapabilities in performing various tasks and thus are applied by recent studies\nto time series forecasting (TSF) tasks, which predict future values with the\ngiven historical time series. Existing LLM-based approaches transfer knowledge\nlearned from text data to time series prediction using prompting or fine-tuning\nstrategies. However, LLMs are proficient at reasoning over discrete tokens and\nsemantic patterns but are not initially designed to model continuous numerical\ntime series data. The gaps between text and time series data lead LLMs to\nachieve inferior performance to a vanilla Transformer model that is directly\ntrained on TSF data. However, the vanilla Transformers often struggle to learn\nhigh-level semantic patterns. In this paper, we design a novel\nTransformer-based architecture that complementarily leverages LLMs and vanilla\nTransformers, so as to integrate the high-level semantic representations\nlearned by LLMs into the temporal information encoded by time series\nTransformers, where a hybrid representation is obtained by fusing the\nrepresentations from the LLM and the Transformer. The resulting fused\nrepresentation contains both historical temporal dynamics and semantic\nvariation patterns, allowing our model to predict more accurate future values.\nExperiments on benchmark datasets demonstrate the effectiveness of the proposed\napproach."}
{"id": "2507.08924", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08924", "abs": "https://arxiv.org/abs/2507.08924", "authors": ["Seokhee Hong", "Sunkyoung Kim", "Guijin Son", "Soyeon Kim", "Yeonjung Hong", "Jinsik Lee"], "title": "From KMMLU-Redux to KMMLU-Pro: A Professional Korean Benchmark Suite for LLM Evaluation", "comment": null, "summary": "The development of Large Language Models (LLMs) requires robust benchmarks\nthat encompass not only academic domains but also industrial fields to\neffectively evaluate their applicability in real-world scenarios. In this\npaper, we introduce two Korean expert-level benchmarks. KMMLU-Redux,\nreconstructed from the existing KMMLU, consists of questions from the Korean\nNational Technical Qualification exams, with critical errors removed to enhance\nreliability. KMMLU-Pro is based on Korean National Professional Licensure exams\nto reflect professional knowledge in Korea. Our experiments demonstrate that\nthese benchmarks comprehensively represent industrial knowledge in Korea. We\nrelease our dataset publicly available."}
{"id": "2507.10155", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.10155", "abs": "https://arxiv.org/abs/2507.10155", "authors": ["Khouloud Saadi", "Di Wang"], "title": "Task-Based Flexible Feature Distillation for LLMs", "comment": null, "summary": "Knowledge Distillation (KD) in general and feature distillation in particular\nare promising techniques for reducing the high computational demand of large\nlanguage models (LLMs). However, traditional feature KD methods typically\nassume that the teacher and the student share the same hidden size, limiting\nthe flexibility of the student's architecture. A common solution to this\nproblem involves training a linear projector to align their feature spaces, but\nthis introduces additional parameters that must be learned from scratch and\noften degrades performance on downstream tasks, especially in generative\nsettings. To address this issue, in this work, we propose a novel task-based\nfeature distillation method that enables knowledge transfer between teacher and\nstudent models with different hidden layer dimensions, without introducing any\nnew parameters. Leveraging the insight that only a subset of LLM components\ncontribute significantly to a specific downstream task, our approach identifies\nthe most task-relevant hidden units in the teacher and directly distills their\nactivations to the student. Our method is flexible and easily integrates with\nother distillation frameworks. Empirical results show consistent improvements\nover prior approaches across diverse tasks, including classification,\ninstruction-following, and summarization, achieving up to a 3\\% performance\ngain over the linear projection baseline."}
{"id": "2507.09037", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09037", "abs": "https://arxiv.org/abs/2507.09037", "authors": ["Bharadwaj Ravichandran", "David Joy", "Paul Elliott", "Brian Hu", "Jadie Adams", "Christopher Funk", "Emily Veenhuis", "Anthony Hoogs", "Arslan Basharat"], "title": "ALIGN: Prompt-based Attribute Alignment for Reliable, Responsible, and Personalized LLM-based Decision-Making", "comment": "10 pages total (including appendix), ICML 2025 Workshop on Reliable\n  and Responsible Foundation Models", "summary": "Large language models (LLMs) are increasingly being used as decision aids.\nHowever, users have diverse values and preferences that can affect their\ndecision-making, which requires novel methods for LLM alignment and\npersonalization. Existing LLM comparison tools largely focus on benchmarking\ntasks, such as knowledge-based question answering. In contrast, our proposed\nALIGN system focuses on dynamic personalization of LLM-based decision-makers\nthrough prompt-based alignment to a set of fine-grained attributes. Key\nfeatures of our system include robust configuration management, structured\noutput generation with reasoning, and several algorithm implementations with\nswappable LLM backbones, enabling different types of analyses. Our user\ninterface enables a qualitative, side-by-side comparison of LLMs and their\nalignment to various attributes, with a modular backend for easy algorithm\nintegration. Additionally, we perform a quantitative analysis comparing\nalignment approaches in two different domains: demographic alignment for public\nopinion surveys and value alignment for medical triage decision-making. The\nentire ALIGN framework is open source and will enable new research on reliable,\nresponsible, and personalized LLM-based decision-makers."}
{"id": "2507.10177", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10177", "abs": "https://arxiv.org/abs/2507.10177", "authors": ["Rohitash Chandra", "Jiyong Choi"], "title": "Abusive text transformation using LLMs", "comment": null, "summary": "Although Large Language Models (LLMs) have demonstrated significant\nadvancements in natural language processing tasks, their effectiveness in the\nclassification and transformation of abusive text into non-abusive versions\nremains an area for exploration. In this study, we aim to use LLMs to transform\nabusive text (tweets and reviews) featuring hate speech and swear words into\nnon-abusive text, while retaining the intent of the text. We evaluate the\nperformance of two state-of-the-art LLMs, such as Gemini, GPT-4o, DeekSeek and\nGroq, on their ability to identify abusive text. We them to transform and\nobtain a text that is clean from abusive and inappropriate content but\nmaintains a similar level of sentiment and semantics, i.e. the transformed text\nneeds to maintain its message. Afterwards, we evaluate the raw and transformed\ndatasets with sentiment analysis and semantic analysis. Our results show Groq\nprovides vastly different results when compared with other LLMs. We have\nidentified similarities between GPT-4o and DeepSeek-V3."}
{"id": "2507.09076", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7; H.5.2"], "pdf": "https://arxiv.org/pdf/2507.09076", "abs": "https://arxiv.org/abs/2507.09076", "authors": ["Jialong Mai", "Xiaofen Xing", "Yawei Li", "Zhipeng Li", "Jingyuan Xing", "Xiangmin Xu"], "title": "Dynamic Parameter Memory: Temporary LoRA-Enhanced LLM for Long-Sequence Emotion Recognition in Conversation", "comment": "submitted to EMNLP 2025", "summary": "Recent research has focused on applying speech large language model (SLLM) to\nimprove speech emotion recognition (SER). However, the inherently high frame\nrate in speech modality severely limits the signal processing and understanding\ncapabilities of SLLM. For example, a SLLM with a 4K context window can only\nprocess 80 seconds of audio at 50Hz feature sampling rate before reaching its\ncapacity limit. Input token compression methods used in SLLM overlook the\ncontinuity and inertia of emotions across multiple conversation turns. This\npaper proposes a Dynamic Parameter Memory (DPM) mechanism with contextual\nsemantics and sentence-level emotion encoding, enabling processing of\nunlimited-length audio with limited context windows in SLLM. Specifically, DPM\nprogressively encodes sentence-level information and emotions into a temporary\nLoRA module during inference to effectively \"memorize\" the contextual\ninformation. We trained an emotion SLLM as a backbone and incorporated our DPM\ninto inference for emotion recognition in conversation (ERC). Experimental\nresults on the IEMOCAP dataset show that DPM significantly improves the emotion\nrecognition capabilities of SLLM when processing long audio sequences,\nachieving state-of-the-art performance."}
{"id": "2507.10216", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10216", "abs": "https://arxiv.org/abs/2507.10216", "authors": ["Renad Al-Monef", "Hassan Alhuzali", "Nora Alturayeif", "Ashwag Alasmari"], "title": "Absher: A Benchmark for Evaluating Large Language Models Understanding of Saudi Dialects", "comment": null, "summary": "As large language models (LLMs) become increasingly central to Arabic NLP\napplications, evaluating their understanding of regional dialects and cultural\nnuances is essential, particularly in linguistically diverse settings like\nSaudi Arabia. This paper introduces \\texttt{Absher}, a comprehensive benchmark\nspecifically designed to assess LLMs performance across major Saudi dialects.\n\\texttt{Absher} comprises over 18,000 multiple-choice questions spanning six\ndistinct categories: Meaning, True/False, Fill-in-the-Blank, Contextual Usage,\nCultural Interpretation, and Location Recognition. These questions are derived\nfrom a curated dataset of dialectal words, phrases, and proverbs sourced from\nvarious regions of Saudi Arabia. We evaluate several state-of-the-art LLMs,\nincluding multilingual and Arabic-specific models. We also provide detailed\ninsights into their capabilities and limitations. Our results reveal notable\nperformance gaps, particularly in tasks requiring cultural inference or\ncontextual understanding. Our findings highlight the urgent need for\ndialect-aware training and culturally aligned evaluation methodologies to\nimprove LLMs performance in real-world Arabic applications."}
{"id": "2507.09104", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09104", "abs": "https://arxiv.org/abs/2507.09104", "authors": ["Taolin Zhang", "Maosong Cao", "Alexander Lam", "Songyang Zhang", "Kai Chen"], "title": "CompassJudger-2: Towards Generalist Judge Model via Verifiable Rewards", "comment": null, "summary": "Recently, the role of LLM-as-judge in evaluating large language models has\ngained prominence. However, current judge models suffer from narrow\nspecialization and limited robustness, undermining their capacity for\ncomprehensive evaluations. In this work, we present CompassJudger-2, a novel\ngeneralist judge model that overcomes these limitations via a task-driven,\nmulti-domain data curation strategy. Central to our approach is supervising\njudgment tasks with verifiable rewards, guiding intrinsic critical reasoning\nthrough rejection sampling to foster robust, generalizable judgment\ncapabilities. We introduce a refined learning objective with margin policy\ngradient loss to enhance performance. Empirically, CompassJudger-2 achieves\nsuperior results across multiple judge and reward benchmarks, and our 7B model\ndemonstrates competitive judgment accuracy with significantly larger models\nlike DeepSeek-V3 and Qwen3-235B-A22B. Additionally, we propose JudgerBenchV2, a\ncomprehensive benchmark evaluating cross-domain judgment accuracy and rank\nconsistency to standardize judge model evaluation. These contributions advance\nrobust, scalable LLM judgment and establish new performance and evaluation\nstandards."}
{"id": "2507.10326", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.10326", "abs": "https://arxiv.org/abs/2507.10326", "authors": ["Muzhaffar Hazman", "Minh-Khoi Pham", "Shweta Soundararajan", "Goncalo Mordido", "Leonardo Custode", "David Lynch", "Giorgio Cruciata", "Yucheng Shi", "Hongmeng Song", "Wang Chao", "Pan Yue", "Aleksandar Milenovic", "Alexandros Agapitos"], "title": "Grammar-Guided Evolutionary Search for Discrete Prompt Optimisation", "comment": "Accepted for Publication at ECAI 2025", "summary": "Prompt engineering has proven to be a crucial step in leveraging pretrained\nlarge language models (LLMs) in solving various real-world tasks. Numerous\nsolutions have been proposed that seek to automate prompt engineering by using\nthe model itself to edit prompts. However, the majority of state-of-the-art\napproaches are evaluated on tasks that require minimal prompt templates and on\nvery large and highly capable LLMs. In contrast, solving complex tasks that\nrequire detailed information to be included in the prompt increases the amount\nof text that needs to be optimised. Furthermore, smaller models have been shown\nto be more sensitive to prompt design. To address these challenges, we propose\nan evolutionary search approach to automated discrete prompt optimisation\nconsisting of two phases. In the first phase, grammar-guided genetic\nprogramming is invoked to synthesise prompt-creating programmes by searching\nthe space of programmes populated by function compositions of syntactic,\ndictionary-based and LLM-based prompt-editing functions. In the second phase,\nlocal search is applied to explore the neighbourhoods of best-performing\nprogrammes in an attempt to further fine-tune their performance. Our approach\noutperforms three state-of-the-art prompt optimisation approaches,\nPromptWizard, OPRO, and RL-Prompt, on three relatively small general-purpose\nLLMs in four domain-specific challenging tasks. We also illustrate several\nexamples where these benchmark methods suffer relatively severe performance\ndegradation, while our approach improves performance in almost all task-model\ncombinations, only incurring minimal degradation when it does not."}
{"id": "2507.09155", "categories": ["cs.CL", "cs.AI", "68T50, 68T07"], "pdf": "https://arxiv.org/pdf/2507.09155", "abs": "https://arxiv.org/abs/2507.09155", "authors": ["Ali Vosoughi", "Ayoub Shahnazari", "Yufeng Xi", "Zeliang Zhang", "Griffin Hess", "Chenliang Xu", "Niaz Abdolrahim"], "title": "OPENXRD: A Comprehensive Benchmark and Enhancement Framework for LLM/MLLM XRD Question Answering", "comment": "10 pages, 6 figures, 5 tables. Code and dataset available at\n  https://github.com/niaz60/OpenXRD. Project webpage:\n  https://niaz60.github.io/OpenXRD/", "summary": "This work presents OPENXRD, an open-book pipeline designed for\ncrystallography question answering, which integrates textual prompts with\nconcise supporting content generated by GPT-4.5. Instead of using scanned\ntextbooks, which may lead to copyright issues, OPENXRD generates compact,\ndomain-specific references that help smaller models understand key concepts in\nX-ray diffraction (XRD). We evaluate OPENXRD on a well-defined set of 217\nexpert-level XRD questions by comparing different vision-language models,\nincluding GPT-4 and LLaVA-based frameworks such as Mistral, LLaMA, and QWEN,\nunder both closed-book (without supporting material) and open-book (with\nsupporting material) conditions. Our experimental results show significant\naccuracy improvements in models that use the GPT-4.5-generated summaries,\nparticularly those with limited prior training in crystallography. OPENXRD uses\nknowledge from larger models to fill knowledge gaps in crystallography and\nshows that AI-generated texts can help smaller models reason more effectively\nin scientific tasks. While the current version of OPENXRD focuses on text-based\ninputs, we also explore future extensions such as adding real crystal diagrams\nor diffraction patterns to improve interpretation in specialized materials\nscience contexts. Overall, OPENXRD shows that specialized open-book systems can\nbe useful in materials science and provides a foundation for broader natural\nlanguage processing (NLP) tools in critical scientific fields."}
{"id": "2507.10330", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10330", "abs": "https://arxiv.org/abs/2507.10330", "authors": ["Mohammed Bouri", "Adnane Saoud"], "title": "Bridging Robustness and Generalization Against Word Substitution Attacks in NLP via the Growth Bound Matrix Approach", "comment": "Accepted to ACL Findings 2025", "summary": "Despite advancements in Natural Language Processing (NLP), models remain\nvulnerable to adversarial attacks, such as synonym substitutions. While prior\nwork has focused on improving robustness for feed-forward and convolutional\narchitectures, the robustness of recurrent networks and modern state space\nmodels (SSMs), such as S4, remains understudied. These architectures pose\nunique challenges due to their sequential processing and complex parameter\ndynamics. In this paper, we introduce a novel regularization technique based on\nGrowth Bound Matrices (GBM) to improve NLP model robustness by reducing the\nimpact of input perturbations on model outputs. We focus on computing the GBM\nfor three architectures: Long Short-Term Memory (LSTM), State Space models\n(S4), and Convolutional Neural Networks (CNN). Our method aims to (1) enhance\nresilience against word substitution attacks, (2) improve generalization on\nclean text, and (3) providing the first systematic analysis of SSM (S4)\nrobustness. Extensive experiments across multiple architectures and benchmark\ndatasets demonstrate that our method improves adversarial robustness by up to\n8.8% over existing baselines. These results highlight the effectiveness of our\napproach, outperforming several state-of-the-art methods in adversarial\ndefense. Codes are available at https://github.com/BouriMohammed/GBM"}
{"id": "2507.09470", "categories": ["cs.CL", "cs.AI", "68T07"], "pdf": "https://arxiv.org/pdf/2507.09470", "abs": "https://arxiv.org/abs/2507.09470", "authors": ["Mingchuan Yang", "Ziyuan Huang"], "title": "Enhancing Clinical Text Classification via Fine-Tuned DRAGON Longformer Models", "comment": "29 pages, 5 tables", "summary": "This study explores the optimization of the DRAGON Longformer base model for\nclinical text classification, specifically targeting the binary classification\nof medical case descriptions. A dataset of 500 clinical cases containing\nstructured medical observations was used, with 400 cases for training and 100\nfor validation. Enhancements to the pre-trained\njoeranbosma/dragon-longformer-base-mixed-domain model included hyperparameter\ntuning, domain-specific preprocessing, and architectural adjustments. Key\nmodifications involved increasing sequence length from 512 to 1024 tokens,\nadjusting learning rates from 1e-05 to 5e-06, extending training epochs from 5\nto 8, and incorporating specialized medical terminology. The optimized model\nachieved notable performance gains: accuracy improved from 72.0% to 85.2%,\nprecision from 68.0% to 84.1%, recall from 75.0% to 86.3%, and F1-score from\n71.0% to 85.2%. Statistical analysis confirmed the significance of these\nimprovements (p < .001). The model demonstrated enhanced capability in\ninterpreting medical terminology, anatomical measurements, and clinical\nobservations. These findings contribute to domain-specific language model\nresearch and offer practical implications for clinical natural language\nprocessing applications. The optimized model's strong performance across\ndiverse medical conditions underscores its potential for broad use in\nhealthcare settings."}
{"id": "2507.10342", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.10342", "abs": "https://arxiv.org/abs/2507.10342", "authors": ["Rosa Illan Castillo", "Javier Valenzuela"], "title": "Using AI to replicate human experimental results: a motion study", "comment": null, "summary": "This paper explores the potential of large language models (LLMs) as reliable\nanalytical tools in linguistic research, focusing on the emergence of affective\nmeanings in temporal expressions involving manner-of-motion verbs. While LLMs\nlike GPT-4 have shown promise across a range of tasks, their ability to\nreplicate nuanced human judgements remains under scrutiny. We conducted four\npsycholinguistic studies (on emergent meanings, valence shifts, verb choice in\nemotional contexts, and sentence-emoji associations) first with human\nparticipants and then replicated the same tasks using an LLM. Results across\nall studies show a striking convergence between human and AI responses, with\nstatistical analyses (e.g., Spearman's rho = .73-.96) indicating strong\ncorrelations in both rating patterns and categorical choices. While minor\ndivergences were observed in some cases, these did not alter the overall\ninterpretative outcomes. These findings offer compelling evidence that LLMs can\naugment traditional human-based experimentation, enabling broader-scale studies\nwithout compromising interpretative validity. This convergence not only\nstrengthens the empirical foundation of prior human-based findings but also\nopens possibilities for hypothesis generation and data expansion through AI.\nUltimately, our study supports the use of LLMs as credible and informative\ncollaborators in linguistic inquiry."}
{"id": "2507.09477", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09477", "abs": "https://arxiv.org/abs/2507.09477", "authors": ["Yangning Li", "Weizhi Zhang", "Yuyao Yang", "Wei-Chieh Huang", "Yaozu Wu", "Junyu Luo", "Yuanchen Bei", "Henry Peng Zou", "Xiao Luo", "Yusheng Zhao", "Chunkit Chan", "Yankai Chen", "Zhongfen Deng", "Yinghui Li", "Hai-Tao Zheng", "Dongyuan Li", "Renhe Jiang", "Ming Zhang", "Yangqiu Song", "Philip S. Yu"], "title": "Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs", "comment": "submitted to ARR May", "summary": "Retrieval-Augmented Generation (RAG) lifts the factuality of Large Language\nModels (LLMs) by injecting external knowledge, yet it falls short on problems\nthat demand multi-step inference; conversely, purely reasoning-oriented\napproaches often hallucinate or mis-ground facts. This survey synthesizes both\nstrands under a unified reasoning-retrieval perspective. We first map how\nadvanced reasoning optimizes each stage of RAG (Reasoning-Enhanced RAG). Then,\nwe show how retrieved knowledge of different type supply missing premises and\nexpand context for complex inference (RAG-Enhanced Reasoning). Finally, we\nspotlight emerging Synergized RAG-Reasoning frameworks, where (agentic) LLMs\niteratively interleave search and reasoning to achieve state-of-the-art\nperformance across knowledge-intensive benchmarks. We categorize methods,\ndatasets, and open challenges, and outline research avenues toward deeper\nRAG-Reasoning systems that are more effective, multimodally-adaptive,\ntrustworthy, and human-centric. The collection is available at\nhttps://github.com/DavidZWZ/Awesome-RAG-Reasoning."}
{"id": "2507.10354", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.10354", "abs": "https://arxiv.org/abs/2507.10354", "authors": ["Silvia Cappa", "Anna Sofia Lippolis", "Stefano Zoia"], "title": "Meanings are like Onions: a Layered Approach to Metaphor Processing", "comment": null, "summary": "Metaphorical meaning is not a flat mapping between concepts, but a complex\ncognitive phenomenon that integrates multiple levels of interpretation. In this\npaper, we propose a stratified model of metaphor processing that treats meaning\nas an onion: a multi-layered structure comprising (1) content analysis, (2)\nconceptual blending, and (3) pragmatic intentionality. This three-dimensional\nframework allows for a richer and more cognitively grounded approach to\nmetaphor interpretation in computational systems. At the first level, metaphors\nare annotated through basic conceptual elements. At the second level, we model\nconceptual combinations, linking components to emergent meanings. Finally, at\nthe third level, we introduce a pragmatic vocabulary to capture speaker intent,\ncommunicative function, and contextual effects, aligning metaphor understanding\nwith pragmatic theories. By unifying these layers into a single formal\nframework, our model lays the groundwork for computational methods capable of\nrepresenting metaphorical meaning beyond surface associations, toward deeper,\nmore context-sensitive reasoning."}
{"id": "2507.09482", "categories": ["cs.CL", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.09482", "abs": "https://arxiv.org/abs/2507.09482", "authors": ["Changli Wang", "Rui Wu", "Fang Yin"], "title": "ViSP: A PPO-Driven Framework for Sarcasm Generation with Contrastive Learning", "comment": null, "summary": "Human emotions are complex, with sarcasm being a subtle and distinctive form.\nDespite progress in sarcasm research, sarcasm generation remains underexplored,\nprimarily due to the overreliance on textual modalities and the neglect of\nvisual cues, as well as the mismatch between image content and sarcastic intent\nin existing datasets. In this paper, we introduce M2SaG, a multimodal sarcasm\ngeneration dataset with 4,970 samples, each containing an image, a sarcastic\ntext, and a sarcasm target. To benchmark M2SaG, we propose ViSP, a generation\nframework that integrates Proximal Policy Optimization (PPO) and contrastive\nlearning. PPO utilizes reward scores from DIP to steer the generation of\nsarcastic texts, while contrastive learning encourages the model to favor\noutputs with higher reward scores. These strategies improve overall generation\nquality and produce texts with more pronounced sarcastic intent. We evaluate\nViSP across five metric sets and find it surpasses all baselines, including\nlarge language models, underscoring their limitations in sarcasm generation.\nFurthermore, we analyze the distributions of Sarcasm Scores and Factual\nIncongruity for both M2SaG and the texts generated by ViSP. The generated texts\nexhibit higher mean Sarcasm Scores (0.898 vs. 0.770) and Factual Incongruity\n(0.768 vs. 0.739), demonstrating that ViSP produces higher-quality sarcastic\ncontent than the original dataset. % The dataset and code will be publicly\navailable. Our dataset and code will be released at\n\\textit{https://github.com/wclapply/ViSP}."}
{"id": "2507.10435", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10435", "abs": "https://arxiv.org/abs/2507.10435", "authors": ["Xinnan Dai", "Kai Yang", "Jay Revolinsky", "Kai Guo", "Aoran Wang", "Bohang Zhang", "Jiliang Tang"], "title": "From Sequence to Structure: Uncovering Substructure Reasoning in Transformers", "comment": null, "summary": "Recent studies suggest that large language models (LLMs) possess the\ncapability to solve graph reasoning tasks. Notably, even when graph structures\nare embedded within textual descriptions, LLMs can still effectively answer\nrelated questions. This raises a fundamental question: How can a decoder-only\nTransformer architecture understand underlying graph structures? To address\nthis, we start with the substructure extraction task, interpreting the inner\nmechanisms inside the transformers and analyzing the impact of the input\nqueries. Specifically, through both empirical results and theoretical analysis,\nwe present Induced Substructure Filtration (ISF), a perspective that captures\nthe substructure identification in the multi-layer transformers. We further\nvalidate the ISF process in LLMs, revealing consistent internal dynamics across\nlayers. Building on these insights, we explore the broader capabilities of\nTransformers in handling diverse graph types. Specifically, we introduce the\nconcept of thinking in substructures to efficiently extract complex composite\npatterns, and demonstrate that decoder-only Transformers can successfully\nextract substructures from attributed graphs, such as molecular graphs.\nTogether, our findings offer a new insight on how sequence-based Transformers\nperform the substructure extraction task over graph data."}
{"id": "2507.09601", "categories": ["cs.CL", "cs.AI", "q-fin.CP"], "pdf": "https://arxiv.org/pdf/2507.09601", "abs": "https://arxiv.org/abs/2507.09601", "authors": ["Hanwool Lee", "Sara Yu", "Yewon Hwang", "Jonghyun Choi", "Heejae Ahn", "Sungbum Jung", "Youngjae Yu"], "title": "NMIXX: Domain-Adapted Neural Embeddings for Cross-Lingual eXploration of Finance", "comment": "Under Review", "summary": "General-purpose sentence embedding models often struggle to capture\nspecialized financial semantics, especially in low-resource languages like\nKorean, due to domain-specific jargon, temporal meaning shifts, and misaligned\nbilingual vocabularies. To address these gaps, we introduce NMIXX (Neural\neMbeddings for Cross-lingual eXploration of Finance), a suite of cross-lingual\nembedding models fine-tuned with 18.8K high-confidence triplets that pair\nin-domain paraphrases, hard negatives derived from a semantic-shift typology,\nand exact Korean-English translations. Concurrently, we release KorFinSTS, a\n1,921-pair Korean financial STS benchmark spanning news, disclosures, research\nreports, and regulations, designed to expose nuances that general benchmarks\nmiss.\n  When evaluated against seven open-license baselines, NMIXX's multilingual\nbge-m3 variant achieves Spearman's rho gains of +0.10 on English FinSTS and\n+0.22 on KorFinSTS, outperforming its pre-adaptation checkpoint and surpassing\nother models by the largest margin, while revealing a modest trade-off in\ngeneral STS performance. Our analysis further shows that models with richer\nKorean token coverage adapt more effectively, underscoring the importance of\ntokenizer design in low-resource, cross-lingual settings. By making both models\nand the benchmark publicly available, we provide the community with robust\ntools for domain-adapted, multilingual representation learning in finance."}
{"id": "2507.10445", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10445", "abs": "https://arxiv.org/abs/2507.10445", "authors": ["Chris Madge", "Matthew Purver", "Massimo Poesio"], "title": "Referential ambiguity and clarification requests: comparing human and LLM behaviour", "comment": null, "summary": "In this work we examine LLMs' ability to ask clarification questions in\ntask-oriented dialogues that follow the asynchronous\ninstruction-giver/instruction-follower format. We present a new corpus that\ncombines two existing annotations of the Minecraft Dialogue Corpus -- one for\nreference and ambiguity in reference, and one for SDRT including clarifications\n-- into a single common format providing the necessary information to\nexperiment with clarifications and their relation to ambiguity. With this\ncorpus we compare LLM actions with original human-generated clarification\nquestions, examining how both humans and LLMs act in the case of ambiguity. We\nfind that there is only a weak link between ambiguity and humans producing\nclarification questions in these dialogues, and low correlation between humans\nand LLMs. Humans hardly ever produce clarification questions for referential\nambiguity, but often do so for task-based uncertainty. Conversely, LLMs produce\nmore clarification questions for referential ambiguity, but less so for task\nuncertainty. We question if LLMs' ability to ask clarification questions is\npredicated on their recent ability to simulate reasoning, and test this with\ndifferent reasoning approaches, finding that reasoning does appear to increase\nquestion frequency and relevancy."}
{"id": "2507.09875", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09875", "abs": "https://arxiv.org/abs/2507.09875", "authors": ["Qinyuan Ye", "Robin Jia", "Xiang Ren"], "title": "Function Induction and Task Generalization: An Interpretability Study with Off-by-One Addition", "comment": "Code: https://github.com/INK-USC/function-induction", "summary": "Large language models demonstrate the intriguing ability to perform unseen\ntasks via in-context learning. However, it remains unclear what mechanisms\ninside the model drive such task-level generalization. In this work, we\napproach this question through the lens of off-by-one addition (i.e., 1+1=3,\n2+2=5, 3+3=?), a two-step, counterfactual task with an unexpected +1 function\nas a second step. Leveraging circuit-style interpretability techniques such as\npath patching, we analyze the models' internal computations behind their\nnotable performance and present three key findings. First, we uncover a\nfunction induction mechanism that explains the model's generalization from\nstandard addition to off-by-one addition. This mechanism resembles the\nstructure of the induction head mechanism found in prior work and elevates it\nto a higher level of abstraction. Second, we show that the induction of the +1\nfunction is governed by multiple attention heads in parallel, each of which\nemits a distinct piece of the +1 function. Finally, we find that this function\ninduction mechanism is reused in a broader range of tasks, including synthetic\ntasks such as shifted multiple-choice QA and algorithmic tasks such as base-8\naddition. Overall, our findings offer deeper insights into how reusable and\ncomposable structures within language models enable task-level generalization."}
{"id": "2507.10468", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10468", "abs": "https://arxiv.org/abs/2507.10468", "authors": ["Ariadna Mon", "Sa√∫l Fenollosa", "Jon Lecumberri"], "title": "From BERT to Qwen: Hate Detection across architectures", "comment": "4 pages, 5 figures. EE-559 Deep Learning course project (Group 11)", "summary": "Online platforms struggle to curb hate speech without over-censoring\nlegitimate discourse. Early bidirectional transformer encoders made big\nstrides, but the arrival of ultra-large autoregressive LLMs promises deeper\ncontext-awareness. Whether this extra scale actually improves practical\nhate-speech detection on real-world text remains unverified. Our study puts\nthis question to the test by benchmarking both model families, classic encoders\nand next-generation LLMs, on curated corpora of online interactions for\nhate-speech detection (Hate or No Hate)."}
{"id": "2507.09935", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09935", "abs": "https://arxiv.org/abs/2507.09935", "authors": ["Hai Toan Nguyen", "Tien Dat Nguyen", "Viet Ha Nguyen"], "title": "Enhancing Retrieval Augmented Generation with Hierarchical Text Segmentation Chunking", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) systems commonly use chunking strategies\nfor retrieval, which enhance large language models (LLMs) by enabling them to\naccess external knowledge, ensuring that the retrieved information is\nup-to-date and domain-specific. However, traditional methods often fail to\ncreate chunks that capture sufficient semantic meaning, as they do not account\nfor the underlying textual structure. This paper proposes a novel framework\nthat enhances RAG by integrating hierarchical text segmentation and clustering\nto generate more meaningful and semantically coherent chunks. During inference,\nthe framework retrieves information by leveraging both segment-level and\ncluster-level vector representations, thereby increasing the likelihood of\nretrieving more precise and contextually relevant information. Evaluations on\nthe NarrativeQA, QuALITY, and QASPER datasets indicate that the proposed method\nachieved improved results compared to traditional chunking techniques."}
{"id": "2507.10472", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.10472", "abs": "https://arxiv.org/abs/2507.10472", "authors": ["Mohamed T. Younes", "Omar Walid", "Mai Hassan", "Ali Hamdi"], "title": "MLAR: Multi-layer Large Language Model-based Robotic Process Automation Applicant Tracking", "comment": null, "summary": "This paper introduces an innovative Applicant Tracking System (ATS) enhanced\nby a novel Robotic process automation (RPA) framework or as further referred to\nas MLAR. Traditional recruitment processes often encounter bottlenecks in\nresume screening and candidate shortlisting due to time and resource\nconstraints. MLAR addresses these challenges employing Large Language Models\n(LLMs) in three distinct layers: extracting key characteristics from job\npostings in the first layer, parsing applicant resume to identify education,\nexperience, skills in the second layer, and similarity matching in the third\nlayer. These features are then matched through advanced semantic algorithms to\nidentify the best candidates efficiently. Our approach integrates seamlessly\ninto existing RPA pipelines, automating resume parsing, job matching, and\ncandidate notifications. Extensive performance benchmarking shows that MLAR\noutperforms the leading RPA platforms, including UiPath and Automation\nAnywhere, in high-volume resume-processing tasks. When processing 2,400\nresumes, MLAR achieved an average processing time of 5.4 seconds per resume,\nreducing processing time by approximately 16.9% compared to Automation Anywhere\nand 17.1% compared to UiPath. These results highlight the potential of MLAR to\ntransform recruitment workflows by providing an efficient, accurate, and\nscalable solution tailored to modern hiring needs."}
{"id": "2507.09973", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09973", "abs": "https://arxiv.org/abs/2507.09973", "authors": ["Sarah Pan"], "title": "Tiny Reward Models", "comment": "2025 ICML Efficient Systems for Foundation Models Workshop", "summary": "Large decoder-based language models have become the dominant architecture for\nreward modeling in reinforcement learning from human feedback (RLHF). However,\nas reward models are increasingly deployed in test-time strategies, their\ninference costs become a growing concern. We present TinyRM, a family of small,\nbidirectional masked language models (MLMs) with as few as 400 million\nparameters, that rival the capabilities of models over 175 times larger on\nreasoning and safety preference modeling tasks. TinyRM combines FLAN-style\nprompting, Directional Low-Rank Adaptation (DoRA), and layer freezing to\nachieve strong performance on RewardBench, despite using significantly fewer\nresources. Our experiments suggest that small models benefit from\ndomain-specific tuning strategies, particularly in reasoning, where lightweight\nfinetuning methods are especially effective. While challenges remain in\nbuilding generalist models and conversational preference modeling, our\npreliminary results highlight the promise of lightweight bidirectional\narchitectures as efficient, scalable alternatives for preference modeling."}
{"id": "2507.10475", "categories": ["cs.CL", "cs.AI", "I.2.7; H.3.3"], "pdf": "https://arxiv.org/pdf/2507.10475", "abs": "https://arxiv.org/abs/2507.10475", "authors": ["ƒ∞smail Tarƒ±m", "Aytuƒü Onan"], "title": "Can You Detect the Difference?", "comment": "11 pages, 3 figures, 2 tables. Code and data:\n  https://github.com/ismailtrm/ceng_404. Cross-list requested to cs.AI for\n  AI-safety relevance", "summary": "The rapid advancement of large language models (LLMs) has raised concerns\nabout reliably detecting AI-generated text. Stylometric metrics work well on\nautoregressive (AR) outputs, but their effectiveness on diffusion-based models\nis unknown. We present the first systematic comparison of diffusion-generated\ntext (LLaDA) and AR-generated text (LLaMA) using 2 000 samples. Perplexity,\nburstiness, lexical diversity, readability, and BLEU/ROUGE scores show that\nLLaDA closely mimics human text in perplexity and burstiness, yielding high\nfalse-negative rates for AR-oriented detectors. LLaMA shows much lower\nperplexity but reduced lexical fidelity. Relying on any single metric fails to\nseparate diffusion outputs from human writing. We highlight the need for\ndiffusion-aware detectors and outline directions such as hybrid models,\ndiffusion-specific stylometric signatures, and robust watermarking."}
{"id": "2507.10073", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10073", "abs": "https://arxiv.org/abs/2507.10073", "authors": ["Simon M√ºnker"], "title": "Cultural Bias in Large Language Models: Evaluating AI Agents through Moral Questionnaires", "comment": "15pages, 1 figure, 2 tables", "summary": "Are AI systems truly representing human values, or merely averaging across\nthem? Our study suggests a concerning reality: Large Language Models (LLMs)\nfail to represent diverse cultural moral frameworks despite their linguistic\ncapabilities. We expose significant gaps between AI-generated and human moral\nintuitions by applying the Moral Foundations Questionnaire across 19 cultural\ncontexts. Comparing multiple state-of-the-art LLMs' origins against human\nbaseline data, we find these models systematically homogenize moral diversity.\nSurprisingly, increased model size doesn't consistently improve cultural\nrepresentation fidelity. Our findings challenge the growing use of LLMs as\nsynthetic populations in social science research and highlight a fundamental\nlimitation in current AI alignment approaches. Without data-driven alignment\nbeyond prompting, these systems cannot capture the nuanced, culturally-specific\nmoral intuitions. Our results call for more grounded alignment objectives and\nevaluation metrics to ensure AI systems represent diverse human values rather\nthan flattening the moral landscape."}
{"id": "2507.10524", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10524", "abs": "https://arxiv.org/abs/2507.10524", "authors": ["Sangmin Bae", "Yujin Kim", "Reza Bayat", "Sungnyun Kim", "Jiyoun Ha", "Tal Schuster", "Adam Fisch", "Hrayr Harutyunyan", "Ziwei Ji", "Aaron Courville", "Se-Young Yun"], "title": "Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation", "comment": "36 pages, 9 figures, 14 tables, codes at\n  https://github.com/raymin0223/mixture_of_recursions", "summary": "Scaling language models unlocks impressive capabilities, but the accompanying\ncomputational and memory demands make both training and deployment expensive.\nExisting efficiency efforts typically target either parameter sharing or\nadaptive computation, leaving open the question of how to attain both\nsimultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework\nthat combines the two axes of efficiency inside a single Recursive Transformer.\nMoR reuses a shared stack of layers across recursion steps to achieve parameter\nefficiency, while lightweight routers enable adaptive token-level thinking by\ndynamically assigning different recursion depths to individual tokens. This\nallows MoR to focus quadratic attention computation only among tokens still\nactive at a given recursion depth, further improving memory access efficiency\nby selectively caching only their key-value pairs. Beyond these core\nmechanisms, we also propose a KV sharing variant that reuses KV pairs from the\nfirst recursion, specifically designed to decrease prefill latency and memory\nfootprint. Across model scales ranging from 135M to 1.7B parameters, MoR forms\na new Pareto frontier: at equal training FLOPs and smaller model sizes, it\nsignificantly lowers validation perplexity and improves few-shot accuracy,\nwhile delivering higher throughput compared with vanilla and existing recursive\nbaselines. These gains demonstrate that MoR is an effective path towards\nlarge-model quality without incurring large-model cost."}
{"id": "2507.10085", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10085", "abs": "https://arxiv.org/abs/2507.10085", "authors": ["Chenxi Huang", "Shaotian Yan", "Liang Xie", "Binbin Lin", "Sinan Fan", "Yue Xin", "Deng Cai", "Chen Shen", "Jieping Ye"], "title": "Enhancing Chain-of-Thought Reasoning with Critical Representation Fine-tuning", "comment": "Accepted by ACL 2025", "summary": "Representation Fine-tuning (ReFT), a recently proposed Parameter-Efficient\nFine-Tuning (PEFT) method, has attracted widespread attention for significantly\nimproving parameter efficiency by editing representation space alone. In this\nwork, we investigate applying ReFT to complex reasoning tasks. However,\ndirectly using the native ReFT method, which modifies fixed representations at\nthe beginning and end of each layer, yields suboptimal performance, as these\nfixed-position representations have uncertain impact on the outputs. We observe\nthat, in complex reasoning tasks, there often exist certain critical\nrepresentations. These representations either integrate significant information\nfrom preceding layers or regulate subsequent layer representations. Through\nlayer-by-layer propagation, they exert a substantial influence on the final\noutput. Naturally, fine-tuning these critical representations has the potential\nto greatly enhance reasoning performance. Building upon these insights, we\npropose Critical Representation Fine-Tuning (CRFT), a novel method that\nidentifies and optimizes these critical representations through information\nflow analysis. CRFT operates within a supervised learning framework,\ndynamically optimizing critical representations in a low-rank linear subspace\nwhile freezing the base model. The effectiveness and efficiency of our method\nare validated across eight benchmarks for arithmetic and commonsense reasoning,\nusing LLaMA and Mistral model families. Furthermore, our method also adapts\neffectively to few-shot settings, boosting one-shot accuracy by 16.4%. Our work\nhighlights the untapped potential of representation-level optimization for CoT\nreasoning, offering a lightweight yet powerful alternative to traditional PEFT\nmethods."}
{"id": "2507.10535", "categories": ["cs.CL", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.10535", "abs": "https://arxiv.org/abs/2507.10535", "authors": ["Hongchao Jiang", "Yiming Chen", "Yushi Cao", "Hung-yi Lee", "Robby T. Tan"], "title": "CodeJudgeBench: Benchmarking LLM-as-a-Judge for Coding Tasks", "comment": "Dataset is available at\n  https://huggingface.co/datasets/mattymchen/codejudgebench", "summary": "Large Language Models (LLMs) have significantly advanced the state-of-the-art\nin various coding tasks. Beyond directly answering user queries, LLMs can also\nserve as judges, assessing and comparing the quality of responses generated by\nother models. Such an evaluation capability is crucial both for benchmarking\ndifferent LLMs and for improving response quality through response ranking.\nHowever, despite the growing adoption of the LLM-as-a-Judge paradigm, its\neffectiveness in coding scenarios remains underexplored due to the absence of\ndedicated benchmarks. To address this gap, we introduce CodeJudgeBench, a\nbenchmark explicitly designed to evaluate the performance of LLM-as-a-Judge\nmodels across three critical coding tasks: code generation, code repair, and\nunit test generation. Through comprehensive benchmarking of 26 LLM-as-a-Judge\nmodels, we find that recent thinking models significantly outperform\nnon-thinking models on our carefully designed code judging tasks. Notably, even\nrelatively small thinking models, such as Qwen3-8B, can outperform specially\ntrained LLM-as-a-Judge models up to 70B in size. Nevertheless, all models still\nexhibit significant randomness in their judgment of coding tasks. For pairwise\njudging tasks, simply changing the order in which responses are presented can\nsubstantially impact accuracy. In addition, when judging code and unit tests\nwritten by different LLMs, LLM-as-a-Judge models also show variance in\nperformance. This sensitivity raises concerns about the reliability and\nconsistency of LLM-as-a-Judge in coding scenarios. Lastly, we study optimal\nprompting strategies for LLM-as-a-Judge. We find that using pair-wise\ncomparison outperforms scalar point-wise judging. Furthermore, retaining\ncomments and reasoning in the full, unprocessed LLM response leads to improved\njudge performance."}
{"id": "2507.10177", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10177", "abs": "https://arxiv.org/abs/2507.10177", "authors": ["Rohitash Chandra", "Jiyong Choi"], "title": "Abusive text transformation using LLMs", "comment": null, "summary": "Although Large Language Models (LLMs) have demonstrated significant\nadvancements in natural language processing tasks, their effectiveness in the\nclassification and transformation of abusive text into non-abusive versions\nremains an area for exploration. In this study, we aim to use LLMs to transform\nabusive text (tweets and reviews) featuring hate speech and swear words into\nnon-abusive text, while retaining the intent of the text. We evaluate the\nperformance of two state-of-the-art LLMs, such as Gemini, GPT-4o, DeekSeek and\nGroq, on their ability to identify abusive text. We them to transform and\nobtain a text that is clean from abusive and inappropriate content but\nmaintains a similar level of sentiment and semantics, i.e. the transformed text\nneeds to maintain its message. Afterwards, we evaluate the raw and transformed\ndatasets with sentiment analysis and semantic analysis. Our results show Groq\nprovides vastly different results when compared with other LLMs. We have\nidentified similarities between GPT-4o and DeepSeek-V3."}
{"id": "2507.10541", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.10541", "abs": "https://arxiv.org/abs/2507.10541", "authors": ["Zhuoshi Pan", "Qizhi Pei", "Yu Li", "Qiyao Sun", "Zinan Tang", "H. Vicky Zhao", "Conghui He", "Lijun Wu"], "title": "REST: Stress Testing Large Reasoning Models by Asking Multiple Problems at Once", "comment": "REST (Reasoning Evaluation through Simultaneous Testing), a\n  stress-testing framework that concurrently exposes LRMs to multiple problems\n  simultaneously", "summary": "Recent Large Reasoning Models (LRMs) have achieved remarkable progress on\ntask-specific benchmarks, yet their evaluation methods remain constrained by\nisolated problem-solving paradigms. Existing benchmarks predominantly assess\nsingle-question reasoning through sequential testing, resulting critical\nlimitations: (1) vulnerability to data contamination and less challenging\n(e.g., DeepSeek-R1 achieves 97.0% on MATH500), forcing costly and perpetual\ncreation of new questions with large human efforts, (2) failure to evaluate\nmodels under multi-context pressure, a key requirement for real-world\ndeployment. To bridge this gap, we present REST (Reasoning Evaluation through\nSimultaneous Testing), a stress-testing framework that concurrently exposes\nLRMs to multiple problems simultaneously. Beyond basic reasoning, REST\nspecifically evaluates several under-tested capabilities: contextual priority\nallocation, cross-problem interference resistance, and dynamic cognitive load\nmanagement. Our evaluation reveals several striking findings: Even\nstate-of-the-art (SOTA) models like DeepSeek-R1 exhibit substantial performance\ndegradation under stress testing. Crucially, REST demonstrates stronger\ndiscriminative power than existing benchmarks, revealing pronounced performance\ndifferences among models that exhibit similar, near-ceiling performance under\nsingle-question evaluations. Some key mechanistic insights emerge from our\nanalysis: (1) the \"overthinking trap\" is a critical factor contributing to the\nperformance degradation; (2) the models trained with \"long2short\" technique\npreserve more accuracy of their single-problem performance under REST,\noutperforming standard-trained counterparts. These results establish REST as a\ncost-efficient, future-proof evaluation paradigm that better reflects\nreal-world reasoning demands while reducing reliance on continuous human\nannotation."}
{"id": "2507.10216", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10216", "abs": "https://arxiv.org/abs/2507.10216", "authors": ["Renad Al-Monef", "Hassan Alhuzali", "Nora Alturayeif", "Ashwag Alasmari"], "title": "Absher: A Benchmark for Evaluating Large Language Models Understanding of Saudi Dialects", "comment": null, "summary": "As large language models (LLMs) become increasingly central to Arabic NLP\napplications, evaluating their understanding of regional dialects and cultural\nnuances is essential, particularly in linguistically diverse settings like\nSaudi Arabia. This paper introduces \\texttt{Absher}, a comprehensive benchmark\nspecifically designed to assess LLMs performance across major Saudi dialects.\n\\texttt{Absher} comprises over 18,000 multiple-choice questions spanning six\ndistinct categories: Meaning, True/False, Fill-in-the-Blank, Contextual Usage,\nCultural Interpretation, and Location Recognition. These questions are derived\nfrom a curated dataset of dialectal words, phrases, and proverbs sourced from\nvarious regions of Saudi Arabia. We evaluate several state-of-the-art LLMs,\nincluding multilingual and Arabic-specific models. We also provide detailed\ninsights into their capabilities and limitations. Our results reveal notable\nperformance gaps, particularly in tasks requiring cultural inference or\ncontextual understanding. Our findings highlight the urgent need for\ndialect-aware training and culturally aligned evaluation methodologies to\nimprove LLMs performance in real-world Arabic applications."}
{"id": "2507.08806", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.08806", "abs": "https://arxiv.org/abs/2507.08806", "authors": ["Daewon Choi", "Jimin Lee", "Jihoon Tack", "Woomin Song", "Saket Dingliwal", "Sai Muralidhar Jayanthi", "Bhavana Ganesh", "Jinwoo Shin", "Aram Galstyan", "Sravan Babu Bodapati"], "title": "Think Clearly: Improving Reasoning via Redundant Token Pruning", "comment": null, "summary": "Recent large language models have shown promising capabilities in long-form\nreasoning, following structured chains of thought before arriving at a final\nanswer. However, we observe that these reasoning paths tend to include\nsubstantial redundancy; analyzing attention patterns reveals that attention\nscores are widely scattered, particularly incorrect answers exhibit greater\nattention sparsity. In this paper, we demonstrate that deliberately removing\nthis redundancy in the reasoning process significantly improves performance\nthrough clear thinking, i.e., removing distraction. Specifically, we\nsystematically identify reasoning redundancy by measuring token-level attention\nscores to a special end-of-thinking token, which is appended to an explicit\ninstruction inserted to conclude each intermediate reasoning step. Furthermore,\nwe propose structure-aware pruning that prioritizes removing tokens in\nlow-contributing reasoning chunks over individual tokens. After evicting\nredundant tokens, we remove the injected end-of-thinking instruction, then\nresume the reasoning generation. We demonstrate that our method significantly\nimproves overall accuracy across reasoning-intensive benchmarks without any\ntraining involved. In particular, our method shows strong performance on\nchallenging mathematical competition benchmarks such as AIME and AMC, where\nreasoning redundancy is more prevalent."}
{"id": "2507.10435", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10435", "abs": "https://arxiv.org/abs/2507.10435", "authors": ["Xinnan Dai", "Kai Yang", "Jay Revolinsky", "Kai Guo", "Aoran Wang", "Bohang Zhang", "Jiliang Tang"], "title": "From Sequence to Structure: Uncovering Substructure Reasoning in Transformers", "comment": null, "summary": "Recent studies suggest that large language models (LLMs) possess the\ncapability to solve graph reasoning tasks. Notably, even when graph structures\nare embedded within textual descriptions, LLMs can still effectively answer\nrelated questions. This raises a fundamental question: How can a decoder-only\nTransformer architecture understand underlying graph structures? To address\nthis, we start with the substructure extraction task, interpreting the inner\nmechanisms inside the transformers and analyzing the impact of the input\nqueries. Specifically, through both empirical results and theoretical analysis,\nwe present Induced Substructure Filtration (ISF), a perspective that captures\nthe substructure identification in the multi-layer transformers. We further\nvalidate the ISF process in LLMs, revealing consistent internal dynamics across\nlayers. Building on these insights, we explore the broader capabilities of\nTransformers in handling diverse graph types. Specifically, we introduce the\nconcept of thinking in substructures to efficiently extract complex composite\npatterns, and demonstrate that decoder-only Transformers can successfully\nextract substructures from attributed graphs, such as molecular graphs.\nTogether, our findings offer a new insight on how sequence-based Transformers\nperform the substructure extraction task over graph data."}
{"id": "2507.09662", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09662", "abs": "https://arxiv.org/abs/2507.09662", "authors": ["Jason Zhu", "Hongyu Li"], "title": "Towards Concise and Adaptive Thinking in Large Reasoning Models: A Survey", "comment": null, "summary": "Large reasoning models (LRMs) like OpenAI o1 and DeepSeek R1 have\ndemonstrated impressive performance on complex reasoning tasks like mathematics\nand programming with long Chain-of-Thought (CoT) reasoning sequences\n(slow-thinking), compared with traditional large language models\n(fast-thinking). However, these reasoning models also face a huge challenge\nthat generating unnecessarily lengthy and redundant reasoning chains even for\ntrivial questions. This phenomenon leads to a significant waste of inference\nresources, increases the response time for simple queries, and hinders the\npractical application of LRMs in real-world products. To this end, it is\ncrucial to shorten lengthy reasoning chains and learn adaptive reasoning\nbetween fast and slow thinking based on input difficulty. In this survey, we\nprovide a comprehensive overview of recent progress in concise and adaptive\nthinking for efficient reasoning of LRMs, including methodologies, benchmarks,\nand challenges for future exploration. We hope this survey can help researchers\nquickly understand the landscape of this field and inspire novel adaptive\nthinking ideas to facilitate better usage of LRMs."}
{"id": "2507.10445", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10445", "abs": "https://arxiv.org/abs/2507.10445", "authors": ["Chris Madge", "Matthew Purver", "Massimo Poesio"], "title": "Referential ambiguity and clarification requests: comparing human and LLM behaviour", "comment": null, "summary": "In this work we examine LLMs' ability to ask clarification questions in\ntask-oriented dialogues that follow the asynchronous\ninstruction-giver/instruction-follower format. We present a new corpus that\ncombines two existing annotations of the Minecraft Dialogue Corpus -- one for\nreference and ambiguity in reference, and one for SDRT including clarifications\n-- into a single common format providing the necessary information to\nexperiment with clarifications and their relation to ambiguity. With this\ncorpus we compare LLM actions with original human-generated clarification\nquestions, examining how both humans and LLMs act in the case of ambiguity. We\nfind that there is only a weak link between ambiguity and humans producing\nclarification questions in these dialogues, and low correlation between humans\nand LLMs. Humans hardly ever produce clarification questions for referential\nambiguity, but often do so for task-based uncertainty. Conversely, LLMs produce\nmore clarification questions for referential ambiguity, but less so for task\nuncertainty. We question if LLMs' ability to ask clarification questions is\npredicated on their recent ability to simulate reasoning, and test this with\ndifferent reasoning approaches, finding that reasoning does appear to increase\nquestion frequency and relevancy."}
{"id": "2507.09751", "categories": ["cs.AI", "cs.CL", "cs.LO"], "pdf": "https://arxiv.org/pdf/2507.09751", "abs": "https://arxiv.org/abs/2507.09751", "authors": ["Bradley P. Allen", "Prateek Chhikara", "Thomas Macaulay Ferguson", "Filip Ilievski", "Paul Groth"], "title": "Sound and Complete Neuro-symbolic Reasoning with LLM-Grounded Interpretations", "comment": "29 pages, 9 tables, 3 figures. Accepted to the 19th Conference on\n  Neurosymbolic Learning and Reasoning (NeSy 2025)", "summary": "Large language models (LLMs) have demonstrated impressive capabilities in\nnatural language understanding and generation, but they exhibit problems with\nlogical consistency in the output they generate. How can we harness LLMs'\nbroad-coverage parametric knowledge in formal reasoning despite their\ninconsistency? We present a method for directly integrating an LLM into the\ninterpretation function of the formal semantics for a paraconsistent logic. We\nprovide experimental evidence for the feasibility of the method by evaluating\nthe function using datasets created from several short-form factuality\nbenchmarks. Unlike prior work, our method offers a theoretical framework for\nneuro-symbolic reasoning that leverages an LLM's knowledge while preserving the\nunderlying logic's soundness and completeness properties."}
{"id": "2507.10475", "categories": ["cs.CL", "cs.AI", "I.2.7; H.3.3"], "pdf": "https://arxiv.org/pdf/2507.10475", "abs": "https://arxiv.org/abs/2507.10475", "authors": ["ƒ∞smail Tarƒ±m", "Aytuƒü Onan"], "title": "Can You Detect the Difference?", "comment": "11 pages, 3 figures, 2 tables. Code and data:\n  https://github.com/ismailtrm/ceng_404. Cross-list requested to cs.AI for\n  AI-safety relevance", "summary": "The rapid advancement of large language models (LLMs) has raised concerns\nabout reliably detecting AI-generated text. Stylometric metrics work well on\nautoregressive (AR) outputs, but their effectiveness on diffusion-based models\nis unknown. We present the first systematic comparison of diffusion-generated\ntext (LLaDA) and AR-generated text (LLaMA) using 2 000 samples. Perplexity,\nburstiness, lexical diversity, readability, and BLEU/ROUGE scores show that\nLLaDA closely mimics human text in perplexity and burstiness, yielding high\nfalse-negative rates for AR-oriented detectors. LLaMA shows much lower\nperplexity but reduced lexical fidelity. Relying on any single metric fails to\nseparate diffusion outputs from human writing. We highlight the need for\ndiffusion-aware detectors and outline directions such as hybrid models,\ndiffusion-specific stylometric signatures, and robust watermarking."}
{"id": "2507.10000", "categories": ["cs.AI", "cs.CL", "I.2.11; F.4.1; I.2.4; G.2.2"], "pdf": "https://arxiv.org/pdf/2507.10000", "abs": "https://arxiv.org/abs/2507.10000", "authors": ["Mark Burgess"], "title": "On The Role of Intentionality in Knowledge Representation: Analyzing Scene Context for Cognitive Agents with a Tiny Language Model", "comment": null, "summary": "Since Searle's work deconstructing intent and intentionality in the realm of\nphilosophy, the practical meaning of intent has received little attention in\nscience and technology. Intentionality and context are both central to the\nscope of Promise Theory's model of Semantic Spacetime, used as an effective\nTiny Language Model. One can identify themes and concepts from a text, on a low\nlevel (without knowledge of the specific language) by using process coherence\nas a guide. Any agent process can assess superficially a degree of latent\n`intentionality' in data by looking for anomalous multi-scale anomalies and\nassessing the work done to form them. Scale separation can be used to sort\nparts into `intended' content and `ambient context', using the spacetime\ncoherence as a measure. This offers an elementary but pragmatic interpretation\nof latent intentionality for very low computational cost, and without reference\nto extensive training or reasoning capabilities. The process is well within the\nreach of basic organisms as it does not require large scale artificial\nprobabilistic batch processing. The level of concept formation depends,\nhowever, on the memory capacity of the agent."}
{"id": "2507.10535", "categories": ["cs.CL", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.10535", "abs": "https://arxiv.org/abs/2507.10535", "authors": ["Hongchao Jiang", "Yiming Chen", "Yushi Cao", "Hung-yi Lee", "Robby T. Tan"], "title": "CodeJudgeBench: Benchmarking LLM-as-a-Judge for Coding Tasks", "comment": "Dataset is available at\n  https://huggingface.co/datasets/mattymchen/codejudgebench", "summary": "Large Language Models (LLMs) have significantly advanced the state-of-the-art\nin various coding tasks. Beyond directly answering user queries, LLMs can also\nserve as judges, assessing and comparing the quality of responses generated by\nother models. Such an evaluation capability is crucial both for benchmarking\ndifferent LLMs and for improving response quality through response ranking.\nHowever, despite the growing adoption of the LLM-as-a-Judge paradigm, its\neffectiveness in coding scenarios remains underexplored due to the absence of\ndedicated benchmarks. To address this gap, we introduce CodeJudgeBench, a\nbenchmark explicitly designed to evaluate the performance of LLM-as-a-Judge\nmodels across three critical coding tasks: code generation, code repair, and\nunit test generation. Through comprehensive benchmarking of 26 LLM-as-a-Judge\nmodels, we find that recent thinking models significantly outperform\nnon-thinking models on our carefully designed code judging tasks. Notably, even\nrelatively small thinking models, such as Qwen3-8B, can outperform specially\ntrained LLM-as-a-Judge models up to 70B in size. Nevertheless, all models still\nexhibit significant randomness in their judgment of coding tasks. For pairwise\njudging tasks, simply changing the order in which responses are presented can\nsubstantially impact accuracy. In addition, when judging code and unit tests\nwritten by different LLMs, LLM-as-a-Judge models also show variance in\nperformance. This sensitivity raises concerns about the reliability and\nconsistency of LLM-as-a-Judge in coding scenarios. Lastly, we study optimal\nprompting strategies for LLM-as-a-Judge. We find that using pair-wise\ncomparison outperforms scalar point-wise judging. Furthermore, retaining\ncomments and reasoning in the full, unprocessed LLM response leads to improved\njudge performance."}
{"id": "2507.10045", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.10045", "abs": "https://arxiv.org/abs/2507.10045", "authors": ["Malte Christian Bartels", "Debayan Banerjee", "Ricardo Usbeck"], "title": "Automating SPARQL Query Translations between DBpedia and Wikidata", "comment": "18 pages, 2 figues. Paper accepted at SEMANTiCS 2025 conference\n  happening on September 2025", "summary": "This paper investigates whether state-of-the-art Large Language Models (LLMs)\ncan automatically translate SPARQL between popular Knowledge Graph (KG)\nschemas. We focus on translations between the DBpedia and Wikidata KG, and\nlater on DBLP and OpenAlex KG. This study addresses a notable gap in KG\ninteroperability research by rigorously evaluating LLM performance on\nSPARQL-to-SPARQL translation. Two benchmarks are assembled, where the first\nalign 100 DBpedia-Wikidata queries from QALD-9-Plus; the second contains 100\nDBLP queries aligned to OpenAlex, testing generalizability beyond encyclopaedic\nKGs. Three open LLMs: Llama-3-8B, DeepSeek-R1-Distill-Llama-70B, and\nMistral-Large-Instruct-2407 are selected based on their sizes and architectures\nand tested with zero-shot, few-shot, and two chain-of-thought variants. Outputs\nwere compared with gold answers, and resulting errors were categorized. We find\nthat the performance varies markedly across models and prompting strategies,\nand that translations for Wikidata to DBpedia work far better than translations\nfor DBpedia to Wikidata."}
{"id": "2507.10522", "categories": ["cs.AI", "cs.CL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.10522", "abs": "https://arxiv.org/abs/2507.10522", "authors": ["Jennifer D'Souza", "Endres Keno Sander", "Andrei Aioanei"], "title": "DeepResearch$^{\\text{Eco}}$: A Recursive Agentic Workflow for Complex Scientific Question Answering in Ecology", "comment": "12 pages, 3 figures", "summary": "We introduce DeepResearch$^{\\text{Eco}}$, a novel agentic LLM-based system\nfor automated scientific synthesis that supports recursive, depth- and\nbreadth-controlled exploration of original research questions -- enhancing\nsearch diversity and nuance in the retrieval of relevant scientific literature.\nUnlike conventional retrieval-augmented generation pipelines, DeepResearch\nenables user-controllable synthesis with transparent reasoning and\nparameter-driven configurability, facilitating high-throughput integration of\ndomain-specific evidence while maintaining analytical rigor. Applied to 49\necological research questions, DeepResearch achieves up to a 21-fold increase\nin source integration and a 14.9-fold rise in sources integrated per 1,000\nwords. High-parameter settings yield expert-level analytical depth and\ncontextual diversity.\n  Source code available at: https://github.com/sciknoworg/deep-research."}
