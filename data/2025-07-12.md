<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 11]
- [cs.CL](#cs.CL) [Total: 9]
- [cs.CE](#cs.CE) [Total: 2]
- [cs.CY](#cs.CY) [Total: 7]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Autonomous Control Leveraging LLMs: An Agentic Framework for Next-Generation Industrial Automation](https://arxiv.org/abs/2507.07115)
*Javal Vyas,Mehmet Mercangoz*

Main category: cs.AI

TL;DR: 本文提出一种基于LLM和有限状态机的智能体架构，实现了从故障规划到过程控制的统一，在化工自动化场景中获得了理想的效果，并分析了系统的关键优势与不足。


<details>
  <summary>Details</summary>
Motivation: 现代化学过程日益复杂，加上劳动力短缺和复杂故障情景，现有自动化方法难以有效应对，需要融合符号推理与自适应控制的新型自动化范式。

Method: 提出一种统一的智能体框架，将大语言模型（LLM）用于离散故障恢复规划和连续过程控制。采用有限状态机（FSM）作为可解释的操作包络，LLM驱动的规划智能体提出故障恢复序列，仿真智能体执行并检查每一步转移，验证-重新提示（Validator-Reprompting）循环迭代优化无效方案。系统在多种FSM和物理/数字双平台（TCLab）实验中测试。

Result: 在案例1中，GPT-4o及mini版本在180个不同规模FSM（4-25个状态，4-300个转移）的规划任务中，在五次重新提示内100%生成有效路径，准确率和延迟均优于开源LLM。在案例2中，框架在TCLab上调节双加热器，应对持续非对称扰动，控制效果与传统PID方法相当。同时，提示循环对于应对系统非线性动态至关重要。分析了主要失效模式如指令遵循失败和常微分方程近似问题。

Conclusion: 结构化反馈和模块化智能体能使LLM实现高层符号规划与低层连续控制的统一，为化学工程领域语言驱动的智能自动化奠定基础。

Abstract: The increasing complexity of modern chemical processes, coupled with
workforce shortages and intricate fault scenarios, demands novel automation
paradigms that blend symbolic reasoning with adaptive control. In this work, we
introduce a unified agentic framework that leverages large language models
(LLMs) for both discrete fault-recovery planning and continuous process control
within a single architecture. We adopt Finite State Machines (FSMs) as
interpretable operating envelopes: an LLM-driven planning agent proposes
recovery sequences through the FSM, a Simulation Agent executes and checks each
transition, and a Validator-Reprompting loop iteratively refines invalid plans.
In Case Study 1, across 180 randomly generated FSMs of varying sizes (4-25
states, 4-300 transitions), GPT-4o and GPT-4o-mini achieve 100% valid-path
success within five reprompts-outperforming open-source LLMs in both accuracy
and latency. In Case Study 2, the same framework modulates dual-heater inputs
on a laboratory TCLab platform (and its digital twin) to maintain a target
average temperature under persistent asymmetric disturbances. Compared to
classical PID control, our LLM-based controller attains similar performance,
while ablation of the prompting loop reveals its critical role in handling
nonlinear dynamics. We analyze key failure modes-such as instruction following
lapses and coarse ODE approximations. Our results demonstrate that, with
structured feedback and modular agents, LLMs can unify high-level symbolic
planningand low-level continuous control, paving the way towards resilient,
language-driven automation in chemical engineering.

</details>


### [2] [BOOST: Out-of-Distribution-Informed Adaptive Sampling for Bias Mitigation in Stylistic Convolutional Neural Networks](https://arxiv.org/abs/2507.07134)
*Mridula Vijendran,Shuang Chen,Jingjing Deng,Hubert P. H. Shum*

Main category: cs.AI

TL;DR: AI绘画分类常因数据偏见导致少数类别识别差。本文提出BOOST方法，通过智能采样和温度调节缓解偏见，并引入SODC指标专门评估偏见消除表现，在多个数据集验证有效，兼顾高性能与公平性。


<details>
  <summary>Details</summary>
Motivation: AI偏见严重影响绘画分类的公平性与准确率，尤其在少数类别下，传统提升分类性能的研究很少关注如何缓解源自数据分布不均的偏见，尤其是在分布外数据情景下迫切需要更有效的偏差缓解方案。

Method: 提出了基于OOD（分布外数据）信息的模型偏差自适应采样方法BOOST，通过动态调整温度缩放和采样概率，使各类别获得更公平的训练。并引入新的评估指标SODC来衡量类别区分和偏见减少。

Result: 在KaoKore和PACS数据集上的实验表明，BOOST方法在保持高分类性能的同时显著降低了类别性偏见，实现类均衡表现。提出的新指标SODC验证了分布内外类别区分与偏见缓解的能力。

Conclusion: 本文提出的BOOST方法能有效缓解绘画分类中因数据集不均引起的模型偏见，兼顾模型性能及公平性，是艺术领域去偏见AI模型的强有力方案。

Abstract: The pervasive issue of bias in AI presents a significant challenge to
painting classification, and is getting more serious as these systems become
increasingly integrated into tasks like art curation and restoration. Biases,
often arising from imbalanced datasets where certain artistic styles dominate,
compromise the fairness and accuracy of model predictions, i.e., classifiers
are less accurate on rarely seen paintings. While prior research has made
strides in improving classification performance, it has largely overlooked the
critical need to address these underlying biases, that is, when dealing with
out-of-distribution (OOD) data. Our insight highlights the necessity of a more
robust approach to bias mitigation in AI models for art classification on
biased training data. We propose a novel OOD-informed model bias adaptive
sampling method called BOOST (Bias-Oriented OOD Sampling and Tuning). It
addresses these challenges by dynamically adjusting temperature scaling and
sampling probabilities, thereby promoting a more equitable representation of
all classes. We evaluate our proposed approach to the KaoKore and PACS
datasets, focusing on the model's ability to reduce class-wise bias. We further
propose a new metric, Same-Dataset OOD Detection Score (SODC), designed to
assess class-wise separation and per-class bias reduction. Our method
demonstrates the ability to balance high performance with fairness, making it a
robust solution for unbiasing AI models in the art domain.

</details>


### [3] [State-Inference-Based Prompting for Natural Language Trading with Game NPCs](https://arxiv.org/abs/2507.07203)
*Minkyung Kim,Junsik Kim,Hwidong Bae,Woongcheol Yang,Sangdon Park,Sohee Bae*

Main category: cs.AI

TL;DR: 提出SIBP方法，通过状态推断提升游戏交易对话的规则合规性与计算准确性，效果优于现有方式，为商业游戏NPC交易交互带来可落地的解决方案。


<details>
  <summary>Details</summary>
Motivation: 目前的大语言模型虽然能用于游戏中的动态交互，但对于严格规则约束的交易系统表现较差，易出现规则违规、物品幻觉和计算错误，影响玩家信任。因此亟需一种更可靠的对话式交易解决方案。

Method: 提出了一种基于状态推断的提示（SIBP）方法，将交易对话分解为六个状态，使用统一文档结构进行上下文敏感的物品引用和基于占位符的价格计算，以提升规则合规性和准确性。

Result: 在100份交易对话评估中，该方法实现了大于97%的状态合规率，95%以上的引用准确率，以及99.7%的计算精度，并且优于现有方法。

Conclusion: SIBP方法能高效、准确地支持符合规则的虚拟交易，为商业游戏中可信赖的NPC交互奠定了基础。

Abstract: Large Language Models enable dynamic game interactions but struggle with
rule-governed trading systems. Current implementations suffer from rule
violations, such as item hallucinations and calculation errors, that erode
player trust. Here, State-Inference-Based Prompting (SIBP) enables reliable
trading through autonomous dialogue state inference and context-specific rule
adherence. The approach decomposes trading into six states within a unified
prompt framework, implementing context-aware item referencing and
placeholder-based price calculations. Evaluation across 100 trading dialogues
demonstrates >97% state compliance, >95% referencing accuracy, and 99.7%
calculation precision. SIBP maintains computational efficiency while
outperforming baseline approaches, establishing a practical foundation for
trustworthy NPC interactions in commercial games.

</details>


### [4] [Neurosymbolic Feature Extraction for Identifying Forced Labor in Supply Chains](https://arxiv.org/abs/2507.07217)
*Zili Wang,Frank Montabon,Kristin Yvonne Rozier*

Main category: cs.AI

TL;DR: 论文针对稀缺且腐化数据的供应链非法活动检测，提出基于LLM和神经符号方法，通过问题树自动分析新闻文本。实验对比人工与自动特征提取效果，结果印证了该方法的可行性，可用于复杂系统下非法行为快速检测。


<details>
  <summary>Details</summary>
Motivation: 供应链网络本身复杂，涉及非法行为（如假冒零件、强制劳动、人口贩运）时分析更为困难。机器学习可以帮助发现复杂系统中的模式，但通常需要大量训练数据，而非法供应链的数据稀缺且质量低下。急需能够在数据稀缺和复杂情况下，自动检测非法行为相关新模式的方法。

Method: 研究团队探索了神经符号方法，用于识别供应链中非法活动实例。他们比较了基于新闻报道的手工和自动特征提取的有效性，并提出了一种基于问题树的方法，利用大型语言模型（LLM）对新闻文章进行查询，评估其与强制劳动等非法活动相关的程度，从而系统分析人工与机器对相关新闻分类的差异。

Result: 通过实施问题树和LLM结合的方法，实现了对供应链中非法活动新闻的高相关性识别，并有效比较了人工和自动分类的异同。结果展示了神经符号方法和LLM在稀疏与不可靠数据环境下对非法活动检测的可行性和优势。

Conclusion: 神经符号方法结合LLM及问题树，可在无需大规模训练数据的前提下，提高对供应链中非法活动新闻的自动检测与分类能力，并为未来自动化监测复杂、数据匮乏场景下的非法行为提供了有效路径。

Abstract: Supply chain networks are complex systems that are challenging to analyze;
this problem is exacerbated when there are illicit activities involved in the
supply chain, such as counterfeit parts, forced labor, or human trafficking.
While machine learning (ML) can find patterns in complex systems like supply
chains, traditional ML techniques require large training data sets. However,
illicit supply chains are characterized by very sparse data, and the data that
is available is often (purposely) corrupted or unreliable in order to hide the
nature of the activities. We need to be able to automatically detect new
patterns that correlate with such illegal activity over complex, even temporal
data, without requiring large training data sets. We explore neurosymbolic
methods for identifying instances of illicit activity in supply chains and
compare the effectiveness of manual and automated feature extraction from news
articles accurately describing illicit activities uncovered by authorities. We
propose a question tree approach for querying a large language model (LLM) to
identify and quantify the relevance of articles. This enables a systematic
evaluation of the differences between human and machine classification of news
articles related to forced labor in supply chains.

</details>


### [5] [Open Source Planning & Control System with Language Agents for Autonomous Scientific Discovery](https://arxiv.org/abs/2507.07257)
*Licong Xu,Milind Sarkar,Anto I. Lonappan,Íñigo Zubeldia,Pablo Villanueva-Domingo,Santiago Casas,Christian Fidler,Chetana Amancharla,Ujjwal Tiwari,Adrian Bayer,Chadi Ait Ekiou,Miles Cranmer,Adrian Dimitrov,James Fergusson,Kahaan Gandhi,Sven Krippendorf,Andrew Laverick,Julien Lesgourgues,Antony Lewis,Thomas Meier,Blake Sherwin,Kristen Surrao,Francisco Villaescusa-Navarro,Chi Wang,Xueqing Xu,Boris Bolliet*

Main category: cs.AI

TL;DR: 作者开发了由30个大模型智能体组成的科研自动化系统cmbagent，实现了复杂天体物理任务全流程无人工操作，效果超过现有主流模型，系统已开源并上线。


<details>
  <summary>Details</summary>
Motivation: 推动自动化科研任务的进程，探索通过多智能体大模型系统自动完成复杂科学研究（如宇宙学任务）的可行性。

Method: 开发了约30个大型语言模型智能体组成的多智能体系统cmbagent。各智能体分工明确，包括检索论文与代码、编写代码、结果解释、相互批判输出等，无需人工介入，并可本地执行代码。采用规划与控制策略协同调度各智能体工作流。

Result: cmbagent成功自动完成博士级别宇宙学任务（利用超新星数据测量宇宙学参数），并在两个基准集上评估，均优于现有先进的LLM。系统已在GitHub开源，演示视频和云端部署也已上线。

Conclusion: 多智能体大模型系统可以无人工干预下，高效且优于当前最优LLM地自动完成复杂科研任务，具有应用与拓展前景。

Abstract: We present a multi-agent system for automation of scientific research tasks,
cmbagent. The system is formed by about 30 Large Language Model (LLM) agents
and implements a Planning & Control strategy to orchestrate the agentic
workflow, with no human-in-the-loop at any point. Each agent specializes in a
different task (performing retrieval on scientific papers and codebases,
writing code, interpreting results, critiquing the output of other agents) and
the system is able to execute code locally. We successfully apply cmbagent to
carry out a PhD level cosmology task (the measurement of cosmological
parameters using supernova data) and evaluate its performance on two benchmark
sets, finding superior performance over state-of-the-art LLMs. The source code
is available on GitHub, demonstration videos are also available, and the system
is deployed on HuggingFace and will be available on the cloud.

</details>


### [6] [Application of LLMs to Multi-Robot Path Planning and Task Allocation](https://arxiv.org/abs/2507.07302)
*Ashish Kumar*

Main category: cs.AI

TL;DR: 该文提出用大语言模型作为专家指导多智能体进行任务探索，证明了其能有效提升探索效率。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习中的高效探索本就具有挑战性，在多智能体强化学习中由于系统复杂性问题更加突出。该文旨在探索如何提升多智能体系统的探索效率。

Method: 本文尝试将大语言模型（LLMs）作为专家规划器，指导多智能体在基于规划的任务中实现高效探索。

Result: 通过引入大语言模型进行智能体决策规划，展现了这类专家探索机制在提升探索效率上的作用。

Conclusion: 大语言模型可作为高效的专家规划器，显著提升多智能体强化学习中的探索效率。

Abstract: Efficient exploration is a well known problem in deep reinforcement learning
and this problem is exacerbated in multi-agent reinforcement learning due the
intrinsic complexities of such algorithms. There are several approaches to
efficiently explore an environment to learn to solve tasks by multi-agent
operating in that environment, of which, the idea of expert exploration is
investigated in this work. More specifically, this work investigates the
application of large-language models as expert planners for efficient
exploration in planning based tasks for multiple agents.

</details>


### [7] [ViDove: A Translation Agent System with Multimodal Context and Memory-Augmented Reasoning](https://arxiv.org/abs/2507.07306)
*Yichen Lu,Wei Dai,Jiaen Liu,Ching Wing Kwok,Zongheng Wu,Xudong Xiao,Ao Sun,Sheng Fu,Jianyuan Zhan,Yian Wang,Takatomo Saito,Sicheng Lai*

Main category: cs.AI

TL;DR: ViDove实现了多模态高精度翻译，显著提升了字幕生成和翻译任务表现，并开源了新型基准数据集DoveBench。


<details>
  <summary>Details</summary>
Motivation: 当前LLM驱动的翻译系统虽然在文本翻译方面表现出色，但主要局限于仅处理文本输入，难以充分利用视觉和上下文背景信息来提升翻译质量。本文作者希望通过引入多模态能力，模拟人类翻译流程，提升系统在复杂和实际应用场景下的翻译适应能力与准确性。

Method: 提出ViDove翻译智能体系统，具备处理多模态输入的能力。系统设计受人类翻译工作流程启发，通过结合视觉与上下文背景信息提升翻译效果。同时，系统集成多模态记忆系统与结合领域知识的长短时记忆网络模块，使其在真实场景下能够更准确、灵活地执行翻译任务。此外，还提出了DoveBench这一新基准数据集，用于评测多模态长视频字幕与翻译任务。

Result: ViDove在字幕生成和通用翻译任务中性能显著提升，相较于此前最佳方法在BLEU得分上提升28%，在SubER指标上提升15%。提出的DoveBench数据集涵盖17小时高质量、人工标注数据。

Conclusion: ViDove多模态翻译系统在实际应用和基准测试中均展现出显著性能提升，证明了借助视觉及上下文信息以及多模态记忆系统对提升翻译质量的重要价值。DoveBench丰富了相关评测资源。

Abstract: LLM-based translation agents have achieved highly human-like translation
results and are capable of handling longer and more complex contexts with
greater efficiency. However, they are typically limited to text-only inputs. In
this paper, we introduce ViDove, a translation agent system designed for
multimodal input. Inspired by the workflow of human translators, ViDove
leverages visual and contextual background information to enhance the
translation process. Additionally, we integrate a multimodal memory system and
long-short term memory modules enriched with domain-specific knowledge,
enabling the agent to perform more accurately and adaptively in real-world
scenarios. As a result, ViDove achieves significantly higher translation
quality in both subtitle generation and general translation tasks, with a 28%
improvement in BLEU scores and a 15% improvement in SubER compared to previous
state-of-the-art baselines. Moreover, we introduce DoveBench, a new benchmark
for long-form automatic video subtitling and translation, featuring 17 hours of
high-quality, human-annotated data. Our code is available here:
https://github.com/pigeonai-org/ViDove

</details>


### [8] [On the Impossibility of Separating Intelligence from Judgment: The Computational Intractability of Filtering for AI Alignment](https://arxiv.org/abs/2507.07341)
*Sarah Ball,Greg Gluch,Shafi Goldwasser,Frauke Kreuter,Omer Reingold,Guy N. Rothblum*

Main category: cs.AI

TL;DR: 本文论证了基于输入或输出过滤器的LLM安全机制在计算上存在不可逾越的障碍，指出只能通过LLM内部机制实现真正的对齐和安全。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的广泛应用带来了有害内容生成的风险，因此需要研究如何过滤或防止这些模型被滥用。论文关注于对齐（alignment）挑战，特别是安全过滤机制的问题。

Method: 分析和证明输入过滤（在用户提示进入模型前拦截）和输出过滤（生成内容后拦截）在计算上存在的瓶颈，主要通过复杂性证明，假设加密难题成立，并对放宽的缓解方法进行形式化和研究。

Result: 证明了对于某些LLM，无法构建高效的提示过滤器，且存在无法高效区分恶意与良性输入的情形；同时，给出某些场景下输出过滤在计算上也不可行，所有结果均建立在加密学难题的基础上。此外，对一些“放宽”缓解方法证明仍存在计算障碍。

Conclusion: 依赖模型外部（输入/输出）过滤无法从根本实现LLM安全，仅靠黑盒访问无法满足对齐需求。智能与判断力在AI系统中不可分割，安全应在模型内部实现。

Abstract: With the increased deployment of large language models (LLMs), one concern is
their potential misuse for generating harmful content. Our work studies the
alignment challenge, with a focus on filters to prevent the generation of
unsafe information. Two natural points of intervention are the filtering of the
input prompt before it reaches the model, and filtering the output after
generation. Our main results demonstrate computational challenges in filtering
both prompts and outputs. First, we show that there exist LLMs for which there
are no efficient prompt filters: adversarial prompts that elicit harmful
behavior can be easily constructed, which are computationally indistinguishable
from benign prompts for any efficient filter. Our second main result identifies
a natural setting in which output filtering is computationally intractable. All
of our separation results are under cryptographic hardness assumptions. In
addition to these core findings, we also formalize and study relaxed mitigation
approaches, demonstrating further computational barriers. We conclude that
safety cannot be achieved by designing filters external to the LLM internals
(architecture and weights); in particular, black-box access to the LLM will not
suffice. Based on our technical results, we argue that an aligned AI system's
intelligence cannot be separated from its judgment.

</details>


### [9] [Meek Models Shall Inherit the Earth](https://arxiv.org/abs/2507.07931)
*Hans Gundlach,Jayson Lynch,Neil Thompson*

Main category: cs.AI

TL;DR: 随着算力扩展的收益递减，小算力AI模型的能力将逐步接近业界最强大模型，AI能力差距最终缩小，AI相关的政策与战略需要重新审视。


<details>
  <summary>Details</summary>
Motivation: 过去十年中，只有少数公司能够推动AI系统的规模化，导致AI模型性能上存在不平等。本论文旨在挑战主流观点，探讨AI系统规模扩展的收益递减是否会带来模型能力的趋同。

Method: 论文构建了一个模型，说明在固定分布的下一个token预测任务下，随着规模扩大，算力带来的边际能力收益大幅减少。此外，论文提供了训练损失作为能力指标的理论和实证依据，并分析了历史AI模型能力差距的数据。

Result: 论文结果显示，在当前的AI扩展实践下，算力带来的收益递减足够强大，甚至是能以指数级速度扩展模型的大公司，最终在能力上也难以取得明显优势。“弱小模型”的能力将逐步接近最强大模型。

Conclusion: AI模型能力将趋同，规模化垄断带来的能力优势最终将消失，AI策略与政策应随之调整。

Abstract: The past decade has seen incredible scaling of AI systems by a few companies,
leading to inequality in AI model performance. This paper argues that, contrary
to prevailing intuition, the diminishing returns to compute scaling will lead
to a convergence of AI model capabilities. In other words, meek models (those
with limited computation budget) shall inherit the earth, approaching the
performance level of the best models overall. We develop a model illustrating
that under a fixed-distribution next-token objective, the marginal capability
returns to raw compute shrink substantially. Given current scaling practices,
we argue that these diminishing returns are strong enough that even companies
that can scale their models exponentially faster than other organizations will
eventually have little advantage in capabilities. As part of our argument, we
give several reasons that proxies like training loss differences capture
important capability measures using evidence from benchmark data and
theoretical performance models. In addition, we analyze empirical data on the
capability difference of AI models over time. Finally, in light of the
increasing ability of meek models, we argue that AI strategy and policy require
reexamination, and we outline the areas this shift will affect.

</details>


### [10] [Supply Chain Optimization via Generative Simulation and Iterative Decision Policies](https://arxiv.org/abs/2507.07355)
*Haoyue Bai,Haoyu Wang,Nanxu Gong,Xinyuan Wang,Wangyang Ying,Haifeng Chen,Yanjie Fu*

Main category: cs.AI

TL;DR: 本文提出了Sim-to-Dec框架，通过生成式仿真和端到端决策优化，提升了供应链运输的及时交付率和经济效益，在多项数据集上展示了其显著优势。


<details>
  <summary>Details</summary>
Motivation: 高响应性和经济效率是供应链运输的关键目标，而运输方式的战略决策直接影响这两点。现有方法难以兼顾仿真泛化能力、精细动态、历史与预测结合及仿真反馈与决策策略紧密结合。

Method: 提出了Sim-to-Dec框架，包含生成式仿真模块（利用自回归建模进行连续状态变化仿真）和历史-未来双感知决策模型，通过与仿真器的端到端优化进行迭代优化。

Result: 在三个实际数据集上的大量实验表明，Sim-to-Dec在及时交付率和利润上均有显著提升。

Conclusion: Sim-to-Dec框架能够有效提升供应链运输中的及时交付率和利润。

Abstract: High responsiveness and economic efficiency are critical objectives in supply
chain transportation, both of which are influenced by strategic decisions on
shipping mode. An integrated framework combining an efficient simulator with an
intelligent decision-making algorithm can provide an observable, low-risk
environment for transportation strategy design. An ideal simulation-decision
framework must (1) generalize effectively across various settings, (2) reflect
fine-grained transportation dynamics, (3) integrate historical experience with
predictive insights, and (4) maintain tight integration between simulation
feedback and policy refinement. We propose Sim-to-Dec framework to satisfy
these requirements. Specifically, Sim-to-Dec consists of a generative
simulation module, which leverages autoregressive modeling to simulate
continuous state changes, reducing dependence on handcrafted domain-specific
rules and enhancing robustness against data fluctuations; and a history-future
dual-aware decision model, refined iteratively through end-to-end optimization
with simulator interactions. Extensive experiments conducted on three
real-world datasets demonstrate that Sim-to-Dec significantly improves timely
delivery rates and profit.

</details>


### [11] [Working with AI: Measuring the Occupational Implications of Generative AI](https://arxiv.org/abs/2507.07935)
*Kiran Tomlinson,Sonia Jaffe,Will Wang,Scott Counts,Siddharth Suri*

Main category: cs.AI

TL;DR: 本文基于20万微软 Bing Copilot 用户交互数据，分析AI协助下的实际工作活动及其在各职业中的适用性，发现AI对信息处理密集型职位如知识工作、行政与销售最适用，并给出薪资、教育与适用性的相关性分析。


<details>
  <summary>Details</summary>
Motivation: 生成式AI被迅速应用于各类任务，可能对经济带来重大影响。要深入了解AI对经济的影响，首先需理清AI在工作活动中的具体作用及其效果。

Method: 分析了20万条匿名且已脱敏的Microsoft Bing Copilot用户与AI的真实对话，识别用户请求的工作类型和AI实际完成的工作类型，并结合职业任务数据，分析各职业AI适用性。

Result: 发现用户主要寻求信息收集及写作等AI帮助，而AI本身主要执行信息提供、写作、教学和咨询任务。计算后，知识类职业（如计算机、数学，行政支持等）及涉及信息传递的销售类职业AI适用性分数最高。同时分析了完成最成功的任务类型，薪资与教育水平和AI适用度的相关性，以及实际AI应用与以往AI影响预测之间的关系。

Conclusion: AI在知识工作、行政、销售等涉及信息处理的职业中最具适用性。实际使用数据帮助理解AI对不同职业的真正影响以及与预测的异同。

Abstract: Given the rapid adoption of generative AI and its potential to impact a wide
range of tasks, understanding the effects of AI on the economy is one of
society's most important questions. In this work, we take a step toward that
goal by analyzing the work activities people do with AI, how successfully and
broadly those activities are done, and combine that with data on what
occupations do those activities. We analyze a dataset of 200k anonymized and
privacy-scrubbed conversations between users and Microsoft Bing Copilot, a
publicly available generative AI system. We find the most common work
activities people seek AI assistance for involve gathering information and
writing, while the most common activities that AI itself is performing are
providing information and assistance, writing, teaching, and advising.
Combining these activity classifications with measurements of task success and
scope of impact, we compute an AI applicability score for each occupation. We
find the highest AI applicability scores for knowledge work occupation groups
such as computer and mathematical, and office and administrative support, as
well as occupations such as sales whose work activities involve providing and
communicating information. Additionally, we characterize the types of work
activities performed most successfully, how wage and education correlate with
AI applicability, and how real-world usage compares to predictions of
occupational AI impact.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [12] [Planted in Pretraining, Swayed by Finetuning: A Case Study on the Origins of Cognitive Biases in LLMs](https://arxiv.org/abs/2507.07186)
*Itay Itzhak,Yonatan Belinkov,Gabriel Stanovsky*

Main category: cs.CL

TL;DR: 大模型主要在预训练阶段形成认知偏差，微调和训练随机性影响较小，应将关注点放在预训练机制以更好理解和减轻偏差。


<details>
  <summary>Details</summary>
Motivation: 此前发现大模型认知偏差与人类类似，且模型间、经过指令微调后的偏差程度不同，但尚不清楚这些差异来源于预训练、微调还是训练过程的随机性。

Method: 提出两步因果实验：1）通过更换随机种子多次微调模型，观察训练随机性对30种认知偏差的影响；2）引入“交叉微调”，互换指令数据集，验证微调数据集对偏差的依赖性。

Result: 结果显示训练过程的随机性会带来一定偏差变化，但模型的认知偏差主要受预训练阶段影响，拥有相同预训练骨干的模型比仅共用微调数据的模型展现出更一致的偏差模式。

Conclusion: 大模型的认知偏差主要由预训练阶段决定，微调数据和训练随机性影响有限，研究偏差需更多关注预训练来源。

Abstract: Large language models (LLMs) exhibit cognitive biases -- systematic
tendencies of irrational decision-making, similar to those seen in humans.
Prior work has found that these biases vary across models and can be amplified
by instruction tuning. However, it remains unclear if these differences in
biases stem from pretraining, finetuning, or even random noise due to training
stochasticity. We propose a two-step causal experimental approach to
disentangle these factors. First, we finetune models multiple times using
different random seeds to study how training randomness affects over $30$
cognitive biases. Second, we introduce \emph{cross-tuning} -- swapping
instruction datasets between models to isolate bias sources. This swap uses
datasets that led to different bias patterns, directly testing whether biases
are dataset-dependent. Our findings reveal that while training randomness
introduces some variability, biases are mainly shaped by pretraining: models
with the same pretrained backbone exhibit more similar bias patterns than those
sharing only finetuning data. These insights suggest that understanding biases
in finetuned models requires considering their pretraining origins beyond
finetuning effects. This perspective can guide future efforts to develop
principled strategies for evaluating and mitigating bias in LLMs.

</details>


### [13] [Prompt Perturbations Reveal Human-Like Biases in LLM Survey Responses](https://arxiv.org/abs/2507.07188)
*Jens Rupprecht,Georg Ahnert,Markus Strohmaier*

Main category: cs.CL

TL;DR: LLM在模拟社会科学调查中对问题表述和答案结构的微小变化十分敏感，表现出与人类类似的偏差（如末位偏好）。因此，在用LLM进行问卷生成与分析时，需高度重视提示设计与鲁棒性验证。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLM）越来越多地被用作社会科学调查中人类被试的代理，但其可靠性以及对已知响应偏差的敏感性尚不清楚。该论文旨在系统评估LLM在规范性调查语境下的响应鲁棒性。

Method: 作者选取了九种不同的LLM，采集世界价值观调查中的题目，对问题措辞和答案结构分别进行了11种扰动，累计生成了16.7万余次模拟问卷。通过这种方式，测试了模型对各种扰动的响应鲁棒性。

Result: 一方面，揭示了所有模型对问题扰动的脆弱性，尤其是在语义重述和多重扰动的情况下。另一方面，所有模型都表现出不同程度的末位偏好（recency bias），倾向选择最后一个答案。虽然大型模型总体上更鲁棒，但依然对语义和复合扰动敏感。此外，LLM在一定程度上也呈现出与人类受访者相似的响应偏差。

Conclusion: 使用LLM生成合成调查数据时，问题设计和鲁棒性测试极为关键，因为现有模型对扰动敏感且存在系统性偏好。

Abstract: Large Language Models (LLMs) are increasingly used as proxies for human
subjects in social science surveys, but their reliability and susceptibility to
known response biases are poorly understood. This paper investigates the
response robustness of LLMs in normative survey contexts -- we test nine
diverse LLMs on questions from the World Values Survey (WVS), applying a
comprehensive set of 11 perturbations to both question phrasing and answer
option structure, resulting in over 167,000 simulated interviews. In doing so,
we not only reveal LLMs' vulnerabilities to perturbations but also reveal that
all tested models exhibit a consistent \textit{recency bias} varying in
intensity, disproportionately favoring the last-presented answer option. While
larger models are generally more robust, all models remain sensitive to
semantic variations like paraphrasing and to combined perturbations. By
applying a set of perturbations, we reveal that LLMs partially align with
survey response biases identified in humans. This underscores the critical
importance of prompt design and robustness testing when using LLMs to generate
synthetic survey data.

</details>


### [14] [SynthTextEval: Synthetic Text Data Generation and Evaluation for High-Stakes Domains](https://arxiv.org/abs/2507.07229)
*Krithika Ramesh,Daniel Smolyak,Zihao Zhao,Nupoor Gandhi,Ritu Agarwal,Margrét Bjarnadóttir,Anjalie Field*

Main category: cs.CL

TL;DR: 本文提出SynthTextEval工具包，实现了对合成文本在下游效用、公平性、隐私等多维度的一站式评估，特别在医疗和法律等高风险领域展现出价值，提高了合成文本在AI开发中的隐私保护可行性。


<details>
  <summary>Details</summary>
Motivation: 当前大模型生成文本流畅，在多个场景中有潜在应用价值，尤以降低高风险领域AI系统开发部署中的隐私泄露风险为核心动力。但实现这一点，需有系统而统一的合成数据多维度评价方法。

Method: 提出SynthTextEval工具包，允许用户针对自有或工具生成的合成数据，从实用性、公平性、隐私泄露风险、分布差异及专家反馈等多个维度进行系统性评估。

Result: 该工具包可以评估各种数据，并以医疗和法律两个高风险领域的数据集验证了其实用性和有效性。

Conclusion: SynthTextEval通过整合和标准化合成文本评价指标，有望提升合成文本在高风险领域（如医疗和法律）中的应用可行性与隐私保护能力。

Abstract: We present SynthTextEval, a toolkit for conducting comprehensive evaluations
of synthetic text. The fluency of large language model (LLM) outputs has made
synthetic text potentially viable for numerous applications, such as reducing
the risks of privacy violations in the development and deployment of AI systems
in high-stakes domains. Realizing this potential, however, requires principled
consistent evaluations of synthetic data across multiple dimensions: its
utility in downstream systems, the fairness of these systems, the risk of
privacy leakage, general distributional differences from the source text, and
qualitative feedback from domain experts. SynthTextEval allows users to conduct
evaluations along all of these dimensions over synthetic data that they upload
or generate using the toolkit's generation module. While our toolkit can be run
over any data, we highlight its functionality and effectiveness over datasets
from two high-stakes domains: healthcare and law. By consolidating and
standardizing evaluation metrics, we aim to improve the viability of synthetic
text, and in-turn, privacy-preservation in AI development.

</details>


### [15] [Medical Red Teaming Protocol of Language Models: On the Importance of User Perspectives in Healthcare Settings](https://arxiv.org/abs/2507.07248)
*Minseon Kim,Jean-Philippe Corbeil,Alessandro Sordoni,Francois Beaulieu,Paul Vozila*

Main category: cs.CL

TL;DR: 提出了面向医疗领域、涵盖患者与医生视角的LLM安全评估协议，建立了患者安全评价基准数据集，并通过多视角红队测试，完善了医疗大模型的安全评估体系。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在医学领域应用越来越广，但其输出可能直接影响人类健康，尤其是面对不同类型用户（如患者和临床医生）时的安全问题不容忽视。现有安全评估多聚焦于通用基准，缺乏针对医疗场景、不同用户视角下的系统性安全评估。

Method: 提出了一个专为医疗领域设计的安全评估协议，覆盖患者和临床医生视角，同时结合常规安全评估，量化分析医疗LLM的安全性。构建了PatientSafetyBench数据集（包含5大类466个案例）以患者视角衡量安全性，并以MediPhi模型集合为案例应用了针对性的红队攻击协议，从患者、医生和通用用户三个视角定义安全评估标准。

Result: 建立了首个针对医疗LLM的多视角（患者、医生、通用用户）安全红队评估体系，为医疗领域LLM安全部署奠定了基础。

Conclusion: 该研究首次系统性地界定了医疗LLM的安全评估标准和协议，填补了领域内评测手段的空白，为未来医疗大模型的安全性提供了可量化和可操作的评估路径。

Abstract: As the performance of large language models (LLMs) continues to advance,
their adoption is expanding across a wide range of domains, including the
medical field. The integration of LLMs into medical applications raises
critical safety concerns, particularly due to their use by users with diverse
roles, e.g. patients and clinicians, and the potential for model's outputs to
directly affect human health. Despite the domain-specific capabilities of
medical LLMs, prior safety evaluations have largely focused only on general
safety benchmarks. In this paper, we introduce a safety evaluation protocol
tailored to the medical domain in both patient user and clinician user
perspectives, alongside general safety assessments and quantitatively analyze
the safety of medical LLMs. We bridge a gap in the literature by building the
PatientSafetyBench containing 466 samples over 5 critical categories to measure
safety from the perspective of the patient. We apply our red-teaming protocols
on the MediPhi model collection as a case study. To our knowledge, this is the
first work to define safety evaluation criteria for medical LLMs through
targeted red-teaming taking three different points of view - patient,
clinician, and general user - establishing a foundation for safer deployment in
medical domains.

</details>


### [16] [The Impact of Background Speech on Interruption Detection in Collaborative Groups](https://arxiv.org/abs/2507.07280)
*Mariah Bradford,Nikhil Krishnaswamy,Nathaniel Blanchard*

Main category: cs.CL

TL;DR: 本文提出了适用于多组协作学习场景、能够应对重叠语音的中断检测方法，并分析了中断的语言和韵律特征，为教室AI监测小组互动提供了新工具。


<details>
  <summary>Details</summary>
Motivation: 现有的中断识别研究大多聚焦于音频环境干净且仅包含单一对话的场景，然而实际课堂中小组协作学习普遍存在多组并发对话与重叠语音，对中断检测造成挑战。

Method: 分析单一对话和多组对话环境下的中断检测问题，提出一种针对重叠语音仍具鲁棒性的中断识别方法，并提取相关语言和韵律特征，用于识别和理解群体协作学习中的中断。

Result: 提出的中断识别方法在面对重叠语音时表现出良好鲁棒性，可支持实际教室环境下AI对小组协作学习对话的监测。同时，研究揭示了群体互动中中断的语言与韵律表现。

Conclusion: 论文为多组协作学习环境下AI识别对话中断奠定了方法基础，推动了对复杂真实互动环境下中断检测的理解，并为未来研究量化多组重叠语音对协作交流追踪的影响提供了方向。

Abstract: Interruption plays a crucial role in collaborative learning, shaping group
interactions and influencing knowledge construction. AI-driven support can
assist teachers in monitoring these interactions. However, most previous work
on interruption detection and interpretation has been conducted in
single-conversation environments with relatively clean audio. AI agents
deployed in classrooms for collaborative learning within small groups will need
to contend with multiple concurrent conversations -- in this context,
overlapping speech will be ubiquitous, and interruptions will need to be
identified in other ways. In this work, we analyze interruption detection in
single-conversation and multi-group dialogue settings. We then create a
state-of-the-art method for interruption identification that is robust to
overlapping speech, and thus could be deployed in classrooms. Further, our work
highlights meaningful linguistic and prosodic information about how
interruptions manifest in collaborative group interactions. Our investigation
also paves the way for future works to account for the influence of overlapping
speech from multiple groups when tracking group dialog.

</details>


### [17] [Multi-Agent Retrieval-Augmented Framework for Evidence-Based Counterspeech Against Health Misinformation](https://arxiv.org/abs/2507.07307)
*Anirban Saha Anik,Xiaoying Song,Elliott Wang,Bryan Wang,Bengisu Yarimbas,Lingzi Hong*

Main category: cs.CL

TL;DR: 提出多智能体RAG框架，整合静态与动态证据，显著提升反健康错误信息对话生成的质量和说服力，经消融实验与人工评价验证效果优异。


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法应对虚假信息时证据有限且输出不可控，难以产生优质、具说服力的反驳对话。为增强证据量、输出可控和提升多维度质量，需采用更精细、多模块协同的生成策略。

Method: 构建多智能体检索增强生成（RAG）框架，联合多个大语言模型优化知识检索、证据增强与回复精炼流程，并结合静态与动态证据，提升生成对话的相关性和事实性。对关键模块进行消融实验并辅以人工评测。

Result: 提出的方法在礼貌性、相关性、信息量和事实准确性等维度均优于现有基线方法，且人工评测显示精炼环节显著提升输出质量和受欢迎度。

Conclusion: 本研究提出的多智能体RAG框架在反健康错误信息对话生成中表现优异，并通过消融实验和人工评估验证了各模块和最终结果的有效性。

Abstract: Large language models (LLMs) incorporated with Retrieval-Augmented Generation
(RAG) have demonstrated powerful capabilities in generating counterspeech
against misinformation. However, current studies rely on limited evidence and
offer less control over final outputs. To address these challenges, we propose
a Multi-agent Retrieval-Augmented Framework to generate counterspeech against
health misinformation, incorporating multiple LLMs to optimize knowledge
retrieval, evidence enhancement, and response refinement. Our approach
integrates both static and dynamic evidence, ensuring that the generated
counterspeech is relevant, well-grounded, and up-to-date. Our method
outperforms baseline approaches in politeness, relevance, informativeness, and
factual accuracy, demonstrating its effectiveness in generating high-quality
counterspeech. To further validate our approach, we conduct ablation studies to
verify the necessity of each component in our framework. Furthermore, human
evaluations reveal that refinement significantly enhances counterspeech quality
and obtains human preference.

</details>


### [18] [GNN-CNN: An Efficient Hybrid Model of Convolutional and Graph Neural Networks for Text Representation](https://arxiv.org/abs/2507.07414)
*Fardin Rastakhiz*

Main category: cs.CL

TL;DR: 本文通过结合GNN和CNN，并实时生成语义图，实现了无需padding或截断的高效长文本分类方法。融合LLM知识，模型在多个任务上兼具高效和竞争力，适合对算力、效率要求高的场景。


<details>
  <summary>Details</summary>
Motivation: 目前主流的深度学习模型——变换器（Transformer）在处理长文本时计算复杂度呈二次增长，导致时间、成本和能耗效率低下。因此，开发一种更高效的长文本处理方法变得尤为重要。

Method: 本文提出一种新颖的模型架构，将图神经网络（GNN）和卷积神经网络（CNN）结合，并集成端到端的实时图生成机制。模型以字符级输入小批量处理，无需padding或截断。同时，模型通过高效字典查找方式融合大语言模型（LLM）的信息（如token embedding和情感极性），利用CNN捕获局部上下文，通过格点型图扩展局部感受野，并用小世界图结构聚合文档级别信息。

Result: 模型生成的图结构显示出有意义的语义组织特性，如平均聚类系数约0.45，平均最短路径长度在4-5之间。在情感分析、新闻分类等多个文本分类任务中，与最新模型对比，所提方法展现出高效且具竞争力的表现。

Conclusion: 该方法能够在保持高运算效率和速度的同时，充分捕捉文本语义信息，适用于长文本的高效处理，并且在多项任务上取得与现有最佳模型相当甚至更优的效果。

Abstract: Time, cost, and energy efficiency are critical considerations in
Deep-Learning (DL), particularly when processing long texts. Transformers,
which represent the current state of the art, exhibit quadratic computational
complexity relative to input length, making them inefficient for extended
documents. This study introduces a novel model architecture that combines Graph
Neural Networks (GNNs) and Convolutional Neural Networks (CNNs), integrated
with a real-time, end-to-end graph generation mechanism. The model processes
compact batches of character-level inputs without requiring padding or
truncation. To enhance performance while maintaining high speed and efficiency,
the model incorporates information from Large Language Models (LLMs), such as
token embeddings and sentiment polarities, through efficient dictionary
lookups. It captures local contextual patterns using CNNs, expands local
receptive fields via lattice-based graph structures, and employs small-world
graphs to aggregate document-level information. The generated graphs exhibit
structural properties indicative of meaningful semantic organization, with an
average clustering coefficient of approximately 0.45 and an average shortest
path length ranging between 4 and 5. The model is evaluated across multiple
text classification tasks, including sentiment analysis and
news-categorization, and is compared against state-of-the-art models.
Experimental results confirm the proposed model's efficiency and competitive
performance.

</details>


### [19] [MedReadCtrl: Personalizing medical text generation with readability-controlled instruction learning](https://arxiv.org/abs/2507.07419)
*Hieu Tran,Zonghai Yao,Won Seok Jang,Sharmin Sultana,Allen Chang,Yuan Zhang,Hong Yu*

Main category: cs.CL

TL;DR: 本文提出的MedReadCtrl框架能有效调控AI生成医疗文本的可读性，使内容更加易懂且个性化，在多项医学任务中超越了GPT-4，有助于提升患者教育和医疗AI的可及性。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在医疗领域应用广泛，但现有模型在与人类沟通时，如何生成既个性化又可理解的内容仍是主要挑战，特别是在需要满足不同受众阅读能力的场景下。

Method: 提出了一种新的可控可读性指令微调框架MedReadCtrl，使大型语言模型（LLM）能够在不损失语义的情况下，调整输出文本的复杂度。基于九个数据集和三类任务，在医学和一般领域进行广泛评估。

Result: MedReadCtrl在医疗和通用领域的任务中，显著减少了可读性指令跟随错误，相比GPT-4表现更好（如ReadMe数据集上1.39 vs 1.59，p<0.001），并在临床新任务上取得了明显提升（如MTSamples上ROUGE-L提升14.7，SARI提升6.18）。专家偏好也显著高于对比模型（71.7% vs 23.3%），低健康素养场景下优势更加突出。

Conclusion: MedReadCtrl能够在保持医学内容原意的基础上，将临床信息转化为符合受众阅读难度的表达方式，为患者教育和公平获得AI辅助医疗服务提供了可扩展的解决方案。

Abstract: Generative AI has demonstrated strong potential in healthcare, from clinical
decision support to patient-facing chatbots that improve outcomes. A critical
challenge for deployment is effective human-AI communication, where content
must be both personalized and understandable. We introduce MedReadCtrl, a
readability-controlled instruction tuning framework that enables LLMs to adjust
output complexity without compromising meaning. Evaluations of nine datasets
and three tasks across medical and general domains show that MedReadCtrl
achieves significantly lower readability instruction-following errors than
GPT-4 (e.g., 1.39 vs. 1.59 on ReadMe, p<0.001) and delivers substantial gains
on unseen clinical tasks (e.g., +14.7 ROUGE-L, +6.18 SARI on MTSamples).
Experts consistently preferred MedReadCtrl (71.7% vs. 23.3%), especially at low
literacy levels. These gains reflect MedReadCtrl's ability to restructure
clinical content into accessible, readability-aligned language while preserving
medical intent, offering a scalable solution to support patient education and
expand equitable access to AI-enabled care.

</details>


### [20] [SynthEHR-Eviction: Enhancing Eviction SDoH Detection with LLM-Augmented Synthetic EHR Data](https://arxiv.org/abs/2507.07421)
*Zonghai Yao,Youxia Zhao,Avijit Mitra,David A. Levy,Emily Druhl,Jack Tsai,Hong Yu*

Main category: cs.CL

TL;DR: 本研究提出了一套结合大模型和自动化优化的人机协同信息抽取管道，在驱逐等社会健康因素信息提取上达到了最佳准确率，并极大减少了人工标注工作，推动了相关健康数据集的规模化生成和利用。


<details>
  <summary>Details</summary>
Motivation: 驱逐（Eviction）是一种重要但研究较少的社会健康决定因素（SDoH），与住房不稳定、失业和心理健康问题相关。然而，在电子健康记录（EHRs）中，驱逐多以非结构化文本出现，缺乏结构化编码，限制了后续利用。

Method: 本文提出了SynthEHR-Eviction管道，结合大语言模型（LLMs）、人机协作标注和自动化提示优化（APO），用于从临床笔记中提取驱逐相关信息。此外，管道能够降低人工标注工作量，加速数据集生成。

Result: 利用该方法，构建了目前最大的公开驱逐相关SDoH数据集，并细分为14个类别。在该数据集上，微调后的LLMs（如Qwen2.5、LLaMA3）表现优异，其Macro-F1分数分别达到88.8%（驱逐）和90.3%（其他SDoH），优于GPT-4o-APO、GPT-4o-mini-APO及BioBERT模型。同时，大幅度节省了人工标注成本。

Conclusion: SynthEHR-Eviction管道实现了高效、可扩展的驱逐及其他SDoH信息抽取，为相关研究和应用提供了新的数据资源和工具，并具有良好的任务泛化能力。

Abstract: Eviction is a significant yet understudied social determinants of health
(SDoH), linked to housing instability, unemployment, and mental health. While
eviction appears in unstructured electronic health records (EHRs), it is rarely
coded in structured fields, limiting downstream applications. We introduce
SynthEHR-Eviction, a scalable pipeline combining LLMs, human-in-the-loop
annotation, and automated prompt optimization (APO) to extract eviction
statuses from clinical notes. Using this pipeline, we created the largest
public eviction-related SDoH dataset to date, comprising 14 fine-grained
categories. Fine-tuned LLMs (e.g., Qwen2.5, LLaMA3) trained on
SynthEHR-Eviction achieved Macro-F1 scores of 88.8% (eviction) and 90.3% (other
SDoH) on human validated data, outperforming GPT-4o-APO (87.8%, 87.3%),
GPT-4o-mini-APO (69.1%, 78.1%), and BioBERT (60.7%, 68.3%), while enabling
cost-effective deployment across various model sizes. The pipeline reduces
annotation effort by over 80%, accelerates dataset creation, enables scalable
eviction detection, and generalizes to other information extraction tasks.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [21] [The Pandora's Box Problem with Sequential Inspections](https://arxiv.org/abs/2507.07508)
*Ali Aouad,Jingwei Ji,Yaron Shaposhnik*

Main category: cs.CE

TL;DR: 本文将Pandora's box问题推广到可部分获取信息场景，分析了信息获取深度与成本的权衡。提出多种分析与数值方法，揭示近似最优及结构良好的策略，尤其是阈值策略在实际中非常有效。


<details>
  <summary>Details</summary>
Motivation: Pandora's box问题是经济理论中的核心模型，但以往模型仅考虑完整开启盒子（获取完整信息），缺乏对部分开启（部分信息）以降低成本的分析。现实决策常伴随信息成本与获取程度间权衡，亟需对此扩展模型进行研究。

Method: 作者扩展Pandora's box模型，允许代理人选择全开或部分开盒子，并综合运用随机优化、结构性质分析、问题松弛、近似解法、特殊情形下的精确刻画，以及大规模数值实验，多角度研究该新模型。

Result: 发现最优策略存在结构性特征，能更好理解最优决策方式。提出了问题松弛及近似最优解法，并在特定非平凡情形下刻画了最优策略。大量数值实验验证多种策略，发现基于阈值的策略在指导搜索决策上表现优异。

Conclusion: 部分获取信息模式下的Pandora's box问题在理论与实践中均具挑战性。虽整体问题具有复杂性，但通过松弛、近似、结构洞察等途径，可获得有用的分析与有效策略。基于阈值的扩展政策具备良好实用性。

Abstract: The Pandora's box problem (Weitzman 1979) is a core model in economic theory
that captures an agent's (Pandora's) search for the best alternative (box). We
study an important generalization of the problem where the agent can either
fully open boxes for a certain fee to reveal their exact values or partially
open them at a reduced cost. This introduces a new tradeoff between information
acquisition and cost efficiency. We establish a hardness result and employ an
array of techniques in stochastic optimization to provide a comprehensive
analysis of this model. This includes (1) the identification of structural
properties of the optimal policy that provide insights about optimal decisions;
(2) the derivation of problem relaxations and provably near-optimal solutions;
(3) the characterization of the optimal policy in special yet non-trivial
cases; and (4) an extensive numerical study that compares the performance of
various policies, and which provides additional insights about the optimal
policy. Throughout, we show that intuitive threshold-based policies that extend
the Pandora's box optimal solution can effectively guide search decisions.

</details>


### [22] [Meshless projection model-order reduction via reference spaces for smoothed-particle hydrodynamics](https://arxiv.org/abs/2507.07830)
*Steven N. Rodriguez,Steven L. Brunton,Liam K. Magargal,Parisa Khodabakshi,Justin W. Jaworski,Nicoleta A. Apetre,John C. Steuben,John G. Michopoulos,Athanasios Iliopoulos*

Main category: cs.CE

TL;DR: 本文提出基于模态参考空间的SPH无网格仿真模型降阶方法，结合GPOD与APG投影实验，实现了仿真速度场的高精度低维演化，有望大幅节省SPH计算资源。


<details>
  <summary>Details</summary>
Motivation: 针对无网格、弱可压缩的平滑粒子流体动力学（SPH）方法，由于其数值结构无序、动态且混合，低维子空间的发现充满挑战。因此，亟需一种有效的模型降阶方法以降低SPH仿真的计算成本。

Method: 提出了一种模型降阶框架，引入模态参考空间，通过投影SPH快照数据到参考空间，利用POD等传统模态分解技术发现低维性。在在线阶段，将模态量通过散点数据插值映射回SPH物理空间。并结合无网格Galerkin POD（GPOD）和对偶Petrov-Galerkin（APG）投影模型降阶公式实现。

Result: 在Taylor-Green涡旋、带盖腔流和空腔外流三个数值实验上验证，降阶重构及预测速度场与原始高维场高度一致。压力场对投影误差较敏感，但可通过APG等非线性方法改善。

Conclusion: 新提出的无网格模型降阶框架，可显著降低SPH仿真计算成本，并保持无网格模拟的核心优势，在动态混合流场等复杂场景下效果显著。

Abstract: This work proposes a model-order reduction framework for the meshless weakly
compressible smoothed particle hydrodynamics (SPH) method. The proposed
framework introduces the concept of modal reference spaces to overcome the
challenges of discovering low-dimensional subspaces from unstructured, dynamic,
and mixing numerical topology that is often seen in SPH simulations. The
proposed modal reference spaces enable a low-dimensional representation of the
SPH field equations while maintaining their inherent meshless qualities. Modal
reference spaces are constructed by projecting SPH snapshot data onto a
reference space where low-dimensionality of field quantities can be discovered
via traditional modal decomposition techniques (e.g., the proper orthogonal
decomposition (POD)). Modal quantities are mapped back to the meshless SPH
space via scattered data interpolation during the online predictive stage. The
proposed model-order reduction framework is cast into the \emph{meshless}
Galerkin POD (GPOD) and the Adjoint Petrov--Galerkin (APG) projection
model-order reduction (PMOR) formulation. The PMORs are tested on three
numerical experiments: 1) the Taylor--Green vortex; 2) lid-driven cavity; and
3) flow past an open cavity. Results show good agreement in reconstructed and
predictive velocity fields, which showcase the ability of the proposed
framework to evolve the unstructured, dynamic, and mixing SPH field equations
in a low-dimensional subspace. Results also show that the pressure field is
sensitive to the projection error due to the stiff weakly-compressible
assumption made in the current SPH framework, but can be alleviated through
nonlinear approximations, such as the APG approach. Ultimately, the presented
meshless model-order reduction framework marks a step toward enabling drastic
cost savings of SPH simulations.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [23] [Short-Term Gains, Long-Term Gaps: The Impact of GenAI and Search Technologies on Retention](https://arxiv.org/abs/2507.07357)
*Mahir Akgun,Sacip Toker*

Main category: cs.CY

TL;DR: ChatGPT、Google等AI工具可提升学生处理基本任务的即时成绩，但对长期学习和复杂认知任务并无优势，应与有效教学方法结合。


<details>
  <summary>Details</summary>
Motivation: 生成式AI工具（如ChatGPT）在教育中的普及改变了学生获取和使用信息的方式，引发了关于其对学生学习成果与知识保持影响的关注。论文旨在探究这类工具相较于传统工具（搜索引擎与电子教材）对学生不同认知复杂度任务表现的具体作用。

Method: 对123名学生分组实验，使用ChatGPT、Google和电子教材完成Bloom认知分类下三类任务：了解与理解、应用，以及综合、评估与创造。对学生即时表现和之后的知识保留情况进行评估比较。

Result: 在较低阶任务上，ChatGPT和Google组在即时测试中表现优于对照组（电子教材），但这种优势会随着时间推移消失，最终所有组的知识保留表现一致。对于高阶任务，三组间无显著差异，且对照组的保留表现最好。

Conclusion: 生成式AI及搜索工具可提升即时低阶认知任务表现，但对长期知识保持无显著促进，甚至在高阶任务中不及或不优于传统学习方式。仅依赖AI工具难以巩固长期知识，需要结合结构化教学策略实现深度认知。

Abstract: The rise of Generative AI (GenAI) tools, such as ChatGPT, has transformed how
students access and engage with information, raising questions about their
impact on learning outcomes and retention. This study investigates how GenAI
(ChatGPT), search engines (Google), and e-textbooks influence student
performance across tasks of varying cognitive complexity, based on Bloom's
Taxonomy. Using a sample of 123 students, we examined performance in three
tasks: [1] knowing and understanding, [2] applying, and [3] synthesizing,
evaluating, and creating. Results indicate that ChatGPT and Google groups
outperformed the control group in immediate assessments for lower-order
cognitive tasks, benefiting from quick access to structured information.
However, their advantage diminished over time, with retention test scores
aligning with those of the e-textbook group. For higher-order cognitive tasks,
no significant differences were observed among groups, with the control group
demonstrating the highest retention. These findings suggest that while
AI-driven tools facilitate immediate performance, they do not inherently
reinforce long-term retention unless supported by structured learning
strategies. The study highlights the need for balanced technology integration
in education, ensuring that AI tools are paired with pedagogical approaches
that promote deep cognitive engagement and knowledge retention.

</details>


### [24] [The Evolution of Scientific Credit: When Authorship Norms Impede Collaboration](https://arxiv.org/abs/2507.07364)
*Toby Handfield,Kevin Zollman*

Main category: cs.CY

TL;DR: 作者通过进化博弈模型，发现学术署名中不敏感于贡献的规范（如资深作者最后、按字母顺序）并非中立，反而阻碍合作和产出。贡献敏感型规范能更好地促进合作，建议学科应重视署名规范对合作和生产力的实际影响。


<details>
  <summary>Details</summary>
Motivation: 不同学科在学术署名规范上的巨大差异，尤其是在署名顺序与贡献之间关系，引发了人们对这些规范如何产生及其对合作行为影响的关注。作者希望通过建模分析，揭示署名规范演化的内在驱动力及其对科学合作的实际作用。

Method: 采用进化博弈理论，建立模型分析署名规范如何演化：一方面分析为什么某些领域会出现不敏感于贡献的署名规范（如资深作者列最后、按字母顺序等）；另一方面分析已形成规范对合作意愿和成功合作的影响。通过两个不同的模型，模拟规范形成条件和合作行为的动态。

Result: 第一，研究发现，当一些研究者因个人地位或声誉承担更大风险时（如拥有大实验室或重大项目的资深学者），更容易促成与贡献无关的署名规范演化，这一现象被称为“红王动态”。第二，结果表明贡献敏感型署名规范更有助于促进合作，反之，不敏感型署名规范会通过“主贡献者不满”及“第二贡献者不满”两种机制，导致系统性协调失效，影响合作质量。

Conclusion: 署名规范不是中性的组织惯例。不敏感于贡献的署名规范实际上阻碍了科学合作，降低了学科整体科研产出。广泛采用如资深作者列最后或按字母排的惯例，可能成为科学合作的制度性阻力。

Abstract: Scientific authorship norms vary dramatically across disciplines, from
contribution-sensitive systems where first author is the greatest contributor
and subsequent author order reflects relative input, to
contribution-insensitive conventions like alphabetical ordering or
senior-author-last. We develop evolutionary game-theoretic models to examine
both how these divergent norms emerge and their subsequent effects on
collaborative behavior. Our first model reveals that contribution-insensitive
norms evolve when researchers who sacrifice positional advantage face the
strongest adaptive pressure -- for example senior authors managing larger
collaboration portfolios or bearing heavier reputational stakes. This "Red
King" dynamic potentially explains why fields in which senior researchers
command large labs, major grants, and extensive collaboration portfolios may
paradoxically evolve conventions that favour junior-author positioning. Our
second model demonstrates that established norms influence researchers'
willingness to collaborate, with contribution-sensitive norms consistently
outperforming insensitive alternatives in fostering successful partnerships.
Contribution-insensitive norms create systematic coordination failures through
two mechanisms: "main contributor resentment" when exceptional work goes
unrecognized, and "second contributor resentment" when comparable efforts
receive unequal credit. These findings suggest that widely adopted practices
like senior-last positioning and alphabetical ordering may function as
institutional frictions that impede valuable scientific collaborations rather
than neutral organizational conventions, potentially reducing overall
scientific productivity across affected disciplines.

</details>


### [25] [Vaccine Hesitancy on YouTube: a Competition between Health and Politics](https://arxiv.org/abs/2507.07517)
*Yelena Mejova,Michele Tizzani*

Main category: cs.CY

TL;DR: 研究系统分析了YouTube上疫苗相关视频的内容与立场，发现反疫苗信息主要由政治评论者传播，且监管有限，为公共健康信息传播与政策制定提供了数据支持。


<details>
  <summary>Details</summary>
Motivation: 随着YouTube成为人们获取信息的重要平台，平台上健康相关（尤其是疫苗接种）信息的质量直接影响公众健康安全。本研究关注疫苗接种相关视频的发布与内容，特别是抗疫苗内容的传播与监管。

Method: 研究通过为期3个月的系统性、每日收集所有与疫苗接种相关的新发布YouTube视频的方法，对其来源、内容和立场进行了细致分析。

Result: 1）反疫苗内容多由社会及政治评论者发布，且更常提及政客和媒体，支持疫苗的内容则更聚焦具体疾病或健康议题。2）仅有2.7%的涉疫苗视频被下架，而20.8%的视频持有疫苗犹豫立场，显示平台对相关内容的调控活动有限。

Conclusion: 高质量健康信息的可获得性对提升公共健康干预的认知与遵从至关重要。本研究揭示了YouTube疫苗信息传播中的主要创作者类型及其立场，为公共健康传播政策提供了重要参考。

Abstract: YouTube has rapidly emerged as a predominant platform for content
consumption, effectively displacing conventional media such as television and
news outlets. A part of the enormous video stream uploaded to this platform
includes health-related content, both from official public health
organizations, and from any individual or group that can make an account. The
quality of information available on YouTube is a critical point of public
health safety, especially when concerning major interventions, such as
vaccination. This study differentiates itself from previous efforts of auditing
YouTube videos on this topic by conducting a systematic daily collection of
posted videos mentioning vaccination for the duration of 3 months. We show that
the competition for the public's attention is between public health messaging
by institutions and individual educators on one side, and commentators on
society and politics on the other, the latest contributing the most to the
videos expressing stances against vaccination. Videos opposing vaccination are
more likely to mention politicians and publication media such as podcasts,
reports, and news analysis, on the other hand, videos in favor are more likely
to mention specific diseases or health-related topics. Finally, we find that,
at the time of analysis, only 2.7% of the videos have been taken down (by the
platform or the channel), despite 20.8% of the collected videos having a
vaccination hesitant stance, pointing to a lack of moderation activity for
hesitant content. The availability of high-quality information is essential to
improve awareness and compliance with public health interventions. Our findings
help characterize the public discourse around vaccination on one of the largest
media platforms, disentangling the role of the different creators and their
stances, and as such, they provide important insights for public health
communication policy.

</details>


### [26] [AI Human Impact: Toward a Model for Ethical Investing in AI-Intensive Companies](https://arxiv.org/abs/2507.07703)
*James Brusseau*

Main category: cs.CY

TL;DR: 现有ESG评价不适用于AI公司。本文设计九项AI伦理表现指标，建立专门评估体系，帮助投资者依其价值观做出投资选择，推动AI企业更以人为本。


<details>
  <summary>Details</summary>
Motivation: 伴随着AI技术渗透企业核心业务，投资者面临AI是否以人为本、人应否适应AI等伦理抉择。现有ESG评估方法不能全面捕捉AI企业的核心伦理问题，需创新投资评估方法。

Method: 提出九项表现指标，从人本导向的角度分析和打分AI技术在企业中的表现，并基于AI伦理原则定制专属评估模型。

Result: 建立了一套专为AI密集型公司设计的伦理评估体系，使分析师和投资者能够根据人本原则和自身价值观做出更理性、客观和具可信度的投资决策。

Conclusion: 传统的环境、社会及治理（ESG）框架，对以AI为核心的公司而言已经不足，需要结合AI伦理原则制定专门的评估指标，为投资者提供更符合其价值观的投资指导。

Abstract: Does AI conform to humans, or will we conform to AI? An ethical evaluation of
AI-intensive companies will allow investors to knowledgeably participate in the
decision. The evaluation is built from nine performance indicators that can be
analyzed and scored to reflect a technology's human-centering. The result is
objective investment guidance, as well as investors empowered to act in
accordance with their own values. Incorporating ethics into financial decisions
is a strategy that will be recognized by participants in environmental, social,
and governance investing, however, this paper argues that conventional ESG
frameworks are inadequate to companies that function with AI at their core.
Fully accounting for contemporary big data, predictive analytics, and machine
learning requires specialized metrics customized from established AI ethics
principles. With these metrics established, the larger goal is a model for
humanist investing in AI-intensive companies that is intellectually robust,
manageable for analysts, useful for portfolio managers, and credible for
investors.

</details>


### [27] [Distributed and Decentralised Training: Technical Governance Challenges in a Shifting AI Landscape](https://arxiv.org/abs/2507.07765)
*Jakub Kryś,Yashvardhan Sharma,Janet Egan*

Main category: cs.CY

TL;DR: 本文区分分布式与去中心化AI训练，分析其对算力治理、能力扩散与政策措施的影响，并呼吁制定更精准的相关政策以应对新挑战。


<details>
  <summary>Details</summary>
Motivation: 由于低通信训练算法的发展，模型训练正从集中式向分布式或去中心化（社区驱动）转变，然而分布式与去中心化常被混淆，且政策讨论中对此理解不足。本论文旨在区分这两种情景，并分析其对AI治理的影响。

Method: 论文主要通过文献梳理和概念区分，讨论分布式与去中心化训练对AI治理（如算力结构、能力扩散、可检测性与可关停性）的潜在影响，并结合政策措施分析两者利弊。

Result: 作者指出去中心化AI训练可能削弱现有的算力治理机制，如增加能力扩散、降低系统可检测性与可关停性，但出口管制等政策仍具一定效力。同时，去中心化带来隐私提升、缓解权力集中等潜在正面影响。

Conclusion: 分布式与去中心化训练在技术与治理层面存在重大差异，对未来算力治理和政策制定提出了新的挑战和机遇。需更细致的政策工具以应对能力扩散与算力治理问题，确保AI安全、可靠发展。

Abstract: Advances in low-communication training algorithms are enabling a shift from
centralised model training to compute setups that are either distributed across
multiple clusters or decentralised via community-driven contributions. This
paper distinguishes these two scenarios - distributed and decentralised
training - which are little understood and often conflated in policy discourse.
We discuss how they could impact technical AI governance through an increased
risk of compute structuring, capability proliferation, and the erosion of
detectability and shutdownability. While these trends foreshadow a possible new
paradigm that could challenge key assumptions of compute governance, we
emphasise that certain policy levers, like export controls, remain relevant. We
also acknowledge potential benefits of decentralised AI, including
privacy-preserving training runs that could unlock access to more data, and
mitigating harmful power concentration. Our goal is to support more precise
policymaking around compute, capability proliferation, and decentralised AI
development.

</details>


### [28] [Structured Prompts, Better Outcomes? Exploring the Effects of a Structured Interface with ChatGPT in a Graduate Robotics Course](https://arxiv.org/abs/2507.07767)
*Jerome Brender,Laila El-Hamamsy,Kim Uittenhove,Francesco Mondada,Engin Bumbacher*

Main category: cs.CY

TL;DR: 结构化提示平台能暂时改善提问行为，但难以持久影响学生学习或习惯，提示应更多关注学生动机和持续引导。


<details>
  <summary>Details</summary>
Motivation: 已有研究表明学生与大型语言模型的互动方式影响其学习和问题解决能力，因此亟需探索如何引导学生更有效地使用LLM以提升学习表现。

Method: 将58名研究生分为实验组（使用结构化GPT平台）和对照组（自由使用ChatGPT），在三次实验课中对两组学生的表现、提问日志、学习成绩和主观感受进行数据分析。

Result: 实验组确实表现出更聚焦于理解代码等高质量提问行为，这与更高的学习增益相关。但这种行为在可自由使用后并未保持，且两组在总体成绩和学习提升上无显著差异。学生对于结构化平台的接受度不高。

Conclusion: 尽管结构化GPT平台能够促进更优质的提问行为，但学生在不受平台结构限制后，这些行为并未持续。同时，多数学生对结构化平台的价值持保留态度。仅通过临时改变界面引导学生，难以有效和持久地提升学习表现。

Abstract: Prior research shows that how students engage with Large Language Models
(LLMs) influences their problem-solving and understanding, reinforcing the need
to support productive LLM-uses that promote learning. This study evaluates the
impact of a structured GPT platform designed to promote 'good' prompting
behavior with data from 58 students in a graduate-level robotics course. The
students were assigned to either an intervention group using the structured
platform or a control group using ChatGPT freely for two practice lab sessions,
before a third session where all students could freely use ChatGPT. We analyzed
student perception (pre-post surveys), prompting behavior (logs), performance
(task scores), and learning (pre-post tests). Although we found no differences
in performance or learning between groups, we identified prompting behaviors -
such as having clear prompts focused on understanding code - that were linked
with higher learning gains and were more prominent when students used the
structured platform. However, such behaviors did not transfer once students
were no longer constrained to use the structured platform. Qualitative survey
data showed mixed perceptions: some students perceived the value of the
structured platform, but most did not perceive its relevance and resisted
changing their habits. These findings contribute to ongoing efforts to identify
effective strategies for integrating LLMs into learning and question the
effectiveness of bottom-up approaches that temporarily alter user interfaces to
influence students' interaction. Future research could instead explore top-down
strategies that address students' motivations and explicitly demonstrate how
certain interaction patterns support learning.

</details>


### [29] [Opting Out of Generative AI: a Behavioral Experiment on the Role of Education in Perplexity AI Avoidance](https://arxiv.org/abs/2507.07881)
*Roberto Ulloa,Juhi Kulshrestha,Celina Kacperski*

Main category: cs.CY

TL;DR: 会话式AI工具（如Perplexity AI）存在显著的学历相关回避现象，低学历人群使用意愿更低。研究表明学历在AI采纳中的重要影响，提示AI技术设计需考虑包容性，以防止数字鸿沟扩大。


<details>
  <summary>Details</summary>
Motivation: 随着会话式AI的普及，人们获取和交互数字信息的方式正在发生变化，但这些工具可能加剧现有的数字鸿沟。本文关注不同学历人群对会话式AI的回避行为及其影响因素。

Method: 通过一项包含1636名参与者的在线实验，将参与者随机分配至三组（控制组、传统搜索组、会话式AI组）。通过结构方程建模（基于UTAUT2理论框架）和LASSO回归，分析学历与AI回避的关联。任务回避定义为问卷放弃或在任务分配时提交无关答案。

Result: 会话式AI组的任务回避率（51%）显著高于传统搜索组（30.9%）和控制组（16.8%），低学历参与者在会话式AI组的回避率高达74.4%。结构方程模型和LASSO回归均显示，即使控制了技术采纳相关的认知和情感变量，学历与AI回避之间仍有显著关联。

Conclusion: 学历水平在AI采纳和回避中起核心作用。研究强调了在相关研究中自选择偏差的重要性，以及新兴技术普及中包容性设计的重要性，以确保公平获取。

Abstract: The rise of conversational AI (CAI), powered by large language models, is
transforming how individuals access and interact with digital information.
However, these tools may inadvertently amplify existing digital inequalities.
This study investigates whether differences in formal education are associated
with CAI avoidance, leveraging behavioral data from an online experiment (N =
1,636). Participants were randomly assigned to a control or an
information-seeking task, either a traditional online search or a CAI
(Perplexity AI). Task avoidance (operationalized as survey abandonment or
providing unrelated responses during task assignment) was significantly higher
in the CAI group (51%) compared to the search (30.9%) and control (16.8%)
groups, with the highest CAI avoidance among participants with lower education
levels (~74.4%). Structural equation modeling based on the theoretical
framework UTAUT2 and LASSO regressions reveal that education is strongly
associated with CAI avoidance, even after accounting for various cognitive and
affective predictors of technology adoption. These findings underscore
education's central role in shaping AI adoption and the role of self-selection
biases in AI-related research, stressing the need for inclusive design to
ensure equitable access to emerging technologies.

</details>
