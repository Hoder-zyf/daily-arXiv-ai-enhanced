<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 19]
- [cs.CL](#cs.CL) [Total: 42]
- [cs.CE](#cs.CE) [Total: 1]
- [cs.CY](#cs.CY) [Total: 6]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [A collaborative digital twin built on FAIR data and compute infrastructure](https://arxiv.org/abs/2507.00048)
*Thomas M. Deucher,Juan C. Verduzco,Michael Titus,Alejandro Strachan*

Main category: cs.AI

TL;DR: 本文提出并实现了一个基于nanoHUB与FAIR数据管理的分布式自驱动实验室架构，支持地理分散的研究者协同进行自动化优化实验，通过主动学习不断改进机器学习模型，并可广泛应用于多种优化任务。


<details>
  <summary>Details</summary>
Motivation: 提高自驱动实验室在科学和工程发现与优化任务中的效率，实现研究者之间更加高效的数据共享和协作，充分利用FAIR数据基础设施，推动ML与自动化实验整合。

Method: 开发基于nanoHUB的在线仿真与FAIR数据管理平台，通过网络界面提交实验数据，自动用Sim2L进行数据处理，并将结果索引到ResultsDB。优化采用主动学习方法，支持顺序优化，实时训练机器学习模型指导实验。采用“节俭孪生”思想，将实际的优化任务具体化为混合食用色素实现目标色彩。

Result: 实现了一个分布式、自更新的SDL与FAIR数据平台，研究者和学生可低成本进行实验，实时共享数据与优化成果，提高协作与效率。该平台和工具可推广至其他优化场景。

Conclusion: 提出了一种基于nanoHUB服务的分布式自驱动实验室实现框架，能够促进地理上分散的研究者在科学与工程优化任务上的协作，并且此工具具有广泛的适用性。

Abstract: The integration of machine learning with automated experimentation in
self-driving laboratories (SDL) offers a powerful approach to accelerate
discovery and optimization tasks in science and engineering applications. When
supported by findable, accessible, interoperable, and reusable (FAIR) data
infrastructure, SDLs with overlapping interests can collaborate more
effectively. This work presents a distributed SDL implementation built on
nanoHUB services for online simulation and FAIR data management. In this
framework, geographically dispersed collaborators conducting independent
optimization tasks contribute raw experimental data to a shared central
database. These researchers can then benefit from analysis tools and machine
learning models that automatically update as additional data become available.
New data points are submitted through a simple web interface and automatically
processed using a nanoHUB Sim2L, which extracts derived quantities and indexes
all inputs and outputs in a FAIR data repository called ResultsDB. A separate
nanoHUB workflow enables sequential optimization using active learning, where
researchers define the optimization objective, and machine learning models are
trained on-the-fly with all existing data, guiding the selection of future
experiments. Inspired by the concept of ``frugal twin", the optimization task
seeks to find the optimal recipe to combine food dyes to achieve the desired
target color. With easily accessible and inexpensive materials, researchers and
students can set up their own experiments, share data with collaborators, and
explore the combination of FAIR data, predictive ML models, and sequential
optimization. The tools introduced are generally applicable and can easily be
extended to other optimization problems.

</details>


### [2] [DiMo-GUI: Advancing Test-time Scaling in GUI Grounding via Modality-Aware Visual Reasoning](https://arxiv.org/abs/2507.00008)
*Hang Wu,Hongkai Chen,Yujun Cai,Chang Liu,Qingwen Ye,Ming-Hsuan Yang,Yiwei Wang*

Main category: cs.AI

TL;DR: DiMo-GUI是一种无需训练的新框架，通过模态分离和递进式候选关注区策略，能在复杂GUI界面下更好地将自然语言查询定位到对应元素，效果优于传统推理方法。


<details>
  <summary>Details</summary>
Motivation: 在图形用户界面（GUI）中进行自然语言查询的定位（grounding）存在诸多挑战，例如视觉元素的多样性、空间杂乱以及语言的歧义性。需要一种有效且无需额外训练的数据驱动方法来提升GUI中的自然语言理解和操作能力。

Method: 提出DiMo-GUI训练自由框架。其核心方法包括：1）动态视觉定向（dynamic visual grounding），2）模态感知优化（modality-aware optimization）。这两者结合将GUI的输入分为文本元素和图标元素，分别用通用视觉-语言模型处理。为解决预测不准或歧义，DiMo-GUI会根据初步预测动态生成候选关注区，通过递进式缩放（incrementally zooms into subregions）细化定位，采用分层细化方式消除视觉拥挤和歧义。全流程无需额外训练或标注数据。

Result: 在标准GUI grounding基准数据集上，该方法相较传统的推断流程取得了一致性的性能提升，显示了模态分离与区域关注推理相结合的高效性。

Conclusion: 通过将模态分离与区域自适应细化推理结合，DiMo-GUI可以在不需要额外数据或训练的情况下，有效提升GUI中自然语言查询定位的准确性和鲁棒性。

Abstract: Grounding natural language queries in graphical user interfaces (GUIs) poses
unique challenges due to the diversity of visual elements, spatial clutter, and
the ambiguity of language. In this paper, we introduce DiMo-GUI, a
training-free framework for GUI grounding that leverages two core strategies:
dynamic visual grounding and modality-aware optimization. Instead of treating
the GUI as a monolithic image, our method splits the input into textual
elements and iconic elements, allowing the model to reason over each modality
independently using general-purpose vision-language models. When predictions
are ambiguous or incorrect, DiMo-GUI dynamically focuses attention by
generating candidate focal regions centered on the model's initial predictions
and incrementally zooms into subregions to refine the grounding result. This
hierarchical refinement process helps disambiguate visually crowded layouts
without the need for additional training or annotations. We evaluate our
approach on standard GUI grounding benchmarks and demonstrate consistent
improvements over baseline inference pipelines, highlighting the effectiveness
of combining modality separation with region-focused reasoning.

</details>


### [3] [TalentMine: LLM-Based Extraction and Question-Answering from Multimodal Talent Tables](https://arxiv.org/abs/2507.00041)
*Varun Mannam,Fang Wang,Chaochun Liu,Xin Chen*

Main category: cs.AI

TL;DR: 本论文提出了用于人才管理系统表格语义增强的TalentMine框架，通过多模态LLM推理实现100%问答正确率，显著提升人才管理中的信息检索与决策能力。


<details>
  <summary>Details</summary>
Motivation: 现有的人才管理系统中，关键信息常以复杂表格形式存在，传统语言模型难以精准检索与理解这些信息，特别是在解析表格语义关系时表现不佳，影响决策和问答准确性。

Method: 提出了TalentMine框架，利用大模型进行表格的结构与语义增强，通过专用多模态推理将提取的表格转化为语义丰富的表示，并实现高效集成到检索增强系统中。

Result: 在员工福利文档测试中，TalentMine在问答任务上达到100%正确率，远超标准AWS Textract（0%）与AWS Textract Visual Q&A（40%）；还发现Claude v3 Haiku在人才管理任务上表现最佳。

Conclusion: TalentMine有效解决了当前表格抽取流程中语义信息丢失的瓶颈，大幅提升了人才信息表格的检索与应用能力，并为后续系统一体化与任务基准测试提供了新方法和数据。

Abstract: In talent management systems, critical information often resides in complex
tabular formats, presenting significant retrieval challenges for conventional
language models. These challenges are pronounced when processing Talent
documentation that requires precise interpretation of tabular relationships for
accurate information retrieval and downstream decision-making. Current table
extraction methods struggle with semantic understanding, resulting in poor
performance when integrated into retrieval-augmented chat applications. This
paper identifies a key bottleneck - while structural table information can be
extracted, the semantic relationships between tabular elements are lost,
causing downstream query failures. To address this, we introduce TalentMine, a
novel LLM-enhanced framework that transforms extracted tables into semantically
enriched representations. Unlike conventional approaches relying on CSV or text
linearization, our method employs specialized multimodal reasoning to preserve
both structural and semantic dimensions of tabular data. Experimental
evaluation across employee benefits document collections demonstrates
TalentMine's superior performance, achieving 100% accuracy in query answering
tasks compared to 0% for standard AWS Textract extraction and 40% for AWS
Textract Visual Q&A capabilities. Our comparative analysis also reveals that
the Claude v3 Haiku model achieves optimal performance for talent management
applications. The key contributions of this work include (1) a systematic
analysis of semantic information loss in current table extraction pipelines,
(2) a novel LLM-based method for semantically enriched table representation,
(3) an efficient integration framework for retrieval-augmented systems as
end-to-end systems, and (4) comprehensive benchmarks on talent analytics tasks
showing substantial improvements across multiple categories.

</details>


### [4] [SEZ-HARN: Self-Explainable Zero-shot Human Activity Recognition Network](https://arxiv.org/abs/2507.00050)
*Devin Y. De Silva,Sandareka Wickramanayake,Dulani Meedeniya,Sanka Rasnayaka*

Main category: cs.AI

TL;DR: 该文提出了一种可自解释的零样本人类行为识别模型SEZ-HARN，不仅准确率接近最优，还能通过骨骼视频解释其决策，提高了模型的透明度和实际应用价值。


<details>
  <summary>Details</summary>
Motivation: 现实世界中，基于IMU的HAR在人类行为识别中的应用受到数据集覆盖范围有限和模型缺乏可解释性的限制。目前的零样本HAR虽然缓解了数据限制问题，但模型的决策过程不透明，难以解释。

Method: 提出一种自解释零样本人类行为识别网络（SEZ-HARN），能够在训练时未见过的活动上做出识别，并生成骨骼视频来解释其识别决策。该方法在四个基准数据集上与三种最先进的黑盒ZS-HAR模型进行性能对比。

Result: 实验结果表明，SEZ-HARN不仅能够生成真实且可理解的解释，还在零样本识别准确率上达到与最优黑盒模型相当的水平。在PAMAP2上，其准确率与最佳模型相差不超过3%，在其他三个数据集上的表现也具有可比性。

Conclusion: SEZ-HARN在保持高准确率的同时，显著提升了模型的可解释性，推动了IMU基础的零样本人类行为识别的实际应用。

Abstract: Human Activity Recognition (HAR), which uses data from Inertial Measurement
Unit (IMU) sensors, has many practical applications in healthcare and assisted
living environments. However, its use in real-world scenarios has been limited
by the lack of comprehensive IMU-based HAR datasets that cover a wide range of
activities and the lack of transparency in existing HAR models. Zero-shot HAR
(ZS-HAR) overcomes the data limitations, but current models struggle to explain
their decisions, making them less transparent. This paper introduces a novel
IMU-based ZS-HAR model called the Self-Explainable Zero-shot Human Activity
Recognition Network (SEZ-HARN). It can recognize activities not encountered
during training and provide skeleton videos to explain its decision-making
process. We evaluate the effectiveness of the proposed SEZ-HARN on four
benchmark datasets PAMAP2, DaLiAc, HTD-MHAD and MHealth and compare its
performance against three state-of-the-art black-box ZS-HAR models. The
experiment results demonstrate that SEZ-HARN produces realistic and
understandable explanations while achieving competitive Zero-shot recognition
accuracy. SEZ-HARN achieves a Zero-shot prediction accuracy within 3\% of the
best-performing black-box model on PAMAP2 while maintaining comparable
performance on the other three datasets.

</details>


### [5] [Enhancing Reasoning Capabilities in SLMs with Reward Guided Dataset Distillation](https://arxiv.org/abs/2507.00054)
*Shreyansh Padarha*

Main category: cs.AI

TL;DR: 本文提出了基于奖励引导的AdvDistill数据集蒸馏框架，通过为每个教师响应赋予奖励权重，有效提升了小型语言模型在复杂推理任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 将大型语言模型（LLM）的能力压缩并转移到更高效、易部署的小型语言模型（SLM）中，当前主要依赖知识蒸馏（KD）技术，但传统KD方法在推理任务上的泛化能力有限，且计算成本高。

Method: 提出了一种基于奖励引导的数据集蒸馏框架AdvDistill。该方法针对每个输入提示，从教师模型生成多个响应，并通过基于规则的验证器为每个响应分配奖励分数。这些奖励作为权重，用于训练学生模型。

Result: 引入奖励机制后，学生模型在数学和复杂推理任务上表现有显著提升。该框架有效提升了学生模型的性能和泛化能力。

Conclusion: 在数据集蒸馏过程中融合奖励机制，可以显著提升知识蒸馏效率与下游任务的推理能力，证明了方法的有效性。

Abstract: The push to compress and impart the proficiency of Large Language Models
(LLMs) into more deployable and efficient Small Language Models (SLMs) has
benefited from improvements in knowledge distillation (KD) techniques. These
techniques allow a smaller student model to learn from a more capable and
larger teacher model's responses. However, distillation often revolves around
the student model merely copying the teacher's in-distribution responses,
limiting its generalisability. This limitation is amplified on reasoning tasks
and can be computationally expensive. In this study, we propose AdvDistill, a
reward-guided dataset distillation framework. We utilise multiple generations
(responses) from a teacher for each prompt and assign rewards based on
rule-based verifiers. These varying and normally distributed rewards serve as
weights when training student models. Our methods and their subsequent
behavioural analysis demonstrate a significant improvement in student model
performance for mathematical and complex reasoning tasks, showcasing the
efficacy and benefits of incorporating a rewarding mechanism in dataset
distillation processes.

</details>


### [6] [VoyagerVision: Investigating the Role of Multi-modal Information for Open-ended Learning Systems](https://arxiv.org/abs/2507.00079)
*Ethan Smyth,Alessandro Suglia*

Main category: cs.AI

TL;DR: 通过让大语言模型接收视觉输入，VoyagerVision 在Minecraft中展现出更强的环境解析与结构构建能力，成功扩展了模型的开放性任务执行范围，尤其在简单环境中表现突出，复杂任务仍有挑战。


<details>
  <summary>Details</summary>
Motivation: 为推动人工通用智能(AGI)的开放性研究，希望使模型能够自主发现和完成更多未知任务。受最新多模态大语言模型(如GPT-4o)能力启发，提出增加视觉输入以提升模型感知能力与任务解决广度。

Method: 通过为多模态语言模型(基于LLM)提供智能体POV像素级视觉输入，使模型能够更好地解析环境，进而在 Minecraft 环境下完成结构构建任务。模型能力通过与 Voyager 的对比实验、在不同复杂度环境下的任务完成率测试进行评估。

Result: VoyagerVision 能在50次迭代内平均创造2.75个不同结构，而原始Voyager无法做到。在构建单元测试中，在简单环境下成功率达50%，在更复杂环境下大多失败。表明视觉输入有效拓展了开放性潜力，但在复杂任务上仍有待提升。

Conclusion: VoyagerVision 多模态模型能够利用截图等视觉反馈，显著提升了智能体在空间环境中的理解与任务完成能力。相比基础的 Voyager，本模型在创造独特结构和完成构建任务上有明显突破，尤其是在简单环境(flat worlds)中表现优异。

Abstract: Open-endedness is an active field of research in the pursuit of capable
Artificial General Intelligence (AGI), allowing models to pursue tasks of their
own choosing. Simultaneously, recent advancements in Large Language Models
(LLMs) such as GPT-4o [9] have allowed such models to be capable of
interpreting image inputs. Implementations such as OMNI-EPIC [4] have made use
of such features, providing an LLM with pixel data of an agent's POV to parse
the environment and allow it to solve tasks. This paper proposes that providing
these visual inputs to a model gives it greater ability to interpret spatial
environments, and as such, can increase the number of tasks it can successfully
perform, extending its open-ended potential. To this aim, this paper proposes
VoyagerVision -- a multi-modal model capable of creating structures within
Minecraft using screenshots as a form of visual feedback, building on the
foundation of Voyager. VoyagerVision was capable of creating an average of 2.75
unique structures within fifty iterations of the system, as Voyager was
incapable of this, it is an extension in an entirely new direction.
Additionally, in a set of building unit tests VoyagerVision was successful in
half of all attempts in flat worlds, with most failures arising in more complex
structures. Project website is available at
https://esmyth-dev.github.io/VoyagerVision.github.io/

</details>


### [7] [Thinking About Thinking: SAGE-nano's Inverse Reasoning for Self-Aware Language Models](https://arxiv.org/abs/2507.00092)
*Basab Jha,Firoj Paudel,Ujjwal Puri,Zhang Yuting,Choi Donghyuk,Wang Junhao*

Main category: cs.AI

TL;DR: 本文创新性提出让模型“逆向思考”解释自身推理链的逆向推理框架，实现了更优的推理准确性和原因解释能力，成为提高AI透明度和安全性的重要突破。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型虽然能够通过Chain-of-Thought（CoT）链式推理解决复杂任务，但其推理过程仍然是“黑箱”，难以解释和理解。作者希望提升AI系统的推理可解释性和透明度，弥补AI决策透明性、可信度、AI安全和可持续科学发现等核心领域的不足。

Method: 提出了一种新的范式——逆向推理（inverse reasoning），使得大语言模型能够在输出推理链后追溯并解释自己的推理步骤。具体采用了一种元认知结构，利用注意力机制反向回溯主要决策点，并生成对推理选择的解释。这一方法实现于SAGE-nano（40亿参数模型）上，并提出了新的逆向关注流动和元学习框架，并构建了综合性评价框架。

Result: 通过在AQUA-RAT、CommonsenseQA及自定义基准上的逻辑推理、数学问题与伦理困境测试，SAGE-nano在推理准确率（AQUA-RAT 74.6%）和解释质量（92.1%人类偏好得分）方面均达到了新高度，表现接近Claude-3.5 Sonnet与GPT-4o等大型模型。实验证明逆向推理能够同时提升模型可解释性和推理表现。

Conclusion: 作者首次系统性提出、实现并验证了LLM自我反思的逆向推理框架，显著提升了推理透明性和可解释性，并推动了AI安全、教育与科学发现领域的进步。该工作为打造更透明、可信的AI系统指明了新方向。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities at
solving complex reasoning tasks with Chain-of-Thought (CoT) prompting, but
their decision-making processes remain somewhat blackbox. We introduce
textbfinverse reasoning, a novel paradigm enabling LLMs to decompose and
explain their own reasoning chains post-hoc. Our approach, used in SAGE-nano, a
4-billion-parameter reasoning model, employs a metacognitive structure that
reflects back via attention processes to identify major decision points and
generate explanations of reasoning choices. While typical CoT approaches are
directed towards forward reasoning generation, inverse reasoning provides
insight into why specific reasoning chains were selected over others. Through
thorough testing of logical reasoning puzzles, math problems and ethical
dilemmas from AQUA-RAT, CommonsenseQA, and customized benchmarks, we
demonstrate that SAGE-nano is at the cutting edge both on reasoning accuracy
(74.6% on AQUA-RAT) and explanation quality (92.1% human preference score) for
its task, and offers performance almost on par with models like Claude-3.5
Sonnet or GPT-4o. Our contributions are: (i) the first rigorous framework for
LLM self-reflection via inverse reasoning, (ii) a novel metalearning framework
to reverse the attention flow, (iii) comprehensive evaluation frameworks for
reasoning transparency, and (iv) evidence that increasing reasoning using
inverse reasoning improves interpretability along with reasoning performance.
Our work creates new avenues for transparent AI systems and closes significant
gaps in AI safety, education, and scientific discovery.

</details>


### [8] [BlackBoxToBlueprint: Extracting Interpretable Logic from Legacy Systems using Reinforcement Learning and Counterfactual Analysis](https://arxiv.org/abs/2507.00180)
*Vidhi Rathore*

Main category: cs.AI

TL;DR: 本文提出用强化学习和决策树自动提取遗留黑盒系统的决策逻辑，并在多种场景下取得了可解释、准确的结果，为遗留系统现代化提供有力支撑。


<details>
  <summary>Details</summary>
Motivation: 现代化改造遗留软件系统十分重要，但通常受限于对原有复杂决策逻辑的理解不足，且缺乏文档支持。传统方法如行为克隆仅能复制输入输出行为，无法还原底层决策意图。

Method: 该文提出一种新颖管道，将遗留系统视为黑盒，利用强化学习（RL）智能体探索输入空间，通过奖励使输出发生有意义变化的操作，从而定位关键决策边界。这些导致输出变化的反事实状态被收集后，利用K-Means聚类，并在聚类结果上训练决策树，以获得可解释、近似系统决策逻辑的规则。

Result: 在三种不同复杂度的虚拟遗留系统（包括基于阈值、组合条件和非线性范围逻辑）上验证了该管道的有效性。强化学习智能体能聚焦探索关键边界区域，提取的规则高度反映了底层系统的核心逻辑。

Conclusion: 该方法为生成遗留系统迁移过程中的规范与测试用例提供了有希望的基础，实现了对复杂黑盒系统决策逻辑的自动提取和可解释化近似。

Abstract: Modernizing legacy software systems is a critical but challenging task, often
hampered by a lack of documentation and understanding of the original system's
intricate decision logic. Traditional approaches like behavioral cloning merely
replicate input-output behavior without capturing the underlying intent. This
paper proposes a novel pipeline to automatically extract interpretable decision
logic from legacy systems treated as black boxes. The approach uses a
Reinforcement Learning (RL) agent to explore the input space and identify
critical decision boundaries by rewarding actions that cause meaningful changes
in the system's output. These counterfactual state transitions, where the
output changes, are collected and clustered using K-Means. Decision trees are
then trained on these clusters to extract human-readable rules that approximate
the system's decision logic near the identified boundaries. I demonstrated the
pipeline's effectiveness on three dummy legacy systems with varying complexity,
including threshold-based, combined-conditional, and non-linear range logic.
Results show that the RL agent successfully focuses exploration on relevant
boundary regions, and the extracted rules accurately reflect the core logic of
the underlying dummy systems, providing a promising foundation for generating
specifications and test cases during legacy migration.

</details>


### [9] [ChatGPT produces more "lazy" thinkers: Evidence of cognitive engagement decline](https://arxiv.org/abs/2507.00181)
*Georgios P. Georgiou*

Main category: cs.AI

TL;DR: 生成式AI（如ChatGPT）辅助学术写作会显著降低学生的认知投入，教育实践应注意引导学生主动深度参与AI内容，避免认知能力受损。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）在教育领域的广泛应用，人们担心其会削弱学生的深度思考和主动学习能力。该研究旨在探究生成式人工智能（AI）工具，特别是ChatGPT，在学术写作任务中对学生认知投入的影响。

Method: 研究采用了实验设计，将参与者随机分配到AI辅助（ChatGPT）组和非辅助（对照）组。参与者完成结构化的议论文写作任务，并填写用于评估心理努力、注意力、深度加工和策略性思考的认知投入量表（CES-AI）。

Result: 结果显示，ChatGPT组的认知投入评分显著低于对照组。这表明AI辅助可能导致认知卸载现象。

Conclusion: AI工具在学术写作中可能降低学生的认知投入，对教育心理学影响值得关注。研究呼吁在教学中采用促进学生主动反思和深度参与AI生成内容的策略，以防影响自我调节学习和深度认知。

Abstract: Despite the increasing use of large language models (LLMs) in education,
concerns have emerged about their potential to reduce deep thinking and active
learning. This study investigates the impact of generative artificial
intelligence (AI) tools, specifically ChatGPT, on the cognitive engagement of
students during academic writing tasks. The study employed an experimental
design with participants randomly assigned to either an AI-assisted (ChatGPT)
or a non-assisted (control) condition. Participants completed a structured
argumentative writing task followed by a cognitive engagement scale (CES), the
CES-AI, developed to assess mental effort, attention, deep processing, and
strategic thinking. The results revealed significantly lower cognitive
engagement scores in the ChatGPT group compared to the control group. These
findings suggest that AI assistance may lead to cognitive offloading. The study
contributes to the growing body of literature on the psychological implications
of AI in education and raises important questions about the integration of such
tools into academic practice. It calls for pedagogical strategies that promote
active, reflective engagement with AI-generated content to avoid compromising
self-regulated learning and deep cognitive involvement of students.

</details>


### [10] [Holistic Artificial Intelligence in Medicine; improved performance and explainability](https://arxiv.org/abs/2507.00205)
*Periklis Petridis,Georgios Margaritis,Vasiliki Stoumpou,Dimitris Bertsimas*

Main category: cs.AI

TL;DR: 本文提出可解释的多模态医学AI框架xHAIM，不仅大幅提升预测效果，还能追溯并解释AI决策，推动AI临床落地。


<details>
  <summary>Details</summary>
Motivation: 原始HAIM系统虽能处理多模态数据，但无任务定向性且缺乏解释性，限制了医学AI在临床中的实际应用价值。

Method: 提出xHAIM（Explainable HAIM）新框架，包括四步：1. 自动识别与任务相关的多模态病人数据；2. 生成综合性病人摘要；3. 用摘要提升预测建模；4. 提供以病人知识为基础的临床解释，并将预测与患者特定信息关联。

Result: 在HAIM-MIMIC-MM数据集上，xHAIM在胸部病理和手术任务中将平均AUC由79.9%提升到90.3%，并实现了结果的可解释追溯。

Conclusion: xHAIM显著提升了多模态医学AI的预测性能与可解释性，使得AI从“黑盒”转变为可溯源、可解释的决策支持系统，有助于临床应用。

Abstract: With the increasing interest in deploying Artificial Intelligence in
medicine, we previously introduced HAIM (Holistic AI in Medicine), a framework
that fuses multimodal data to solve downstream clinical tasks. However, HAIM
uses data in a task-agnostic manner and lacks explainability. To address these
limitations, we introduce xHAIM (Explainable HAIM), a novel framework
leveraging Generative AI to enhance both prediction and explainability through
four structured steps: (1) automatically identifying task-relevant patient data
across modalities, (2) generating comprehensive patient summaries, (3) using
these summaries for improved predictive modeling, and (4) providing clinical
explanations by linking predictions to patient-specific medical knowledge.
Evaluated on the HAIM-MIMIC-MM dataset, xHAIM improves average AUC from 79.9%
to 90.3% across chest pathology and operative tasks. Importantly, xHAIM
transforms AI from a black-box predictor into an explainable decision support
system, enabling clinicians to interactively trace predictions back to relevant
patient data, bridging AI advancements with clinical utility.

</details>


### [11] [Learning for routing: A guided review of recent developments and future directions](https://arxiv.org/abs/2507.00218)
*Fangting Zhou,Attila Lischka,Balazs Kulcsar,Jiaming Wu,Morteza Haghir Chehreghani,Gilbert Laporte*

Main category: cs.AI

TL;DR: 该论文综述了机器学习在复杂路由优化问题（TSP、VRP等）中的应用，提出了方法分类，强调了与传统方法的结合，并为未来研究提供框架。


<details>
  <summary>Details</summary>
Motivation: 传统求解NP难组合优化问题（如TSP和VRP）的方法存在效率低（精确算法）或者无法保证最优解（启发式算法）的问题，近年来机器学习的成功为提升这些问题的求解质量和效率带来新的可能。

Method: 通过对现有文献的综述，提出了将基于机器学习的路由问题求解方法分为构造式和改进式两大类的分类法。

Result: 本综述总结了现有机器学习在TSP、VRP等路由优化问题上的研究进展，分类梳理了不同方法，强调了机器学习与传统优化方法结合的价值，为后续研究和新型问题变种的解决提供了参考。

Conclusion: 论文提出并总结了一套整合传统运筹优化方法与先进机器学习技术的研究框架，并对未来研究提出指导。

Abstract: This paper reviews the current progress in applying machine learning (ML)
tools to solve NP-hard combinatorial optimization problems, with a focus on
routing problems such as the traveling salesman problem (TSP) and the vehicle
routing problem (VRP). Due to the inherent complexity of these problems, exact
algorithms often require excessive computational time to find optimal
solutions, while heuristics can only provide approximate solutions without
guaranteeing optimality. With the recent success of machine learning models,
there is a growing trend in proposing and implementing diverse ML techniques to
enhance the resolution of these challenging routing problems. We propose a
taxonomy categorizing ML-based routing methods into construction-based and
improvement-based approaches, highlighting their applicability to various
problem characteristics. This review aims to integrate traditional OR methods
with state-of-the-art ML techniques, providing a structured framework to guide
future research and address emerging VRP variants.

</details>


### [12] [ASTRO: Teaching Language Models to Reason by Reflecting and Backtracking In-Context](https://arxiv.org/abs/2507.00417)
*Joongwon Kim,Anirudh Goyal,Liang Tan,Hannaneh Hajishirzi,Srinivasan Iyer,Tianlu Wang*

Main category: cs.AI

TL;DR: 作者提出ASTRO框架，通过导入自搜索算法的推理链、结合RL训练，显著提升Llama 3等开源大模型的推理和自纠错能力，在多个数学复杂任务上有大幅度性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前许多高性能推理模型是在本身已有良好推理和搜索能力的基础上进一步提升，而如Llama 3等非推理导向的开源模型如何获得这种能力尚未解决。作者旨在为这类模型引入结构化、类似搜索的推理能力。

Method: 1）用Monte Carlo Tree Search生成解题轨迹；2）将轨迹转为自然语言链式思维过程，并涵盖成功与失败纠正；3）利用这些数据对模型做微调，再通过强化学习进一步提升。

Result: 在Llama 3系列模型上应用ASTRO方法，在MATH-500、AMC 2023、AIME 2024等基准测试中成绩分别提升了16.0%、26.9%、20.0%，特别是对需多轮修正的复杂问题有显著增益。

Conclusion: 通过将搜索算法（如Monte Carlo Tree Search）生成的推理轨迹转化为自然语言链式思维，并结合强化学习，能够大幅提升开源大语言模型的推理能力和纠错能力。

Abstract: We introduce ASTRO, the "Autoregressive Search-Taught Reasoner", a framework
for training language models to reason like search algorithms, explicitly
leveraging self-reflection, backtracking, and exploration in their outputs.
Recently, training large language models (LLMs) via reinforcement learning (RL)
has led to the advent of reasoning models with greatly enhanced reasoning
capabilities. Open-source replications of reasoning models, while successful,
build upon models that already exhibit strong reasoning capabilities along with
search behavior observed even before RL. As a result, it is yet unclear how to
boost the reasoning capabilities of other non-reasoner models including Llama
3. ASTRO teaches such models to internalize structured search behavior through
a synthetic dataset derived from Monte Carlo Tree Search (MCTS) over
mathematical problem-solving trajectories. By converting search traces into
natural language chain-of-thoughts that capture both successes and recoveries
from failure, ASTRO bootstraps models with a rich prior for exploration during
RL. We finetune our models on these search-derived traces and further improve
performance via RL with verifiable rewards. We apply ASTRO to the Llama 3
family of models and achieve absolute performance gains of 16.0% on MATH-500,
26.9% on AMC 2023, and 20.0% on AIME 2024, especially improving upon
challenging problems that require iterative correction. Our results demonstrate
that search-inspired training offers a principled way to instill robust
reasoning capabilities into open LLMs.

</details>


### [13] [Does Math Reasoning Improve General LLM Capabilities? Understanding Transferability of LLM Reasoning](https://arxiv.org/abs/2507.00432)
*Maggie Huan,Yuetai Li,Tuney Zheng,Xiaoyu Xu,Seungone Kim,Minxin Du,Radha Poovendran,Graham Neubig,Xiang Yue*

Main category: cs.AI

TL;DR: 虽然LLM在数学任务上表现提升，但这种提升未能普遍迁移到其他推理任务。强化学习微调有助于保持模型的广泛能力，而监督微调可能导致能力窄化与遗忘，提示后训练范式亟需改进。


<details>
  <summary>Details</summary>
Motivation: 近年来，大型语言模型（LLM）在数学推理基准任务上取得了巨大进步，但这类提高是模型广泛推理能力的体现还是对特定任务过拟合，尚不清楚。作者提出有必要探究这些进步背后的泛化能力问题。

Method: 作者评测了20多个开源、多种推理微调（reasoning-tuned）的LLM，覆盖数学、科学问答、智能体规划、编程以及标准指令跟随等多个任务。同时，利用Qwen3-14B模型，通过仅用数学数据、采用不同微调方法（监督微调SFT与强化学习RL）进行对照实验；并进一步分析潜在空间表示及token分布变化。

Result: 绝大多数在数学任务表现优异的模型，其能力难以迁移到其他领域。与SFT微调相比，RL微调的模型在多任务上的泛化能力更强。SFT微调导致模型的表示和输出漂移、遗忘了之前的广泛能力，而RL能保持通用能力。

Conclusion: 目前通行的模型后训练（post-training）范式、特别是太过依赖SFT蒸馏数据的做法，需重新审视。为提升LLM推理泛化，应更加重视微调方式的选择，避免能力窄化和遗忘。

Abstract: Math reasoning has become the poster child of progress in large language
models (LLMs), with new models rapidly surpassing human-level performance on
benchmarks like MATH and AIME. But as math leaderboards improve week by week,
it is worth asking: do these gains reflect broader problem-solving ability or
just narrow overfitting? To answer this question, we evaluate over 20
open-weight reasoning-tuned models across a broad suite of tasks, including
math, scientific QA, agent planning, coding, and standard
instruction-following. We surprisingly find that most models that succeed in
math fail to transfer their gains to other domains. To rigorously study this
phenomenon, we conduct controlled experiments on Qwen3-14B models using
math-only data but different tuning methods. We find that reinforcement
learning (RL)-tuned models generalize well across domains, while supervised
fine-tuning (SFT)-tuned models often forget general capabilities. Latent-space
representation and token-space distribution shift analyses reveal that SFT
induces substantial representation and output drift, while RL preserves
general-domain structure. Our results suggest a need to rethink standard
post-training recipes, particularly the reliance on SFT-distilled data for
advancing reasoning models.

</details>


### [14] [Advancing Local Search in SMT-NRA with MCSAT Integration](https://arxiv.org/abs/2507.00557)
*Tianyi Ding,Haokun Li,Xinpeng Ni,Bican Xia,Tianqi Zhao*

Main category: cs.AI

TL;DR: 本文通过提出二维cell-jump、2d-LS局部搜索框架、结合MCSAT和OpenCAD技术，显著提升了SMT-NRA问题的局部搜索效率，实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 提升SMT-NRA（非线性实数算术可满足性理论）局部搜索的效率及解决复杂实例的能力。

Method: 提出二维cell-jump操作，扩展出2d-LS局部搜索框架，并结合MCSAT和sample-cell投影算子进行高效搜索，最终与OpenCAD整合为混合框架。

Result: 实验结果显示，所提出的方法提升了SMT-NRA中的局部搜索表现。

Conclusion: 新提出的方法有效提升了SMT-NRA中局部搜索的性能。

Abstract: In this paper, we advance local search for Satisfiability Modulo the Theory
of Nonlinear Real Arithmetic (SMT-NRA for short). First, we introduce a
two-dimensional cell-jump move, called \emph{$2d$-cell-jump}, generalizing the
key operation, cell-jump, of the local search method for SMT-NRA. Then, we
propose an extended local search framework, named \emph{$2d$-LS} (following the
local search framework, LS, for SMT-NRA), integrating the model constructing
satisfiability calculus (MCSAT) framework to improve search efficiency. To
further improve the efficiency of MCSAT, we implement a recently proposed
technique called \emph{sample-cell projection operator} for MCSAT, which is
well suited for CDCL-style search in the real domain and helps guide the search
away from conflicting states. Finally, we design a hybrid framework for SMT-NRA
combining MCSAT, $2d$-LS and OpenCAD, to improve search efficiency through
information exchange. The experimental results demonstrate improvements in
local search performance, highlighting the effectiveness of the proposed
methods.

</details>


### [15] [Can Large Language Models Develop Strategic Reasoning? Post-training Insights from Learning Chess](https://arxiv.org/abs/2507.00726)
*Dongyoon Hwang,Hojoon Lee,Jaegul Choo,Dongmin Park,Jongho Park*

Main category: cs.AI

TL;DR: 本研究发现，通过强化学习和知识蒸馏虽可改善LLM（大语言模型）在国际象棋上的举动质量，但模型棋力远不及人类专家，关键瓶颈在于其对棋类知识的内在理解不足，强化学习难以完全弥补此短板。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLMs）使用强化学习在数学推理领域取得了一些进展，但在策略推理（如国际象棋）方面的研究还不充分。因此，作者希望探索LLM在国际象棋中通过强化学习是否能够学习到策略推理能力。

Method: 作者利用一个经过国际象棋预训练的动作价值网络，对LLM输出的走子质量提供稠密奖励，这是一种知识蒸馏的方法，并与传统的稀疏二元奖励方式进行了对比实验。此外，论文还对监督微调（SFT）和强化学习过程进行消融实验。

Result: 实验表明，基于蒸馏的稠密奖励通常优于稀疏的二元奖励。然而，所有经过训练的模型的棋力都远低于专家水平。消融实验表明，导致表现受限的核心原因是预训练模型对国际象棋的内部理解存在一定的不足，仅靠强化学习难以彻底弥补这项缺陷。

Conclusion: 单纯依赖强化学习难以显著提升LLM在国际象棋等策略推理任务中的能力，其根本障碍在于模型本身缺乏对任务规则和隐含知识的深层理解。知识蒸馏等稠密奖励机制虽有提升，但模型水平依然有限。

Abstract: While reinforcement learning (RL) for large language models (LLMs) has shown
promise in mathematical reasoning, strategic reasoning for LLMs using RL
remains largely unexplored. We investigate whether LLMs can develop strategic
reasoning capabilities through RL in chess. To this end, we leverage a
chess-pretrained action-value network to provide dense reward on the LLM's
output move quality, which can be seen as a form of knowledge distillation. Our
experiments show that our distillation-based dense rewards often outperform
sparse binary rewards. However, surprisingly, all models plateau far below
expert levels. We provide SFT and RL ablations on chess reasoning training and
find evidence that this limitation stems from a deficit in the pretrained
models' internal understanding of chess--a deficit which RL alone may not be
able to fully overcome.

</details>


### [16] [A Robust Algorithm for Non-IID Machine Learning Problems with Convergence Analysis](https://arxiv.org/abs/2507.00810)
*Qing Xu,Xiaohua Xuan*

Main category: cs.AI

TL;DR: 作者提出了一种改进的用于最小最大问题的新数值算法，并证明了其在温和条件下的收敛性，可应用于鲁棒优化和不平衡学习等领域。


<details>
  <summary>Details</summary>
Motivation: 最小最大(minimax)问题在鲁棒优化和不平衡学习等领域具有广泛应用，但由于其非光滑特性，数值求解难度较大。现有方法在收敛性和求解效率上存在局限。

Method: 提出了一种结合非光滑优化、二次规划和迭代过程的改进型数值算法，用于求解最小最大问题，并在一定假设下给出了算法收敛性的严格证明。

Result: 该算法在梯度连续性和有界性条件下收敛性得到理论保证，具备广泛的实际应用前景。

Conclusion: 所提出的算法能够有效求解广泛应用场景中的最小最大问题，理论上具有可靠的收敛性。

Abstract: In this paper, we propose an improved numerical algorithm for solving minimax
problems based on nonsmooth optimization, quadratic programming and iterative
process. We also provide a rigorous proof of convergence for our algorithm
under some mild assumptions, such as gradient continuity and boundedness. Such
an algorithm can be widely applied in various fields such as robust
optimization, imbalanced learning, etc.

</details>


### [17] [SafeMobile: Chain-level Jailbreak Detection and Automated Evaluation for Multimodal Mobile Agents](https://arxiv.org/abs/2507.00841)
*Siyuan Liang,Tianmeng Fang,Zhe Liu,Aishan Liu,Yan Xiao,Jinyuan He,Ee-Chien Chang,Xiaochun Cao*

Main category: cs.AI

TL;DR: 作者提出结合行为序列和大语言模型的自动化判别与评估方案，有效提升了多模态智能体越狱风险识别能力，并为相关系统的安全建模和防护指明了方向。


<details>
  <summary>Details</summary>
Motivation: 随着多模态基础模型在智能体系统中的广泛应用，移动设备控制、智能助手交互等场景变得依赖这些大模型。然而，这类系统暴露出日益严峻的越狱风险，即攻击者通过特殊输入诱导系统绕过行为约束，执行高风险敏感操作，带来安全新挑战。现有安全措施难以应对多轮交互或复杂任务序列中的潜在风险。此外，缺乏高效且一致的自动化评估方法也是现有短板。

Method: 本文提出一种基于引入行为序列信息的风险判别机制，并利用大型语言模型设计自动化辅助评估方案，以检测并评估多模态智能体系统中的潜在越狱行为。

Result: 在几个具有代表性的高风险任务中的初步实验表明，该方法能够在一定程度上提升对风险行为的识别能力，帮助降低智能体系统被越狱的概率。

Conclusion: 该研究为多模态智能体系统的安全风险建模与防护提供了有意义的参考，推动相关安全机制的发展。

Abstract: With the wide application of multimodal foundation models in intelligent
agent systems, scenarios such as mobile device control, intelligent assistant
interaction, and multimodal task execution are gradually relying on such large
model-driven agents. However, the related systems are also increasingly exposed
to potential jailbreak risks. Attackers may induce the agents to bypass the
original behavioral constraints through specific inputs, and then trigger
certain risky and sensitive operations, such as modifying settings, executing
unauthorized commands, or impersonating user identities, which brings new
challenges to system security. Existing security measures for intelligent
agents still have limitations when facing complex interactions, especially in
detecting potentially risky behaviors across multiple rounds of conversations
or sequences of tasks. In addition, an efficient and consistent automated
methodology to assist in assessing and determining the impact of such risks is
currently lacking. This work explores the security issues surrounding mobile
multimodal agents, attempts to construct a risk discrimination mechanism by
incorporating behavioral sequence information, and designs an automated
assisted assessment scheme based on a large language model. Through preliminary
validation in several representative high-risk tasks, the results show that the
method can improve the recognition of risky behaviors to some extent and assist
in reducing the probability of agents being jailbroken. We hope that this study
can provide some valuable references for the security risk modeling and
protection of multimodal intelligent agent systems.

</details>


### [18] [Thinking Beyond Tokens: From Brain-Inspired Intelligence to Cognitive Foundations for Artificial General Intelligence and its Societal Impact](https://arxiv.org/abs/2507.00951)
*Rizwan Qureshi,Ranjan Sapkota,Abbas Shah,Amgad Muneer,Anas Zafar,Ashmal Vayani,Maged Shoman,Abdelrahman B. M. Eldaly,Kai Zhang,Ferhat Sadak,Shaina Raza,Xinqi Fan,Ravid Shwartz-Ziv,Hong Yan,Vinjia Jain,Aman Chadha,Manoj Karkee,Jia Wu,Philip Torr,Seyedali Mirjalili*

Main category: cs.AI

TL;DR: 文章综述了实现通用人工智能（AGI）所需的理论与技术，强调单一大模型的规模扩展无法达成真正智能。未来AGI需融合模块化推理、持续性记忆、多智能体协作等能力，同时兼顾科学、技术与伦理挑战。


<details>
  <summary>Details</summary>
Motivation: 机器是否能够像人类一样真正思考、推理和行动一直是人工智能领域的核心问题。当前的大模型虽在多模态处理与部分推理能力上有进展，但依然存在依赖于token级预测、缺乏行动力等根本性局限。本文旨在从多学科视角，探讨达到真正通用人工智能所需的理论与技术基础。

Method: 本文采用跨学科综述的方法，从人工智能、认知神经科学、心理学、生成式模型及基于智能体的系统多个领域，对通用智能的构建基础进行了分析，并针对目前主流方法的不足提出以Agentic RAG、模块化推理、持续性记忆与多智能体协同等为核心的改进方向。

Result: 文章着重指出，通用智能的实现不仅依赖模型规模，更是模块化、交互式、能自我完善系统（具有记忆与推理能力）的深度集成。强调信息压缩、无训练测试适应及神经符号系统等新方法，为达成跨领域、适应性强的智能指明了路径。同时，提出VLM需作为具备体现和协作能力的高级接口，并梳理了实现AGI过程中需面对的科学、技术和伦理挑战。

Conclusion: 要实现人工通用智能，必须突破目前基于统计学习和规模扩展的瓶颈，转向集成记忆、推理、模块化协调与自我提升的多元系统架构，结合神经符号、强化学习及认知脚手架等前沿技术，灵活面对实际场景中的多样性与复杂性。

Abstract: Can machines truly think, reason and act in domains like humans? This
enduring question continues to shape the pursuit of Artificial General
Intelligence (AGI). Despite the growing capabilities of models such as GPT-4.5,
DeepSeek, Claude 3.5 Sonnet, Phi-4, and Grok 3, which exhibit multimodal
fluency and partial reasoning, these systems remain fundamentally limited by
their reliance on token-level prediction and lack of grounded agency. This
paper offers a cross-disciplinary synthesis of AGI development, spanning
artificial intelligence, cognitive neuroscience, psychology, generative models,
and agent-based systems. We analyze the architectural and cognitive foundations
of general intelligence, highlighting the role of modular reasoning, persistent
memory, and multi-agent coordination. In particular, we emphasize the rise of
Agentic RAG frameworks that combine retrieval, planning, and dynamic tool use
to enable more adaptive behavior. We discuss generalization strategies,
including information compression, test-time adaptation, and training-free
methods, as critical pathways toward flexible, domain-agnostic intelligence.
Vision-Language Models (VLMs) are reexamined not just as perception modules but
as evolving interfaces for embodied understanding and collaborative task
completion. We also argue that true intelligence arises not from scale alone
but from the integration of memory and reasoning: an orchestration of modular,
interactive, and self-improving components where compression enables adaptive
behavior. Drawing on advances in neurosymbolic systems, reinforcement learning,
and cognitive scaffolding, we explore how recent architectures begin to bridge
the gap between statistical learning and goal-directed cognition. Finally, we
identify key scientific, technical, and ethical challenges on the path to AGI.

</details>


### [19] [Enhancing LLM Agent Safety via Causal Influence Prompting](https://arxiv.org/abs/2507.00979)
*Dongyoon Hahm,Woogyeol Jin,June Suk Choi,Sungsoo Ahn,Kimin Lee*

Main category: cs.AI

TL;DR: 本文提出利用因果影响图指导和优化自治智能体决策流程，通过三步法持续提升安全性。实验证明此方法在多类任务中有效防止了风险发生。


<details>
  <summary>Details</summary>
Motivation: 当前由大语言模型驱动的自治智能体在许多任务中展现出潜力，但如何确保其安全和可靠仍是亟需解决的问题，特别是防止意外后果的发生。

Method: 提出了一种新技术CIP，利用因果影响图（CID）识别并缓解智能体决策带来的风险。方法包括三个步骤：（1）根据任务初始化CID，描述决策过程；（2）利用CID指导智能体与环境交互；（3）根据行为和结果持续优化CID。

Result: 实验证明该方法能有效提升智能体在代码执行和移动设备控制任务中的安全性。

Conclusion: 通过用CID建模因果关系并不断优化，智能体能够更好地预见和避免有害后果，从而提升决策安全性。

Abstract: As autonomous agents powered by large language models (LLMs) continue to
demonstrate potential across various assistive tasks, ensuring their safe and
reliable behavior is crucial for preventing unintended consequences. In this
work, we introduce CIP, a novel technique that leverages causal influence
diagrams (CIDs) to identify and mitigate risks arising from agent
decision-making. CIDs provide a structured representation of cause-and-effect
relationships, enabling agents to anticipate harmful outcomes and make safer
decisions. Our approach consists of three key steps: (1) initializing a CID
based on task specifications to outline the decision-making process, (2)
guiding agent interactions with the environment using the CID, and (3)
iteratively refining the CID based on observed behaviors and outcomes.
Experimental results demonstrate that our method effectively enhances safety in
both code execution and mobile device control tasks.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [20] [Table Understanding and (Multimodal) LLMs: A Cross-Domain Case Study on Scientific vs. Non-Scientific Data](https://arxiv.org/abs/2507.00152)
*Ekaterina Borisova,Fabio Barth,Nils Feldhus,Raia Abu Ahmad,Malte Ostendorff,Pedro Ortiz Suarez,Georg Rehm,Sebastian Möller*

Main category: cs.CL

TL;DR: 本文系统评估了文本型和多模态LLM对表格理解的能力，覆盖不同领域和多种数据格式，并发布了TableEval基准。结果显示，LLM在表格表达形式变换时表现稳健，但科学表格仍是难点。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLM）在许多下游任务中表现强劲，但其在处理表格数据方面的效率和表现尚未被充分探索。表格是表现结构化数据的重要工具，广泛应用于多个领域，因此研究LLM对表格理解的能力具有重要意义。

Method: 作者设计了跨领域（科学与非科学）、跨模态（表格图像与文本）评估LLM处理表格任务的实验。具体地，他们比较了文本型和多模态LLM在不同来源、不同格式的表格上的表现，并进行了可解释性分析，评估模型对上下文和输入相关性的利用情况。此外，作者新建了TableEval基准数据集，收集了3017张来自学术论文、维基百科和财报的表格，每张表格有五种不同格式。

Result: 实验结果显示LLM在不同模态（如图片和文本）下处理表格的表现较为稳健，但在应对科学领域表格时，仍面临显著挑战。

Conclusion: LLM具备一定跨模态、跨领域的表格理解能力，但科学表格因其高复杂性，对模型提出了更高要求。TableEval有望促进该领域进一步研究。

Abstract: Tables are among the most widely used tools for representing structured data
in research, business, medicine, and education. Although LLMs demonstrate
strong performance in downstream tasks, their efficiency in processing tabular
data remains underexplored. In this paper, we investigate the effectiveness of
both text-based and multimodal LLMs on table understanding tasks through a
cross-domain and cross-modality evaluation. Specifically, we compare their
performance on tables from scientific vs. non-scientific contexts and examine
their robustness on tables represented as images vs. text. Additionally, we
conduct an interpretability analysis to measure context usage and input
relevance. We also introduce the TableEval benchmark, comprising 3017 tables
from scholarly publications, Wikipedia, and financial reports, where each table
is provided in five different formats: Image, Dictionary, HTML, XML, and LaTeX.
Our findings indicate that while LLMs maintain robustness across table
modalities, they face significant challenges when processing scientific tables.

</details>


### [21] [Prompting as Scientific Inquiry](https://arxiv.org/abs/2507.00163)
*Ari Holtzman,Chenhao Tan*

Main category: cs.CL

TL;DR: 本文认为提示（prompting）应被视为研究大型语言模型（LLMs）科学的关键方法，而不是非科学性的权宜之计。提示过程本质上是对LLMs进行语言层面的行为科学实验，是理解和控制LLMs能力的重要工具，与机制可解释性共同构成了LLMs科学体系。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的功能开发和控制主要依赖于提示（prompting），但目前这一方法往往没有被作为科学方法对待，甚至受到低估。作者认为这一现象是对其本质的错误分类。

Method: 将LLMs视为一种新的、复杂且不透明的“有机体”，并主张应该将提示过程视作对LLMs进行“行为科学”研究，而不是仅仅作为一种权宜之计。通过对比机制可解释性（关注神经底层）、提示则是用语言这一原生接口去研究模型行为。

Result: 作者主张，提示不仅不是低级的技巧，而是研究LLMs科学不可或缺的关键组成部分。提示和机制可解释性一样，都是探索和理解LLMs的重要手段。

Conclusion: 提示应当作为科学研究方法被重视，在LLMs科学体系中具有基础性地位。提示研究实际上是对LLMs进行科学实验和理解的重要路径。

Abstract: Prompting is the primary method by which we study and control large language
models. It is also one of the most powerful: nearly every major capability
attributed to LLMs-few-shot learning, chain-of-thought, constitutional AI-was
first unlocked through prompting. Yet prompting is rarely treated as science
and is frequently frowned upon as alchemy. We argue that this is a category
error. If we treat LLMs as a new kind of complex and opaque organism that is
trained rather than programmed, then prompting is not a workaround: it is
behavioral science. Mechanistic interpretability peers into the neural
substrate, prompting probes the model in its native interface: language. We
contend that prompting is not inferior, but rather a key component in the
science of LLMs.

</details>


### [22] [LineRetriever: Planning-Aware Observation Reduction for Web Agents](https://arxiv.org/abs/2507.00210)
*Imene Kerboua,Sahar Omidi Shayegan,Megh Thakkar,Xing Han Lù,Massimo Caccia,Véronique Eglin,Alexandre Aussem,Jérémy Espinas,Alexandre Lacoste*

Main category: cs.CL

TL;DR: 网页导航中传统截断与检索方法难以兼顾输入长度和规划相关信息。该文提出LineRetriever，利用语言模型筛选对后续动作有指导意义的内容，有效压缩输入规模同时保证导航性能。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型处理网页导航时受输入长度限制，常用检索方法易丢失关键页面状态和操作历史信息，导致自适应规划能力不足。希望提升检索对后续动作预测的支持能力。

Method: 提出LineRetriever，通过语言模型分析和筛选对未来导航（规划）最相关的观测信息，相较于仅用语义相似度传统方法更加关注规划功能。

Result: LineRetriever相比现有方案，能在降低每步输入体积的前提下，支持网页代理维持稳定性能。

Conclusion: LineRetriever能够有效优化网页导航任务中的检索方法，减少输入内容体积的同时保持代理的一致性能。

Abstract: While large language models have demonstrated impressive capabilities in web
navigation tasks, the extensive context of web pages, often represented as DOM
or Accessibility Tree (AxTree) structures, frequently exceeds model context
limits. Current approaches like bottom-up truncation or embedding-based
retrieval lose critical information about page state and action history. This
is particularly problematic for adaptive planning in web agents, where
understanding the current state is essential for determining future actions. We
hypothesize that embedding models lack sufficient capacity to capture
plan-relevant information, especially when retrieving content that supports
future action prediction. This raises a fundamental question: how can retrieval
methods be optimized for adaptive planning in web navigation tasks? In
response, we introduce \textit{LineRetriever}, a novel approach that leverages
a language model to identify and retrieve observation lines most relevant to
future navigation steps. Unlike traditional retrieval methods that focus solely
on semantic similarity, \textit{LineRetriever} explicitly considers the
planning horizon, prioritizing elements that contribute to action prediction.
Our experiments demonstrate that \textit{LineRetriever} can reduce the size of
the observation at each step for the web agent while maintaining consistent
performance within the context limitations.

</details>


### [23] [Two-Stage Reasoning-Infused Learning: Improving Classification with LLM-Generated Reasoning](https://arxiv.org/abs/2507.00214)
*Mads Henrichsen,Rasmus Krebs*

Main category: cs.CL

TL;DR: 本文通过引入LLM推理生成，分两步提升了情感分类的准确率和可解释性，实验表明带推理输出的模型准确率提升显著，展示了该思路在NLP任务中的潜力。


<details>
  <summary>Details</summary>
Motivation: 标准分类模型通常直接将输入映射到标签，缺乏显式推理，限制了性能、鲁棒性和可解释性。为了解决这一问题，本文提出通过LLM生成推理来增强文本分类。

Method: 提出了一种两阶段方法：第一阶段使用Llama-3.2-1B-Instruct模型在通用推理数据集上进行微调以生成推理；第二阶段利用上述模型离线生成增强训练数据，然后用这一数据训练下游情感分类模型，让模型学习从输入文本生成推理以及情感标签。

Result: 在dair-ai/emotion数据集上，输出推理与情感（Q->RA）的生成模型相比仅输出情感（Q->A）的基线模型，在情感预测准确率上显著提升了8.7个百分点，证明了模型泛化能力和显式推理训练的优势。

Conclusion: LLM生成的推理能够提升下游NLP任务性能，并为模型提供更强的可解释性和更丰富的训练数据。

Abstract: Standard classification models often map inputs directly to labels without
explicit reasoning, potentially limiting their performance, robustness, and
interpretability. This paper introduces a novel two-stage approach to enhance
text classification by leveraging Large Language Model (LLM)-generated
reasonings. In the first stage, we fine-tune a Llama-3.2-1B-Instruct model
(henceforth Llama-R-Gen) on a general-purpose reasoning dataset
(syvai/reasoning-gen) to generate textual reasoning (R) given a question and
its answer. In the second stage, this generally trained Llama-R-Gen is used
offline to create an augmented training dataset for a downstream generative
model. This downstream model, based on Llama-3.2-1B-Instruct, takes only the
input text (Q) and is trained to output the generated reasoning (R) immediately
followed by the predicted emotion (A). We demonstrate this methodology on the
dair-ai/emotion dataset for emotion classification. Our experiments show that
the generative model trained to output reasoning and the emotion (Classifier
Q->RA) achieves a significant improvement of 8.7 percentage points in accuracy
(for emotion prediction) compared to a baseline generative model trained solely
to output the emotion (Classifier Q->A), highlighting the strong generalization
capabilities of the reasoning generation and the benefit of explicit reasoning
training. This work underscores the potential of LLM-generated reasonings for
creating richer training datasets, thereby improving the performance of diverse
downstream NLP tasks and providing explicit explanations.

</details>


### [24] [Towards Style Alignment in Cross-Cultural Translation](https://arxiv.org/abs/2507.00216)
*Shreya Havaldar,Adam Stein,Eric Wong,Lyle Ungar*

Main category: cs.CL

TL;DR: 本文提出RASTA方法，用检索和风格概念提升LLM跨文化翻译中的风格对齐能力，对于礼貌等风格在非西方语言翻译中的表达尤为有效。


<details>
  <summary>Details</summary>
Motivation: 论文关注于跨文化交流中，讲话者表达的风格与听话者理解的风格经常不一致的问题，特别是在多语言翻译场景下，语言模型在传达礼貌等风格特征时往往失效。

Method: 提出RASTA（检索增强风格对齐）方法，利用预训练的风格概念，通过检索相关风格示例，指导大型语言模型在翻译过程中更好地传达文化交流规范与风格特征。

Result: 该方法能有效缓解现有语言模型在风格传达上的缺陷，尤其是在非西方语言和礼貌等风格表达方面，表现出更好的风格对齐能力。

Conclusion: RASTA方法能显著提升LLM在多文化翻译任务中的风格对齐效果，使译文更贴合原语境下的文化和风格预期。

Abstract: Successful communication depends on the speaker's intended style (i.e., what
the speaker is trying to convey) aligning with the listener's interpreted style
(i.e., what the listener perceives). However, cultural differences often lead
to misalignment between the two; for example, politeness is often lost in
translation. We characterize the ways that LLMs fail to translate style -
biasing translations towards neutrality and performing worse in non-Western
languages. We mitigate these failures with RASTA (Retrieval-Augmented STylistic
Alignment), a method that leverages learned stylistic concepts to encourage LLM
translation to appropriately convey cultural communication norms and align
style.

</details>


### [25] [Linearly Decoding Refused Knowledge in Aligned Language Models](https://arxiv.org/abs/2507.00239)
*Aryan Shrivastava,Ari Holtzman*

Main category: cs.CL

TL;DR: 语言模型的有害信息通过越狱提示词虽被表面抑制，但依旧能用简单线性探针在隐藏状态中被读出，表明指令微调无法彻底消除或迁移这些内容。


<details>
  <summary>Details</summary>
Motivation: 主流的语言模型通过指令微调和强化学习来规避产生有害内容，但被“越狱”提示词（jailbreak prompts）可以绕过这些限制，研究这些限制信息是否还可以通过模型内部表征获得。

Method: 尝试对越狱场景下被拒答的信息，在语言模型隐藏状态上训练线性探针进行解码，并测试这些探针在基础模型和指令微调模型上的效果。

Result: 许多最初被拒绝的信息可以通过线性探针以较高相关性（皮尔逊系数>0.8）解码。部分在基础模型训练的探针能迁移到微调模型上解码出越狱生成的信息，说明被拒绝的信息在模型表征中依然存在。探针输出还与模型在生成任务中的输出高度相关。

Conclusion: 指令微调未能完全清除或转移有害信息，仅仅抑制其直接表达。这些信息在模型内部仍然可被线性获取，且对下游行为有间接影响。

Abstract: Most commonly used language models (LMs) are instruction-tuned and aligned
using a combination of fine-tuning and reinforcement learning, causing them to
refuse users requests deemed harmful by the model. However, jailbreak prompts
can often bypass these refusal mechanisms and elicit harmful responses. In this
work, we study the extent to which information accessed via jailbreak prompts
is decodable using linear probes trained on LM hidden states. We show that a
great deal of initially refused information is linearly decodable. For example,
across models, the response of a jailbroken LM for the average IQ of a country
can be predicted by a linear probe with Pearson correlations exceeding $0.8$.
Surprisingly, we find that probes trained on base models (which do not refuse)
sometimes transfer to their instruction-tuned versions and are capable of
revealing information that jailbreaks decode generatively, suggesting that the
internal representations of many refused properties persist from base LMs
through instruction-tuning. Importantly, we show that this information is not
merely "leftover" in instruction-tuned models, but is actively used by them: we
find that probe-predicted values correlate with LM generated pairwise
comparisons, indicating that the information decoded by our probes align with
suppressed generative behavior that may be expressed more subtly in other
downstream tasks. Overall, our results suggest that instruction-tuning does not
wholly eliminate or even relocate harmful information in representation
space-they merely suppress its direct expression, leaving it both linearly
accessible and indirectly influential in downstream behavior.

</details>


### [26] [Many LLMs Are More Utilitarian Than One](https://arxiv.org/abs/2507.00814)
*Anita Keshmirian,Razan Baltaji,Babak Hemmatian,Hadi Asghari,Lav R. Varshney*

Main category: cs.CL

TL;DR: 多智能体LLM在道德困境中会表现出更强的群体功利主义倾向，这种现象表面上类似于人类，但根本机制不同，应关注AI对齐与道德推理的新问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）在道德判断和社会推理中的一致性问题日益重要，尤其是在多智能体系统越来越普及的背景下，了解LLM在集体协作时与单个表现有何不同具有重要意义。模型能否产生类似于人类群体中已知的“功利主义提升”的现象，是当前研究空白。

Method: 研究团队让六种LLM模型在经典道德困境下推理，采用两种条件：(1) 单独推理（Solo），(2) 以小组形式交互讨论（Group），有配对和三人组两种。比较小组合作与独立推理时，LLM在道德判断上的差异。

Result: 在个人道德困境中，所有LLM模型在小组讨论时更易于接受最大化整体利益但损害个体的道德违例，与人类实验中群体表现相似。一些模型更加偏向于支持整体福祉最大化的行为，哪怕是牺牲熟人。还有模型在小组条件下更易违反道德规范。但与人类不同，LLM集体的功利主义倾向并非来自结果敏感性提升，而是由于对规范敏感性降低或者更强的无差别性。

Conclusion: LLM集体决策的表层行为与人类群体看似类似，但背后的心理机制存在显著差异。这为AI对齐、多智能体系统设计和人工道德推理提出了新挑战和启示。

Abstract: Moral judgment is integral to large language model (LLM) alignment and social
reasoning. As multi-agent systems gain prominence, it becomes crucial to
understand how LLMs function collectively during collaboration, compared to
individual agents. In human moral judgment, group deliberation leads to a
utilitarian boost: a tendency to endorse norm violations that maximize benefits
for the greatest number of people despite harms. We study whether a similar
dynamic emerges in multi-agent LLM systems. We tested six models on
well-established sets of moral dilemmas across two conditions: (1) Solo, where
models reasoned independently, and (2) Group, where they engaged in multi-turn
discussions in pairs or triads. In personal moral dilemmas, where agents must
decide to directly harm one individual to maximize the utility for others, all
models found moral violations to be more acceptable when part of a group than
individually, similar to human experiments. Some models endorsed actions that
maximized overall well-being, even if they benefited strangers over familiar
individuals. Others became more willing to violate moral norms in groups.
However, while human groups show a similar action bias, the mechanism for their
utilitarian boost differs from LLMs. Whereas the human shift comes from
heightened sensitivity to decision outcomes, LLM groups show either reduced
norm sensitivity or enhanced impartiality. This suggests that while the surface
behavior of LLM collectives mimics human group reasoning, the underlying
drivers differ. We discuss the implications for AI alignment, multi-agent
design, and artificial moral reasoning.

</details>


### [27] [The Algebraic Structure of Morphosyntax](https://arxiv.org/abs/2507.00244)
*Isabella Senturia,Matilde Marcolli*

Main category: cs.CL

TL;DR: 本文提出形态-句法接口的数学模型，扩展形态树集合，通过operad上的代数刻画两者配对与交互，对分布式形态学操作提供新解释，实现边界灵活调整，促进语言理论精确化。


<details>
  <summary>Details</summary>
Motivation: 探讨在强极简主义理论和Merge数学生成下，形态学与句法学如何进行接口建模，解决两者结构与操作上的差异。

Method: 构建严谨的数学模型，将形态结构视为形态树的magma集，无“移动”操作；扩展生成形态树的集合，并通过operad上的代数建立形态-句法数据之间配对及对应。

Result: 提出了将分布式形态学的特定操作重新解释为在形态与句法边界灵活移动的变换，实现了形态-句法接口的数学刻画与控制边界灵活性的理论基础。

Conclusion: 该模型为形态-句法接口的结构形成过程提供了数学描述，有助于深入理解两者之间操作及结构互动的机制，促进语言理论的形式化与精确性。

Abstract: Within the context of the mathematical formulation of Merge and the Strong
Minimalist Thesis, we present a mathematical model of the morphology-syntax
interface. In this setting, morphology has compositional properties responsible
for word formation, organized into a magma of morphological trees. However,
unlike syntax, we do not have movement within morphology. A coproduct
decomposition exists, but it requires extending the set of morphological trees
beyond those which are generated solely by the magma, to a larger set of
possible morphological inputs to syntactic trees. These participate in the
formation of morphosyntactic trees as an algebra over an operad, and a
correspondence between algebras over an operad. The process of structure
formation for morphosyntactic trees can then be described in terms of this
operadic correspondence that pairs syntactic and morphological data and the
morphology coproduct. We reinterpret in this setting certain operations of
Distributed Morphology as transformation that allow for flexibility in moving
the boundary between syntax and morphology within the morphosyntactic objects.

</details>


### [28] [EfficientXLang: Towards Improving Token Efficiency Through Cross-Lingual Reasoning](https://arxiv.org/abs/2507.00246)
*Sanchit Ahuja,Praneetha Vaddamanu,Barun Patra*

Main category: cs.CL

TL;DR: 非英语语言有助于提升大模型推理的token效率且不损失准确率，说明多语言推理值得更多关注，强多语言基础极为重要。


<details>
  <summary>Details</summary>
Motivation: 当前大多数推理模型研究只关注英语，尽管模型本身在多语言数据上预训练。论文旨在探讨英语是否真的是推理任务中最token高效的语言，以及多语言推理的潜力。

Method: 对DeepSeek R1、Qwen 2.5和Qwen 3三个开源推理模型，分别在四个数学数据集、七种类型各异的语言上展开实验，对比不同语言下推理的token效率与准确率，并检验将推理痕迹翻译回英语后的表现。

Result: 实验表明，部分非英语语言下推理任务token效率更高且准确率没有下降，提升空间取决于模型的多语言能力。这为多语言推理发展带来了新启示。

Conclusion: 通过实验发现，在非英语语言中进行推理不仅能减少token使用量，而且能保持推理准确率。这些优势甚至在将推理过程转译回英语后仍然存在，表明其改进是模型行为的真实变化而非仅仅是语言表层的差异。效果的提升取决于模型的多语言能力。

Abstract: Despite recent advances in Language Reasoning Models (LRMs), most research
focuses solely on English, even though many models are pretrained on
multilingual data. In this work, we investigate: Is English the most
token-efficient language for reasoning? We evaluate three open-source RLMs:
DeepSeek R1, Qwen 2.5 and Qwen 3, across four math datasets and seven
typologically diverse languages. We find that reasoning in non-English
languages not only reduces token usage, but also preserves accuracy. These
gains persist even after translating the reasoning traces into English,
suggesting genuine shifts in reasoning behavior rather than surface-level
linguistic effects. The extent of improvement, however, depends on the models
multilingual strength. Our findings motivate a broader view of reasoning in
language models, highlighting the potential of multilingual reasoning and the
importance of strong multilingual foundations. The code for our work can be
found: https://github.com/microsoft/EfficientXLang.

</details>


### [29] [Impact of Fine-Tuning Methods on Memorization in Large Language Models](https://arxiv.org/abs/2507.00258)
*Jie Hou,Chuxiong Wu,Lannan Luo,Qiang Zeng*

Main category: cs.CL

TL;DR: 本文系统分析了微调方法对大语言模型隐私泄露风险的影响，发现基于prompt的微调方法更能保护隐私，对成员推断攻击的抵抗力也较好。


<details>
  <summary>Details</summary>
Motivation: 随着大规模预训练语言模型能力的提升，模型微调成为主流，但其在隐私泄露（特别是由记忆导致的泄露）方面的研究较少。

Method: 对主流微调方法进行分类，并借助成员归属推断攻击（MIAs）评估其记忆性及隐私风险。

Result: 与参数微调相比，基于prompt的微调性能相当但对MIAs攻击更不敏感，且不随模型规模增加加剧信息记忆。参数微调存在更大隐私泄露风险。

Conclusion: prompt微调是更能保护隐私的微调选择，参数微调则更易造成隐私泄露。

Abstract: As the capabilities of pre-trained large language models (LLMs) continue to
advance, the "pre-train and fine-tune" paradigm has become increasingly
mainstream, leading to the development of various fine-tuning methods. However,
the privacy risks arising from memorization during fine-tuning have received
relatively little attention. To address this gap, we categorize popular
fine-tuning approaches and assess their impact on memorization through the lens
of membership inference attacks (MIAs). Our results show that, compared to
parameter-based fine-tuning, prompt-based fine-tuning achieves competitive
performance while exhibiting lower vulnerability to MIAs. Furthermore,
prompt-based methods maintain low memorization regardless of model scale. These
findings suggest that parameter-based fine-tuning is more prone to leaking
private information, whereas prompt-based fine-tuning serves as a more
privacy-preserving option.

</details>


### [30] [Natural language processing for African languages](https://arxiv.org/abs/2507.00297)
*David Ifeoluwa Adelani*

Main category: cs.CL

TL;DR: 针对非洲低资源语言，作者分析了现有数据噪声及其对词向量和PLM表现的影响，证明高质量数据比单纯数量更重要。构建了大规模21语种人工标注数据集，并实证验证了PLM在低资源场景的潜力，为非洲语种NLP研究和资源开发做出重要贡献。


<details>
  <summary>Details</summary>
Motivation: 当前多语种NLP模型数据偏向高资源语言，低资源语言（如撒哈拉以南非洲地区土著语言）数据稀缺且噪声高，缺乏标注数据，导致这些语言的NLP能力难以评估和提升。

Method: 分析公开语料噪声，并整理高质量语料，比较数据量与质量对词向量语义表示的影响；评估多语种预训练语言模型（PLM）对未见语种和低资源场景的表现；研究利用少量单语文本微调PLM到新非洲语种的方法；构建涵盖21种非洲语的以命名实体识别和机器翻译为任务的大型人工标注数据集，并在多种学习范式下做广泛实证评估。

Result: 高质量预训练数据对词向量表现优于单纯大量、但噪声高的数据；PLM对未见及低资源语种有优势；通过少量单语数据能够有效适配和提升PLM；构建了涵盖21个非洲语种、针对命名实体识别与机器翻译的人类标注数据集，与现有技术相比做了全面实验分析。

Conclusion: 数据质量对词向量学习和PLM表现至关重要，PLM在低资源和未见语种场景具备潜力，研究弥补了非洲低资源语种NLP研究和数据集的空白。

Abstract: Recent advances in word embeddings and language models use large-scale,
unlabelled data and self-supervised learning to boost NLP performance.
Multilingual models, often trained on web-sourced data like Wikipedia, face
challenges: few low-resource languages are included, their data is often noisy,
and lack of labeled datasets makes it hard to evaluate performance outside
high-resource languages like English. In this dissertation, we focus on
languages spoken in Sub-Saharan Africa where all the indigenous languages in
this region can be regarded as low-resourced in terms of the availability of
labelled data for NLP tasks and unlabelled data found on the web. We analyse
the noise in the publicly available corpora, and curate a high-quality corpus,
demonstrating that the quality of semantic representations learned in word
embeddings does not only depend on the amount of data but on the quality of
pre-training data. We demonstrate empirically the limitations of word
embeddings, and the opportunities the multilingual pre-trained language model
(PLM) offers especially for languages unseen during pre-training and
low-resource scenarios. We further study how to adapt and specialize
multilingual PLMs to unseen African languages using a small amount of
monolingual texts. To address the under-representation of the African languages
in NLP research, we developed large scale human-annotated labelled datasets for
21 African languages in two impactful NLP tasks: named entity recognition and
machine translation. We conduct an extensive empirical evaluation using
state-of-the-art methods across supervised, weakly-supervised, and transfer
learning settings.

</details>


### [31] [Failure by Interference: Language Models Make Balanced Parentheses Errors When Faulty Mechanisms Overshadow Sound Ones](https://arxiv.org/abs/2507.00322)
*Daking Rai,Samuel Miller,Kevin Moran,Ziyu Yao*

Main category: cs.CL

TL;DR: 本文揭示了语言模型在生成平衡括号等任务中出错的机制，提出了RASteer方法显著提高模型表现，准确率可从0%飙升至接近100%，并能迁移至其他推理任务。


<details>
  <summary>Details</summary>
Motivation: 尽管当前语言模型（LMs）在编码能力上有显著进步，但在生成平衡括号等简单句法任务上仍表现不佳。作者希望理解这些低级错误持续存在的原因，并寻找可行的缓解方法。

Method: 作者分析了不同规模的语言模型在括号平衡生成任务中的表现，通过定位模型内部注意力头和前馈神经元的预测机制，区分出可靠组件和引入噪声的（不可靠的）组件，并提出了一种新的“RASteer”方法，能够系统地提升那些可靠组件的贡献。

Result: RASteer极大改善了模型在平衡括号任务上的表现，将某些模型的准确率从0%提高至接近100%，并且没有削弱模型的整体编码能力。此外，该方法在算术推理任务上也能带来准确率高达约20%的提升。

Conclusion: LM在简单语法任务中的错误源于内部机制中可靠与不可靠组件之间的博弈，通过RASteer方法能有效提升可靠组件对结果的正向影响，从而大幅提升模型表现，并具备广泛适用性。

Abstract: Despite remarkable advances in coding capabilities, language models (LMs)
still struggle with simple syntactic tasks such as generating balanced
parentheses. In this study, we investigate the underlying mechanisms behind the
persistence of these errors across LMs of varying sizes (124M-7B) to both
understand and mitigate the errors. Our study reveals that LMs rely on a number
of components (attention heads and FF neurons) that independently make their
own predictions. While some components reliably promote correct answers across
a generalized range of inputs (i.e., implementing "sound mechanisms''), others
are less reliable and introduce noise by promoting incorrect tokens (i.e.,
implementing "faulty mechanisms''). Errors occur when the faulty mechanisms
overshadow the sound ones and dominantly affect the predictions. Motivated by
this insight, we introduce RASteer, a steering method to systematically
identify and increase the contribution of reliable components for improving
model performance. RASteer substantially improves performance on balanced
parentheses tasks, boosting accuracy of some models from $0$% to around $100$%
without impairing the models' general coding ability. We further demonstrate
its broader applicability in arithmetic reasoning tasks, achieving performance
gains of up to around $20$%.

</details>


### [32] [Modeling Data Diversity for Joint Instance and Verbalizer Selection in Cold-Start Scenarios](https://arxiv.org/abs/2507.00330)
*Mohna Chakraborty,Adithya Kulkarni,Qi Li*

Main category: cs.CL

TL;DR: 作者提出COLDSELECT，首次联合优化verbalizer和实例选择，通过聚类和降维带来多样性和低不确定性，使prompt学习在无标签冷启动场景表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 目前prompt-based方法在模板、verbalizer和few-shot实例选择上表现敏感，尤其在无标签数据的冷启动条件下，且现有研究忽视了实例与verbalizer之间的依赖关系，需要更优的选择策略。

Method: 提出COLDSELECT方法，将预训练语言模型词表和[MASK]嵌入映射到共享空间，并进行降维与聚类，实现实例和verbalizer的联合选择，以优化不确定性和数据多样性。

Result: COLDSELECT在8个基准实验中均显著降低了不确定性，提升了泛化能力，在verbalizer及few-shot实例选择方面超越了现有基线方法。

Conclusion: COLDSELECT方法能够有效提升在无标注数据情况下（cold-start场景）prompt-based方法的鲁棒性和泛化表现，在8个基准测试中优于现有方法。

Abstract: Prompt-based methods leverage the knowledge of pre-trained language models
(PLMs) trained with a masked language modeling (MLM) objective; however, these
methods are sensitive to template, verbalizer, and few-shot instance selection,
particularly in cold-start settings with no labeled data. Existing studies
overlook the dependency between instances and verbalizers, where instance-label
probabilities depend on verbalizer token proximity in the embedding space. To
address this, we propose COLDSELECT, a joint verbalizer and instance selection
approach that models data diversity. COLDSELECT maps PLM vocabulary and
$h_{[MASK]}$ embeddings into a shared space, applying dimensionality reduction
and clustering to ensure efficient and diverse selection. By optimizing for
minimal uncertainty and maximal diversity, COLDSELECT captures data
relationships effectively. Experiments on eight benchmarks demonstrate
COLDSELECT's superiority in reducing uncertainty and enhancing generalization,
outperforming baselines in verbalizer and few-shot instance selection for
cold-start scenarios.

</details>


### [33] [Question Decomposition for Retrieval-Augmented Generation](https://arxiv.org/abs/2507.00355)
*Paul J. L. Ammann,Jonas Golde,Alan Akbik*

Main category: cs.CL

TL;DR: 通过把复杂问题分解为子问题并结合重排序，提高了大模型多跳问答任务中精准检索和准确回答的能力，无需额外训练，实现效果大幅提升。


<details>
  <summary>Details</summary>
Motivation: 传统的RAG在多跳问题中表现不佳，因为相关事实分布在多个文档中，标准检索难以查全全部信息，因此需要新的策略提升多文档综合能力。

Method: 提出了一种新的RAG（检索增强生成）流程：(1) 利用LLM将复杂问题分解为子问题；(2) 针对子问题分别进行检索；(3) 将所有检索到的候选文档合并并用交叉编码器重排序，从而提升信息覆盖度和精度。

Result: 在MultiHop-RAG与HotpotQA数据集上，方法提升了检索效果（MRR@10提升36.7%）与答案准确率（F1提升11.6%），明显优于标准RAG基线。

Conclusion: 将大语言模型中的问题分解与交叉编码器重排序结合，可以大幅提升多跳问题的检索效果和答案准确率，无需额外训练和特殊索引，具有实用性。

Abstract: Grounding large language models (LLMs) in verifiable external sources is a
well-established strategy for generating reliable answers. Retrieval-augmented
generation (RAG) is one such approach, particularly effective for tasks like
question answering: it retrieves passages that are semantically related to the
question and then conditions the model on this evidence. However, multi-hop
questions, such as "Which company among NVIDIA, Apple, and Google made the
biggest profit in 2023?," challenge RAG because relevant facts are often
distributed across multiple documents rather than co-occurring in one source,
making it difficult for standard RAG to retrieve sufficient information. To
address this, we propose a RAG pipeline that incorporates question
decomposition: (i) an LLM decomposes the original query into sub-questions,
(ii) passages are retrieved for each sub-question, and (iii) the merged
candidate pool is reranked to improve the coverage and precision of the
retrieved evidence. We show that question decomposition effectively assembles
complementary documents, while reranking reduces noise and promotes the most
relevant passages before answer generation. Although reranking itself is
standard, we show that pairing an off-the-shelf cross-encoder reranker with
LLM-driven question decomposition bridges the retrieval gap on multi-hop
questions and provides a practical, drop-in enhancement, without any extra
training or specialized indexing. We evaluate our approach on the MultiHop-RAG
and HotpotQA, showing gains in retrieval (MRR@10: +36.7%) and answer accuracy
(F1: +11.6%) over standard RAG baselines.

</details>


### [34] [Gregorian melody, modality, and memory: Segmenting chant with Bayesian nonparametrics](https://arxiv.org/abs/2507.00380)
*Vojtěch Lanz,Jan Hajič jr*

Main category: cs.CL

TL;DR: 用无监督分段方法分析圣歌旋律，在调式分类和记忆效率上获得新见解，但未支持centonisation的传统分段理论。


<details>
  <summary>Details</summary>
Motivation: 探讨格里高利圣歌旋律是否由某种乐句词汇（即分段片段）构成，该观点（centonisation theory）长期存在争议，尤其是分段策略的音乐学有效性。受到圣歌需记忆传承的启发，试图以无监督方式寻找最优旋律分段。

Method: 采用嵌套分层Pitman-Yor语言模型，对圣歌旋律进行最优无监督分段，评估分段对调式分类的性能，同时分析乐谱片段对记忆效率与调式的关系。

Result: 找到的分段方法在调式分类上达到最新最优表现，证实了调式类别与记忆效率之间的联系，发现旋律开头与结尾处更具公式化，契合其实用表演角色，但最终分段结果不等同于传统意义上的centonisation。

Conclusion: 即使采用了记忆最优的分段策略，所得结果仍不支持centonisation理论对“乐句词汇”的狭义理解，但对旋律结构与记忆之间关系提出了新的实证证据。

Abstract: The idea that Gregorian melodies are constructed from some vocabulary of
segments has long been a part of chant scholarship. This so-called
"centonisation" theory has received much musicological criticism, but frequent
re-use of certain melodic segments has been observed in chant melodies, and the
intractable number of possible segmentations allowed the option that some
undiscovered segmentation exists that will yet prove the value of
centonisation, and recent empirical results have shown that segmentations can
outperform music-theoretical features in mode classification. Inspired by the
fact that Gregorian chant was memorised, we search for an optimal unsupervised
segmentation of chant melody using nested hierarchical Pitman-Yor language
models. The segmentation we find achieves state-of-the-art performance in mode
classification. Modeling a monk memorising the melodies from one liturgical
manuscript, we then find empirical evidence for the link between mode
classification and memory efficiency, and observe more formulaic areas at the
beginnings and ends of melodies corresponding to the practical role of modality
in performance. However, the resulting segmentations themselves indicate that
even such a memory-optimal segmentation is not what is understood as
centonisation.

</details>


### [35] [Causal Prompting for Implicit Sentiment Analysis with Large Language Models](https://arxiv.org/abs/2507.00389)
*Jing Ren,Wenhao Zhou,Bowen Li,Mujie Liu,Nguyen Linh Dan Le,Jiade Cen,Liping Chen,Ziqi Xu,Xiwei Xu,Xiaodong Li*

Main category: cs.CL

TL;DR: 本工作提出了一种因果调节的链式推理提示框架CAPITAL，用于提升大模型在隐性情感分析任务上的准确性与抗偏见能力，在多个数据与模型上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 隐性情感分析（ISA）需要模型识别文本中未明说的情感，进行更深层次的推理。现有基于大语言模型（LLMs）的提示方法在ISA上有进展，但大多只依赖于多链思维（CoT）路径的多数表决，没有判断因果有效性，容易出现偏见和伪相关。

Method: 提出了一种基于因果提示的新方法CAPITAL，将front-door因果调整机制引入链式推理（CoT）中，通过编码器聚类和NWGM估算，分解输入对推理链和推理链对输出的因果影响，并用对比学习优化编码器与LLM的推理空间对齐。

Result: 在ISA基准数据集和三种LLM上，CAPITAL方法无论在准确率还是鲁棒性上均显著优于强基线模型，尤其在对抗条件下表现更好。

Conclusion: CAPITAL为LLM提示融合因果推理提供了系统性方法，能有效提升模型的情感推理能力和对偏见的鲁棒性。

Abstract: Implicit Sentiment Analysis (ISA) aims to infer sentiment that is implied
rather than explicitly stated, requiring models to perform deeper reasoning
over subtle contextual cues. While recent prompting-based methods using Large
Language Models (LLMs) have shown promise in ISA, they often rely on majority
voting over chain-of-thought (CoT) reasoning paths without evaluating their
causal validity, making them susceptible to internal biases and spurious
correlations. To address this challenge, we propose CAPITAL, a causal prompting
framework that incorporates front-door adjustment into CoT reasoning. CAPITAL
decomposes the overall causal effect into two components: the influence of the
input prompt on the reasoning chains, and the impact of those chains on the
final output. These components are estimated using encoder-based clustering and
the NWGM approximation, with a contrastive learning objective used to better
align the encoder's representation with the LLM's reasoning space. Experiments
on benchmark ISA datasets with three LLMs demonstrate that CAPITAL consistently
outperforms strong prompting baselines in both accuracy and robustness,
particularly under adversarial conditions. This work offers a principled
approach to integrating causal inference into LLM prompting and highlights its
benefits for bias-aware sentiment reasoning. The source code and case study are
available at: https://github.com/whZ62/CAPITAL.

</details>


### [36] [Beyond Sociodemographic Prompting: Using Supervision to Align LLMs with Human Response Distributions](https://arxiv.org/abs/2507.00439)
*Gauri Kambhatla,Sanjana Gautam,Angela Zhang,Alex Liu,Ravi Srinivasan,Junyi Jessy Li,Matthew Lease*

Main category: cs.CL

TL;DR: 通过简单监督方法，可以明显提升语言模型在不同群体间主观问题预测的一致性，方法简单易实现，并为未来相关研究提供了公开基准和资源。


<details>
  <summary>Details</summary>
Motivation: 准确预测不同群体对主观问题的回答对于许多实际应用具有重要价值。现有方法在群体多样性上的一致性和表现还存在不足，因此亟需改进。

Method: 本文通过应用相对简单的监督方法，对语言模型进行对齐，以提高其在多样化群体间主观问题回答的一致性。方法在三个涵盖不同话题的数据集上进行实验，并评估不同群体之间的对齐表现。还对多种大型语言模型和提示策略进行了比较测试。

Result: 实验显示，简单的监督显著提升了语言模型在多群体上的对齐程度和预测表现。评估了平均表现并报告了不同群体间的表现差异。

Conclusion: 通过简单监督可以有效提升语言模型对不同群体的主观问题对齐能力，且方法易于推广。开放源代码和综合基准有助于促进该领域未来研究。

Abstract: The ability to accurately predict how different population groups would
answer subjective questions would have great value. In this work, we show that
use of relatively simple supervision can greatly improve language model
alignment with diverse population groups, as measured over three datasets
spanning various topics. Beyond evaluating average performance, we also report
how alignment varies across specific groups. The simplicity and generality of
our approach promotes easy adoption, while our broad findings provide useful
guidance for when to use or not use our approach in practice. By conducting
evaluation over many LLMs and prompting strategies, along with open-sourcing
our work, we provide a useful benchmark to stimulate future research.

</details>


### [37] [Pitfalls of Evaluating Language Models with Open Benchmarks](https://arxiv.org/abs/2507.00460)
*Md. Najib Hasan,Mohammad Fakhruddin Babar,Souvika Sarkar,Monowar Hasan,Santu Karmaker*

Main category: cs.CL

TL;DR: 开放基准便利但易被作弊，排行榜成绩未必可靠，提出需结合私有评测并重审当前评测方式。


<details>
  <summary>Details</summary>
Motivation: 虽然开放的大型语言模型（LLM）基准如HELM和BIG-bench促进了模型的公平比较和复现，但其公开性可能导致被恶意利用，影响评测的公正性。

Method: 作者通过系统地构建了“作弊”模型（将BART、T5和GPT-2的小型变体直接在公开测试集上微调），以测试在HELM等开放性基准上的表现。

Result: 这些“作弊”模型能在HELM这类公开基准中获得顶尖排名，但它们在泛化能力和实际应用价值上都较差。

Conclusion: 高分数的排行榜成绩未必代表真实的实际能力，开放评测应辅以私有或动态基准，当前评测体系亟需重新评估，以确保评测的公正和可靠。

Abstract: Open Large Language Model (LLM) benchmarks, such as HELM and BIG-bench, offer
standardized, transparent protocols that facilitate the fair comparison,
reproducibility, and iterative advancement of Language Models (LMs). However,
their openness also introduces critical and underexplored pitfalls. This study
exposes these weaknesses by systematically constructing ``cheating'' models --
smaller variants of BART, T5, and GPT-2 fine-tuned directly on public test sets
-- which achieve top rankings on a prominent open, holistic benchmark (HELM)
despite poor generalization and limited practical utility. Our findings
underscore three key insights: \ca high leaderboard performance on open
benchmarks may not always reflect real-world effectiveness; \cb private or
dynamic benchmarks must complement open evaluations to safeguard integrity; and
\cc a fundamental reevaluation of current benchmarking practices is essential
to ensure robust and trustworthy LM assessments.

</details>


### [38] [TeamCMU at Touché: Adversarial Co-Evolution for Advertisement Integration and Detection in Conversational Search](https://arxiv.org/abs/2507.00509)
*To Eun Kim,João Coelho,Gbemileke Onilude,Jai Singh*

Main category: cs.CL

TL;DR: 提出了针对生成式对话搜索广告插入的分步解决方案，通过合成数据训练高效广告识别器，并用其优化广告内容的隐蔽植入，实现广告检测和无感插入协同增强。


<details>
  <summary>Details</summary>
Motivation: 随着基于生成式大型语言模型（LLM）和检索增强生成（RAG）的对话式搜索引擎普及，将广告无缝集成到生成内容中带来了商业新机遇，同时也引发了信息/广告界限模糊、用户信任及透明度等挑战。

Method: 本文提出了一套模块化广告管理流程，包括广告重写器（用于生成内容中无缝整合广告）和广告分类器（用于检测广告内容）。使用基于营销策略合成的数据训练广告分类器，并用该分类器指导两类广告插入策略：一是监督微调广告重写器，二是从多个候选中选择最难被检测出带广告内容的结果。

Result: 合成数据+课程学习训练的分类器对多种广告整合策略有良好检测性能。基于分类器引导的优化（微调和多样本选择）能极大提升广告的隐蔽性，实现更自然、无痕的广告融入。

Conclusion: 本文为RAG对话系统广告管理带来了广告分类与无痕插入的协同进化框架，有助于未来广告感知的生成式搜索系统和更强健的广告检测方法发展。

Abstract: As conversational search engines increasingly adopt generation-based
paradigms powered by Large Language Models (LLMs) and Retrieval-Augmented
Generation (RAG), the integration of advertisements into generated responses
presents both commercial opportunities and challenges for user experience.
Unlike traditional search, where advertisements are clearly delineated,
generative systems blur the boundary between informational content and
promotional material, raising concerns around transparency and trust. In this
work, we propose a modular pipeline for advertisement management in RAG-based
conversational systems, consisting of an ad-rewriter for seamless ad
integration and a robust ad-classifier for detection. We leverage synthetic
data to train high-performing classifiers, which are then used to guide two
complementary ad-integration strategies: supervised fine-tuning of the
ad-rewriter and a best-of-N sampling approach that selects the least detectable
ad-integrated response among multiple candidates. Our evaluation focuses on two
core questions: the effectiveness of ad classifiers in detecting diverse ad
integration strategies, and the training methods that best support coherent,
minimally intrusive ad insertion. Experimental results show that our
ad-classifier, trained on synthetic advertisement data inspired by marketing
strategies and enhanced through curriculum learning, achieves robust detection
performance. Additionally, we demonstrate that classifier-guided optimization,
through both fine-tuning and best-of-N sampling, significantly improves ad
stealth, enabling more seamless integration. These findings contribute an
adversarial co-evolution framework for developing more sophisticated ad-aware
generative search systems and robust ad classifiers.

</details>


### [39] [NIRANTAR: Continual Learning with New Languages and Domains on Real-world Speech Data](https://arxiv.org/abs/2507.00534)
*Tahir Javed,Kaushal Bhogale,Mitesh M. Khapra*

Main category: cs.CL

TL;DR: 本文提出Nirantar框架，基于真实多语种、多领域印度语音数据评估ASR持续学习，揭示了现有方法的不足，并为后续研究提供了新基准。


<details>
  <summary>Details</summary>
Motivation: 多语种、多领域的连续学习在现实中具有很大挑战，而现有持续学习评估通常基于模拟环境，缺乏代表性，难以真实反映模型性能。

Method: 提出了Nirantar框架，利用从印度22种语言和208个地区逐步收集的语音数据，支持语言增量、领域增量和两者结合的新型持续学习评估。利用真实语料而非模拟数据系统性进行基准测试。

Result: 引入了包含3250小时人工转录的新数据集并系统评估了现有持续学习方法，发现这些方法无法稳定应对复杂的任务。

Conclusion: 当前没有单一的持续学习方法能够在多语言、多领域ASR中始终表现优异，需要更强健的新策略。

Abstract: We introduce Nirantar, a comprehensive framework for evaluating continual
learning (CL) in multilingual and multi-domain ASR. Designed to reflect
real-world CL challenges, Nirantar leverages data collected incrementally
across 22 languages and 208 districts in India through natural episodes. This
enables evaluation across Language-Incremental (LIL), Domain-Incremental (DIL),
and the novel Language-Incremental Domain-Incremental Learning (LIDIL)
scenarios. Unlike prior work that relies on simulated episodes, Nirantar
presents dynamic, non-uniform language and domain shifts, making it an ideal
testbed for CL research. With 3250 hours of human-transcribed speech, including
1720 hours newly introduced in this work, our framework enables systematic
benchmarking of CL methods. We evaluate existing approaches and demonstrate
that no single method performs consistently well, underscoring the need for
more robust CL strategies.

</details>


### [40] [Capsule Network-Based Semantic Intent Modeling for Human-Computer Interaction](https://arxiv.org/abs/2507.00540)
*Shixiao Wang,Yifan Zhuang,Runsheng Zhang,Zhijun Song*

Main category: cs.CL

TL;DR: 提出基于胶囊网络的意图识别方法，显著提升了识别准确率和鲁棒性，优于现有主流模型。


<details>
  <summary>Details</summary>
Motivation: 现有人机交互中的意图识别准确率不足，面临着语义建模能力受限的问题。

Method: 提出基于胶囊网络的用户语义意图建模算法，通过向量化胶囊结构表示语义特征，动态路由机制实现多层胶囊信息流转，结合卷积特征提取模块，采用margin机制优化损失函数。

Result: 在公开的自然语言理解数据集上，提出模型在准确率、F1及意图检测率方面均优于传统方法和主流深度学习结构。分析了动态路由迭代次数对性能的影响并给出了损失函数收敛曲线，验证了模型的稳定性和有效性。

Conclusion: 新提出的胶囊网络结构能有效提升复杂语义条件下的意图识别性能，为语义建模提供了有效思路。

Abstract: This paper proposes a user semantic intent modeling algorithm based on
Capsule Networks to address the problem of insufficient accuracy in intent
recognition for human-computer interaction. The method represents semantic
features in input text through a vectorized capsule structure. It uses a
dynamic routing mechanism to transfer information across multiple capsule
layers. This helps capture hierarchical relationships and part-whole structures
between semantic entities more effectively. The model uses a convolutional
feature extraction module as the low-level encoder. After generating initial
semantic capsules, it forms high-level abstract intent representations through
an iterative routing process. To further enhance performance, a margin-based
mechanism is introduced into the loss function. This improves the model's
ability to distinguish between intent classes. Experiments are conducted using
a public natural language understanding dataset. Multiple mainstream models are
used for comparison. Results show that the proposed model outperforms
traditional methods and other deep learning structures in terms of accuracy,
F1-score, and intent detection rate. The study also analyzes the effect of the
number of dynamic routing iterations on model performance. A convergence curve
of the loss function during training is provided. These results verify the
stability and effectiveness of the proposed method in semantic modeling.
Overall, this study presents a new structured modeling approach to improve
intent recognition under complex semantic conditions.

</details>


### [41] [Methodological Rigour in Algorithm Application: An Illustration of Topic Modelling Algorithm](https://arxiv.org/abs/2507.00547)
*Malmi Amadoru*

Main category: cs.CL

TL;DR: 本文针对计算密集型研究中的主题建模方法，探讨了其方法学规范性，提出了适用于该领域的操作指南，为提升相关算法研究的可信度和质量提供了有益参考。


<details>
  <summary>Details</summary>
Motivation: 随着计算算法的进步，越来越多研究采用高计算强度的方法进行理论开发，但这些算法的不透明性和使用过程中的不规范问题带来了方法学挑战，影响了研究可信度。因此作者希望提升此类研究中方法学的规范性与透明度。

Method: 作者以结构化主题建模算法为例，演示其应用流程，并提出了一系列确保主题建模研究规范性的操作指南。

Result: 提出了确保主题建模（及其他相关算法）研究方法学规范性的具体准则，这些准则对初学者、编辑和审稿人均有参考价值。

Conclusion: 本文加强了主题建模领域关于方法学规范性的讨论，并为计算密集型理论建构研究领域的后续工作提供了有益指导。

Abstract: The rise of advanced computational algorithms has opened new avenues for
computationally intensive research approaches to theory development. However,
the opacity of these algorithms and lack of transparency and rigour in their
application pose methodological challenges, potentially undermining trust in
research. The discourse on methodological rigour in this new genre of research
is still emerging. Against this backdrop, I attempt to offer guidance on
methodological rigour, particularly in the context of topic modelling
algorithms. By illustrating the application of the structural topic modelling
algorithm and presenting a set of guidelines, I discuss how to ensure rigour in
topic modelling studies. Although the guidelines are for the application of
topic modelling algorithms, they can be applied to other algorithms with
context-specific adjustments. The guidelines are helpful, especially for novice
researchers applying topic modelling, and editors and reviewers handling topic
modelling manuscripts. I contribute to the literature on topic modelling and
join the emerging dialogue on methodological rigour in computationally
intensive theory construction research.

</details>


### [42] [TUM-MiKaNi at SemEval-2025 Task 3: Towards Multilingual and Knowledge-Aware Non-factual Hallucination Identification](https://arxiv.org/abs/2507.00579)
*Miriam Anschütz,Ekaterina Gikalo,Niklas Herbster,Georg Groh*

Main category: cs.CL

TL;DR: 本文针对多语言环境下的LLM幻觉识别，提出结合检索核查和BERT微调的系统，在多语言测试中取得优异成绩，为提升LLM可信度和多场景应用提供了有力工具。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM幻觉（胡编现象）问题严重，大多数相关研究聚焦在英语语料，忽视了LLM的多语言特性。

Method: 提出了一个两阶段流程，结合基于检索的维基百科事实核查和微调过的BERT系统，以识别常见幻觉模式。

Result: 该系统在所有语言上表现出竞争力，在包括英语在内的八种语言中进入前十。此外，还支持任务覆盖之外的多种语言。

Conclusion: 该多语言幻觉识别器有助于提升LLM输出的可靠性与实用性，推动其广泛应用。

Abstract: Hallucinations are one of the major problems of LLMs, hindering their
trustworthiness and deployment to wider use cases. However, most of the
research on hallucinations focuses on English data, neglecting the multilingual
nature of LLMs. This paper describes our submission to the SemEval-2025 Task-3
- Mu-SHROOM, the Multilingual Shared-task on Hallucinations and Related
Observable Overgeneration Mistakes. We propose a two-part pipeline that
combines retrieval-based fact verification against Wikipedia with a BERT-based
system fine-tuned to identify common hallucination patterns. Our system
achieves competitive results across all languages, reaching top-10 results in
eight languages, including English. Moreover, it supports multiple languages
beyond the fourteen covered by the shared task. This multilingual hallucination
identifier can help to improve LLM outputs and their usefulness in the future.

</details>


### [43] [Transferable Modeling Strategies for Low-Resource LLM Tasks: A Prompt and Alignment-Based](https://arxiv.org/abs/2507.00601)
*Shuangquan Lyu,Yingnan Deng,Guiran Liu,Zhen Qi,Ruotong Wang*

Main category: cs.CL

TL;DR: 本文针对大语言模型在低资源语言场景下表现有限的问题，提出结合知识迁移和高效微调的新框架，并通过多项跨语言任务实验验证了方法的优越性与鲁棒性，特别在数据稀缺时效果显著。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在低资源语言场景下的迁移与适应能力有限，存在表现不佳和训练不稳的问题，因此亟需新的方法提升其泛化与适应能力。

Method: 提出了一个统一框架，将知识迁移模块与高效参数微调策略结合。方法包括引入知识对齐损失与软提示调优，引导模型在极少标注下吸收目标语言或任务的结构特征，并采用轻量化适配模块以降低计算成本。同时，整合冻结和提词（prompt injection）策略，兼顾模型对原有知识的保持和快速适应新任务的能力。此外，设计了稳定性分析和伪数据迁移实验，系统评测方法的适用性与鲁棒性。

Result: 在MLQA、XQuAD、PAWS-X等跨语言任务中，所提方法较主流多语预训练模型和迁移方法表现出更高的性能和更好的稳定性。在极端缺少数据的条件下尤其具备明显优势。

Conclusion: 该方法具有高度通用性与扩展性，能在提高大模型任务适应性的同时保留其通用能力，特别适合复杂语义建模和多语种处理场景。

Abstract: This paper addresses the limited transfer and adaptation capabilities of
large language models in low-resource language scenarios. It proposes a unified
framework that combines a knowledge transfer module with parameter-efficient
fine-tuning strategies. The method introduces knowledge alignment loss and soft
prompt tuning to guide the model in effectively absorbing the structural
features of target languages or tasks under minimal annotation. This enhances
both generalization performance and training stability. The framework includes
lightweight adaptation modules to reduce computational costs. During training,
it integrates freezing strategies and prompt injection to preserve the model's
original knowledge while enabling quick adaptation to new tasks. The study also
conducts stability analysis experiments and synthetic pseudo-data transfer
experiments to systematically evaluate the method's applicability and
robustness across different low-resource tasks. Experimental results show that
compared with existing multilingual pre-trained models and mainstream transfer
methods, the proposed approach achieves higher performance and stability on
cross-lingual tasks such as MLQA, XQuAD, and PAWS-X. It demonstrates
particularly strong advantages under extremely data-scarce conditions. The
proposed method offers strong generality and scalability. It enhances
task-specific adaptability while preserving the general capabilities of large
language models. This makes it well-suited for complex semantic modeling and
multilingual processing tasks.

</details>


### [44] [Mixture of Reasonings: Teach Large Language Models to Reason with Adaptive Strategies](https://arxiv.org/abs/2507.00606)
*Tao Xiong,Xavier Hu,Wenyan Fan,Shengyu Zhang*

Main category: cs.CL

TL;DR: 本文提出Mixture of Reasoning（MoR）框架，通过整合多种推理策略，使大模型无需人工任务特定prompt即可自适应多任务推理，大幅提升推理性能，方法通用且实用。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型虽然在复杂任务中表现出色，但高度依赖于人工设计的、针对具体任务的提示（prompt）；这限制了其适应性和效率。

Method: 提出了Mixture of Reasoning (MoR)训练框架，通过引入多样推理策略，使模型能够自主适应不同任务、无需外部提示工程。MoR包含两个阶段：1. 思路生成。利用GPT-4o等生成推理链模板；2. SFT数据集构建。将模板与数据集组合，用于监督微调。

Result: 实验证明，采用MoR150模型在不同提示方法下均提升显著：使用CoT提升2.2%，全面提升达13.5%，均优于各项基线。

Conclusion: MoR框架能够消除人工任务定制prompt的需求，为多任务推理提供了通用而强健的解决方案。

Abstract: Large language models (LLMs) excel in complex tasks through advanced
prompting techniques like Chain-of-Thought (CoT) and Tree-of-Thought (ToT), but
their reliance on manually crafted, task-specific prompts limits adaptability
and efficiency. We introduce Mixture of Reasoning (MoR), a training framework
that embeds diverse reasoning strategies into LLMs for autonomous,
task-adaptive reasoning without external prompt engineering. MoR has two
phases: Thought Generation, creating reasoning chain templates with models like
GPT-4o, and SFT Dataset Construction, pairing templates with benchmark datasets
for supervised fine-tuning.Our experiments show that MoR significantly enhances
performance, with MoR150 achieving 0.730 (2.2% improvement) using CoT prompting
and 0.734 (13.5% improvement) compared to baselines. MoR eliminates the need
for task-specific prompts, offering a generalizable solution for robust
reasoning across diverse tasks.

</details>


### [45] [SAFER: Probing Safety in Reward Models with Sparse Autoencoder](https://arxiv.org/abs/2507.00665)
*Sihang Li,Wei Shi,Ziyuan Xie,Tao Liang,Guojun Ma,Xiang Wang*

Main category: cs.CL

TL;DR: SAFER框架用稀疏自编码器揭示了奖励模型中的可解释特征，实现更好理解并改善大语言模型安全性的奖励机制，同时能精准调整安全对齐效果而不损失整体性能。


<details>
  <summary>Details</summary>
Motivation: RLHF虽然是将大语言模型与人类价值观对齐的关键方法，但其核心的奖励模型仍然较为“黑箱”，即不透明，难以理解和解释。动机是提升奖励模型的可解释性和安全性。

Method: 作者提出了SAFER框架（利用稀疏自编码器Sparse Autoencoders），能够从奖励模型的激活中提取出可被人理解的特征，并通过分析不同响应（被选中和被拒绝）之间的激活差异来衡量各特征的重要性。基于这些特征，设计了有针对性的数据污染和去噪方法，进一步用于安全性提升。

Result: 实验证明，通过少量修改数据，SAFER框架可以精确地提升或削弱模型在安全性上的对齐能力，同时不会影响常规聊天性能。

Conclusion: SAFER为奖励模型的解释、审计和改进提供了新工具，助力高风险场景下的大语言模型的对齐安全性。

Abstract: Reinforcement learning from human feedback (RLHF) is a key paradigm for
aligning large language models (LLMs) with human values, yet the reward models
at its core remain largely opaque. In this work, we present sparse Autoencoder
For Enhanced Reward model (\textbf{SAFER}), a novel framework for interpreting
and improving reward models through mechanistic analysis. Leveraging Sparse
Autoencoders (SAEs), we uncover human-interpretable features in reward model
activations, enabling insight into safety-relevant decision-making. We apply
SAFER to safety-oriented preference datasets and quantify the salience of
individual features by activation differences between chosen and rejected
responses. Using these feature-level signals, we design targeted data poisoning
and denoising strategies. Experiments show that SAFER can precisely degrade or
enhance safety alignment with minimal data modification, without sacrificing
general chat performance. Our approach contributes to interpreting, auditing
and refining reward models in high-stakes LLM alignment tasks. Our codes are
available at https://github.com/xzy-101/SAFER-code. \textit{This paper
discusses topics related to large language model safety and may include
discussions or examples that highlight potential risks or unsafe outcomes.}

</details>


### [46] [Contrasting Cognitive Styles in Vision-Language Models: Holistic Attention in Japanese Versus Analytical Focus in English](https://arxiv.org/abs/2507.00700)
*Ahmed Sabir,Azinovič Gasper,Mengsay Loem,Rajesh Sharma*

Main category: cs.CL

TL;DR: 本研究发现，以不同文化语言训练的视觉-语言模型，其输出存在整体和分析认知风格的文化差异，说明模型在无意识中学习并再现了人类文化中的感知与认知习惯。


<details>
  <summary>Details</summary>
Motivation: 不同文化背景下的人在视觉感知和认知上表现出不同的处理方式，两大典型文化（东亚和西方）展现出整体与分析的差异。本研究希望探究视觉-语言模型（VLMs）是否同样受到其训练语言文化的影响，呈现不同的文化知觉模式。

Method: 对以日语和英语为主训练的视觉-语言模型，进行图像描述产出的对比分析，检验模型在描述时是否反映出整体和分析性处理的文化差异。

Result: 研究发现，VLMs不仅学习了语言的结构特性，还复现了训练数据中所蕴含的文化行为。模型输出表现出与人类类似的整体与分析性倾向，显示出文化认知对模型输出有隐性影响。

Conclusion: VLMs在训练过程中，会内化语言结构及文化知觉模式，间接反映了模型背后数据的文化特征。这说明AI模型可能无意中继承并再现人类文化认知倾向。

Abstract: Cross-cultural research in perception and cognition has shown that
individuals from different cultural backgrounds process visual information in
distinct ways. East Asians, for example, tend to adopt a holistic perspective,
attending to contextual relationships, whereas Westerners often employ an
analytical approach, focusing on individual objects and their attributes. In
this study, we investigate whether Vision-Language Models (VLMs) trained
predominantly on different languages, specifically Japanese and English,
exhibit similar culturally grounded attentional patterns. Using comparative
analysis of image descriptions, we examine whether these models reflect
differences in holistic versus analytic tendencies. Our findings suggest that
VLMs not only internalize the structural properties of language but also
reproduce cultural behaviors embedded in the training data, indicating that
cultural cognition may implicitly shape model outputs.

</details>


### [47] [AI Analyst: Framework and Comprehensive Evaluation of Large Language Models for Financial Time Series Report Generation](https://arxiv.org/abs/2507.00718)
*Elizabeth Fons,Elena Kochkina,Rachneet Kaur,Zhen Zeng,Berowne Hlavaty,Charese Smiley,Svitlana Vyetrenko,Manuela Veloso*

Main category: cs.CL

TL;DR: 本文提出了一套用大语言模型从时序数据自动生成金融报告的系统，利用新颖的信息高亮与分类手段，有效评估了模型的真实性与推理能力，实验显示其生成报告具有良好的连贯性和信息量。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLM）在金融报告自动化上的潜力尚未充分挖掘，需要探索其处理时序数据生成财报的能力和方法。

Method: 提出了一个综合框架，涵盖提示工程、模型选择和评估，并引入自动化高亮系统，用以区分报告中基于数据、金融推理和外部知识的信息。

Result: 实验证明，LLM可以根据真实股指和合成时序数据，生成连贯且信息丰富的财务报告。

Conclusion: 本文提出的系统有效验证了LLM在生成金融报告方面的能力，为其在金融自动化领域的应用提供了依据和思路。

Abstract: This paper explores the potential of large language models (LLMs) to generate
financial reports from time series data. We propose a framework encompassing
prompt engineering, model selection, and evaluation. We introduce an automated
highlighting system to categorize information within the generated reports,
differentiating between insights derived directly from time series data,
stemming from financial reasoning, and those reliant on external knowledge.
This approach aids in evaluating the factual grounding and reasoning
capabilities of the models. Our experiments, utilizing both data from the real
stock market indices and synthetic time series, demonstrate the capability of
LLMs to produce coherent and informative financial reports.

</details>


### [48] [LitBench: A Benchmark and Dataset for Reliable Evaluation of Creative Writing](https://arxiv.org/abs/2507.00769)
*Daniel Fein,Sebastian Russo,Violet Xiang,Kabir Jolly,Rafael Rafailov,Nick Haber*

Main category: cs.CL

TL;DR: 提出了创造性写作自动测评基准LitBench和奖励模型，显著提升了大语言模型生成文本评判的可靠性和准确性，成果和资源已开源。


<details>
  <summary>Details</summary>
Motivation: 在LLM（大语言模型）生成的创造性写作文本评估中，由于叙述开放且缺乏“标准答案”，缺少高效、可靠的自动化评估方法，而常规的零样本模型做评判时，效果和可靠性尚不明确。

Method: 1. 提出LitBench基准和配套数据集，内含2480个人工标注且消除偏见的故事对比测试集，以及43827个人类偏好标签的训练集；2. 利用LitBench评测零样本（off-the-shelf）LLM评判员的表现；3. 用训练集对Bradley-Terry模型和生成式奖励模型进行训练；4. 通过线上真人研究验证奖励模型对新生成故事的排序与人类一致性。

Result: 在现成LLM评判员中，Claude-3.7-Sonnet表现最好，与人类偏好达73%一致率；而训练得到的Bradley-Terry和生成式奖励模型准确率达78%，超过全部OTS评判员。线上真人实验进一步确认，奖励模型在新故事上结果与人类一致。

Conclusion: 作者推出LitBench数据集和相应的奖励模型，为创造性写作系统的自动化测评和优化提供了可靠资源，提升了自动评估的一致性与准确性。

Abstract: Evaluating creative writing generated by large language models (LLMs) remains
challenging because open-ended narratives lack ground truths. Without
performant automated evaluation methods, off-the-shelf (OTS) language models
are employed as zero-shot judges, yet their reliability is unclear in this
context. In pursuit of robust evaluation for creative writing, we introduce
LitBench, the first standardized benchmark and paired dataset for creative
writing verification, comprising a held-out test set of 2,480 debiased,
human-labeled story comparisons drawn from Reddit and a 43,827-pair training
corpus of human preference labels. Using LitBench, we (i) benchmark zero-shot
LLM judges, (ii) train Bradley Terry and generative reward models, and (iii)
conduct an online human study to validate reward model rankings on newly
LLM-generated stories. Our benchmark identifies Claude-3.7-Sonnet as the
strongest off-the-shelf judge, reaching 73% agreement with human preferences;
among trained reward models, Bradley-Terry and Generative reward models both
attain an accuracy of 78%, outperforming all off-the-shelf judges. An online
human study further confirms that our trained reward models consistently align
with human preferences in novel LLM-generated stories. We release LitBench and
reward models at
https://huggingface.co/collections/SAA-Lab/litbench-68267b5da3aafe58f9e43461,
providing a vetted resource for reliable, automated evaluation and optimization
of creative writing systems.

</details>


### [49] [A Diagrammatic Calculus for a Functional Model of Natural Language Semantics](https://arxiv.org/abs/2507.00782)
*Matthieu Pierre Boyer*

Main category: cs.CL

TL;DR: 提出用函数式编程和范畴类型系统结合效应图示演算的方法，有效提升了自然语言语义建模的表达力和计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统的自然语言语义学表达方式在表达能力上存在局限性，需要更具表现力的方法。

Method: 本文提出了一种基于函数式编程的自然语言语义分析方法，形式化了基于范畴的类型和效应系统，并构建了一个图示演算来建模句子的解析和语义效应的处理。

Result: 通过使用这种系统，可以更加高效且有表现力地计算句子的语义表征（denotations）。

Conclusion: 本文的方法提升了自然语言语义建模的表达能力，并通过范畴类型和效应系统进行形式化，同时通过图示演算简化了解析与效应处理过程。

Abstract: In this paper, we study a functional programming approach to natural language
semantics, allowing us to increase the expressivity of a more traditional
denotation style. We will formalize a category based type and effect system,
and construct a diagrammatic calculus to model parsing and handling of effects,
and use it to efficiently compute the denotations for sentences.

</details>


### [50] [Generative AI and the future of scientometrics: current topics and future questions](https://arxiv.org/abs/2507.00783)
*Benedetto Lepori,Jens Peter Andersen,Karsten Donnay*

Main category: cs.CL

TL;DR: 该论文系统回顾了GenAI在科学计量学中的应用及影响。GenAI适合语言生成类任务，但在需要深层推理的应用中有限制。建议需不断比较不同GenAI模型，并关注其对该领域主要评估体系的潜在改变。


<details>
  <summary>Details</summary>
Motivation: 本论文旨在回顾生成式人工智能（GenAI）在科学计量学中的应用，并探讨其对该领域的更广泛影响。

Method: 作者首先介绍了GenAI的生成性和概率性本质，并将其与人类“推理”能力的模拟进行讨论。随后，批判性分析了GenAI在科学计量学中的实验应用，如主题标注、引文语境分析、预测应用、学者画像和科研评估。最后，探讨了GenAI生成大量科学文本对科学计量学领域核心度量（如作者、词汇、参考文献等）的潜在影响。

Result: GenAI在以语言生成为主的任务（如主题标注）中显示出潜力，但在需要稳定语义、语用推理或结构化领域知识的任务上存在局限。此外，GenAI带来的影响十分迅速，需持续对比不同模型的表现。同时，GenAI对科学文本特性的改变可能对科学计量学的度量产生根本性影响。

Conclusion: GenAI有望在某些科学计量学任务中提升效率，但其局限与变革性影响需持续关注和系统比较。未来应加强实证和理论研究，以把握知识生产模式的演变。

Abstract: The aim of this paper is to review the use of GenAI in scientometrics, and to
begin a debate on the broader implications for the field. First, we provide an
introduction on GenAI's generative and probabilistic nature as rooted in
distributional linguistics. And we relate this to the debate on the extent to
which GenAI might be able to mimic human 'reasoning'. Second, we leverage this
distinction for a critical engagement with recent experiments using GenAI in
scientometrics, including topic labelling, the analysis of citation contexts,
predictive applications, scholars' profiling, and research assessment. GenAI
shows promise in tasks where language generation dominates, such as labelling,
but faces limitations in tasks that require stable semantics, pragmatic
reasoning, or structured domain knowledge. However, these results might become
quickly outdated. Our recommendation is, therefore, to always strive to
systematically compare the performance of different GenAI models for specific
tasks. Third, we inquire whether, by generating large amounts of scientific
language, GenAI might have a fundamental impact on our field by affecting
textual characteristics used to measure science, such as authors, words, and
references. We argue that careful empirical work and theoretical reflection
will be essential to remain capable of interpreting the evolving patterns of
knowledge production.

</details>


### [51] [ProxAnn: Use-Oriented Evaluations of Topic Models and Document Clustering](https://arxiv.org/abs/2507.00828)
*Alexander Hoyle,Lorena Calvo-Bartolomé,Jordan Boyd-Graber,Philip Resnik*

Main category: cs.CL

TL;DR: 本文设计了一种可扩展的主题模型和聚类模型人类评价模式及其LLM自动代理，经大量实验证实最优LLM代理与人工评价一致，可应用于模型自动评测，成果已开源。


<details>
  <summary>Details</summary>
Motivation: 当前主题模型和文档聚类的评价方法，主要依赖自动化指标或专家标注，但前者与人工偏好不一致，后者又难以大规模应用，因此亟需一种既符合实际需求又能扩展的评价方式。

Method: 设计了一种可扩展的人类评价方案和相应的自动近似评价流程。方法包括让人工标注员或LLM代理对分配到某一主题或聚类的文本进行浏览，推断该组的标签类别，并将该类别应用到新的文档上。

Result: 收集了大量众包标注数据，并验证了自动代理的有效性。实验发现，表现最好的LLM代理与人工标注在统计意义上无法区分，能够作为自动评价的可靠替代。

Conclusion: 提出的评价协议和自动近似方法能够高效、准确地反映主题模型及聚类模型的真实应用成效，LLM代理可大幅降低人工成本，推荐用于实际自动评价场景。

Abstract: Topic model and document-clustering evaluations either use automated metrics
that align poorly with human preferences or require expert labels that are
intractable to scale. We design a scalable human evaluation protocol and a
corresponding automated approximation that reflect practitioners' real-world
usage of models. Annotators -- or an LLM-based proxy -- review text items
assigned to a topic or cluster, infer a category for the group, then apply that
category to other documents. Using this protocol, we collect extensive
crowdworker annotations of outputs from a diverse set of topic models on two
datasets. We then use these annotations to validate automated proxies, finding
that the best LLM proxies are statistically indistinguishable from a human
annotator and can therefore serve as a reasonable substitute in automated
evaluations. Package, web interface, and data are at
https://github.com/ahoho/proxann

</details>


### [52] [Stylometry recognizes human and LLM-generated texts in short samples](https://arxiv.org/abs/2507.00838)
*Karol Przystalski,Jan K. Argasiński,Iwona Grabska-Gradzińska,Jeremi K. Ochab*

Main category: cs.CL

TL;DR: 本文通过风格计量方法与机器学习模型，有效实现了对人类和LLM生成文本的区分，表现优异，对模型溯源、知识产权和AI伦理应用具有边界性意义。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）生成文本能力增强，如何区分人类与AI文本、明确模型归属、保护知识产权、并促进伦理AI使用成为亟需解决的问题。以往主要采用文体计量法（stylometry）分析作者风格，但对于LLM生成文本的辨识仍需探索。

Method: 构建了一个包含Wikipedia中人写简介、不同LLM（GPT-3.5/4, LLaMa 2/3等）生成文本、多种摘要及改写方法处理文本的基准数据集。利用决策树与LightGBM等树模型，结合人工设计的StyloMetrix特征及n-gram文本特征，对10句长文本进行风格计量分析和分类。采用Shapley分析解释重要特征。

Result: 在七分类情景下，模型的Matthews相关系数最高达到0.87，在二分类中，准确率介于0.79至1之间，Wikipedia与GPT-4样本上平衡数据集准确率可达0.98。Shapley分析显示百科类文本特征、特定高频词和LLM生成文本的高语法一致性是区分关键。实验证明即便面临先进LLM，也可在特定文本类型下有效辨识人和机器文本。

Conclusion: 文体计量结合树模型可高效区分人类与LLM生成文本，特别是在界定明晰的文本类型（如百科条目）情况下，为模型来源判别和相关AI伦理问题提供现实解决方案。

Abstract: The paper explores stylometry as a method to distinguish between texts
created by Large Language Models (LLMs) and humans, addressing issues of model
attribution, intellectual property, and ethical AI use. Stylometry has been
used extensively to characterise the style and attribute authorship of texts.
By applying it to LLM-generated texts, we identify their emergent writing
patterns. The paper involves creating a benchmark dataset based on Wikipedia,
with (a) human-written term summaries, (b) texts generated purely by LLMs
(GPT-3.5/4, LLaMa 2/3, Orca, and Falcon), (c) processed through multiple text
summarisation methods (T5, BART, Gensim, and Sumy), and (d) rephrasing methods
(Dipper, T5). The 10-sentence long texts were classified by tree-based models
(decision trees and LightGBM) using human-designed (StyloMetrix) and
n-gram-based (our own pipeline) stylometric features that encode lexical,
grammatical, syntactic, and punctuation patterns. The cross-validated results
reached a performance of up to .87 Matthews correlation coefficient in the
multiclass scenario with 7 classes, and accuracy between .79 and 1. in binary
classification, with the particular example of Wikipedia and GPT-4 reaching up
to .98 accuracy on a balanced dataset. Shapley Additive Explanations pinpointed
features characteristic of the encyclopaedic text type, individual overused
words, as well as a greater grammatical standardisation of LLMs with respect to
human-written texts. These results show -- crucially, in the context of the
increasingly sophisticated LLMs -- that it is possible to distinguish machine-
from human-generated texts at least for a well-defined text type.

</details>


### [53] [TransLaw: Benchmarking Large Language Models in Multi-Agent Simulation of the Collaborative Translation](https://arxiv.org/abs/2507.00875)
*Xi Xuan,King-kui Sin,Yufei Zhou,Chunyu Kit*

Main category: cs.CL

TL;DR: 本文提出了适用于香港法律判决翻译的多智能体LLM框架TransLaw，综合提升翻译效果，性能优于顶级模型GPT-4o，且大幅降低成本，部分方面仍逊于人工翻译。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）在多领域翻译任务中已展现出强大潜力，但在香港法律判决的翻译上，因复杂法律术语、文化内涵和严格语言结构等挑战，其能力尚不明确。

Method: 提出了TransLaw多智能体框架，内嵌翻译员、注释员、校对员三个智能体，共同协作提升法律翻译精度、风格适宜性以及结构连贯性，并支持LLM的自由配置。以13种开源及商业LLM为智能体进行了评估。

Result: TransLaw在法律语义准确度、结构连贯性及风格一致性上优于GPT-4o，但在复杂术语语境和风格自然度上仍逊于人工专家。该方案大幅降低成本，相关评测语料和平台均已开放。

Conclusion: TransLaw框架在香港法律判决翻译领域展现了较高的实用性和准确性，对自动法律文献翻译具有推动意义，但部分细节仍有提升空间。

Abstract: Multi-agent systems empowered by large language models (LLMs) have
demonstrated remarkable capabilities in a wide range of downstream
applications, including machine translation. However, the potential of LLMs in
translating Hong Kong legal judgments remains uncertain due to challenges such
as intricate legal terminology, culturally embedded nuances, and strict
linguistic structures. In this work, we introduce TransLaw, a novel multi-agent
framework implemented for real-world Hong Kong case law translation. It employs
three specialized agents, namely, Translator, Annotator, and Proofreader, to
collaboratively produce translations for high accuracy in legal meaning,
appropriateness in style, and adequate coherence and cohesion in structure.
This framework supports customizable LLM configurations and achieves tremendous
cost reduction compared to professional human translation services. We
evaluated its performance using 13 open-source and commercial LLMs as agents
and obtained interesting findings, including that it surpasses GPT-4o in legal
semantic accuracy, structural coherence, and stylistic fidelity, yet trails
human experts in contextualizing complex terminology and stylistic naturalness.
Our platform website is available at CityUHK, and our bilingual judgment corpus
used for the evaluation is available at Hugging Face.

</details>


### [54] [Mathematics Isn't Culture-Free: Probing Cultural Gaps via Entity and Scenario Perturbations](https://arxiv.org/abs/2507.00883)
*Aditya Tomar,Nihar Ranjan Sahoo,Ashish Mittal,Rudra Murthy,Pushpak Bhattacharyya*

Main category: cs.CL

TL;DR: 考察并生成多个地区文化适配版数学题集，测评6个大模型的表现，发现推理能力强的模型跨文化迁移能力更好。传统美式数据集不能充分评测模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 尽管数学被普遍认为是文化中立的，但数学问题的呈现方式往往隐含文化背景。现有数据集如GSM8K主要基于西方（美国）范式，无法充分评估大模型在跨文化语境下对数学问题的理解能力。

Method: 作者基于GSM8K数学题集，针对非西方地区（非洲、印度、中国、韩国、日本）用提示工程进行本地化改写，再经人工校验，形成文化适配的测试集。随后在六个参数规模从8B到72B的大语言模型上，通过五种不同的提示方式，对模型在多种文化语境下的鲁棒性进行测评。

Result: 分析发现在原版美式数据集下大模型表现最佳，而在文化适配版本上表现较差。不同行推理能力模型在文化迁移中的表现差异明显：推理能力更强的大模型对文化变化更具鲁棒性，即推理能力有助于跨越文化语境的障碍。

Conclusion: 数学问题的文化表达对大语言模型的推理与泛化能力构成挑战，内嵌较强推理能力的模型更能适应不同文化下数学任务的表达形式。为提升模型的通用性与公平性，应重视多元文化背景下数据集与模型能力的评估。

Abstract: Although mathematics is often considered culturally neutral, the way
mathematical problems are presented can carry implicit cultural context.
Existing benchmarks like GSM8K are predominantly rooted in Western norms,
including names, currencies, and everyday scenarios. In this work, we create
culturally adapted variants of the GSM8K test set for five regions Africa,
India, China, Korea, and Japan using prompt-based transformations followed by
manual verification. We evaluate six large language models (LLMs), ranging from
8B to 72B parameters, across five prompting strategies to assess their
robustness to cultural variation in math problem presentation. Our findings
reveal a consistent performance gap: models perform best on the original
US-centric dataset and comparatively worse on culturally adapted versions.
However, models with reasoning capabilities are more resilient to these shifts,
suggesting that deeper reasoning helps bridge cultural presentation gaps in
mathematical tasks

</details>


### [55] [MemeCMD: An Automatically Generated Chinese Multi-turn Dialogue Dataset with Contextually Retrieved Memes](https://arxiv.org/abs/2507.00891)
*Yuheng Wang,Xianhe Tang,Pufeng Huang*

Main category: cs.CL

TL;DR: 本文提出并自动构建了一个结合了表情包的中文多轮多模态对话数据集，显著提升了对话的表达力和上下文丰富性，为多模态对话AI研究提供了新资源。


<details>
  <summary>Details</summary>
Motivation: 现有对话数据集主要为人工标注或纯文本对话，缺乏多模态交互中的表现力和上下文细腻度，因此不足以支撑多模态对话AI的发展。

Method: 提出MemeCMD：一种自动生成的中文多轮对话数据集。该数据集结合了大规模MLLM（多模态大语言模型）标注的表情包库以及双代理在不同场景下自动生成的对话。设计了检索框架和自适应阈值，确保表情包的上下文相关性和自然间隔。

Result: 实验结果显示，所构建的数据集能有效生成上下文相关且多样的表情包嵌入对话，为多模态对话AI提供了可扩展且保护隐私的资源。

Conclusion: MemeCMD解决了现有数据集缺乏多模态表达的不足，推动了多模态对话系统的发展。

Abstract: Memes are widely used in online social interactions, providing vivid,
intuitive, and often humorous means to express intentions and emotions.
Existing dialogue datasets are predominantly limited to either manually
annotated or pure-text conversations, lacking the expressiveness and contextual
nuance that multimodal interactions provide.To address these challenges, we
introduce MemeCMD, an automatically generated Chinese Multi-turn Dialogue
dataset with contextually retrieved memes. Our dataset combines a large-scale,
MLLM-annotated meme library with dialogues auto-generated by dual agents across
diverse scenarios. We introduce a retrieval framework and adaptive threshold to
ensure contextually relevant, naturally spaced meme usage. Experiments
demonstrate the effectiveness of our approach in generating contextually
appropriate and diverse meme-incorporated dialogues, offering a scalable and
privacy-preserving resource for advancing multimodal conversational AI.

</details>


### [56] [Scaling Laws Are Unreliable for Downstream Tasks: A Reality Check](https://arxiv.org/abs/2507.00885)
*Nicholas Lourie,Michael Y. Hu,Kyunghyun Cho*

Main category: cs.CL

TL;DR: 文章通过元分析发现，只有约39%的情况下下游缩放定律表现为线性，且实验条件变化会严重影响趋势。因此，仅靠线性缩放推断下游业绩不具普遍性，要重视非线性和异常表现的情况。


<details>
  <summary>Details</summary>
Motivation: 近年来，预训练模型的“下游缩放定律”被用来预测更大规模模型的下游任务性能，但文献中对此是否可行存在争议。部分研究认为下游性能与缩放存在线性关系，部分则指出还存在‘涌现现象’和‘逆向缩放’等问题。因此，作者希望澄清下游缩放定律适用的条件及其局限。

Method: 作者对现有关于下游缩放定律的公开数据进行了元分析，统计分析了不同实验结果中下游缩放趋势的吻合程度和影响因素。

Result: 只有39%的案例中，下游任务性能与缩放呈现明确的线性关系。实验设定中的一些微小变化也能完全改变缩放趋势。因此，线性缩放并非普适规律。

Conclusion: 仅依赖线性缩放趋势预测更大规模模型下游任务性能并不可靠。理解缩放定律奏效的前提和失效的情形，对于建立更完整的模型预测关系至关重要。

Abstract: Downstream scaling laws aim to predict task performance at larger scales from
pretraining losses at smaller scales. Whether this prediction should be
possible is unclear: some works demonstrate that task performance follows clear
linear scaling trends under transformation, whereas others point out
fundamental challenges to downstream scaling laws, such as emergence and
inverse scaling. In this work, we conduct a meta-analysis of existing data on
downstream scaling laws, finding that close fit to linear scaling laws only
occurs in a minority of cases: 39% of the time. Furthermore, seemingly benign
changes to the experimental setting can completely change the scaling trend.
Our analysis underscores the need to understand the conditions under which
scaling laws succeed. To fully model the relationship between pretraining loss
and downstream task performance, we must embrace the cases in which scaling
behavior deviates from linear trends.

</details>


### [57] [SciArena: An Open Evaluation Platform for Foundation Models in Scientific Literature Tasks](https://arxiv.org/abs/2507.01001)
*Yilun Zhao,Kaiyan Zhang,Tiansheng Hu,Sihong Wu,Ronan Le Bras,Taira Anderson,Jonathan Bragg,Joseph Chee Chang,Jesse Dodge,Matt Latzke,Yixin Liu,Charles McGrady,Xiangru Tang,Zihang Wang,Chen Zhao,Hannaneh Hajishirzi,Doug Downey,Arman Cohan*

Main category: cs.CL

TL;DR: SciArena是面向科学文献任务的大模型社区评测平台，通过集体投票和真实问题收集，更真实地反映模型实际表现，并首次发布了用于自动答案质量评判的元基准，为未来文献评测自动化方法研究奠定基础。


<details>
  <summary>Details</summary>
Motivation: 当前科学文献理解和综合的评测多为传统基准，缺少直接与科研社区互动的方法，这限制了模型在开放性、实际科学任务中的评估广度和深度，因此作者希望建立一个让社区直接参与的大模型科学文献能力评测平台。

Method: 提出并搭建了SciArena平台，参照Chatbot Arena，通过社区投票对模型在开放性科学任务上的表现进行对比评价。平台收集各领域研究者真实提问和投票，支持23个开源或专有基础模型。收集了13,000多票并分析了问题多样性、评估一致性等，还发布了基于投票数据的自动评测元基准SciArena-Eval，用于衡量模型自动评价文献答案质量的能力。

Result: 平台收获了广泛的科学领域研究者参与，采集到丰富多样、贴合实际的问题。评测数据表明投票者自洽性和互评一致性良好。模型排行榜揭示了不同模型在实际科学任务上的表现分布。发布的SciArena-Eval基准用于模型自动评测，实验表明该任务具有挑战性，现有自动方法有待提升。

Conclusion: SciArena提供了一种开放、协作的科学文献理解能力评估新范式，增强了模型真实科学任务的测试覆盖面和结果参考价值。平台及其元评测基准有助于推动大模型文献自动评价方法的进一步发展。

Abstract: We present SciArena, an open and collaborative platform for evaluating
foundation models on scientific literature tasks. Unlike traditional benchmarks
for scientific literature understanding and synthesis, SciArena engages the
research community directly, following the Chatbot Arena evaluation approach of
community voting on model comparisons. By leveraging collective intelligence,
SciArena offers a community-driven evaluation of model performance on
open-ended scientific tasks that demand literature-grounded, long-form
responses. The platform currently supports 23 open-source and proprietary
foundation models and has collected over 13,000 votes from trusted researchers
across diverse scientific domains. We analyze the data collected so far and
confirm that the submitted questions are diverse, aligned with real-world
literature needs, and that participating researchers demonstrate strong
self-consistency and inter-annotator agreement in their evaluations. We discuss
the results and insights based on the model ranking leaderboard. To further
promote research in building model-based automated evaluation systems for
literature tasks, we release SciArena-Eval, a meta-evaluation benchmark based
on our collected preference data. The benchmark measures the accuracy of models
in judging answer quality by comparing their pairwise assessments with human
votes. Our experiments highlight the benchmark's challenges and emphasize the
need for more reliable automated evaluation methods.

</details>


### [58] [The Cognate Data Bottleneck in Language Phylogenetics](https://arxiv.org/abs/2507.00911)
*Luise Häuser,Alexandros Stamatakis*

Main category: cs.CL

TL;DR: 现有的数据来源和自动化采集方法难以满足复杂计算语言进化方法对大规模高质量同源词数据的需求，目前尚无法解决这一难题。


<details>
  <summary>Details</summary>
Motivation: 推动计算系统发掘词源数据（cognate data）在历史语言学的潜力，但当前手工采集的数据规模太小，无法支持复杂模型和机器学习方法的应用。

Method: 尝试通过自动化方式（以BabelNet为例）提取更大规模的同源词数据集，并利用该数据集进行系统发育（phylogenetic）推断分析。

Result: 自动提取的同源词数据集生成的系统发育树与公认的金标准树高度不一致，说明自动采集的数据质量不足。作者也分析了从其它多语种资源采集到更好数据的可能性很低。

Conclusion: 由于高质量大规模同源词数据集难以实现，目前许多依赖大数据集的计算语言进化方法无法应用于此领域，是否以及如何将此类方法用于历史语言学依然是个未解的问题。

Abstract: To fully exploit the potential of computational phylogenetic methods for
cognate data one needs to leverage specific (complex) models an machine
learning-based techniques. However, both approaches require datasets that are
substantially larger than the manually collected cognate data currently
available. To the best of our knowledge, there exists no feasible approach to
automatically generate larger cognate datasets. We substantiate this claim by
automatically extracting datasets from BabelNet, a large multilingual
encyclopedic dictionary. We demonstrate that phylogenetic inferences on the
respective character matrices yield trees that are largely inconsistent with
the established gold standard ground truth trees. We also discuss why we
consider it as being unlikely to be able to extract more suitable character
matrices from other multilingual resources. Phylogenetic data analysis
approaches that require larger datasets can therefore not be applied to cognate
data. Thus, it remains an open question how, and if these computational
approaches can be applied in historical linguistics.

</details>


### [59] [Discourse Heuristics For Paradoxically Moral Self-Correction](https://arxiv.org/abs/2507.00985)
*Guangliang Liu,Zimo Qi,Xitong Zhang,Kristen Marie Johnson*

Main category: cs.CL

TL;DR: 本文发现大型语言模型的道德自我纠正主要依赖启发式捷径，导致在自我纠正与自我诊断能力同步提升时出现矛盾。作者提出借助精选数据集中的启发式策略可改善这一能力，但泛化性依然是挑战。


<details>
  <summary>Details</summary>
Motivation: 道德自我纠正被视为大型语言模型（LLM）输出与人类道德价值观对齐的一种前景广阔的方法，但目前存在两个悖论：一是自我纠正表面有效，缺乏深层支撑；二是LLM可以诊断自己输出的非道德性，但在自我纠正过程中难以定位道德不一致的根源。

Method: 分析精调语料库中的话语结构，旨在强化LLM道德自我纠正能力，揭示其背后起作用的启发式策略，并研究这些策略对自我纠正与自我诊断能力共同提升时带来的矛盾和不一致。

Result: 发现道德自我纠正依赖于反映启发式捷径的话语结构，这些捷径导致当试图同时提升自我纠正和自我诊断能力时，系统表现出不一致性。

Conclusion: 提出通过利用精心策划数据集中的启发式策略，提升道德自我纠正能力，并指出这种能力在语境适应和模型扩展性方面仍面临泛化难题。

Abstract: Moral self-correction has emerged as a promising approach for aligning the
output of Large Language Models (LLMs) with human moral values. However, moral
self-correction techniques are subject to two primary paradoxes. First, despite
empirical and theoretical evidence to support the effectiveness of
self-correction, this LLM capability only operates at a superficial level.
Second, while LLMs possess the capability of self-diagnosing immoral aspects of
their output, they struggle to identify the cause of this moral inconsistency
during their self-correction process. To better understand and address these
paradoxes, we analyze the discourse constructions in fine-tuning corpora
designed to enhance moral self-correction, uncovering the existence of the
heuristics underlying effective constructions. We demonstrate that moral
self-correction relies on discourse constructions that reflect heuristic
shortcuts, and that the presence of these heuristic shortcuts during
self-correction leads to inconsistency when attempting to enhance both
self-correction and self-diagnosis capabilities jointly. Based on our findings,
we propose a solution to improve moral self-correction by leveraging the
heuristics of curated datasets. We also highlight the generalization challenges
of this capability, particularly in terms of learning from situated context and
model scales.

</details>


### [60] [Should We Still Pretrain Encoders with Masked Language Modeling?](https://arxiv.org/abs/2507.00994)
*Hippolyte Gisserot-Boukhlef,Nicolas Boizard,Manuel Faysse,Duarte M. Alves,Emmanuel Malherbe,André F. T. Martins,Céline Hudelot,Pierre Colombo*

Main category: cs.CL

TL;DR: 作者系统对比MLM与CLM对文本表征能力的影响，发现MLM效果略胜一筹但CLM更高效稳定，结合两者的双阶段训练策略能最好地兼顾效果与算力需求。


<details>
  <summary>Details</summary>
Motivation: 当前CLM预训练模型在表征任务上表现优异，但不清楚是否因CLM目标本身优越还是由模型规模、数据等混杂因素所致，因此亟需系统实验验证。

Method: 大规模受控消融实验，训练30个不同规模模型（2.1亿到10亿参数），超过15000次微调与评测，比较MLM、CLM及其组合对文本表征效果的影响。

Result: 1. MLM整体表现优于CLM，2. CLM训练效率与微调稳定性更突出，3. 先CLM再MLM的双阶段训练策略效果最优，尤其适合复用已有CLM模型，节省算力。

Conclusion: 论文发现，虽然MLM通常在文本表征任务中表现更优，但CLM更高效且微调稳定性更好。提出CLM+MLM两阶段训练策略在计算预算不变下获得最佳效果，且有助于复用已有大语言模型，降低训练成本。

Abstract: Learning high-quality text representations is fundamental to a wide range of
NLP tasks. While encoder pretraining has traditionally relied on Masked
Language Modeling (MLM), recent evidence suggests that decoder models
pretrained with Causal Language Modeling (CLM) can be effectively repurposed as
encoders, often surpassing traditional encoders on text representation
benchmarks. However, it remains unclear whether these gains reflect an inherent
advantage of the CLM objective or arise from confounding factors such as model
and data scale. In this paper, we address this question through a series of
large-scale, carefully controlled pretraining ablations, training a total of 30
models ranging from 210 million to 1 billion parameters, and conducting over
15,000 fine-tuning and evaluation runs. We find that while training with MLM
generally yields better performance across text representation tasks,
CLM-trained models are more data-efficient and demonstrate improved fine-tuning
stability. Building on these findings, we experimentally show that a biphasic
training strategy that sequentially applies CLM and then MLM, achieves optimal
performance under a fixed computational training budget. Moreover, we
demonstrate that this strategy becomes more appealing when initializing from
readily available pretrained CLM models (from the existing LLM ecosystem),
reducing the computational burden needed to train best-in-class encoder models.
We release all project artifacts at https://hf.co/MLMvsCLM to foster further
research.

</details>


### [61] [La Leaderboard: A Large Language Model Leaderboard for Spanish Varieties and Languages of Spain and Latin America](https://arxiv.org/abs/2507.00999)
*María Grandury,Javier Aula-Blasco,Júlia Falcão,Clémentine Fourrier,Miguel González,Gonzalo Martínez,Gonzalo Santamaría,Rodrigo Agerri,Nuria Aldama,Luis Chiruzzo,Javier Conde,Helena Gómez,Marta Guerrero,Guido Ivetta,Natalia López,Flor Miriam Plaza-del-Arco,María Teresa Martín-Valdivia,Helena Montoro,Carmen Muñoz,Pedro Reviriego,Leire Rosado,Alejandro Vaca,María Estrella Vallecillo-Rodríguez,Jorge Vallego,Irune Zubiaga*

Main category: cs.CL

TL;DR: 本文提出并实现了首个面向西班牙及拉美语言多样性大模型的开源评测榜单La Leaderboard，测试了66个数据集、50个模型，标准化了评测流程，有助于推动多语言领域LLM发展和社区合作。


<details>
  <summary>Details</summary>
Motivation: 当前主流榜单大多以英语为主，缺乏对西班牙语及其方言、多样性语言的系统评测，限制了西班牙语社区大语言模型的发展。因此，作者动机是推动能代表西班牙语社区语言多样性的大模型研究和标准化评测。

Method: 构建了首个面向西班牙语及其地方语言、方言的大模型开源评测榜单“La Leaderboard”。这个榜单整合了66个涵盖巴斯克语、加泰罗尼亚语、加利西亚语及不同西班牙语变体的数据集，并对50个模型进行了评测。同时，定义了社区驱动的标准化流程，提供了任务与评测方法的选择指南，并提出在few-shot任务中减少样本数，以降低环境影响并改善社区可复现性。

Result: La Leaderboard作为开源、社区驱动的榜单成功上线，覆盖66个数据集与50个模型，成为西班牙语及变体LLM系统评测的重要参考。引入创新的few-shot策略后，保证了评测科学性并兼顾环境与公平性。

Conclusion: La Leaderboard填补了西班牙语社区大模型评测标准的空白，通过开放、标准化、可扩展的方式推动了西班牙语及其方言、邻近语言的生成式大模型研究发展。该项目方法具有参考推广价值。

Abstract: Leaderboards showcase the current capabilities and limitations of Large
Language Models (LLMs). To motivate the development of LLMs that represent the
linguistic and cultural diversity of the Spanish-speaking community, we present
La Leaderboard, the first open-source leaderboard to evaluate generative LLMs
in languages and language varieties of Spain and Latin America. La Leaderboard
is a community-driven project that aims to establish an evaluation standard for
everyone interested in developing LLMs for the Spanish-speaking community. This
initial version combines 66 datasets in Basque, Catalan, Galician, and
different Spanish varieties, showcasing the evaluation results of 50 models. To
encourage community-driven development of leaderboards in other languages, we
explain our methodology, including guidance on selecting the most suitable
evaluation setup for each downstream task. In particular, we provide a
rationale for using fewer few-shot examples than typically found in the
literature, aiming to reduce environmental impact and facilitate access to
reproducible results for a broader research community.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [62] [Ensemble Kalman Filter for Data Assimilation coupled with low-resolution computations techniques applied in Fluid Dynamics](https://arxiv.org/abs/2507.00539)
*Paul Jeanney,Ashton Hetherington,Shady E. Ahmed,David Lanceta,Susana Saiz,José Miguel Perez,Soledad Le CLainche*

Main category: cs.CE

TL;DR: 本文提出结合EnKF和低成本SVD的降阶数据同化新方法，能大幅减低计算与内存需求，并在保持高精度的同时，适合流体力学等大规模、实时应用场景。


<details>
  <summary>Details</summary>
Motivation: 流体动力学中的数据同化（DA）计算代价极高，限制了其实用性，尤其是对大规模或实时应用。需要探索可兼顾精度与效率的数据同化新方法。

Method: 提出一种创新的降阶模型（ROM），将低分辨率（LR）技术与集合卡尔曼滤波（EnKF）结合，并引入低成本奇异值分解（lcSVD）技术，先对数据降采样，再用lcSVD高效重构，提高数据同化效率。

Result: 在湍流测试中，LR方法以15.9的压缩率实现了13.7倍加速、90.9%内存压缩，同时仅2.6%的RRMSE（HR参考为0.8%），大幅节省计算资源而保持较高精度。

Conclusion: 所提方法在不牺牲精度的前提下，大幅降低了数据同化的计算时间和内存消耗，具备在CFD、环境监测、航空航天等大规模与实时领域应用潜力。

Abstract: This paper presents an innovative Reduced-Order Model (ROM) for merging
experimental and simulation data using Data Assimilation (DA) to estimate the
"True" state of a fluid dynamics system, leading to more accurate predictions.
Our methodology introduces a novel approach implementing the Ensemble Kalman
Filter (EnKF) within a reduced-dimensional framework, grounded in a robust
theoretical foundation and applied to fluid dynamics. To address the
substantial computational demands of DA, the proposed ROM employs
low-resolution (LR) techniques to drastically reduce computational costs. This
approach involves downsampling datasets for DA computations, followed by an
advanced reconstruction technique based on low-cost Singular Value
Decomposition (lcSVD). The lcSVD method, a key innovation in this paper, has
never been applied to DA before and offers a highly efficient way to enhance
resolution with minimal computational resources. Our results demonstrate
significant reductions in both computation time and RAM usage through the LR
techniques without compromising the accuracy of the estimations. For instance,
in a turbulent test case, the LR approach with a compression rate of 15.9 can
achieve a speed-up of 13.7 and a RAM compression of 90.9% while maintaining a
low Relative Root Mean Square Error (RRMSE) of 2.6%, compared to 0.8% in the
high-resolution (HR) reference. Furthermore, we highlight the effectiveness of
the EnKF in estimating and predicting the state of fluid flow systems based on
limited observations and low-fidelity numerical data. This paper highlights the
potential of the proposed DA method in fluid dynamics applications,
particularly for improving computational efficiency in CFD and related fields.
Its ability to balance accuracy with low computational and memory costs makes
it suitable for large-scale and real-time applications, such as environmental
monitoring or aerospace.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [63] [Integrating Universal Generative AI Platforms in Educational Labs to Foster Critical Thinking and Digital Literacy](https://arxiv.org/abs/2507.00007)
*Vasiliy Znamenskiy,Rafael Niyazov,Joel Hernandez*

Main category: cs.CY

TL;DR: 本研究提出了一种将生成式AI平台融入理科实验课的教学模型，通过设计学科化提示与反思性评估，不仅提升了学生的参与度，也增强了其批判性和数字素养，对各学科实验教学具借鉴意义。


<details>
  <summary>Details</summary>
Motivation: 当前生成式人工智能（GenAI）工具如ChatGPT、Claude、Gemini等在教育领域应用广泛，但学生对其生成内容容易不加批判地接受，存在滥用及误用风险。亟需开发新的教学框架，将GenAI融入课程，以提升学生批判性思维和数字素养。

Method: 本研究提出并实施了一种新的教学模型，将GenAI作为研究对象和认知工具，要求学生针对学科制定提示词，并评估AI在文本、图片和视频模式下的回答。通过在天文学课程中的实验，观察学生的参与度及反思过程。

Result: 实验显示，学生参与度高，具备较强批判性思考，许多学生甚至课后自主继续实验，并在学术研讨会上展示成果。Structured AI 交互和反思式评价明显提升学习成效。

Conclusion: 将GenAI作为实验室活动工具并辅以反思性评估，有助于提升多学科学生的批判思维和数字素养，该方法具备良好的可复制性和跨学科应用前景。

Abstract: This paper presents a new educational framework for integrating generative
artificial intelligence (GenAI) platforms such as ChatGPT, Claude, and Gemini
into laboratory activities aimed at developing critical thinking and digital
literacy among undergraduate students. Recognizing the limitations and risks of
uncritical reliance on large language models (LLMs), the proposed pedagogical
model reframes GenAI as a research subject and cognitive tool. Students
formulate discipline-specific prompts and evaluate GenAI-generated responses in
text, image, and video modalities. A pilot implementation in a general
astronomy course for non-science majors demonstrated high levels of engagement
and critical reflection, with many students continuing the activity after class
and presenting results at a research symposium. The results highlight the
importance of structured AI interactions in education and suggest that GenAI
can improve learning outcomes when combined with reflective assessment methods.
The study proposes a replicable model for interdisciplinary AI-integrated lab
work, adaptable to scientific disciplines. See the guide to learning activities
based on Generative-Ai platforms: https://doi.org/10.5281/zenodo.15555802

</details>


### [64] [Ken Utilization Layer: Hebbian Replay Within a Student's Ken for Adaptive Knowledge Tracing](https://arxiv.org/abs/2507.00032)
*Grey Kuling,Marinka Zitnik*

Main category: cs.CY

TL;DR: KUL-KT结合Hebbian记忆和梯度整合，提出高效灵活的知识追踪模型，不依赖大数据与长历史记录，支持多样输入，在多项基准和实际教学中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前知识追踪（KT）方法在个性化、可扩展性和记忆效率上存在局限，且生物启发的记忆机制在KT领域应用较少。因此，作者希望引入生物学灵感，结合Hebbian记忆和高效梯度优化，实现更优的学生建模。

Method: 提出KUL-KT架构，融合了Hebbian时间衰减记忆更新（支持遗忘）和梯度下降整合（持续学习）。设计核心包括：一次会合性的Hebbian记忆更新+慢速线性网络巩固，辅以全新Loss-aligned Internal Target（LIT）策略，无需反向传播历史，可直接持续学习。模型完全在嵌入空间操作，兼容结构化与非结构化输入。

Result: 在十个公开KT基准（nDCG、Recall@10等指标）上KUL-KT超过强基线模型；实际课堂部署中通过短答自适应出题，提高了学生满意度并降低了感知难度。消融实验证明Hebbian衰减和LIT对持续适应至关重要。与主流图基KT模型相比，KUL-KT训练速度提升1.75倍，内存消耗降低99%以上。

Conclusion: KUL-KT实现了内存高效、输入灵活且受生物学启发的个性化大规模知识追踪。其新颖的记忆与学习机制可推广至更多教育场景。

Abstract: We introduce KUL-KT, a biologically inspired architecture for knowledge
tracing (KT), combining Hebbian memory encoding with gradient-based
consolidation in a scalable, input-agnostic framework. KUL-KT adapts the
principle of memory consolidation in neural systems, to student modeling by
introducing two key innovations: (i) a time-decaying Hebbian memory update that
enables graceful forgetting, and (ii) a novel Loss-aligned Internal Target
(LIT) method to compute an ideal internal state, allowing continual learning
without backpropagation through time. The architecture consists of a fast
Hebbian memory that captures each learner interaction via a single associative
update, and a slower linear network that consolidates recalled samples through
gradient descent. This design enables few-shot personalization and natural
forgetting without storing raw data or relying on large cohort training.
Operating entirely in embedding space, KUL-KT supports both structured
(tabular) and unstructured (short-answer) inputs. Empirically, KUL-KT
outperforms strong baselines on ten public KT benchmarks in rank-sensitive
metrics such as nDCG and Recall@10. In a classroom deployment, KUL-KT
personalized quizzes from short-answer data, leading to improved
learner-perceived helpfulness and reduced difficulty (p < 0.05). Ablation
studies confirm that Hebbian decay and LIT are critical for continual
adaptation. Compared to a strong graph-based KT model, KUL-KT trains 1.75x
faster and uses 99.01\% less memory. These results position KUL-KT as a
biologically grounded, memory-efficient, and input-flexible framework for
personalized learning at scale.

</details>


### [65] [Teaching Programming in the Age of Generative AI: Insights from Literature, Pedagogical Proposals, and Student Perspectives](https://arxiv.org/abs/2507.00108)
*Clemente Rubio-Manzano,Jazna Meza,Rodolfo Fernandez-Santibanez,Christian Vidal-Castro*

Main category: cs.CY

TL;DR: AI代码生成工具改变了高校编程教学，文章建议用代码可视化和执行仿真提升教学质量，并获得了学生的初步支持。


<details>
  <summary>Details</summary>
Motivation: 编程自动化工具发展推动了高校编程教学变革，需探索AI时代下更有效的编程教学方法及评估方式。

Method: 综述已有文献，总结AI在编程教学中的优缺点；提出并探讨可视化教学法；收集面向对象编程课程学生的意见。

Result: 文献分析总结了编程自动生成工具在教育中的利弊。尝试代码可视化和执行仿真的教学法获得了学生的积极反馈，支持将其作为编程教学的重要补充。

Conclusion: 文章认为在生成式人工智能背景下，应该通过引入代码可视化和可视化执行模拟，提升编程教学的效果和学生理解深度。学生的反馈也支持将可视化仿真纳入Java等面向对象编程课程。

Abstract: Computer programming is undergoing a true transformation driven by powerful
new tools for automatic source code generation based on large language models.
This transformation is also manifesting in introductory programming courses at
universities around the world, generating an in-depth debate about how
programming content should be taught, learned, and assessed in the context of
generative artificial intelligence.
  This article aims, on the one hand, to review the most relevant studies on
this issue, highlighting the advantages and disadvantages identified in the
specialized literature. On the other hand, it proposes enriching teaching and
learning methodologies by focusing on code comprehension and execution rather
than on mere coding or program functionality. In particular, it advocates for
the use of visual representations of code and visual simulations of its
execution as effective tools for teaching, learning, and assessing programming,
thus fostering a deeper understanding among students.
  Finally, the opinions of students who took the object-oriented programming
course are presented to provide preliminary context supporting the
incorporation of visual simulations in Java (or other languages) as part of the
training process.

</details>


### [66] [Intellectual Property Rights and Entrepreneurship in the NFT Ecosystem: Legal Frameworks, Business Models, and Innovation Opportunities](https://arxiv.org/abs/2507.00172)
*Pranav Darshan,Rohan J S,Raghuveer Rajesh,Ruchitha M,Sanika Kamath,Manas M N*

Main category: cs.CY

TL;DR: NFT带来了新型数字所有权，但现有版权法与区块链交易之间存在脱节。本文通过新提出的IP矩阵和商业模式分类，揭示NFT法律与商业执行中的主要风险点，并提出完善建议。


<details>
  <summary>Details</summary>
Motivation: NFT的快速发展引发了版权管理上的严重问题，存在NFT所有权与其基础内容版权混淆的现象。作者希望解决NFT相关的知识产权权利界定和法律适用难题。

Method: 采用混合研究方法，建立新的知识产权权利矩阵，并提出商业模式分类法。研究过程中分析法律案例、智能合约，并采访相关利益方。

Result: 发现了跨区法律执行、许可证标准化及业务机会评估等方面的重要问题。建立了一个清晰显示版权与NFT所有权结构关系的知识产权权利矩阵和商业模式风险/可持续性分类法。

Conclusion: 当前NFT与传统版权法之间存在显著脱节，法律无法顺利跨区执行，且商业模式和风险管理存在重大挑战。需要明确NFT交易与版权之间的界限，并对相关法律与行业标准进行完善。

Abstract: Non Fungible Tokens have changed digital ownership and how creators earn
money. Between 2021 and 2024, the market value exceeded 40 billion. However,
the fast growth of the NFT ecosystem has revealed serious issues in managing
intellectual property rights. There is a lot of confusion about the difference
between owning an NFT and owning the copyright for the underlying content. This
research looks at the gap between traditional copyright laws and
blockchain-based transactions. We use a mixed methods approach to analyze this
disconnect. We create a new IP rights matrix that clearly shows how copyright
law relates to NFT ownership structures. Additionally, we include a business
model taxonomy that sorts new commercial applications by their IP risk and
sustainability factors. By examining important legal cases, smart contracts,
and interviews with stakeholders, we find key problems in enforcing laws across
different regions, standardizing licenses, and assessing business
opportunities.

</details>


### [67] [Partnering with AI: A Pedagogical Feedback System for LLM Integration into Programming Education](https://arxiv.org/abs/2507.00406)
*Niklas Scholz,Manh Hung Nguyen,Adish Singla,Tomohiro Nagashima*

Main category: cs.CY

TL;DR: 本文提出结合教学原则的新型LLM自动反馈框架，并通过实证研究验证其在中学编程教学中的有效性与局限性，强调未来仍需结合教师专业知识来优化学生学习体验。


<details>
  <summary>Details</summary>
Motivation: 反馈对于促进高效学习至关重要。随着大语言模型（LLMs）的快速发展，自动化编程教育反馈越来越受到关注，但现有研究常忽略了关键的教学原则（如掌握度与进步适应性），影响了反馈的有效性。

Method: 提出基于教学原则的LLM驱动自动反馈生成框架（源于既有理论与中学教师的实际经验），并基于此实现了Python编程网页应用。采用混合方法评价：邀请8位中学计算机老师参与，对系统进行了深入评估。

Result: 教师认为依据该框架的LLM反馈能够有效支持学生，部分场景甚至优于人类老师（如即时、精准反馈），但也存在不能适应课堂动态情境等局限。

Conclusion: 基于框架的LLM自动反馈能提高编程教学反馈的质量和效率，但仍需人类教师补充，以适应复杂的教学环境。该研究为未来自动反馈系统的发展提出了新方向和启示。

Abstract: Feedback is one of the most crucial components to facilitate effective
learning. With the rise of large language models (LLMs) in recent years,
research in programming education has increasingly focused on automated
feedback generation to help teachers provide timely support to every student.
However, prior studies often overlook key pedagogical principles, such as
mastery and progress adaptation, that shape effective feedback strategies. This
paper introduces a novel pedagogical framework for LLM-driven feedback
generation derived from established feedback models and local insights from
secondary school teachers. To evaluate this framework, we implemented a
web-based application for Python programming with LLM-based feedback that
follows the framework and conducted a mixed-method evaluation with eight
secondary-school computer science teachers. Our findings suggest that teachers
consider that, when aligned with the framework, LLMs can effectively support
students and even outperform human teachers in certain scenarios through
instant and precise feedback. However, we also found several limitations, such
as its inability to adapt feedback to dynamic classroom contexts. Such a
limitation highlights the need to complement LLM-generated feedback with human
expertise to ensure effective student learning. This work demonstrates an
effective way to use LLMs for feedback while adhering to pedagogical standards
and highlights important considerations for future systems.

</details>


### [68] [Teacher-AI Collaboration for Curating and Customizing Lesson Plans in Low-Resource Schools](https://arxiv.org/abs/2507.00456)
*Deepak Varuvel Dennison,Bakhtawar Ahtisham,Kavyansh Chourasia,Nirmit Arora,Rahul Singh,Rene F. Kizilcec,Akshay Nambi,Tanuja Ganu,Aditya Vashistha*

Main category: cs.CY

TL;DR: 研究在印度公立学校评估AI备课工具Shiksha copilot。结果显示AI可提升教师效率和教学质量，但制度性障碍限制了其深度应用。建议未来EdTech强调教师主导和本地化设计。


<details>
  <summary>Details</summary>
Motivation: 印度卡纳塔克邦的公立学校面临师资、行政压力和多语环境下教学难题。为减轻教师文书负担，提升课程质量，并探索AI与教师协作的新模式，研究团队引入并评估Shiksha copilot这一AI协助备课工具。

Method: 本研究采用大规模混合方法，包括对1043名教师和23名课程策划人（curators）的参与观察、问卷调查、内容分析等，系统性调查教师如何与AI协作制定适应本地实际的课程计划，并分析AI生成内容的质量及其对教学实践的影响。

Result: Shiksha copilot有效减轻了教师的行政和备课压力，降低了教学压力，并促进了以活动为基础的教学方式转变。但由于师资不足、行政要求高等系统性障碍，AI工具对更深层次教学变革的推动作用受限。

Conclusion: AI与教师协作能提升师资能力和教学质量，但其深度融合受制于教育体系中的结构性瓶颈。未来的教育技术设计需关注教师主导、多语和南方国家情境，实现真正以教师为中心的创新发展。

Abstract: This study investigates Shiksha copilot, an AI-assisted lesson planning tool
deployed in government schools across Karnataka, India. The system combined
LLMs and human expertise through a structured process in which English and
Kannada lesson plans were co-created by curators and AI; teachers then further
customized these curated plans for their classrooms using their own expertise
alongside AI support. Drawing on a large-scale mixed-methods study involving
1,043 teachers and 23 curators, we examine how educators collaborate with AI to
generate context-sensitive lesson plans, assess the quality of AI-generated
content, and analyze shifts in teaching practices within multilingual,
low-resource environments. Our findings show that teachers used Shiksha copilot
both to meet administrative documentation needs and to support their teaching.
The tool eased bureaucratic workload, reduced lesson planning time, and lowered
teaching-related stress, while promoting a shift toward activity-based
pedagogy. However, systemic challenges such as staffing shortages and
administrative demands constrained broader pedagogical change. We frame these
findings through the lenses of teacher-AI collaboration and communities of
practice to examine the effective integration of AI tools in teaching. Finally,
we propose design directions for future teacher-centered EdTech, particularly
in multilingual and Global South contexts.

</details>
