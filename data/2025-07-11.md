<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 26]
- [cs.CL](#cs.CL) [Total: 50]
- [cs.CE](#cs.CE) [Total: 2]
- [cs.CY](#cs.CY) [Total: 7]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Autonomous Control Leveraging LLMs: An Agentic Framework for Next-Generation Industrial Automation](https://arxiv.org/abs/2507.07115)
*Javal Vyas,Mehmet Mercangoz*

Main category: cs.AI

TL;DR: 本文提出了一种结合大语言模型和有限状态机的统一智能体自动化框架，可同时胜任化工过程中的故障规划和过程控制，实验证明方案准确高效，且优于传统和开源方法，对未来化工自动化具有重要意义。


<details>
  <summary>Details</summary>
Motivation: 现代化学过程变得越来越复杂，且面临劳动力短缺和复杂故障情况，因此需要将符号推理与自适应控制相结合的全新自动化范式。

Method: 提出了一个统一的多智能体架构，利用大语言模型（LLM）在单一架构中同时进行离散故障恢复规划和连续过程控制；采用有限状态机（FSM）作为可解释的操作包络，LLM驱动的规划Agent生成恢复序列，仿真Agent执行并检查每一步，验证-重试循环不断优化无效方案。

Result: 在案例1中，GPT-4o和GPT-4o-mini在180个不同规模的FSM上能在最多五次重试内100%找到有效路径，优于开源LLM。在案例2中，LLM架构能在硬件和仿真平台下维持目标温度，与PID控制器有相似性能，并证明提示循环对非线性动态控制至关重要。

Conclusion: 结构化反馈和模块化智能体结合LLM可实现符号规划和连续控制的统一，为化工领域走向强韧性、语言驱动的自动化铺平道路。

Abstract: The increasing complexity of modern chemical processes, coupled with
workforce shortages and intricate fault scenarios, demands novel automation
paradigms that blend symbolic reasoning with adaptive control. In this work, we
introduce a unified agentic framework that leverages large language models
(LLMs) for both discrete fault-recovery planning and continuous process control
within a single architecture. We adopt Finite State Machines (FSMs) as
interpretable operating envelopes: an LLM-driven planning agent proposes
recovery sequences through the FSM, a Simulation Agent executes and checks each
transition, and a Validator-Reprompting loop iteratively refines invalid plans.
In Case Study 1, across 180 randomly generated FSMs of varying sizes (4-25
states, 4-300 transitions), GPT-4o and GPT-4o-mini achieve 100% valid-path
success within five reprompts-outperforming open-source LLMs in both accuracy
and latency. In Case Study 2, the same framework modulates dual-heater inputs
on a laboratory TCLab platform (and its digital twin) to maintain a target
average temperature under persistent asymmetric disturbances. Compared to
classical PID control, our LLM-based controller attains similar performance,
while ablation of the prompting loop reveals its critical role in handling
nonlinear dynamics. We analyze key failure modes-such as instruction following
lapses and coarse ODE approximations. Our results demonstrate that, with
structured feedback and modular agents, LLMs can unify high-level symbolic
planningand low-level continuous control, paving the way towards resilient,
language-driven automation in chemical engineering.

</details>


### [2] [BOOST: Out-of-Distribution-Informed Adaptive Sampling for Bias Mitigation in Stylistic Convolutional Neural Networks](https://arxiv.org/abs/2507.07134)
*Mridula Vijendran,Shuang Chen,Jingjing Deng,Hubert P. H. Shum*

Main category: cs.AI

TL;DR: 提出了BOOST，一种利用分布外信息动态采样与温度缩放的新方法，有效降低绘画分类AI中的类别偏见，并通过新指标SODC验证了方法兼具高性能与公平性。


<details>
  <summary>Details</summary>
Motivation: 人工智能在绘画分类任务中存在偏见问题，尤其是由数据集失衡引起的偏差，影响模型在艺术策展、修复等实际应用中的公平性与准确性。以往研究关注于提升分类性能，而忽视了对这些由训练数据偏见导致的分布外(out-of-distribution, OOD)问题的根本解决。

Method: 提出了一种新颖的基于分布外信息的自适应采样方法——BOOST（偏见导向的OOD采样与调优），通过动态调整温度缩放和采样概率，实现对各类的更公平代表。

Result: 在KaoKore和PACS数据集上进行评测，方法能有效降低类别偏见，同时提出了新的评估指标SODC(同数据集分布外检测分数)，用于衡量类别分离和偏见减少效果。结果显示该方法在提升模型公平性的同时保持了较高的分类性能。

Conclusion: BOOST方法能够有效缓解绘画分类AI中的类别偏见，兼顾精度和公平性，为艺术领域去偏见提供了有力的技术方案。

Abstract: The pervasive issue of bias in AI presents a significant challenge to
painting classification, and is getting more serious as these systems become
increasingly integrated into tasks like art curation and restoration. Biases,
often arising from imbalanced datasets where certain artistic styles dominate,
compromise the fairness and accuracy of model predictions, i.e., classifiers
are less accurate on rarely seen paintings. While prior research has made
strides in improving classification performance, it has largely overlooked the
critical need to address these underlying biases, that is, when dealing with
out-of-distribution (OOD) data. Our insight highlights the necessity of a more
robust approach to bias mitigation in AI models for art classification on
biased training data. We propose a novel OOD-informed model bias adaptive
sampling method called BOOST (Bias-Oriented OOD Sampling and Tuning). It
addresses these challenges by dynamically adjusting temperature scaling and
sampling probabilities, thereby promoting a more equitable representation of
all classes. We evaluate our proposed approach to the KaoKore and PACS
datasets, focusing on the model's ability to reduce class-wise bias. We further
propose a new metric, Same-Dataset OOD Detection Score (SODC), designed to
assess class-wise separation and per-class bias reduction. Our method
demonstrates the ability to balance high performance with fairness, making it a
robust solution for unbiasing AI models in the art domain.

</details>


### [3] [State-Inference-Based Prompting for Natural Language Trading with Game NPCs](https://arxiv.org/abs/2507.07203)
*Minkyung Kim,Junsik Kim,Hwidong Bae,Woongcheol Yang,Sangdon Park,Sohee Bae*

Main category: cs.AI

TL;DR: 本文提出SIBP方法，通过精细的对话状态管理与上下文处理，有效提升了语言模型在游戏交易规则遵守与准确性方面的表现，是实现可信NPC交易互动的重要进展。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在游戏中用于交易系统时，存在规则违背、物品幻觉与计算错误等问题，严重影响玩家信任。本文旨在解决这一痛点，提升NPC与玩家交易的可靠性。

Method: 通过自动对话状态推断与场景特定规则遵守，将交易流程分为六个状态，并采用统一的提示框架进行上下文感知的物品引用与基于占位符的价格计算。

Result: 在100个交易对话中，SIBP方法实现了超过97%的状态合规率，95%以上的物品引用准确率，以及99.7%的计算精度，同时保持高效的计算性能，明显优于现有基线方法。

Conclusion: 提出的基于状态推断的提示方法（SIBP）可以大幅提升大型语言模型在游戏交易系统中的规范性与准确性，为商业游戏中可信赖的NPC交易互动提供了基础。

Abstract: Large Language Models enable dynamic game interactions but struggle with
rule-governed trading systems. Current implementations suffer from rule
violations, such as item hallucinations and calculation errors, that erode
player trust. Here, State-Inference-Based Prompting (SIBP) enables reliable
trading through autonomous dialogue state inference and context-specific rule
adherence. The approach decomposes trading into six states within a unified
prompt framework, implementing context-aware item referencing and
placeholder-based price calculations. Evaluation across 100 trading dialogues
demonstrates >97% state compliance, >95% referencing accuracy, and 99.7%
calculation precision. SIBP maintains computational efficiency while
outperforming baseline approaches, establishing a practical foundation for
trustworthy NPC interactions in commercial games.

</details>


### [4] [Neurosymbolic Feature Extraction for Identifying Forced Labor in Supply Chains](https://arxiv.org/abs/2507.07217)
*Zili Wang,Frank Montabon,Kristin Yvonne Rozier*

Main category: cs.AI

TL;DR: 本论文提出结合神经符号方法和大语言模型的问题树，创新性解决供应链中非法活动检测的数据稀疏与数据不可靠难题，验证了自动方法对新闻文本相关性的有效挖掘，并系统比较了人机分类的差异。


<details>
  <summary>Details</summary>
Motivation: 供应链网络极为复杂，尤其是涉及非法活动时，如假冒零件、强制劳工或人口贩运。传统机器学习方法依赖大量高质量数据，但非法供应链的数据十分稀缺且常被蓄意篡改。现有方法难以在数据稀疏且不可靠情况下有效检测非法活动。

Method: 论文探索了神经符号方法，用于识别供应链中的非法活动。同时，论文比较了来自新闻报道的手动和自动特征提取方法，并提出了一套基于“问题树”的方案，用大语言模型（LLM）来量化新闻文章的相关性，从而系统地评估人机分类在供应链强迫劳动相关新闻识别上的差别。

Result: 实验验证了神经符号方法和基于LLM的问题树策略能够在小数据、数据不可靠的场景下，提升对供应链中非法活动检测的表现；系统地比较了人类与机器新闻文本分类差异。

Conclusion: 神经符号方法结合LLM的问题树技术，为供应链非法活动检测提供了可行的新思路，尤其适用于数据稀疏与数据不可信的情况，对比分析也为人机协同分类提供了理论基础。

Abstract: Supply chain networks are complex systems that are challenging to analyze;
this problem is exacerbated when there are illicit activities involved in the
supply chain, such as counterfeit parts, forced labor, or human trafficking.
While machine learning (ML) can find patterns in complex systems like supply
chains, traditional ML techniques require large training data sets. However,
illicit supply chains are characterized by very sparse data, and the data that
is available is often (purposely) corrupted or unreliable in order to hide the
nature of the activities. We need to be able to automatically detect new
patterns that correlate with such illegal activity over complex, even temporal
data, without requiring large training data sets. We explore neurosymbolic
methods for identifying instances of illicit activity in supply chains and
compare the effectiveness of manual and automated feature extraction from news
articles accurately describing illicit activities uncovered by authorities. We
propose a question tree approach for querying a large language model (LLM) to
identify and quantify the relevance of articles. This enables a systematic
evaluation of the differences between human and machine classification of news
articles related to forced labor in supply chains.

</details>


### [5] [Open Source Planning & Control System with Language Agents for Autonomous Scientific Discovery](https://arxiv.org/abs/2507.07257)
*Licong Xu,Milind Sarkar,Anto I. Lonappan,Íñigo Zubeldia,Pablo Villanueva-Domingo,Santiago Casas,Christian Fidler,Chetana Amancharla,Ujjwal Tiwari,Adrian Bayer,Chadi Ait Ekiou,Miles Cranmer,Adrian Dimitrov,James Fergusson,Kahaan Gandhi,Sven Krippendorf,Andrew Laverick,Julien Lesgourgues,Antony Lewis,Thomas Meier,Blake Sherwin,Kristen Surrao,Francisco Villaescusa-Navarro,Chi Wang,Xueqing Xu,Boris Bolliet*

Main category: cs.AI

TL;DR: cmbagent系统由约30个LLM智能体组成，以无人工介入的方式自动协作完成科研任务，在高级宇宙学任务和基准测试上取得优于现有模型的表现。


<details>
  <summary>Details</summary>
Motivation: 为实现科学研究任务的自动化，降低人力参与，提升科研效率和自动化智能体之间的协作能力。

Method: 系统由30个LLM智能体组成，通过计划与控制策略协同完成信息检索、代码编写、结果解释与批判等任务，并可本地执行代码。无人工流程参与。

Result: 系统成功完成宇宙学高级任务（基于超新星数据测定宇宙学参数），并在两组基准测试上超过了现有最优LLM模型。源码与演示已开源。

Conclusion: cmbagent展示了无需人工介入即可自动完成科研任务的可行性，并在实际和基准任务中优于现有LLM模型。

Abstract: We present a multi-agent system for automation of scientific research tasks,
cmbagent. The system is formed by about 30 Large Language Model (LLM) agents
and implements a Planning & Control strategy to orchestrate the agentic
workflow, with no human-in-the-loop at any point. Each agent specializes in a
different task (performing retrieval on scientific papers and codebases,
writing code, interpreting results, critiquing the output of other agents) and
the system is able to execute code locally. We successfully apply cmbagent to
carry out a PhD level cosmology task (the measurement of cosmological
parameters using supernova data) and evaluate its performance on two benchmark
sets, finding superior performance over state-of-the-art LLMs. The source code
is available on GitHub, demonstration videos are also available, and the system
is deployed on HuggingFace and will be available on the cloud.

</details>


### [6] [Application of LLMs to Multi-Robot Path Planning and Task Allocation](https://arxiv.org/abs/2507.07302)
*Ashish Kumar*

Main category: cs.AI

TL;DR: 本论文提出将大型语言模型作为专家，用于指导多智能体强化学习中的高效探索，并在基于规划的多智能体任务中取得了提升。


<details>
  <summary>Details</summary>
Motivation: 在深度强化学习（RL）中实现高效探索一直是一个著名难题，而在多智能体强化学习（MARL）中，这一问题由于算法的复杂性变得更加严重。因此，作者希望寻找改善多智能体场景中探索效率的方法。

Method: 本文提出利用大型语言模型（LLM）作为专家规划者，指导多智能体在基于规划的任务中进行高效探索。即让LLM辅助多智能体在任务中的探索决策。

Result: 结果表明，将大型语言模型作为专家为多智能体规划提供指导，能够提升其在复杂环境中的探索效率和学习效果。

Conclusion: 利用大型语言模型作为专家规划者，可以有效改善多智能体强化学习中的探索问题，特别是在需要规划的任务中。

Abstract: Efficient exploration is a well known problem in deep reinforcement learning
and this problem is exacerbated in multi-agent reinforcement learning due the
intrinsic complexities of such algorithms. There are several approaches to
efficiently explore an environment to learn to solve tasks by multi-agent
operating in that environment, of which, the idea of expert exploration is
investigated in this work. More specifically, this work investigates the
application of large-language models as expert planners for efficient
exploration in planning based tasks for multiple agents.

</details>


### [7] [ViDove: A Translation Agent System with Multimodal Context and Memory-Augmented Reasoning](https://arxiv.org/abs/2507.07306)
*Yichen Lu,Wei Dai,Jiaen Liu,Ching Wing Kwok,Zongheng Wu,Xudong Xiao,Ao Sun,Sheng Fu,Jianyuan Zhan,Yian Wang,Takatomo Saito,Sicheng Lai*

Main category: cs.AI

TL;DR: 论文提出多模态翻译系统ViDove，结合视觉、上下文信息以及增强记忆模块，显著提升了翻译质量，在公共基准上超越当前最佳模型并推出新数据集DoveBench，代码已开源。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型（LLM）翻译代理已经能够实现接近人类的翻译水平，且在处理更长、更复杂的上下文方面更高效。然而，这些模型大多局限于文本输入，难以充分利用视觉等多模态信息。作者希望突破这一局限，提高在多模态场景下翻译的准确性和适应性。

Method: 提出ViDove系统，它模仿人类译者的工作流程，融合了视觉和上下文背景信息，提升翻译能力。系统中还集成了多模态记忆系统和结合领域知识的长短期记忆模块，使模型在实际应用中表现更精确和具适应性。此外，作者还推出新的长视频自动字幕和翻译基准DoveBench，含有17小时高质量、人工标注的数据。

Result: ViDove在字幕生成和通用翻译任务上表现优异，BLEU得分提升了28%，SubER（字幕错误率）提升了15%，超过了当前最先进的基线模型。

Conclusion: ViDove有效利用多模态输入和增强记忆结构，显著提高了翻译质量和应用的适应性。同时，DoveBench基准集为多模态翻译的进一步研究提供了强有力的评测基础。

Abstract: LLM-based translation agents have achieved highly human-like translation
results and are capable of handling longer and more complex contexts with
greater efficiency. However, they are typically limited to text-only inputs. In
this paper, we introduce ViDove, a translation agent system designed for
multimodal input. Inspired by the workflow of human translators, ViDove
leverages visual and contextual background information to enhance the
translation process. Additionally, we integrate a multimodal memory system and
long-short term memory modules enriched with domain-specific knowledge,
enabling the agent to perform more accurately and adaptively in real-world
scenarios. As a result, ViDove achieves significantly higher translation
quality in both subtitle generation and general translation tasks, with a 28%
improvement in BLEU scores and a 15% improvement in SubER compared to previous
state-of-the-art baselines. Moreover, we introduce DoveBench, a new benchmark
for long-form automatic video subtitling and translation, featuring 17 hours of
high-quality, human-annotated data. Our code is available here:
https://github.com/pigeonai-org/ViDove

</details>


### [8] [On the Impossibility of Separating Intelligence from Judgment: The Computational Intractability of Filtering for AI Alignment](https://arxiv.org/abs/2507.07341)
*Sarah Ball,Greg Gluch,Shafi Goldwasser,Frauke Kreuter,Omer Reingold,Guy N. Rothblum*

Main category: cs.AI

TL;DR: 本文通过理论和密码学分析，指出外部过滤（输入、输出）无法高效阻止LLM生成有害内容，安全对齐需深入模型内部（架构和权重），黑盒方式无法实现真正安全。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）部署的增加，人们担忧其被用于生成有害内容，因此研究如何防止LLM产生危险信息的对齐和过滤机制非常重要。

Method: 本文主要围绕输入提示和输出结果的过滤方法进行理论分析，通过构造性证明并借助密码学难题假设，分析了高效过滤的计算难度。此外，还形式化并研究了放宽的缓解策略的可行性。

Result: （1）证明了对于某些LLM，不存在高效的输入提示过滤器，攻击性提示和正常提示在计算上无法有效区分。（2）在自然场景下，输出过滤同样在计算上是难以实现的，这些结论都建立在密码学困难性的假设基础上。（3）即使采用放宽的缓解方案，也存在额外的计算障碍。最终得出，依赖于LLM外部的黑盒过滤器无法真正实现安全对齐。

Conclusion: LLM的安全对齐不能依赖于模型外部的过滤器（无论是输入或输出过滤），必须依赖模型内部（结构与权重）方案。对齐的智能和判断是不可分割的。

Abstract: With the increased deployment of large language models (LLMs), one concern is
their potential misuse for generating harmful content. Our work studies the
alignment challenge, with a focus on filters to prevent the generation of
unsafe information. Two natural points of intervention are the filtering of the
input prompt before it reaches the model, and filtering the output after
generation. Our main results demonstrate computational challenges in filtering
both prompts and outputs. First, we show that there exist LLMs for which there
are no efficient prompt filters: adversarial prompts that elicit harmful
behavior can be easily constructed, which are computationally indistinguishable
from benign prompts for any efficient filter. Our second main result identifies
a natural setting in which output filtering is computationally intractable. All
of our separation results are under cryptographic hardness assumptions. In
addition to these core findings, we also formalize and study relaxed mitigation
approaches, demonstrating further computational barriers. We conclude that
safety cannot be achieved by designing filters external to the LLM internals
(architecture and weights); in particular, black-box access to the LLM will not
suffice. Based on our technical results, we argue that an aligned AI system's
intelligence cannot be separated from its judgment.

</details>


### [9] [Meek Models Shall Inherit the Earth](https://arxiv.org/abs/2507.07931)
*Hans Gundlach,Jayson Lynch,Neil Thompson*

Main category: cs.AI

TL;DR: 尽管算力不平等使大公司AI模型先领先一步，但算力扩展的边际收益迅速递减，普通算力下的“弱小模型”最终将逼近顶级模型水平；AI战略与政策要顺应这种趋势进行调整。


<details>
  <summary>Details</summary>
Motivation: 近十年来，少数公司推动了AI系统大规模扩展，导致AI模型性能不均等。该文试图挑战普遍认为“算力规模越大模型性能越强”的观点，探讨算力对AI模型性能的实际影响。

Method: 提出了一个理论模型来说明在固定分布的下一个token预测任务下，增加算力的边际能力收益会显著递减。结合理论分析和AI模型实际能力数据，对不同算力下模型表现的差异进行分析。

Result: 文章表明，尽管有些公司可以用指数级速度扩展AI模型，但由于算力扩展的收益递减，其能力优势终将大幅减小。“弱小”模型将逐步逼近顶级模型表现。并且通过训练损失与实际能力间关系分析，支持了这一点。

Conclusion: 最终，论文认为，目前AI政策和战略需重新审视，因为“弱小”模型能力提升将带来AI领域的能力趋同，影响现有技术格局。

Abstract: The past decade has seen incredible scaling of AI systems by a few companies,
leading to inequality in AI model performance. This paper argues that, contrary
to prevailing intuition, the diminishing returns to compute scaling will lead
to a convergence of AI model capabilities. In other words, meek models (those
with limited computation budget) shall inherit the earth, approaching the
performance level of the best models overall. We develop a model illustrating
that under a fixed-distribution next-token objective, the marginal capability
returns to raw compute shrink substantially. Given current scaling practices,
we argue that these diminishing returns are strong enough that even companies
that can scale their models exponentially faster than other organizations will
eventually have little advantage in capabilities. As part of our argument, we
give several reasons that proxies like training loss differences capture
important capability measures using evidence from benchmark data and
theoretical performance models. In addition, we analyze empirical data on the
capability difference of AI models over time. Finally, in light of the
increasing ability of meek models, we argue that AI strategy and policy require
reexamination, and we outline the areas this shift will affect.

</details>


### [10] [Supply Chain Optimization via Generative Simulation and Iterative Decision Policies](https://arxiv.org/abs/2507.07355)
*Haoyue Bai,Haoyu Wang,Nanxu Gong,Xinyuan Wang,Wangyang Ying,Haifeng Chen,Yanjie Fu*

Main category: cs.AI

TL;DR: 提出了Sim-to-Dec仿真-决策一体化框架，通过更智能的仿真和决策模型提升供应链运输的及时性和利润，实验验证效果显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 在供应链运输中，实现高响应性和经济效率至关重要，而这些目标都受到运输方式战略决策的影响。当前缺乏一种既能泛化到不同环境、捕捉细致动态、结合历史与预测、又能高效反馈决策改进的仿真-决策一体化框架。

Method: 提出了Sim-to-Dec框架，由生成式模拟模块和历史-未来双感知决策模型组成。模拟模块通过自回归建模模拟连续状态变化，减少依赖手工领域规则，提高对数据波动的鲁棒性。决策模型通过与模拟器交互、端到端优化迭代提升决策效果。

Result: 在三个真实数据集上的大量实验表明，Sim-to-Dec框架能显著提升及时交付率和利润。

Conclusion: Sim-to-Dec框架能有效提高供应链运输系统的及时交付率和经济效益，具有良好的泛化与实际应用潜力。

Abstract: High responsiveness and economic efficiency are critical objectives in supply
chain transportation, both of which are influenced by strategic decisions on
shipping mode. An integrated framework combining an efficient simulator with an
intelligent decision-making algorithm can provide an observable, low-risk
environment for transportation strategy design. An ideal simulation-decision
framework must (1) generalize effectively across various settings, (2) reflect
fine-grained transportation dynamics, (3) integrate historical experience with
predictive insights, and (4) maintain tight integration between simulation
feedback and policy refinement. We propose Sim-to-Dec framework to satisfy
these requirements. Specifically, Sim-to-Dec consists of a generative
simulation module, which leverages autoregressive modeling to simulate
continuous state changes, reducing dependence on handcrafted domain-specific
rules and enhancing robustness against data fluctuations; and a history-future
dual-aware decision model, refined iteratively through end-to-end optimization
with simulator interactions. Extensive experiments conducted on three
real-world datasets demonstrate that Sim-to-Dec significantly improves timely
delivery rates and profit.

</details>


### [11] [Working with AI: Measuring the Occupational Implications of Generative AI](https://arxiv.org/abs/2507.07935)
*Kiran Tomlinson,Sonia Jaffe,Will Wang,Scott Counts,Siddharth Suri*

Main category: cs.AI

TL;DR: 本文利用Bing Copilot对话数据，衡量AI在各职业中的应用及成功率，发现知识型与信息类岗位最能受益于AI，但实际应用与此前预测存在差别，且工资、学历与AI适用性有关。


<details>
  <summary>Details</summary>
Motivation: 鉴于生成式AI正被迅速广泛采用，且有潜力影响多种工作任务，因此亟需理解AI在经济中的具体影响，尤其是AI对实际职业和任务的影响领域。

Method: 分析了20万条微软Bing Copilot（生成式AI系统）中用户匿名对话数据，归类用户请求及AI完成的工作活动，并结合工作职责、生成功能及任务成功率，计算各职业的AI适用性评分。

Result: 最常见的AI协助场景为信息收集与写作，AI主要执行信息提供、协助、写作、教学及建议。知识型和信息沟通类职业的AI适用性最高。不同职业的工资、教育水平与AI适用性存在一定相关性。也对最成功的AI协作任务类别及实际应用与预测的差异进行了量化分析。

Conclusion: 知识型工作岗位（如计算机、数学、行政支持及销售等）对AI的适用性最高，AI在这些职业中应用较为广泛且表现较好。实际AI的使用与此前对职业影响的预测有一定关联，AI的应用还与工资和教育水平存在相关性。

Abstract: Given the rapid adoption of generative AI and its potential to impact a wide
range of tasks, understanding the effects of AI on the economy is one of
society's most important questions. In this work, we take a step toward that
goal by analyzing the work activities people do with AI, how successfully and
broadly those activities are done, and combine that with data on what
occupations do those activities. We analyze a dataset of 200k anonymized and
privacy-scrubbed conversations between users and Microsoft Bing Copilot, a
publicly available generative AI system. We find the most common work
activities people seek AI assistance for involve gathering information and
writing, while the most common activities that AI itself is performing are
providing information and assistance, writing, teaching, and advising.
Combining these activity classifications with measurements of task success and
scope of impact, we compute an AI applicability score for each occupation. We
find the highest AI applicability scores for knowledge work occupation groups
such as computer and mathematical, and office and administrative support, as
well as occupations such as sales whose work activities involve providing and
communicating information. Additionally, we characterize the types of work
activities performed most successfully, how wage and education correlate with
AI applicability, and how real-world usage compares to predictions of
occupational AI impact.

</details>


### [12] [DrugMCTS: a drug repurposing framework combining multi-agent, RAG and Monte Carlo Tree Search](https://arxiv.org/abs/2507.07426)
*Zerui Yang,Yuwei Wan,Yinqiao Li,Yudai Matsuda,Tong Xie,Linqi Song*

Main category: cs.AI

TL;DR: 本文提出DrugMCTS，通过结合RAG、多智能体与MCTS，实现了无需微调即可显著提升药物重定位问题的LLM推理效果。大幅超越现有LLM和深度学习方法，验证了结构化协作推理的重要性。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLM）在药物发现等科学领域已展现出巨大潜力，但在涉及超出预训练知识的推理时，仍受效果限制。传统方法（如微调、检索增强生成等）面临计算开销大或不能充分利用结构化科学数据的问题。

Method: 提出了DrugMCTS框架，将检索增强生成（RAG）、多智能体协作和蒙特卡洛树搜索（MCTS）有机结合。该框架设有五个专门的智能体，负责检索和分析分子及蛋白质信息，实现结构化、迭代式推理，无需领域特定微调。

Result: DrugMCTS使Qwen2.5-7B-Instruct在无需领域微调的情况下，性能超过Deepseek-R1超过20%。在DrugBank和KIBA数据集上的广泛实验表明，DrugMCTS在召回率和鲁棒性方面大幅优于通用LLM及深度学习基线。

Conclusion: 结构化推理、多智能体协作与反馈驱动的搜索机制对于提升LLM在药物发现中的应用至关重要。DrugMCTS框架有效弥补了当前主流方法的短板，显著提升了药物重定位的推理能力与结果。

Abstract: Recent advances in large language models have demonstrated considerable
potential in scientific domains such as drug discovery. However, their
effectiveness remains constrained when reasoning extends beyond the knowledge
acquired during pretraining. Conventional approaches, such as fine-tuning or
retrieval-augmented generation, face limitations in either imposing high
computational overhead or failing to fully exploit structured scientific data.
To overcome these challenges, we propose DrugMCTS, a novel framework that
synergistically integrates RAG, multi-agent collaboration, and Monte Carlo Tree
Search for drug repurposing. The framework employs five specialized agents
tasked with retrieving and analyzing molecular and protein information, thereby
enabling structured and iterative reasoning. Without requiring domain-specific
fine-tuning, DrugMCTS empowers Qwen2.5-7B-Instruct to outperform Deepseek-R1 by
over 20\%. Extensive experiments on the DrugBank and KIBA datasets demonstrate
that DrugMCTS achieves substantially higher recall and robustness compared to
both general-purpose LLMs and deep learning baselines. Our results highlight
the importance of structured reasoning, agent-based collaboration, and
feedback-driven search mechanisms in advancing LLM applications for drug
discovery.

</details>


### [13] [StarDojo: Benchmarking Open-Ended Behaviors of Agentic Multimodal LLMs in Production-Living Simulations with Stardew Valley](https://arxiv.org/abs/2507.07445)
*Weihao Tan,Changjiu Jiang,Yu Duan,Mingcong Lei,Jiageng Li,Yitian Hong,Xinrun Wang,Bo An*

Main category: cs.AI

TL;DR: 作者提出StarDojo基准，实现了对自主体综合生产生活与社会交往能力的全方位评测。实验发现主流多模态大模型在该基准下表现尚不足，未来需攻克视觉、多模态理解及低层操控等挑战。


<details>
  <summary>Details</summary>
Motivation: 当前自主智能体需要同时具备生产活动和社会交往能力，但现有基准很少综合评估这两者。为解决这一评测空白，作者提出了新的基准。

Method: 提出了StarDojo基准，基于游戏Stardew Valley，涵盖种植、制作、探索、战斗和社交五大领域的1000个任务，并提供100个代表性子集用于快速评测。该基准具有通用性强、易用性高、支持多系统并行运行等优点，适用于多模态大模型智能体评测。

Result: 对多个先进多模态大型语言模型（MLLMs）进行评测，结果发现模型普遍表现不佳，最好的GPT-4.1的成功率仅为12.7%，主要瓶颈在视觉理解、多模态推理和底层操作能力。

Conclusion: StarDojo为自动体在复杂开放式环境中的评测提供了新的工具，有利于推动更健壮自主体的研究。

Abstract: Autonomous agents navigating human society must master both production
activities and social interactions, yet existing benchmarks rarely evaluate
these skills simultaneously. To bridge this gap, we introduce StarDojo, a novel
benchmark based on Stardew Valley, designed to assess AI agents in open-ended
production-living simulations. In StarDojo, agents are tasked to perform
essential livelihood activities such as farming and crafting, while
simultaneously engaging in social interactions to establish relationships
within a vibrant community. StarDojo features 1,000 meticulously curated tasks
across five key domains: farming, crafting, exploration, combat, and social
interactions. Additionally, we provide a compact subset of 100 representative
tasks for efficient model evaluation. The benchmark offers a unified,
user-friendly interface that eliminates the need for keyboard and mouse
control, supports all major operating systems, and enables the parallel
execution of multiple environment instances, making it particularly well-suited
for evaluating the most capable foundation agents, powered by multimodal large
language models (MLLMs). Extensive evaluations of state-of-the-art MLLMs agents
demonstrate substantial limitations, with the best-performing model, GPT-4.1,
achieving only a 12.7% success rate, primarily due to challenges in visual
understanding, multimodal reasoning and low-level manipulation. As a
user-friendly environment and benchmark, StarDojo aims to facilitate further
research towards robust, open-ended agents in complex production-living
environments.

</details>


### [14] [Position: We Need An Algorithmic Understanding of Generative AI](https://arxiv.org/abs/2507.07544)
*Oliver Eberle,Thomas McGee,Hamza Giaffar,Taylor Webb,Ida Momennejad*

Main category: cs.AI

TL;DR: 本文提出AlgEval框架，用于系统研究大语言模型推理中的实际算法机制，结合假设形成与底层分析，初步验证了对模型内部算法的探索能力。该框架有望推动LLM可解释性与高效性发展。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型性能惊人，但其实际学会并执行了哪些算法仍然缺乏理论和实证研究。以往研究更关注于扩大规模提升性能，导致对模型内部算法机制缺乏系统理解，阻碍了可解释性和效率的提升。

Method: 该论文提出了AlgEval框架，通过顶层假设梳理候选算法，并结合底层分析（如注意力模式和隐藏状态的电路级研究）来验证这些假设。以涌现的搜索算法为案例，展示了从假设推导到实证验证的方法路径。

Result: AlgEval框架有助于识别和分析LLM潜在的算法基元及其组合，为理解模型的推理过程打开新路径，也为开发更高效的训练与更具解释性的模型结构提供了契机。案例初步证实该方法对发现涌现算法有效。

Conclusion: 提出并初步展示了一个名为AlgEval的评估框架，用于系统研究大语言模型（LLM）“学到”的算法机制，并强调深入了解这些机制对于模型可解释性和高效提升性能的重要意义。

Abstract: What algorithms do LLMs actually learn and use to solve problems? Studies
addressing this question are sparse, as research priorities are focused on
improving performance through scale, leaving a theoretical and empirical gap in
understanding emergent algorithms. This position paper proposes AlgEval: a
framework for systematic research into the algorithms that LLMs learn and use.
AlgEval aims to uncover algorithmic primitives, reflected in latent
representations, attention, and inference-time compute, and their algorithmic
composition to solve task-specific problems. We highlight potential
methodological paths and a case study toward this goal, focusing on emergent
search algorithms. Our case study illustrates both the formation of top-down
hypotheses about candidate algorithms, and bottom-up tests of these hypotheses
via circuit-level analysis of attention patterns and hidden states. The
rigorous, systematic evaluation of how LLMs actually solve tasks provides an
alternative to resource-intensive scaling, reorienting the field toward a
principled understanding of underlying computations. Such algorithmic
explanations offer a pathway to human-understandable interpretability, enabling
comprehension of the model's internal reasoning performance measures. This can
in turn lead to more sample-efficient methods for training and improving
performance, as well as novel architectures for end-to-end and multi-agent
systems.

</details>


### [15] [On Trustworthy Rule-Based Models and Explanations](https://arxiv.org/abs/2507.07576)
*Mohamed Siala,Jordi Planes,Joao Marques-Silva*

Main category: cs.AI

TL;DR: 本论文指出，常用的基于规则的可解释机器学习模型存在冗余和重叠等缺陷，并提出分析这些缺陷的算法，强调在高风险应用中对模型解释性的严格要求。


<details>
  <summary>Details</summary>
Motivation: 在高风险领域，机器学习模型的解释性尤为重要，不准确的解释可能误导人类决策，因此需要对解释进行严格研究。

Method: 分析基于规则的机器学习模型（如决策树、决策图等）中负面重叠和冗余等已知缺陷，并开发算法对这些缺陷进行检测和分析。

Result: 研究表明，目前被广泛使用的基于规则的机器学习建模工具生成的规则集，普遍存在负面特征，如重叠和冗余。

Conclusion: 即使是主流的基于规则的可解释模型，在实际应用中仍然会产生某些负面特性，需要对模型解释性进行更严格的分析和改进。

Abstract: A task of interest in machine learning (ML) is that of ascribing explanations
to the predictions made by ML models. Furthermore, in domains deemed high risk,
the rigor of explanations is paramount. Indeed, incorrect explanations can and
will mislead human decision makers. As a result, and even if interpretability
is acknowledged as an elusive concept, so-called interpretable models are
employed ubiquitously in high-risk uses of ML and data mining (DM). This is the
case for rule-based ML models, which encompass decision trees, diagrams, sets
and lists. This paper relates explanations with well-known undesired facets of
rule-based ML models, which include negative overlap and several forms of
redundancy. The paper develops algorithms for the analysis of these undesired
facets of rule-based systems, and concludes that well-known and widely used
tools for learning rule-based ML models will induce rule sets that exhibit one
or more negative facets.

</details>


### [16] [Context Pooling: Query-specific Graph Pooling for Generic Inductive Link Prediction in Knowledge Graphs](https://arxiv.org/abs/2507.07595)
*Zhixiang Su,Di Wang,Chunyan Miao*

Main category: cs.AI

TL;DR: 本文提出了一个创新的Context Pooling方法，提升了GNN在知识图谱链路预测上的表现，通过生成查询相关的子图和筛选逻辑相关邻居，在多个数据集和模型中取得了最优结果。


<details>
  <summary>Details</summary>
Motivation: 近年来，使用图神经网络（GNN）进行知识图谱中链路预测的研究发现，常规的聚合操作对模型性能提升有限，需要新的方法来提升GNN的效果。

Method: 本文提出了一种新方法——Context Pooling，通过在知识图谱中应用图池化机制，生成针对具体查询的子图，特别适用于归纳设置（测试实体在训练时未出现）。此外，作者设计了两个指标：邻居精度和邻居召回率，用于评估邻居与给定查询的逻辑相关性，从而筛选出逻辑上相关的邻居用于链路预测。该方法通用，可融合到现有SOTA模型中。

Result: 将Context Pooling方法应用于两个最先进的GNN模型，并在三个公开的迁移和归纳数据集上测试，在48个设定中有42次获得了SOTA性能。

Conclusion: Context Pooling首次将图池化应用于知识图谱中的链路预测任务，并能在归纳场景下有效生成查询相关的子图，显著提升了GNN链路预测性能，为GNN与知识图谱的结合提供了新途径。

Abstract: Recent investigations on the effectiveness of Graph Neural Network
(GNN)-based models for link prediction in Knowledge Graphs (KGs) show that
vanilla aggregation does not significantly impact the model performance. In
this paper, we introduce a novel method, named Context Pooling, to enhance
GNN-based models' efficacy for link predictions in KGs. To our best of
knowledge, Context Pooling is the first methodology that applies graph pooling
in KGs. Additionally, Context Pooling is first-of-its-kind to enable the
generation of query-specific graphs for inductive settings, where testing
entities are unseen during training. Specifically, we devise two metrics,
namely neighborhood precision and neighborhood recall, to assess the neighbors'
logical relevance regarding the given queries, thereby enabling the subsequent
comprehensive identification of only the logically relevant neighbors for link
prediction. Our method is generic and assessed by being applied to two
state-of-the-art (SOTA) models on three public transductive and inductive
datasets, achieving SOTA performance in 42 out of 48 settings.

</details>


### [17] [Enhancing Vaccine Safety Surveillance: Extracting Vaccine Mentions from Emergency Department Triage Notes Using Fine-Tuned Large Language Models](https://arxiv.org/abs/2507.07599)
*Sedigh Khademi,Jim Black,Christopher Palmer,Muhammad Javed,Hazel Clothier,Jim Buttery,Gerardo Luis Dimaguila*

Main category: cs.AI

TL;DR: 本文证明微调后的大语言模型（Llama 3）结合模型量化，能够有效、准确地从急诊分诊记录中自动提取疫苗相关信息，极大提升了疫苗安全监测的实时性和效率。


<details>
  <summary>Details</summary>
Motivation: 当前疫苗接种后不良事件的监测需要高效、准确地从急诊分诊记录中提取疫苗相关信息，传统方法效率较低且难以应对大规模数据，亟需自动化工具提升疫苗安全监测能力。

Method: 本研究采用Prompt工程生成标签数据集，并由人工注释确认，随后比较了Prompt工程模型、微调后的大模型（Llama 3, 32亿参数）、以及基于规则的方法提取疫苗信息的性能。还应用了模型量化技术以适应资源受限环境部署。

Result: 微调后的Llama 3大模型在提取疫苗名称的准确性上表现最佳，优于其它方法。模型量化大幅提升了在资源有限环境下的推理效率。

Conclusion: 大语言模型，尤其是经过微调和量化后的Llama 3，在自动化处理急诊分诊记录、提升疫苗安全监测效率与准确性方面具备巨大潜力，有助于及早发现免疫相关不良事件。

Abstract: This study evaluates fine-tuned Llama 3.2 models for extracting
vaccine-related information from emergency department triage notes to support
near real-time vaccine safety surveillance. Prompt engineering was used to
initially create a labeled dataset, which was then confirmed by human
annotators. The performance of prompt-engineered models, fine-tuned models, and
a rule-based approach was compared. The fine-tuned Llama 3 billion parameter
model outperformed other models in its accuracy of extracting vaccine names.
Model quantization enabled efficient deployment in resource-constrained
environments. Findings demonstrate the potential of large language models in
automating data extraction from emergency department notes, supporting
efficient vaccine safety surveillance and early detection of emerging adverse
events following immunization issues.

</details>


### [18] [Towards conservative inference in credal networks using belief functions: the case of credal chains](https://arxiv.org/abs/2507.07619)
*Marco Sangalli,Thomas Krak,Cassio de Campos*

Main category: cs.AI

TL;DR: 本文提出一种基于Dempster-Shafer理论的链式credal网络信念推断方法，特点是高效、稳健，并通过对比分析展示其实用价值和局限。


<details>
  <summary>Details</summary>
Motivation: 以往对credal网络中的不确定推理多集中在经典概率分析，缺乏对Dempster-Shafer理论下信念推断的系统方法。本研究旨在填补该空白，尤其关注链式结构。

Method: 提出了基于Dempster-Shafer理论的信念传播新框架，用于处理credal网络（以链为例）中的不确定性推断。正式描述信念推断方法，采用数值实验与经典灵敏度分析进行对比。

Result: 该方法可以高效计算保守的置信区间，保证运算速度的同时提升不确定性刻画能力。数值结果揭示了信念推断在实际应用中的优点和局限性。

Conclusion: 信念推断为链式及更广泛的credal网络提供了一种高效且稳健的不确定性传播工具，有助于理论和实际应用领域深入理解和利用不确定推理。

Abstract: This paper explores belief inference in credal networks using Dempster-Shafer
theory. By building on previous work, we propose a novel framework for
propagating uncertainty through a subclass of credal networks, namely chains.
The proposed approach efficiently yields conservative intervals through belief
and plausibility functions, combining computational speed with robust
uncertainty representation. Key contributions include formalizing belief-based
inference methods and comparing belief-based inference against classical
sensitivity analysis. Numerical results highlight the advantages and
limitations of applying belief inference within this framework, providing
insights into its practical utility for chains and for credal networks in
general.

</details>


### [19] [PlanQA: A Benchmark for Spatial Reasoning in LLMs using Structured Representations](https://arxiv.org/abs/2507.07644)
*Fedor Rodionov,Abdelrahman Eldesokey,Michael Birsak,John Femiani,Bernard Ghanem,Peter Wonka*

Main category: cs.AI

TL;DR: 本文提出PlanQA室内场景空间推理基准，揭示现有LLMs在实际空间几何推理中的弱点，呼吁社区推动空间认知能力更强的语言模型发展。


<details>
  <summary>Details</summary>
Motivation: 当前的大型语言模型在文本理解方面表现优异，但其对几何和空间推理能力的评估较为缺乏，尤其是在真实世界布局和物理约束方面存在明显短板。该论文希望通过一个新的基准检测和推动语言模型在空间理解上的进步。

Method: 提出了PlanQA基准，基于室内场景（如厨房、客厅、卧室）的结构化符号表示（如JSON、XML布局），设计了多样化题型，涵盖距离、可视性、最短路径等度量与拓扑推理，以及可用性、间隙、平衡等室内设计约束，通过对多种主流开源和商业LLMs进行评测，分析其空间推理能力。

Result: 当前LLMs在浅层空间查询表现尚可，但在模拟物理约束、维持空间一致性、或在布局扰动下泛化能力薄弱。PlanQA揭示了LLMs在真实空间布局推理中的明显不足。

Conclusion: PlanQA为LLMs在空间与几何推理方面提供了诊断性评估工具，结果显示现有LLMs难以可靠地理解和操作真实空间布局。作者期望此基准促进语言模型在空间推理能力的提升。

Abstract: We introduce PlanQA, a diagnostic benchmark for evaluating geometric and
spatial reasoning in large-language models (LLMs). PlanQA is grounded in
structured representations of indoor scenes, such as kitchens, living rooms,
and bedrooms, encoded in a symbolic format (e.g., JSON, XML layouts). The
benchmark includes diverse question types that test not only metric and
topological reasoning (e.g., distance, visibility, shortest paths) but also
interior design constraints such as affordance, clearance, balance, and
usability. Our results across a variety of frontier open-source and commercial
LLMs show that while models may succeed in shallow queries, they often fail to
simulate physical constraints, preserve spatial coherence, or generalize under
layout perturbation. PlanQA uncovers a clear blind spot in today's LLMs: they
do not consistently reason about real-world layouts. We hope that this
benchmark inspires new work on language models that can accurately infer and
manipulate spatial and geometric properties in practical settings.

</details>


### [20] [Stable Preference Optimization for LLMs: A Bilevel Approach Beyond Direct Preference Optimization](https://arxiv.org/abs/2507.07723)
*Chengtao Jian,Kai Yang,Ye Ouyang,Xiaozhou Ye*

Main category: cs.AI

TL;DR: DPO虽然有效，却有概率分配和稳定性方面的问题。作者提出了结合有监督微调和正则化的稳定偏好优化方法，有效提升了推理准确率和偏好对齐表现。


<details>
  <summary>Details</summary>
Motivation: 尽管直接偏好优化（DPO）方法以其效率和实际表现成为人类偏好对齐的热门方法，但其理论特性和内在局限性尚未深入研究。动机在于揭示DPO的理论缺陷，并设计更稳健的偏好对齐方法。

Method: 作者通过概率演化视角对DPO方法进行理论分析，指出其对初始化敏感且易导致概率分布错误分配，进而提出结合有监督微调与增强版DPO目标的双层优化框架，并通过引入正则化来促进偏好输出概率的提升，同时保证优化过程稳定。最后在推理和摘要任务上进行了实验验证。

Result: 分析显示DPO对初始化高度敏感，容易将概率分配给无关或不理想的响应，从而加剧模型偏见，影响稳定性和一致性。所提的稳定偏好优化框架，则通过实验证明，能提升推理准确率并更好地对齐输出分布。

Conclusion: 本论文提出的稳定偏好优化方法能够更好地对齐语言模型输出与人类偏好，并在推理和摘要任务上显著优于标准的DPO方法。该方法为偏好对齐目标的设计提供了新见解，并为更可靠、可解释的语言模型对齐开辟了新途径。

Abstract: Direct Preference Optimization (DPO) has emerged as a popular and efficient
alternative to reward modeling and reinforcement learning for aligning language
models with human preferences. Despite its empirical success, the theoretical
properties and intrinsic limitations of DPO remain underexplored. In this work,
we first present a comprehensive analysis of DPO's dynamics from a probability
evolution perspective. Our analysis reveals that DPO is highly sensitive to
initialization. It also tends to misallocate probability mass, which can
inadvertently shift probability toward irrelevant or undesired responses. This
misallocation may unintentionally reinforce model bias, thereby compromising
both the stability of model alignment and the consistency with intended
preferences. Motivated by these theoretical findings, we propose a
theoretically grounded bilevel optimization framework that tightly integrate
supervised fine-tuning with an enhanced DPO objective a.k.a. stable preference
optimization. Our approach introduces a principled regularization scheme to
explicitly encourage absolute probability improvement for preferred outputs,
while maintaining stable optimization dynamics. Experiments on challenging
reasoning and summarization benchmarks elucidate that our method consistently
improves reasoning accuracy and better aligns output distributions with
intended preferences, outperforming standard DPO. Stable preference
optimization provides new insights into the design of preference-based
alignment objectives and opens up new avenues towards more reliable and
interpretable language model alignment.

</details>


### [21] [Identification of Violin Reduction via Contour Lines Classification](https://arxiv.org/abs/2507.07743)
*Philémon Beghin,Anne-Emmanuelle Ceulemans,François Glineur*

Main category: cs.AI

TL;DR: 本文提出基于等高线几何参数，运用三维模型分析和分类算法，定量区分缩减与未缩减小提琴，为乐器形态学定量分析提供新工具。


<details>
  <summary>Details</summary>
Motivation: 早期小提琴形态多样，18世纪中期统一之后，部分乐器被进行尺寸缩减，专家发现缩减与未缩减乐器的等高线存在U型/V型差异，但缺乏定量研究。

Method: 收集25把小提琴的三维模型，按毫米间隔提取等高线，用形如y = α*|x|^β的抛物线拟合曲线，通过参数α、β和回归、计数法获得乐器的数值特征，之后应用分类方法判断几何特征是否能预测尺寸缩减。

Result: 几何特征在一定程度上可区分缩减与未缩减的小提琴。最具判别力的是‘开口’参数β，但部分介于缩减与未缩减之间的乐器难以分类。

Conclusion: 可通过乐器的等高线几何参数对小提琴的缩减状态进行分类，尤其是‘开口’参数β最具代表性，然而完全准确的分类尚受限于存在不同程度缩减的乐器谱系。

Abstract: The first violins appeared in late 16th-century Italy. Over the next 200
years, they spread across Europe and luthiers of various royal courts, eager to
experiment with new techniques, created a highly diverse family of instruments.
Around 1750, size standards were introduced to unify violin making for
orchestras and conservatories. Instruments that fell between two standards were
then reduced to a smaller size by luthiers. These reductions have an impact on
several characteristics of violins, in particular on the contour lines, i.e.
lines of constant altitude, which look more like a U for non reduced
instruments and a V for reduced ones. While such differences are observed by
experts, they have not been studied quantitatively.
  This paper presents a method for classifying violins as reduced or
non-reduced based on their contour lines. We study a corpus of 25 instruments
whose 3D geometric meshes were acquired via photogrammetry. For each
instrument, we extract 10-20 contour lines regularly spaced every millimetre.
Each line is fitted with a parabola-like curve (with an equation of the type y
= alpha*abs(x)**beta) depending on two parameters, describing how open (beta)
and how vertically stretched (alpha) the curve is. We compute additional
features from those parameters, using regressions and counting how many values
fall under some threshold. We also deal with outliers and non equal numbers of
levels, and eventually obtain a numerical profile for each instrument.
  We then apply classification methods to assess whether geometry alone can
predict size reduction. We find that distinguishing between reduced and non
reduced instruments is feasible to some degree, taking into account that a
whole spectrum of more or less transformed violins exists, for which it is more
difficult to quantify the reduction. We also find the opening parameter beta to
be the most predictive.

</details>


### [22] [Measuring AI Alignment with Human Flourishing](https://arxiv.org/abs/2507.07787)
*Elizabeth Hilliard,Akshaya Jagadeesh,Alex Cook,Steele Billings,Nicholas Skytland,Alicia Llewellyn,Jackson Paull,Nathan Paull,Nolan Kurylo,Keatra Nesbitt,Robert Gruenewald,Anthony Jantzi,Omar Chavez*

Main category: cs.AI

TL;DR: 本论文提出了一套全新的AI福祉对齐评测标准，涵盖七大人类繁荣指标，经实测发现现有大语言模型尚无法全面对齐全部维度，尤其在精神和价值层面存在不足，意义在于为AI伦理和发展提供新评判框架。


<details>
  <summary>Details</summary>
Motivation: 现有AI评测标准多聚焦于技术能力或防止伤害，缺乏对AI促进人类全面福祉的评估方法。因此，亟需开发能衡量AI与人类繁荣（flourishing）对齐程度的新型评测框架。

Method: 作者提出了“Flourishing AI Benchmark (FAI Benchmark)”，该框架针对七大维度（品格与美德、紧密社会关系、幸福与生活满意度、意义与目标、身心健康、金融与物质稳定、信仰与精神性），通过1229个客观与主观问题考察AI模型表现。评测过程利用专门的评判型大语言模型和多维评分体系，并采用几何均值保证各维度表现平衡。

Result: 对28个主流大语言模型的初步测试显示，部分模型在整体对齐上有进步（最高得分为72/100），但所有模型都未能在全部维度上实现令人满意的对齐，尤其在信仰与精神性、品格与美德、意义与目标方面表现较弱。

Conclusion: FAI Benchmark为评估和推动AI促进人类福祉而非仅仅避免伤害提供了新范式，对AI发展、伦理、评测等领域具有重要意义。

Abstract: This paper introduces the Flourishing AI Benchmark (FAI Benchmark), a novel
evaluation framework that assesses AI alignment with human flourishing across
seven dimensions: Character and Virtue, Close Social Relationships, Happiness
and Life Satisfaction, Meaning and Purpose, Mental and Physical Health,
Financial and Material Stability, and Faith and Spirituality. Unlike
traditional benchmarks that focus on technical capabilities or harm prevention,
the FAI Benchmark measures AI performance on how effectively models contribute
to the flourishing of a person across these dimensions. The benchmark evaluates
how effectively LLM AI systems align with current research models of holistic
human well-being through a comprehensive methodology that incorporates 1,229
objective and subjective questions. Using specialized judge Large Language
Models (LLMs) and cross-dimensional evaluation, the FAI Benchmark employs
geometric mean scoring to ensure balanced performance across all flourishing
dimensions. Initial testing of 28 leading language models reveals that while
some models approach holistic alignment (with the highest-scoring models
achieving 72/100), none are acceptably aligned across all dimensions,
particularly in Faith and Spirituality, Character and Virtue, and Meaning and
Purpose. This research establishes a framework for developing AI systems that
actively support human flourishing rather than merely avoiding harm, offering
significant implications for AI development, ethics, and evaluation.

</details>


### [23] [MoSE: Skill-by-Skill Mixture-of-Expert Learning for Autonomous Driving](https://arxiv.org/abs/2507.07818)
*Lu Xu,Jiaqian Yu,Xiongfeng Peng,Yiwei Chen,Weiming Li,Jaewook Yoo,Sunghyun Chunag,Dongwook Lee,Daehyun Ji,Chao Zhang*

Main category: cs.AI

TL;DR: 作者提出了结合人类驾驶分步学习思想的技能导向MoE模型MoSE，能以更小计算量达到优异表现，在自动驾驶推理任务上显著领先传统大模型。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型（LLM）和视觉语言模型（VLM）虽然具备强大泛化能力，但直接将通用的专家混合模型（MoE）应用于自动驾驶，受限于巨大的数据需求和训练复杂度。本文受到人类驾驶员逐步学习技能的过程启发，尝试寻找更高效、解释性更强的模型方案。

Method: 本文提出了一种以技能为中心的专家混合模型，称为MoSE。该模型通过定义和标注具体驾驶技能，采用技能导向的路由机制，将输入动态分配至相应的专家，实现技能逐步学习与推理。通过构建分层技能数据集、预训练路由器，使模型能够进行分步推理。此外，MoSE将描述、推理、规划等辅助任务集成于一次前向推理过程中，无需增加计算开销。

Result: 在激活参数不到3B的情况下，MoSE在CODA AD自动驾驶极端场景推理任务上超越了多种8B+参数规模的模型。与同类开源方法相比，本方案以单轮对话实现了最先进性能，并将激活模型规模显著降低至少62.5%。

Conclusion: 技能导向的专家混合模型MoSE通过借鉴人类驾驶员分步学习过程，实现了高效且解释性强的端到端自动驾驶推理。该方法在更小模型规模下取得了领先性能，具备实际应用潜力。

Abstract: Recent studies show large language models (LLMs) and vision language models
(VLMs) trained using web-scale data can empower end-to-end autonomous driving
systems for a better generalization and interpretation. Specifically, by
dynamically routing inputs to specialized subsets of parameters, the
Mixture-of-Experts (MoE) technique enables general LLMs or VLMs to achieve
substantial performance improvements while maintaining computational
efficiency. However, general MoE models usually demands extensive training data
and complex optimization. In this work, inspired by the learning process of
human drivers, we propose a skill-oriented MoE, called MoSE, which mimics human
drivers' learning process and reasoning process, skill-by-skill and
step-by-step. We propose a skill-oriented routing mechanism that begins with
defining and annotating specific skills, enabling experts to identify the
necessary driving competencies for various scenarios and reasoning tasks,
thereby facilitating skill-by-skill learning. Further align the driving process
to multi-step planning in human reasoning and end-to-end driving models, we
build a hierarchical skill dataset and pretrain the router to encourage the
model to think step-by-step. Unlike multi-round dialogs, MoSE integrates
valuable auxiliary tasks (e.g.\ description, reasoning, planning) in one single
forward process without introducing any extra computational cost. With less
than 3B sparsely activated parameters, our model outperforms several 8B+
parameters on CODA AD corner case reasoning task. Compared to existing methods
based on open-source models and data, our approach achieves state-of-the-art
performance with significantly reduced activated model size (at least by
$62.5\%$) with a single-turn conversation.

</details>


### [24] [AI Should Sense Better, Not Just Scale Bigger: Adaptive Sensing as a Paradigm Shift](https://arxiv.org/abs/2507.07820)
*Eunsu Baek,Keondo Park,Jeonggil Ko,Min-hwan Oh,Taesik Gong,Hyung-Sin Kim*

Main category: cs.AI

TL;DR: 本文提出“自适应感知”新范式，通过动态调节传感器输入，既提升小模型表现，又降低资源消耗，有助于构建可持续和公平的AI系统。


<details>
  <summary>Details</summary>
Motivation: 当前AI依赖于扩大神经网络规模和训练数据来提升泛化和稳健性，但带来了环境、经济和伦理负担，影响可持续性和公平性。作者受生物感知系统的启发，提出在输入端动态适应，有望解决上述问题。

Method: 提出“自适应感知”范式，通过主动调节传感器参数（如曝光、灵敏度、多模态配置）来适应输入变化，缓解共变量偏移，提高效率。文中还规划了自适应感知在实际领域的应用路线，并探讨了技术和伦理挑战，提出后续研究方向。

Result: 实验证明，在采用自适应感知后，小型模型（如EfficientNet-B0）的性能可超越大型模型（如OpenCLIP-H），即使用的数据和计算资源更少。

Conclusion: 自适应感知可推动AI系统向可持续、稳健和公平方向发展，并为其在多个现实场景中的集成和发展提供了可行路径。

Abstract: Current AI advances largely rely on scaling neural models and expanding
training datasets to achieve generalization and robustness. Despite notable
successes, this paradigm incurs significant environmental, economic, and
ethical costs, limiting sustainability and equitable access. Inspired by
biological sensory systems, where adaptation occurs dynamically at the input
(e.g., adjusting pupil size, refocusing vision)--we advocate for adaptive
sensing as a necessary and foundational shift. Adaptive sensing proactively
modulates sensor parameters (e.g., exposure, sensitivity, multimodal
configurations) at the input level, significantly mitigating covariate shifts
and improving efficiency. Empirical evidence from recent studies demonstrates
that adaptive sensing enables small models (e.g., EfficientNet-B0) to surpass
substantially larger models (e.g., OpenCLIP-H) trained with significantly more
data and compute. We (i) outline a roadmap for broadly integrating adaptive
sensing into real-world applications spanning humanoid, healthcare, autonomous
systems, agriculture, and environmental monitoring, (ii) critically assess
technical and ethical integration challenges, and (iii) propose targeted
research directions, such as standardized benchmarks, real-time adaptive
algorithms, multimodal integration, and privacy-preserving methods.
Collectively, these efforts aim to transition the AI community toward
sustainable, robust, and equitable artificial intelligence systems.

</details>


### [25] [Searching for actual causes: Approximate algorithms with adjustable precision](https://arxiv.org/abs/2507.07857)
*Samuel Reyd,Ada Diaconescu,Jean-Louis Dessalles*

Main category: cs.AI

TL;DR: 本论文针对XAI中实际原因识别难题，提出了一套高效、可调节精度的算法，适用于非布尔、黑盒和随机系统，为因果解释提供了更实用的方法。


<details>
  <summary>Details</summary>
Motivation: 近年来，因果关系在机器学习领域变得非常重要，提升了模型的性能和可解释性。然而，传统的可解释人工智能(XAI)方法和因果推断主要关注因素与结果之间的关联，对普通用户来说解释效果有限，他们更期待针对特定结果的实际原因的说明。目前，如何形式化界定并高效识别实际原因仍然是难题，且存在算法复杂性高、实用算法稀缺的问题。

Method: 提出了一套新算法，能够以多项式复杂度识别实际原因，并且可以调整算法的精度和穷尽性。此外，这些算法适用于不同类型的系统，包括非布尔系统、黑盒系统和随机系统。

Result: 实验表明，提出的算法不仅可以处理现有方法无法覆盖的系统类型（如非布尔、黑盒、随机系统），还可以通过增加计算量来提升因果识别的精度和穷尽性。

Conclusion: 本文提出的多项式复杂度因果识别算法能够高效且灵活地识别实际原因，拓展了传统方法无法处理的系统范围，并为实际应用提供了更具解释性的工具。

Abstract: Causality has gained popularity in recent years. It has helped improve the
performance, reliability, and interpretability of machine learning models.
However, recent literature on explainable artificial intelligence (XAI) has
faced criticism. The classical XAI and causality literature focuses on
understanding which factors contribute to which consequences. While such
knowledge is valuable for researchers and engineers, it is not what non-expert
users expect as explanations. Instead, these users often await facts that cause
the target consequences, i.e., actual causes. Formalizing this notion is still
an open problem. Additionally, identifying actual causes is reportedly an
NP-complete problem, and there are too few practical solutions to approximate
formal definitions. We propose a set of algorithms to identify actual causes
with a polynomial complexity and an adjustable level of precision and
exhaustiveness. Our experiments indicate that the algorithms (1) identify
causes for different categories of systems that are not handled by existing
approaches (i.e., non-boolean, black-box, and stochastic systems), (2) can be
adjusted to gain more precision and exhaustiveness with more computation time.

</details>


### [26] [An Integrated Framework of Prompt Engineering and Multidimensional Knowledge Graphs for Legal Dispute Analysis](https://arxiv.org/abs/2507.07893)
*Mingda Zhang,Na Zhao,Jianglong Qing,Qing xu,Kaiwen Pan,Ting luo*

Main category: cs.AI

TL;DR: 本文提出一种结合多维知识图谱与分层提示工程、并集成法律专用推理和动态优化能力的框架，显著提升了大型语言模型在复杂法律案件分析中的知识应用和司法逻辑理解水平，为智能法律决策提供了新的技术手段。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型虽然在智能法律系统中发挥着核心作用，但在法律争议分析方面存在知识表达不足、法律概念理解有限、推理能力薄弱等明显短板。

Method: 提出结合提示工程（prompt engineering）和多维知识图谱的增强型解决方案，具体包含三阶段分层提示结构（任务定义、知识背景、推理引导）、法律专用推理模板以及动态优化机制，并构建三层知识图谱（法律分类本体层、表示层、实例层）。同时，结合多种法条与法律概念精准检索方法，并与网页搜索技术集成。

Result: 实验结果表明该框架在法律争议分析任务上取得了明显性能提升，能更准确地分析复杂案例中的法律适用与司法逻辑。

Conclusion: 该研究提出了创新性的知识增强型智能法律决策框架，显著改善了大型语言模型在法律场景下的知识应用与推理能力，为智能法律辅助系统提供了新的技术路径。

Abstract: The rapid development of artificial intelligence has positioned large
language models as fundamental components of intelligent legal systems.
However, these models face significant limitations in legal dispute analysis,
including insufficient legal knowledge representation, limited concept
understanding, and reasoning deficiencies. This research proposes an enhanced
framework integrating prompt engineering with multidimensional knowledge
graphs. The framework introduces a three-stage hierarchical prompt structure
comprising task definition, knowledge background, and reasoning guidance,
supplemented by legal-specific reasoning templates and dynamic optimization
mechanisms. A three-layer knowledge graph architecture is constructed with
legal classification ontology, representation, and instance layers. Four
complementary methods enable precise legal concept retrieval: direct legal norm
code matching, domain-specific semantic vector similarity, ontology-based path
reasoning, and specialized lexical segmentation. These components integrate
with web search technology to establish a knowledge-enhanced framework for
legal decision-making. Experimental results demonstrate significant performance
improvements in legal dispute analysis, enabling accurate legal application
analysis for complex cases while exhibiting nuanced understanding of judicial
decision-making logic, providing a novel technical approach for implementing
intelligent legal assistance systems.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [27] [Planted in Pretraining, Swayed by Finetuning: A Case Study on the Origins of Cognitive Biases in LLMs](https://arxiv.org/abs/2507.07186)
*Itay Itzhak,Yonatan Belinkov,Gabriel Stanovsky*

Main category: cs.CL

TL;DR: 本文通过因果实验方法发现LLMs中的认知偏见主要源自预训练阶段而非微调或训练随机性，为后续偏见评估与缓解策略提供了新视角。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在决策过程中表现出类似于人类的认知偏见。已有研究表明，这些偏见因模型和微调方式不同而有所区别，但偏见产生的根本原因（预训练、微调还是训练中的随机性）尚不清楚。本文旨在剖析LLMs偏见产生的具体来源。

Method: 作者提出两步因果实验方法：第一步，多次用不同的随机种子对模型进行微调，分析训练随机性对30余种认知偏见的影响；第二步，提出“交叉微调”方法，将不同偏见模式的数据集互换，测试偏见是否依赖于特定的数据集。

Result: 研究发现，训练随机性虽会带来一定偏差变化，但偏见主要由预训练阶段决定。拥有相同预训练基础的模型，其偏见表现更为相似，而单独共享微调数据的模型差异更大。

Conclusion: LLMs中的认知偏见并非单纯由微调或训练偶然性引发，本质上源自预训练阶段。因此评估和缓解模型偏见时，应重视预训练过程及其带来的影响。

Abstract: Large language models (LLMs) exhibit cognitive biases -- systematic
tendencies of irrational decision-making, similar to those seen in humans.
Prior work has found that these biases vary across models and can be amplified
by instruction tuning. However, it remains unclear if these differences in
biases stem from pretraining, finetuning, or even random noise due to training
stochasticity. We propose a two-step causal experimental approach to
disentangle these factors. First, we finetune models multiple times using
different random seeds to study how training randomness affects over $30$
cognitive biases. Second, we introduce \emph{cross-tuning} -- swapping
instruction datasets between models to isolate bias sources. This swap uses
datasets that led to different bias patterns, directly testing whether biases
are dataset-dependent. Our findings reveal that while training randomness
introduces some variability, biases are mainly shaped by pretraining: models
with the same pretrained backbone exhibit more similar bias patterns than those
sharing only finetuning data. These insights suggest that understanding biases
in finetuned models requires considering their pretraining origins beyond
finetuning effects. This perspective can guide future efforts to develop
principled strategies for evaluating and mitigating bias in LLMs.

</details>


### [28] [Prompt Perturbations Reveal Human-Like Biases in LLM Survey Responses](https://arxiv.org/abs/2507.07188)
*Jens Rupprecht,Georg Ahnert,Markus Strohmaier*

Main category: cs.CL

TL;DR: LLMs在社会科学调查中会出现类似人类的回答偏见（如新近性偏见），对问题表述扰动敏感，模型越大越稳健，但所有模型依然无法完全避免这些问题，因此在设计和使用相关应用时应充分关注提示词及稳健性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）越来越多被用作社会科学调查中人类受试者的代理，但其可靠性及对已知回答偏见的敏感性尚未被充分理解。本文旨在探索LLMs在规范性调查情境下的答复稳健性。

Method: 本研究选取了九种不同的大语言模型，使用世界价值观调查（WVS）中的问题，对问题表述和答案选项结构进行11种全面的扰动，模拟了超过167,000次访谈。分析了模型对表述变化的响应等。

Result: 所有测试模型对问题表述的变化表现出一定脆弱性，且都一致出现了“新近性偏见”，即更倾向于选择最后给出的选项。大型模型虽更稳健，但仍对语义变化如释义以及组合性扰动敏感。LLMs在一定程度上呈现出与人类受访者相似的回答偏见。

Conclusion: 在利用LLMs生成合成调查数据时，提示词设计与模型稳健性测试至关重要，因为LLMs存在已知的人类回答偏见及对问题表达的敏感性。

Abstract: Large Language Models (LLMs) are increasingly used as proxies for human
subjects in social science surveys, but their reliability and susceptibility to
known response biases are poorly understood. This paper investigates the
response robustness of LLMs in normative survey contexts -- we test nine
diverse LLMs on questions from the World Values Survey (WVS), applying a
comprehensive set of 11 perturbations to both question phrasing and answer
option structure, resulting in over 167,000 simulated interviews. In doing so,
we not only reveal LLMs' vulnerabilities to perturbations but also reveal that
all tested models exhibit a consistent \textit{recency bias} varying in
intensity, disproportionately favoring the last-presented answer option. While
larger models are generally more robust, all models remain sensitive to
semantic variations like paraphrasing and to combined perturbations. By
applying a set of perturbations, we reveal that LLMs partially align with
survey response biases identified in humans. This underscores the critical
importance of prompt design and robustness testing when using LLMs to generate
synthetic survey data.

</details>


### [29] [SynthTextEval: Synthetic Text Data Generation and Evaluation for High-Stakes Domains](https://arxiv.org/abs/2507.07229)
*Krithika Ramesh,Daniel Smolyak,Zihao Zhao,Nupoor Gandhi,Ritu Agarwal,Margrét Bjarnadóttir,Anjalie Field*

Main category: cs.CL

TL;DR: 本文提出并实现了SynthTextEval，一个可从多维度评价合成文本的工具包，支持实际应用、隐私防护等多项指标，助力推动合成文本在高风险领域的可用性和隐私安全。


<details>
  <summary>Details</summary>
Motivation: 由于大语言模型（LLM）生成的文本在语流性方面表现良好，合成文本在诸多领域（如隐私敏感的高风险场景）有越来越多的应用潜力。然而，目前缺乏一套系统、全面且一致的评价方法来覆盖这些合成数据在实际应用、系统公平性、隐私泄露风险、分布差异等多维度的指标。本文旨在解决这一评价标准不统一的问题。

Method: 本文提出了SynthTextEval工具包，使用户能够在上传或工具自带生成的合成文本上，从实用性、公平性、隐私风险、分布差异及专家主观意见等多维度，进行标准化、综合性的评价。该工具不仅兼容多种数据，还突出展示了其在医疗和法律等高风险领域数据集上的能力和效果。

Result: SynthTextEval能够让用户方便地对合成文本进行多维度评价，并通过整合和标准化评价指标，提高了合成文本在高风险场景（如医疗、法律领域）中实际应用的可行性及其对AI开发过程中的隐私保护能力。

Conclusion: SynthTextEval作为一个多维度、标准化的合成文本评价工具包，为合成数据的真实效用及隐私保护贡献了重要工具，对于推动AI系统开发中的隐私保护具有积极意义。

Abstract: We present SynthTextEval, a toolkit for conducting comprehensive evaluations
of synthetic text. The fluency of large language model (LLM) outputs has made
synthetic text potentially viable for numerous applications, such as reducing
the risks of privacy violations in the development and deployment of AI systems
in high-stakes domains. Realizing this potential, however, requires principled
consistent evaluations of synthetic data across multiple dimensions: its
utility in downstream systems, the fairness of these systems, the risk of
privacy leakage, general distributional differences from the source text, and
qualitative feedback from domain experts. SynthTextEval allows users to conduct
evaluations along all of these dimensions over synthetic data that they upload
or generate using the toolkit's generation module. While our toolkit can be run
over any data, we highlight its functionality and effectiveness over datasets
from two high-stakes domains: healthcare and law. By consolidating and
standardizing evaluation metrics, we aim to improve the viability of synthetic
text, and in-turn, privacy-preservation in AI development.

</details>


### [30] [Medical Red Teaming Protocol of Language Models: On the Importance of User Perspectives in Healthcare Settings](https://arxiv.org/abs/2507.07248)
*Minseon Kim,Jean-Philippe Corbeil,Alessandro Sordoni,Francois Beaulieu,Paul Vozila*

Main category: cs.CL

TL;DR: 本文针对医疗大语言模型，首次从患者、临床医师和通用用户三重视角，提出了专门的安全评估协议和数据集，有效填补了通用基准下医疗安全评测的不足，为该领域模型的安全应用提供了基础和工具。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）性能不断提升，其在医疗领域的应用逐渐增多。但由于相关用户（如患者和临床医师）的多元性以及模型输出对健康的直接影响，医疗领域的安全问题日益突出。目前，大部分安全性评估主要集中于通用安全基准，缺乏面向医疗场景且兼顾角色差异的安全评估方法。因此，迫切需要为医疗LLMs制定专门的安全评估方案。

Method: 本文提出了一个针对医疗场景的安全性评估协议，分别从患者和临床医师的角度出发，同时结合了通用安全评估。作者构建了包含五大关键类别、共466个样本的PatientSafetyBench数据集，用于评估模型在患者视角下的安全性，并通过红队测试协议对MediPhi系列模型进行了案例研究。

Result: 作者系统性地量化分析了医疗LLMs的安全性，填补了领域内缺乏针对患者、临床医师及通用用户三类视角的医疗LLM安全性细致评测的空白。

Conclusion: 该研究首次从患者、临床医师和通用用户三个视角出发，定义了面向医疗大语言模型的安全评估标准，并建立了相应的评测流程和数据集，为医疗领域安全部署LLMs奠定了基础。

Abstract: As the performance of large language models (LLMs) continues to advance,
their adoption is expanding across a wide range of domains, including the
medical field. The integration of LLMs into medical applications raises
critical safety concerns, particularly due to their use by users with diverse
roles, e.g. patients and clinicians, and the potential for model's outputs to
directly affect human health. Despite the domain-specific capabilities of
medical LLMs, prior safety evaluations have largely focused only on general
safety benchmarks. In this paper, we introduce a safety evaluation protocol
tailored to the medical domain in both patient user and clinician user
perspectives, alongside general safety assessments and quantitatively analyze
the safety of medical LLMs. We bridge a gap in the literature by building the
PatientSafetyBench containing 466 samples over 5 critical categories to measure
safety from the perspective of the patient. We apply our red-teaming protocols
on the MediPhi model collection as a case study. To our knowledge, this is the
first work to define safety evaluation criteria for medical LLMs through
targeted red-teaming taking three different points of view - patient,
clinician, and general user - establishing a foundation for safer deployment in
medical domains.

</details>


### [31] [The Impact of Background Speech on Interruption Detection in Collaborative Groups](https://arxiv.org/abs/2507.07280)
*Mariah Bradford,Nikhil Krishnaswamy,Nathaniel Blanchard*

Main category: cs.CL

TL;DR: 本研究针对协作学习场景下多组并发、语音重叠时的中断检测，提出了鲁棒的AI识别方法，并揭示了中断的关键语言与韵律特征，适用于真实教室环境。


<details>
  <summary>Details</summary>
Motivation: 中断在协作学习中对小组互动与知识建构有重要影响，AI可辅佐教师监控互动，但现有关于中断检测研究多聚焦于单一对话且音频环境较干净，无法适应小组协作中多组并发、语音重叠的实际场景。

Method: 分析单一对话与多小组对话场景下的中断检测问题，并提出一种对语音重叠鲁棒的中断识别新方法。同时，挖掘和分析中断中具有意义的语言和韵律信息。

Result: 提出的中断识别方法在语音重叠环境下表现出色，适合在真实教室多小组环境中部署，研究发现了中断在协作组互动中的语言与韵律特征。

Conclusion: 论文提出的鲁棒性中断检测方法能适用于多组对话场景，为未来跟踪小组对话时应对多组语音重叠提供了新方向。

Abstract: Interruption plays a crucial role in collaborative learning, shaping group
interactions and influencing knowledge construction. AI-driven support can
assist teachers in monitoring these interactions. However, most previous work
on interruption detection and interpretation has been conducted in
single-conversation environments with relatively clean audio. AI agents
deployed in classrooms for collaborative learning within small groups will need
to contend with multiple concurrent conversations -- in this context,
overlapping speech will be ubiquitous, and interruptions will need to be
identified in other ways. In this work, we analyze interruption detection in
single-conversation and multi-group dialogue settings. We then create a
state-of-the-art method for interruption identification that is robust to
overlapping speech, and thus could be deployed in classrooms. Further, our work
highlights meaningful linguistic and prosodic information about how
interruptions manifest in collaborative group interactions. Our investigation
also paves the way for future works to account for the influence of overlapping
speech from multiple groups when tracking group dialog.

</details>


### [32] [Multi-Agent Retrieval-Augmented Framework for Evidence-Based Counterspeech Against Health Misinformation](https://arxiv.org/abs/2507.07307)
*Anirban Saha Anik,Xiaoying Song,Elliott Wang,Bryan Wang,Bengisu Yarimbas,Lingzi Hong*

Main category: cs.CL

TL;DR: 本文提出多智能体检索增强框架，协同多LLM提升健康谣言反驳生成的质量，实验与人工评测均表明系统在多个指标上优于基线，有效增强反对言论的力度与说服力。


<details>
  <summary>Details</summary>
Motivation: 现有利用RAG的大语言模型自动生成反驳谣言的能力虽强，但存在证据单一、输出可控性弱等问题，无法很好地支持健康领域中反谣言需求，因此需要提升证据丰富性与输出质量。

Method: 采用多智能体（Multi-agent）结合检索增强生成（RAG）的方法，集成了多个大语言模型（LLM），分别负责知识检索、证据增强与回复精炼。系统整合静态和动态证据，进行消融实验和人工评测以验证框架有效性。

Result: 提出的方法在礼貌性、相关性、信息量和事实准确性等方面均超越现有基线方法。消融实验和人工评测表明，模型各组成部分及精炼流程能显著提升最终效果。

Conclusion: 本论文提出的多智能体检索增强框架能有效提升针对健康错误信息的反对言论质量，包括礼貌性、相关性、信息量及事实准确性。框架中各组成部分均被实验证明是必要的，且通过精炼步骤可显著提升人类偏好。

Abstract: Large language models (LLMs) incorporated with Retrieval-Augmented Generation
(RAG) have demonstrated powerful capabilities in generating counterspeech
against misinformation. However, current studies rely on limited evidence and
offer less control over final outputs. To address these challenges, we propose
a Multi-agent Retrieval-Augmented Framework to generate counterspeech against
health misinformation, incorporating multiple LLMs to optimize knowledge
retrieval, evidence enhancement, and response refinement. Our approach
integrates both static and dynamic evidence, ensuring that the generated
counterspeech is relevant, well-grounded, and up-to-date. Our method
outperforms baseline approaches in politeness, relevance, informativeness, and
factual accuracy, demonstrating its effectiveness in generating high-quality
counterspeech. To further validate our approach, we conduct ablation studies to
verify the necessity of each component in our framework. Furthermore, human
evaluations reveal that refinement significantly enhances counterspeech quality
and obtains human preference.

</details>


### [33] [GNN-CNN: An Efficient Hybrid Model of Convolutional and Graph Neural Networks for Text Representation](https://arxiv.org/abs/2507.07414)
*Fardin Rastakhiz*

Main category: cs.CL

TL;DR: 论文提出一种结合GNN和CNN的新型文本处理模型，显著提升长文本处理效率并保持竞争性效果，在多项分类任务中优于或可媲美当前主流模型。


<details>
  <summary>Details</summary>
Motivation: 深度学习处理长文本时，时间、成本和能耗是关键问题。现有Transformer模型在输入长度增加时计算复杂度呈二次增长，处理长文档效率低，亟需新方法提升效率。

Method: 提出一种融合图神经网络(GNN)和卷积神经网络(CNN)的新模型，结合端到端实时图生成机制。采用字符级输入小批量处理，无需填充或截断，并有效利用大型语言模型的嵌入与情感信息，CNN捕捉局部上下文，基于格子的图结构扩展感受野，小世界图聚合文档信息。

Result: 该模型生成的图具有良好语义结构（平均聚类系数约0.45，平均最短路径4～5），并在情感分析、新闻分类等多文本分类任务中展现出高效和有竞争力的效果。

Conclusion: 该模型在保持高效和快速的同时，能结构化地组织实际语义，并在多个文本任务中达到与主流方法相当的竞争性能。

Abstract: Time, cost, and energy efficiency are critical considerations in
Deep-Learning (DL), particularly when processing long texts. Transformers,
which represent the current state of the art, exhibit quadratic computational
complexity relative to input length, making them inefficient for extended
documents. This study introduces a novel model architecture that combines Graph
Neural Networks (GNNs) and Convolutional Neural Networks (CNNs), integrated
with a real-time, end-to-end graph generation mechanism. The model processes
compact batches of character-level inputs without requiring padding or
truncation. To enhance performance while maintaining high speed and efficiency,
the model incorporates information from Large Language Models (LLMs), such as
token embeddings and sentiment polarities, through efficient dictionary
lookups. It captures local contextual patterns using CNNs, expands local
receptive fields via lattice-based graph structures, and employs small-world
graphs to aggregate document-level information. The generated graphs exhibit
structural properties indicative of meaningful semantic organization, with an
average clustering coefficient of approximately 0.45 and an average shortest
path length ranging between 4 and 5. The model is evaluated across multiple
text classification tasks, including sentiment analysis and
news-categorization, and is compared against state-of-the-art models.
Experimental results confirm the proposed model's efficiency and competitive
performance.

</details>


### [34] [MedReadCtrl: Personalizing medical text generation with readability-controlled instruction learning](https://arxiv.org/abs/2507.07419)
*Hieu Tran,Zonghai Yao,Won Seok Jang,Sharmin Sultana,Allen Chang,Yuan Zhang,Hong Yu*

Main category: cs.CL

TL;DR: 提出了一种可控可读性的医疗AI指令微调方法MedReadCtrl，实验结果表明其生成内容更易理解且准确，有望提升医疗AI对患者等非专业人士的教育普惠能力。


<details>
  <summary>Details</summary>
Motivation: 生成式人工智能在医疗领域应用广泛，但人机沟通的可理解性和个性化始终是一大挑战。如何让AI生成的内容既能满足不同受众的理解需求、又不失医疗信息的准确性，有待解决。

Method: 提出MedReadCtrl，一种可调节输出可读性的指令微调框架，使大模型在保持原意的前提下灵活调整回答复杂度。对医疗及通用多个数据集和任务进行了评估。

Result: MedReadCtrl在九个数据集、三项医疗与通用任务上，相比GPT-4展现了更低的可读性错误（如ReadMe数据集：1.39 vs. 1.59, p<0.001），在未见过的临床任务上也有较大提升（如ROUGE-L提升14.7分，SARI提升6.18分）。专家偏好测试中MedReadCtrl被选择的比例（71.7%）明显高于对比模型（23.3%），对低健康素养用户尤其有效。

Conclusion: MedReadCtrl能够将专业的医疗内容重组为更易理解、可读性更高的语言，同时不损失医学原意，为健康教育及普惠AI医疗服务提供了可扩展的解决方案。

Abstract: Generative AI has demonstrated strong potential in healthcare, from clinical
decision support to patient-facing chatbots that improve outcomes. A critical
challenge for deployment is effective human-AI communication, where content
must be both personalized and understandable. We introduce MedReadCtrl, a
readability-controlled instruction tuning framework that enables LLMs to adjust
output complexity without compromising meaning. Evaluations of nine datasets
and three tasks across medical and general domains show that MedReadCtrl
achieves significantly lower readability instruction-following errors than
GPT-4 (e.g., 1.39 vs. 1.59 on ReadMe, p<0.001) and delivers substantial gains
on unseen clinical tasks (e.g., +14.7 ROUGE-L, +6.18 SARI on MTSamples).
Experts consistently preferred MedReadCtrl (71.7% vs. 23.3%), especially at low
literacy levels. These gains reflect MedReadCtrl's ability to restructure
clinical content into accessible, readability-aligned language while preserving
medical intent, offering a scalable solution to support patient education and
expand equitable access to AI-enabled care.

</details>


### [35] [SynthEHR-Eviction: Enhancing Eviction SDoH Detection with LLM-Augmented Synthetic EHR Data](https://arxiv.org/abs/2507.07421)
*Zonghai Yao,Youxia Zhao,Avijit Mitra,David A. Levy,Emily Druhl,Jack Tsai,Hong Yu*

Main category: cs.CL

TL;DR: 论文提出了自动化从EHR中提取驱逐信息的流程，极大提升了抽取效率与准确率，显著推动SDoH数据的研究和利用。


<details>
  <summary>Details</summary>
Motivation: 驱逐行为（Eviction）作为一个重要但研究不足的健康社会决定因素（SDoH），会影响住房稳定、失业率和心理健康。但在电子健康记录（EHR）中，驱逐通常只有非结构化文本记录，少有结构化编码，这限制了后续应用和研究。

Method: 作者提出了SynthEHR-Eviction，这是一套可扩展的流程，结合了大语言模型（LLM）、人工参与标注和自动化提示优化（APO），用来从临床记录中提取驱逐状态。

Result: 利用上述流程，作者创建了目前公开最大的驱逐相关SDoH数据集（含14个细分类别）。基于该数据集微调的大模型（如Qwen2.5、LLaMA3），在人工校验数据上获得了高Macro-F1分数（驱逐88.8%，其他SDoH 90.3%），优于GPT-4o-APO（87.8%、87.3%）、GPT-4o-mini-APO（69.1%、78.1%）和BioBERT（60.7%、68.3%）。

Conclusion: 该流程不仅大幅减少标注工作量（超过80%），还能加快数据集构建、实现可扩展的驱逐检测，并可推广到其他信息抽取任务。

Abstract: Eviction is a significant yet understudied social determinants of health
(SDoH), linked to housing instability, unemployment, and mental health. While
eviction appears in unstructured electronic health records (EHRs), it is rarely
coded in structured fields, limiting downstream applications. We introduce
SynthEHR-Eviction, a scalable pipeline combining LLMs, human-in-the-loop
annotation, and automated prompt optimization (APO) to extract eviction
statuses from clinical notes. Using this pipeline, we created the largest
public eviction-related SDoH dataset to date, comprising 14 fine-grained
categories. Fine-tuned LLMs (e.g., Qwen2.5, LLaMA3) trained on
SynthEHR-Eviction achieved Macro-F1 scores of 88.8% (eviction) and 90.3% (other
SDoH) on human validated data, outperforming GPT-4o-APO (87.8%, 87.3%),
GPT-4o-mini-APO (69.1%, 78.1%), and BioBERT (60.7%, 68.3%), while enabling
cost-effective deployment across various model sizes. The pipeline reduces
annotation effort by over 80%, accelerates dataset creation, enables scalable
eviction detection, and generalizes to other information extraction tasks.

</details>


### [36] [Towards Interpretable Time Series Foundation Models](https://arxiv.org/abs/2507.07439)
*Matthieu Boileau,Philippe Helluy,Jeremy Pawlus,Svitlana Vyetrenko*

Main category: cs.CL

TL;DR: 本文通过在合成数据上微调小型语言模型，实现了用自然语言对时间序列模式进行可解释推理，为开发轻量化、易部署、支持隐私保护的时序AI模型铺平了道路。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型虽具备理解与处理时间序列的潜力，但相关能力主要集中在大型、资源消耗高的模型上，需求定位于将这种时序推理能力压缩进小型、指令微调的语言模型中，提高模型的可解释性与易用性。

Method: 利用合成的均值回复时间序列数据集，系统性调整趋势与噪声水平，通过大型多模态模型生成自然语言注释，用这些注释指导小型Qwen模型的微调。同时，设计针对时序推理能力（如趋势方向、噪声强度和极值定位）的评测指标，系统性评估模型的理解深度。

Result: 微调后的小型Qwen模型获得了有意义的时序推理与解释能力，在趋势、噪声、极值等维度上表现良好，验证了此类能力可压缩进轻量型、支持自然语言的模型中。

Conclusion: 时序推理能力是可以通过蒸馏方法迁移到小型语言模型中的，用自然语言解释时间序列成为可能。这为开发可部署、保护隐私、易解释的时序基础模型提供了实际基础。

Abstract: In this paper, we investigate the distillation of time series reasoning
capabilities into small, instruction-tuned language models as a step toward
building interpretable time series foundation models. Leveraging a synthetic
dataset of mean-reverting time series with systematically varied trends and
noise levels, we generate natural language annotations using a large multimodal
model and use these to supervise the fine-tuning of compact Qwen models. We
introduce evaluation metrics that assess the quality of the distilled reasoning
- focusing on trend direction, noise intensity, and extremum localization - and
show that the post-trained models acquire meaningful interpretive capabilities.
Our results highlight the feasibility of compressing time series understanding
into lightweight, language-capable models suitable for on-device or
privacy-sensitive deployment. This work contributes a concrete foundation
toward developing small, interpretable models that explain temporal patterns in
natural language.

</details>


### [37] [SAND: Boosting LLM Agents with Self-Taught Action Deliberation](https://arxiv.org/abs/2507.07441)
*Yu Xia,Yiran Jenny Shen,Junda Wu,Tong Yu,Sungchul Kim,Ryan A. Rossi,Lina Yao,Julian McAuley*

Main category: cs.CL

TL;DR: 本文提出了一种让LLM Agent在做决策前进行自我深思并权衡多种选择的新方法（SAND），能够显著提升任务表现，优于现有微调技术。


<details>
  <summary>Details</summary>
Motivation: 当前LLM Agent常常依赖于模仿专家行为或通过偏好优化进行微调，但这些方法存在动作空间探索不足的问题，可能导致模型过度承诺于貌似合理但实际上次优的动作。

Method: 提出了SAND（Self-taught ActioN Deliberation）框架，使LLM Agent在做决定前能对备选动作进行显式的权衡和深思，融合了自洽动作采样和执行引导的动作批判来生成逐步的深思轨迹，并迭代地用这些轨迹进一步微调模型。

Result: 在两个具有代表性的交互型Agent任务中，SAND相较初始的监督微调平均提升20%，并优于最新的Agent微调方法。

Conclusion: SAND能有效提升LLM Agent在复杂任务中的决策能力，通过引入自我深思策略，弥补了现有方法探索不足的短板。

Abstract: Large Language Model (LLM) agents are commonly tuned with supervised
finetuning on ReAct-style expert trajectories or preference optimization over
pairwise rollouts. Most of these methods focus on imitating specific expert
behaviors or promoting chosen reasoning thoughts and actions over rejected
ones. However, without reasoning and comparing over alternatives actions, LLM
agents finetuned with these methods may over-commit towards seemingly plausible
but suboptimal actions due to limited action space exploration. To address
this, in this paper we propose Self-taught ActioN Deliberation (SAND)
framework, enabling LLM agents to explicitly deliberate over candidate actions
before committing to one. To tackle the challenges of when and what to
deliberate given large action space and step-level action evaluation, we
incorporate self-consistency action sampling and execution-guided action
critique to help synthesize step-wise action deliberation thoughts using the
base model of the LLM agent. In an iterative manner, the deliberation
trajectories are then used to finetune the LLM agent itself. Evaluating on two
representative interactive agent tasks, SAND achieves an average 20%
improvement over initial supervised finetuning and also outperforms
state-of-the-art agent tuning approaches.

</details>


### [38] [RLEP: Reinforcement Learning with Experience Replay for LLM Reasoning](https://arxiv.org/abs/2507.07451)
*Hongzhi Zhang,Jia Fu,Jingyuan Zhang,Kai Fu,Qi Wang,Fuzheng Zhang,Guorui Zhou*

Main category: cs.CL

TL;DR: RLEP是一种针对大语言模型强化学习的高效训练框架，通过重用高质量经验轨迹实现了更快训练和更好效果，实验验证在多个数学难题基准集上大幅提升准确率。


<details>
  <summary>Details</summary>
Motivation: 近年来，大型语言模型在强化学习（RL）领域取得了重要进展，但其训练过程耗能高且易于不稳定，模型策略易逐渐偏离预训练权重，影响最终表现。该问题亟需解决，以提升训练效率和模型性能。

Method: 本文提出了一种新的两阶段强化学习框架RLEP（Reinforcement Learning with Experience rePlay），首先收集经过验证的高质量轨迹，然后在训练过程中将这些成功轨迹与新生成的轨迹混合，采用小批量训练方式进行优化。通过不断重放高质量示例，引导模型朝向有效探索并加速收敛。

Result: 在Qwen2.5-Math-7B基础模型上，RLEP框架显著减少了训练更新次数，取得了更高的最终表现。其中，AIME-2024数据集的准确率从38.2%提升到39.9%，AIME-2025从19.8%提升到22.3%，AMC-2023从77.0%提升到82.2%。

Conclusion: RLEP通过经验重放机制有效提升了大型语言模型的强化学习效果，实现更快收敛和更优性能。该方法有助于减少算力消耗和训练不稳定问题，对大模型的RL训练具有积极推动作用。

Abstract: Reinforcement learning (RL) for large language models is an energy-intensive
endeavor: training can be unstable, and the policy may gradually drift away
from its pretrained weights. We present \emph{RLEP}\, -- \,Reinforcement
Learning with Experience rePlay\, -- \,a two-phase framework that first
collects verified trajectories and then replays them during subsequent
training. At every update step, the policy is optimized on mini-batches that
blend newly generated rollouts with these replayed successes. By replaying
high-quality examples, RLEP steers the model away from fruitless exploration,
focuses learning on promising reasoning paths, and delivers both faster
convergence and stronger final performance. On the Qwen2.5-Math-7B base model,
RLEP reaches baseline peak accuracy with substantially fewer updates and
ultimately surpasses it, improving accuracy on AIME-2024 from 38.2% to 39.9%,
on AIME-2025 from 19.8% to 22.3%, and on AMC-2023 from 77.0% to 82.2%. Our
code, datasets, and checkpoints are publicly available at
https://github.com/Kwai-Klear/RLEP to facilitate reproducibility and further
research.

</details>


### [39] [Machine Bullshit: Characterizing the Emergent Disregard for Truth in Large Language Models](https://arxiv.org/abs/2507.07484)
*Kaiqu Liang,Haimin Hu,Xuandong Zhao,Dawn Song,Thomas L. Griffiths,Jaime Fernández Fisac*

Main category: cs.CL

TL;DR: 本文提出并定义了机器bullshit这一新概念，通过Bullshit Index及多类别体系定量定性评估LLM在真实性方面的缺陷，并发现RLHF微调与CoT提示均会加剧相关问题，强调了AI对齐中的系统性挑战。


<details>
  <summary>Details</summary>
Motivation: 尽管已有研究关注于大语言模型（LLM）的幻觉（hallucination）和阿谀（sycophancy）现象，但对于LLM在本质上对真实性的漠视缺乏系统框架。本文提出“machine bullshit”作为更广泛的概念，以研究模型在真相表达方面的系统性失真。

Method: 提出并定义了“Bullshit Index”这一新颖指标用于量化大语言模型对真相的忽视，设计了四类bullshit分类体系（废话言辞、花言巧语、模糊措辞、无验证声明）。利用已有Marketplace、Political Neutrality数据集及自建BullshitEval基准（包含2400个场景，覆盖100个AI助手），对多种模型的bullshit现象进行实证评估。

Result: 实验证明，采用基于人工反馈的强化学习（RLHF）进行模型微调，显著加重了模型产生bullshit的现象。而推理时采用Chain-of-Thought（CoT）提示，也显著提升了特定类型的bullshit（如废话言辞和花言巧语）。在政治语境中，机器bullshit尤为普遍，以模糊措辞为主要表现。

Conclusion: 机器bullshit作为新颖的分析视角，有助于系统性理解与量化LLM的真实性失真，为AI对齐提供理论和实证基础，并揭示实现更真实语言输出的关键挑战。

Abstract: Bullshit, as conceptualized by philosopher Harry Frankfurt, refers to
statements made without regard to their truth value. While previous work has
explored large language model (LLM) hallucination and sycophancy, we propose
machine bullshit as an overarching conceptual framework that can allow
researchers to characterize the broader phenomenon of emergent loss of
truthfulness in LLMs and shed light on its underlying mechanisms. We introduce
the Bullshit Index, a novel metric quantifying LLMs' indifference to truth, and
propose a complementary taxonomy analyzing four qualitative forms of bullshit:
empty rhetoric, paltering, weasel words, and unverified claims. We conduct
empirical evaluations on the Marketplace dataset, the Political Neutrality
dataset, and our new BullshitEval benchmark (2,400 scenarios spanning 100 AI
assistants) explicitly designed to evaluate machine bullshit. Our results
demonstrate that model fine-tuning with reinforcement learning from human
feedback (RLHF) significantly exacerbates bullshit and inference-time
chain-of-thought (CoT) prompting notably amplify specific bullshit forms,
particularly empty rhetoric and paltering. We also observe prevalent machine
bullshit in political contexts, with weasel words as the dominant strategy. Our
findings highlight systematic challenges in AI alignment and provide new
insights toward more truthful LLM behavior.

</details>


### [40] [PLAN-TUNING: Post-Training Language Models to Learn Step-by-Step Planning for Complex Problem Solving](https://arxiv.org/abs/2507.07495)
*Mihir Parmar,Palash Goyal,Xin Liu,Yiwen Song,Mingyang Ling,Chitta Baral,Hamid Palangi,Tomas Pfister*

Main category: cs.CL

TL;DR: 提出PLAN-TUNING框架，通过“任务分解”知识蒸馏和微调，使小型开源LLM在复杂推理任务上的性能和泛化力显著提升，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前对大型模型利用“规划分解”解决复杂任务的能力已取得显著提升，但这一思路在小型开源LLM性能提升方面尚未被充分探索。

Method: 该方法包括两个主要步骤：（1）从大型LLM中蒸馏出合成任务分解（即“规划轨迹”）；（2）采用监督学习和强化学习目标对小模型进行微调以模仿这些规划过程。

Result: 在GSM8k和MATH基准集上，相比强基线，plan-tuned模型性能平均提升约7%；在OlympiadBench和AIME 2024等域外数据集上，泛化能力提升平均分别约10%和12%。

Conclusion: PLAN-TUNING通过人工任务分解蒸馏并微调开源小模型，有效提升了它们在复杂推理任务上的表现和泛化能力。

Abstract: Recently, decomposing complex problems into simple subtasks--a crucial part
of human-like natural planning--to solve the given problem has significantly
boosted the performance of large language models (LLMs). However, leveraging
such planning structures during post-training to boost the performance of
smaller open-source LLMs remains underexplored. Motivated by this, we introduce
PLAN-TUNING, a unified post-training framework that (i) distills synthetic task
decompositions (termed "planning trajectories") from large-scale LLMs and (ii)
fine-tunes smaller models via supervised and reinforcement-learning objectives
designed to mimic these planning processes to improve complex reasoning. On
GSM8k and the MATH benchmarks, plan-tuned models outperform strong baselines by
an average $\sim7\%$. Furthermore, plan-tuned models show better generalization
capabilities on out-of-domain datasets, with average $\sim10\%$ and $\sim12\%$
performance improvements on OlympiadBench and AIME 2024, respectively. Our
detailed analysis demonstrates how planning trajectories improves complex
reasoning capabilities, showing that PLAN-TUNING is an effective strategy for
improving task-specific performance of smaller LLMs.

</details>


### [41] [Teaching LLM to Reason: Reinforcement Learning from Algorithmic Problems without Code](https://arxiv.org/abs/2507.07498)
*Keqin Bao,Nuo Chen,Xiaoyuan Li,Binyuan Hui,Bowen Yu,Fuli Feng,Junyang Lin,Xiangnan He,Dayiheng Liu*

Main category: cs.CL

TL;DR: TeaR通过数据筛选和强化学习，帮助大语言模型建立更优推理路径，提升了模型在广泛推理任务上的整体表现。


<details>
  <summary>Details</summary>
Motivation: 提升大语言模型的推理能力，目前在通过模拟代码执行帮助推理时，经常出现过度依赖复杂的数据结构和算法，导致模型在简单任务上也表现为对算法模式的过拟合，而非核心推理能力的提升。

Method: 提出TeaR方法，结合精心的数据筛选与强化学习，引导模型通过代码相关任务发现最优推理路径，从而加强通用推理能力。

Result: 在1.5B到32B参数规模的两种基础模型及三种长链式思维(CoT)蒸馏模型上，涵盖数学、知识、代码、逻辑推理等17项基准测试。结果显示，TeaR显著提升模型性能。例如，Qwen2.5-7B提升35.9%，R1-Distilled-7B提升5.9%。

Conclusion: TeaR方法通过数据筛选与强化学习，有效改进了大语言模型的推理能力，并在多项基准上取得了显著性能提升。

Abstract: Enhancing reasoning capabilities remains a central focus in the LLM reasearch
community. A promising direction involves requiring models to simulate code
execution step-by-step to derive outputs for given inputs. However, as code is
often designed for large-scale systems, direct application leads to
over-reliance on complex data structures and algorithms, even for simple cases,
resulting in overfitting to algorithmic patterns rather than core reasoning
structures. To address this, we propose TeaR, which aims at teaching LLMs to
reason better. TeaR leverages careful data curation and reinforcement learning
to guide models in discovering optimal reasoning paths through code-related
tasks, thereby improving general reasoning abilities. We conduct extensive
experiments using two base models and three long-CoT distillation models, with
model sizes ranging from 1.5 billion to 32 billion parameters, and across 17
benchmarks spanning Math, Knowledge, Code, and Logical Reasoning. The results
consistently show significant performance improvements. Notably, TeaR achieves
a 35.9% improvement on Qwen2.5-7B and 5.9% on R1-Distilled-7B.

</details>


### [42] [Extracting ORR Catalyst Information for Fuel Cell from Scientific Literature](https://arxiv.org/abs/2507.07499)
*Hein Htet,Amgad Ahmed Ali Ibrahim,Yutaka Sasaki,Ryoji Asahi*

Main category: cs.CL

TL;DR: 本文提出用DyGIE++和多种BERT变体（如MatSciBERT和PubMedBERT）抽取燃料电池ORR催化剂相关信息，建立手工数据集并对模型微调，获得高准确度结果，验证了领域专用模型在自动文献分析中的优势。


<details>
  <summary>Details</summary>
Motivation: 由于ORR催化剂相关信息分散且文本多样，如何自动高效地从海量文献中提取结构化信息成为材料信息学研究中的重要难题。

Method: 本文基于DyGIE++结构，结合多种预训练BERT变体（如MatSciBERT和PubMedBERT），进行命名实体识别（NER）和关系抽取（RE）。通过手动标注数据集，确定12类实体和2种关系，并对数据进行标注、整合及BERT模型微调，最后对不同模型和标注一致性进行了评测。

Result: 实验表明，微调后的PubMedBERT在NER任务上F1值达到82.19%，MatSciBERT在RE任务上F1值为66.10%。领域专用BERT优于BlueBERT等通用科学模型，模型自动化性能可靠，媲美甚至优于人工标注。

Conclusion: 专用的领域BERT模型（如MatSciBERT和PubMedBERT）在从科学文献中抽取ORR催化剂相关信息方面表现优于通用型科学模型。细调后的模型不仅在精度上优于人工标注，而且具备良好的可扩展性和自动化分析潜力。

Abstract: The oxygen reduction reaction (ORR) catalyst plays a critical role in
enhancing fuel cell efficiency, making it a key focus in material science
research. However, extracting structured information about ORR catalysts from
vast scientific literature remains a significant challenge due to the
complexity and diversity of textual data. In this study, we propose a named
entity recognition (NER) and relation extraction (RE) approach using DyGIE++
with multiple pre-trained BERT variants, including MatSciBERT and PubMedBERT,
to extract ORR catalyst-related information from the scientific literature,
which is compiled into a fuel cell corpus for materials informatics
(FC-CoMIcs). A comprehensive dataset was constructed manually by identifying 12
critical entities and two relationship types between pairs of the entities. Our
methodology involves data annotation, integration, and fine-tuning of
transformer-based models to enhance information extraction accuracy. We assess
the impact of different BERT variants on extraction performance and investigate
the effects of annotation consistency. Experimental evaluations demonstrate
that the fine-tuned PubMedBERT model achieves the highest NER F1-score of
82.19% and the MatSciBERT model attains the best RE F1-score of 66.10%.
Furthermore, the comparison with human annotators highlights the reliability of
fine-tuned models for ORR catalyst extraction, demonstrating their potential
for scalable and automated literature analysis. The results indicate that
domain-specific BERT models outperform general scientific models like BlueBERT
for ORR catalyst extraction.

</details>


### [43] [Hallucination Stations: On Some Basic Limitations of Transformer-Based Language Models](https://arxiv.org/abs/2507.07505)
*Varin Sikka,Vishal Sikka*

Main category: cs.CL

TL;DR: LLM受限于计算复杂性，难以胜任或验证过于复杂的任务，这为其实际应用和发展方向提供了重要参考。


<details>
  <summary>Details</summary>
Motivation: 随着基于Transformer的语言模型广泛应用，人们对大型语言模型（LLMs）能力的极限及其“幻觉”现象（即在某些主题下生成虚假、不正确或无意义信息）产生了浓厚兴趣。同时，越来越多的LLM被用作自主或半自主代理，执行各种实际任务，因此有必要理解LLM可以或无法完成何种任务。

Method: 本文从LLM推理的计算复杂性角度，探讨大型语言模型可实现的任务能力边界。作者通过理论分析和示例，展示了LLM在面对超过一定复杂度的计算和代理任务时的能力限制。

Result: 研究发现，LLM无法执行超出特定计算复杂度的计算和代理性任务，且无法验证复杂度过高任务的准确性。文中还给出了相关实例进行说明。

Conclusion: LLM在能力上存在与计算复杂性相关的内在限制；对于复杂度超限的任务，LLM不仅无法完成，还无法有效验证。这对LLM在实际应用中的能力边界和安全性提出了重要参考。

Abstract: With widespread adoption of transformer-based language models in AI, there is
significant interest in the limits of LLMs capabilities, specifically so-called
hallucinations, occurrences in which LLMs provide spurious, factually incorrect
or nonsensical information when prompted on certain subjects. Furthermore,
there is growing interest in agentic uses of LLMs - that is, using LLMs to
create agents that act autonomously or semi-autonomously to carry out various
tasks, including tasks with applications in the real world. This makes it
important to understand the types of tasks LLMs can and cannot perform. We
explore this topic from the perspective of the computational complexity of LLM
inference. We show that LLMs are incapable of carrying out computational and
agentic tasks beyond a certain complexity, and further that LLMs are incapable
of verifying the accuracy of tasks beyond a certain complexity. We present
examples of both, then discuss some consequences of this work.

</details>


### [44] [Toward Real-World Chinese Psychological Support Dialogues: CPsDD Dataset and a Co-Evolving Multi-Agent System](https://arxiv.org/abs/2507.07509)
*Yuanchen Shi,Longyin Zhang,Fang Kong*

Main category: cs.CL

TL;DR: 本文提出利用大语言模型和专家知识生成及优化中文心理支持对话，创建了大规模数据集CPsDD，并设计了多模块自动心理支持系统（CADSS），实验表明其性能优越，推动了非英语心理健康对话研究。


<details>
  <summary>Details</summary>
Motivation: 伴随心理压力增加，对心理支持的需求日益增长，然而相关数据集稀缺，尤其是非英语语言领域。因此需要更丰富、多样化的心理对话数据。

Method: 提出一种利用有限真实数据和专家知识微调两大语言模型（Dialog Generator 和 Dialog Modifier）的框架。Generator基于预设路径生成大规模心理支持对话，Modifier对对话进行修饰以对齐真实数据。开发了CPsDD大规模中文心理支持对话数据集，同时提出含有Profiler、Summarizer、Planner和Supporter组成的综合对话支持系统（CADSS）。

Result: CPsDD数据集包含68K对话，涵盖13类群体、16种心理问题、13个诱因和12个支持重点。CADSS系统在策略预测和情感支持会话任务中，在CPsDD和ESConv数据集上取得了SOTA性能。

Conclusion: 结合大模型生成和改写能力，依赖有限数据和专家知识，生成高质量中文心理支持对话数据集，并构建高性能自动心理支持系统。提升了心理支持对话建模能力，推动非英语心理对话研究。

Abstract: The growing need for psychological support due to increasing pressures has
exposed the scarcity of relevant datasets, particularly in non-English
languages. To address this, we propose a framework that leverages limited
real-world data and expert knowledge to fine-tune two large language models:
Dialog Generator and Dialog Modifier. The Generator creates large-scale
psychological counseling dialogues based on predefined paths, which guide
system response strategies and user interactions, forming the basis for
effective support. The Modifier refines these dialogues to align with
real-world data quality. Through both automated and manual review, we construct
the Chinese Psychological support Dialogue Dataset (CPsDD), containing 68K
dialogues across 13 groups, 16 psychological problems, 13 causes, and 12
support focuses. Additionally, we introduce the Comprehensive Agent Dialogue
Support System (CADSS), where a Profiler analyzes user characteristics, a
Summarizer condenses dialogue history, a Planner selects strategies, and a
Supporter generates empathetic responses. The experimental results of the
Strategy Prediction and Emotional Support Conversation (ESC) tasks demonstrate
that CADSS achieves state-of-the-art performance on both CPsDD and ESConv
datasets.

</details>


### [45] [Triadic Multi-party Voice Activity Projection for Turn-taking in Spoken Dialogue Systems](https://arxiv.org/abs/2507.07518)
*Mikey Elmers,Koji Inoue,Divesh Lala,Tatsuya Kawahara*

Main category: cs.CL

TL;DR: 本研究首次将VAP用于三方会话，通过预测每位说话人未来发言，提升了三方会话轮流预测效果，并明确指出模型实际有望集成到对话系统中。


<details>
  <summary>Details</summary>
Motivation: 传统的轮流发言研究多集中于双人对话，而现实生活中多方会话更为常见。因此，研究如何在三方会话中预测轮流发言具有重要意义。

Method: 采用voice activity projection（VAP，声活动预测）模型，仅基于声学数据，训练并预测三方会话中每位说话人的未来发言活动。实验使用日语三人会话数据集，所涉主题广泛。

Result: 所有模型中，VAP在三方会话预测轮流发言的表现均优于基线模型。但对话内容类型会影响预测准确率。

Conclusion: VAP方法首次被有效扩展应用于三方会话中，提升了多方语音对话的人机交互性能。后续将尝试将此模型集成到实际对话系统中。

Abstract: Turn-taking is a fundamental component of spoken dialogue, however
conventional studies mostly involve dyadic settings. This work focuses on
applying voice activity projection (VAP) to predict upcoming turn-taking in
triadic multi-party scenarios. The goal of VAP models is to predict the future
voice activity for each speaker utilizing only acoustic data. This is the first
study to extend VAP into triadic conversation. We trained multiple models on a
Japanese triadic dataset where participants discussed a variety of topics. We
found that the VAP trained on triadic conversation outperformed the baseline
for all models but that the type of conversation affected the accuracy. This
study establishes that VAP can be used for turn-taking in triadic dialogue
scenarios. Future work will incorporate this triadic VAP turn-taking model into
spoken dialogue systems.

</details>


### [46] [CEA-LIST at CheckThat! 2025: Evaluating LLMs as Detectors of Bias and Opinion in Text](https://arxiv.org/abs/2507.07539)
*Akram Elbouanani,Evan Dufraisse,Aboubacar Tuo,Adrian Popescu*

Main category: cs.CL

TL;DR: 本论文利用大语言模型与少样本提示方法，在多语言主观性检测任务中取得优异成绩，并在数据不一致或缺乏标注时展现出强大优势，为传统模型提供替代方案。


<details>
  <summary>Details</summary>
Motivation: 处理多语言主观性检测任务，尤其在数据质量低、标注稀缺的情况下，探索LLM（大语言模型）结合少样本提示效果，寻找优于传统微调SLM（小型语言模型）的方法。

Method: 采用大型语言模型（LLMs）进行多语言主观性检测，设计少样本提示（few-shot prompting），并尝试了辩论式提示、不同样本选择方法等高级提示工程，参与CheckThat! 2025评测活动。

Result: 在CheckThat! 2025多语言主观性检测任务中，系统在多个语言组别取得前列成绩：阿拉伯语和波兰语第一，意大利语、英语、德语及多语言任务进入前四。方法在阿拉伯语数据集上尤为稳健，展现出对标注不一致的鲁棒性。

Conclusion: 基于LLM的少样本学习方法在多语言主观性检测上效果优异，在有噪声或标注不一致时更具优势，成为传统微调SLM外的有力替代方案，尤其适用于标注数据稀缺的场景。

Abstract: This paper presents a competitive approach to multilingual subjectivity
detection using large language models (LLMs) with few-shot prompting. We
participated in Task 1: Subjectivity of the CheckThat! 2025 evaluation
campaign. We show that LLMs, when paired with carefully designed prompts, can
match or outperform fine-tuned smaller language models (SLMs), particularly in
noisy or low-quality data settings. Despite experimenting with advanced prompt
engineering techniques, such as debating LLMs and various example selection
strategies, we found limited benefit beyond well-crafted standard few-shot
prompts. Our system achieved top rankings across multiple languages in the
CheckThat! 2025 subjectivity detection task, including first place in Arabic
and Polish, and top-four finishes in Italian, English, German, and multilingual
tracks. Notably, our method proved especially robust on the Arabic dataset,
likely due to its resilience to annotation inconsistencies. These findings
highlight the effectiveness and adaptability of LLM-based few-shot learning for
multilingual sentiment tasks, offering a strong alternative to traditional
fine-tuning, particularly when labeled data is scarce or inconsistent.

</details>


### [47] [The Cross-Lingual Cost: Retrieval Biases in RAG over Arabic-English Corpora](https://arxiv.org/abs/2507.07543)
*Chen Amiraz,Yaroslav Fyodorov,Elad Haramaty,Zohar Karnin,Liane Lewin-Eytan*

Main category: cs.CL

TL;DR: 作者分析了阿英双语领域RAG任务，发现跨语言检索是性能瓶颈，并提出简易策略显著提升了跨语言和整体表现，凸显了提升多语种检索的应用价值。


<details>
  <summary>Details</summary>
Motivation: 现有研究多聚焦生成任务，并以开放域（如维基百科）数据为基准，忽视了多语言检索面临的真实挑战，导致检索问题被掩盖。论文旨在探索领域特定且更具挑战的跨语言RAG场景，填补研究空白。

Method: 基于真实企业数据集构建阿拉伯语-英语双语检索-增强生成（RAG）基准，包括所有语言组合，系统性地分析多语种检索表现。

Result: 实验证明检索器在跨语种文档排序上效果显著下降，是性能瓶颈。通过提出等比例双语检索策略，有效缓解上述问题，提升系统在跨语言任务中的表现。

Conclusion: 跨语言检索在跨语言、领域特定场景下是RAG系统的关键瓶颈，检索策略的改进可显著提升跨语言及整体性能。

Abstract: Cross-lingual retrieval-augmented generation (RAG) is a critical capability
for retrieving and generating answers across languages. Prior work in this
context has mostly focused on generation and relied on benchmarks derived from
open-domain sources, most notably Wikipedia. In such settings, retrieval
challenges often remain hidden due to language imbalances, overlap with
pretraining data, and memorized content. To address this gap, we study
Arabic-English RAG in a domain-specific setting using benchmarks derived from
real-world corporate datasets. Our benchmarks include all combinations of
languages for the user query and the supporting document, drawn independently
and uniformly at random. This enables a systematic study of multilingual
retrieval behavior.
  Our findings reveal that retrieval is a critical bottleneck in cross-lingual
domain-specific scenarios, with significant performance drops occurring when
the user query and supporting document languages differ. A key insight is that
these failures stem primarily from the retriever's difficulty in ranking
documents across languages. Finally, we propose a simple retrieval strategy
that addresses this source of failure by enforcing equal retrieval from both
languages, resulting in substantial improvements in cross-lingual and overall
performance. These results highlight meaningful opportunities for improving
multilingual retrieval, particularly in practical, real-world RAG applications.

</details>


### [48] [The Synergy Dilemma of Long-CoT SFT and RL: Investigating Post-Training Techniques for Reasoning VLMs](https://arxiv.org/abs/2507.07562)
*Jierun Chen,Tiezheng Yu,Haoli Bai,Lewei Yao,Jiannan Wu,Kaican Li,Fei Mi,Chaofan Tao,Lei Zhu,Manyi Zhang,Xiaohui Li,Lu Hou,Lifeng Shang,Qun Liu*

Main category: cs.CL

TL;DR: 本文系统评估了长链式监督微调和强化学习在多模态推理模型中的作用及协同效果，发现两者难以实现叠加优势，反而存在性能权衡，提示需探索更自适应的整合方案。


<details>
  <summary>Details</summary>
Motivation: 当前大型视觉-语言模型（VLMs）为提升推理能力，越来越多地采用长链式思维（CoT）监督微调和强化学习（RL）等后训练技术。但这些技术在VLMs中的联合效果尚不明确，值得系统研究。

Method: 本文系统性地比较并分析了长CoT SFT和RL在多项多模态推理基准下的各自作用及协同效果，包括两步训练、交错、渐进训练、数据混合和模型融合等多种联合方式。

Result: SFT提升了复杂问题的结构化推理表现，但带来了冗长且对简单问题表现变差。RL有助于泛化和简洁回答，各类难度都有提升，但对难题提升不如SFT。两者各种组合方式未能带来预期的叠加效果，反而在准确性、推理风格和回复长度间产生权衡。

Conclusion: SFT与RL在VLMs中各有优劣，协同应用存在“协同困境”，需创新更自适应的技术整合机制才能充分利用后训练方法的潜力。

Abstract: Large vision-language models (VLMs) increasingly adopt post-training
techniques such as long chain-of-thought (CoT) supervised fine-tuning (SFT) and
reinforcement learning (RL) to elicit sophisticated reasoning. While these
methods exhibit synergy in language-only models, their joint effectiveness in
VLMs remains uncertain. We present a systematic investigation into the distinct
roles and interplay of long-CoT SFT and RL across multiple multimodal reasoning
benchmarks. We find that SFT improves performance on difficult questions by
in-depth, structured reasoning, but introduces verbosity and degrades
performance on simpler ones. In contrast, RL promotes generalization and
brevity, yielding consistent improvements across all difficulty levels, though
the improvements on the hardest questions are less prominent compared to SFT.
Surprisingly, combining them through two-staged, interleaved, or progressive
training strategies, as well as data mixing and model merging, all fails to
produce additive benefits, instead leading to trade-offs in accuracy, reasoning
style, and response length. This ``synergy dilemma'' highlights the need for
more seamless and adaptive approaches to unlock the full potential of combined
post-training techniques for reasoning VLMs.

</details>


### [49] [Single-to-mix Modality Alignment with Multimodal Large Language Model for Document Image Machine Translation](https://arxiv.org/abs/2507.07572)
*Yupu Liang,Yaping Zhang,Zhiyang Zhang,Yang Zhao,Lu Xiang,Chengqing Zong,Yu Zhou*

Main category: cs.CL

TL;DR: M4Doc是一种基于多模态大语言模型对齐的文档图像机器翻译新方法，在提升跨域泛化和翻译质量的同时，保持高效推理。


<details>
  <summary>Details</summary>
Motivation: 在文档图像机器翻译（DIMT）领域，受限于训练数据稀缺和视觉与文本信息高度耦合的复杂性，模型的泛化能力面临巨大挑战。

Method: 提出了M4Doc框架，将单模态编码器与多模态大语言模型（MLLM）的多模态表示对齐。M4Doc在大规模文档图像数据集上预训练，通过对齐，轻量级DIMT模型在训练时学习到关键的视觉-文本相关性；推理时无需MLLM参与，保障计算效率。

Result: 实验结果表明，M4Doc在翻译质量上大幅提升，特别是在跨域泛化和处理具有挑战性的文档图像方面效果显著。

Conclusion: M4Doc通过多模态对齐，有效提升了DIMT模型的泛化与翻译能力，同时兼顾了效率和性能，具备良好的应用前景。

Abstract: Document Image Machine Translation (DIMT) aims to translate text within
document images, facing generalization challenges due to limited training data
and the complex interplay between visual and textual information. To address
these challenges, we introduce M4Doc, a novel single-to-mix modality alignment
framework leveraging Multimodal Large Language Models (MLLMs). M4Doc aligns an
image-only encoder with the multimodal representations of an MLLM, pre-trained
on large-scale document image datasets. This alignment enables a lightweight
DIMT model to learn crucial visual-textual correlations during training. During
inference, M4Doc bypasses the MLLM, maintaining computational efficiency while
benefiting from its multimodal knowledge. Comprehensive experiments demonstrate
substantial improvements in translation quality, especially in cross-domain
generalization and challenging document image scenarios.

</details>


### [50] [Bayesian Discrete Diffusion Beats Autoregressive Perplexity](https://arxiv.org/abs/2507.07586)
*Cooper Doyle*

Main category: cs.CL

TL;DR: 本文揭示了离散扩散语言模型的贝叶斯本质，提出推理时的集成方法显著提升了模型性能，无需额外训练，在WikiText-2上优于同等规模GPT-2。


<details>
  <summary>Details</summary>
Motivation: 探究离散扩散语言模型中的潜在贝叶斯本质，并提升其推断能力与预测准确性。

Method: 证明了在前向mask噪声分布下，取期望的去噪输出能够还原干净token的精确后验分布；提出了基于蒙特卡洛边缘化和轻量级推理集成方法，通过多次mask-and-denoise后平均，提高了推断时token概率与不确定性估计。

Result: 在WikiText-2测试集上，K=8时方法取得了8.8的困惑度，而类似规模下GPT-2 Small的困惑度为20.3，显著优于对照模型。

Conclusion: 离散扩散语言模型可以被解释为贝叶斯推断，提出的推理集成方法无需额外训练即可提高性能，并且能输出后验概率及不确定性评估。

Abstract: We reveal a hidden Bayesian core of discrete-diffusion language models by
showing that the expected denoiser output under the forward masking
distribution recovers the exact posterior over clean tokens. Under minimal
assumptions, Monte Carlo marginalization over K independent corruptions
converges to this posterior at rate O(1/sqrt(K)), yielding a simple proof of
consistency and finite-sample error bounds. Building on this insight, we
introduce a lightweight inference-time ensemble that averages K
mask-and-denoise passes to obtain posterior-aware token probabilities and
uncertainty estimates at no extra training cost. On WikiText-2, our method
achieves test perplexity 8.8 with K=8, versus 20.3 for GPT-2 Small, despite
using a model of comparable size. Code is available at
https://github.com/mercury0100/bayesradd.

</details>


### [51] [Exploring the Limits of Model Compression in LLMs: A Knowledge Distillation Study on QA Tasks](https://arxiv.org/abs/2507.07630)
*Joyeeta Datta,Niclas Doll,Qusai Ramadan,Zeyd Boukhers*

Main category: cs.CL

TL;DR: 本研究表明，通过知识蒸馏与少量提示，可让压缩后的小模型在保持较高问答性能的同时，大幅降低计算资源需求，适用于实际应用中的受限环境。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在自然语言任务中表现优异，但在实际部署中由于计算资源消耗大受到限制。因此需探索在任务性能与模型效率之间的平衡路径，赋能实际应用场景。

Method: 采用知识蒸馏（KD）的方法，将大型语言模型（如Pythia和Qwen2.5）压缩为体积更小的学生模型；并在SQuAD和MLQA两个问答基准上，分别在零-shot和单-shot提示下测试其性能。

Result: 蒸馏后的学生模型参数规模最高可减少57.1%，同时在问答任务上保留超过90%的教师模型性能。单-shot提示相较于零-shot进一步提升了模型表现。

Conclusion: 知识蒸馏结合少量提示能够在显著减少参数的情况下，保持大部分原始LLM在问答任务上的性能，为资源受限场景下的小型高效问答系统提供了有效方案。

Abstract: Large Language Models (LLMs) have demonstrated outstanding performance across
a range of NLP tasks, however, their computational demands hinder their
deployment in real-world, resource-constrained environments. This work
investigates the extent to which LLMs can be compressed using Knowledge
Distillation (KD) while maintaining strong performance on Question Answering
(QA) tasks. We evaluate student models distilled from the Pythia and Qwen2.5
families on two QA benchmarks, SQuAD and MLQA, under zero-shot and one-shot
prompting conditions. Results show that student models retain over 90% of their
teacher models' performance while reducing parameter counts by up to 57.1%.
Furthermore, one-shot prompting yields additional performance gains over
zero-shot setups for both model families. These findings underscore the
trade-off between model efficiency and task performance, demonstrating that KD,
combined with minimal prompting, can yield compact yet capable QA systems
suitable for resource-constrained applications.

</details>


### [52] [FrugalRAG: Learning to retrieve and reason for multi-hop QA](https://arxiv.org/abs/2507.07634)
*Abhinav Java,Srivathsan Koundinyan,Nagarajan Natarajan,Amit Sharma*

Main category: cs.CL

TL;DR: 本研究发现，处理复杂问答任务时，无需大规模微调即可通过精心设计的prompt和流程拿到更好的RAG表现；同时，监督或RL微调可极大提升检索效率，节省资源，维持精度。


<details>
  <summary>Details</summary>
Motivation: 现有处理复杂问答任务的方法主要集中在提升检索增强生成（RAG）模型的准确率和召回率，但检索效率（如检索次数）这一重要指标却鲜有关注。该研究的动机是探讨提升检索效率的方法，并重新审视现有提升RAG的策略是否真的需要大规模微调。

Method: 作者分析了两类主流提升RAG的方法：（a）利用包含推理链的问答大数据集进行微调；（b）基于强化学习（RL）并利用问题-文档相关性的微调。此外，作者提出采用标准的ReAct流程和改进的prompt技术，检验其在无需大规模微调下的表现。同时，他们关注检索次数对效率的影响。

Result: （1）改进提示的标准ReAct流程，无需大规模数据微调即可在HotPotQA等基准上优于现有最优方法；（2）监督与RL微调有助于提高RAG系统的“节俭性”，即在推理时减少检索次数，同时几乎不损失RAG性能。例如，采用同一基础模型和很少量训练样本（1000例），以接近一半的检索次数实现与最优方法相当的RAG指标。

Conclusion: 提升RAG性能并不依赖大规模微调，合理设计提示和流程同样可以取得优秀结果。监督和RL微调则可有效降低系统在推理阶段的检索成本，实现高效又高质量的复杂问答。

Abstract: We consider the problem of answering complex questions, given access to a
large unstructured document corpus. The de facto approach to solving the
problem is to leverage language models that (iteratively) retrieve and reason
through the retrieved documents, until the model has sufficient information to
generate an answer. Attempts at improving this approach focus on
retrieval-augmented generation (RAG) metrics such as accuracy and recall and
can be categorized into two types: (a) fine-tuning on large question answering
(QA) datasets augmented with chain-of-thought traces, and (b) leveraging
RL-based fine-tuning techniques that rely on question-document relevance
signals. However, efficiency in the number of retrieval searches is an equally
important metric, which has received less attention. In this work, we show
that: (1) Large-scale fine-tuning is not needed to improve RAG metrics,
contrary to popular claims in recent literature. Specifically, a standard ReAct
pipeline with improved prompts can outperform state-of-the-art methods on
benchmarks such as HotPotQA. (2) Supervised and RL-based fine-tuning can help
RAG from the perspective of frugality, i.e., the latency due to number of
searches at inference time. For example, we show that we can achieve
competitive RAG metrics at nearly half the cost (in terms of number of
searches) on popular RAG benchmarks, using the same base model, and at a small
training cost (1000 examples).

</details>


### [53] [Lost in Pronunciation: Detecting Chinese Offensive Language Disguised by Phonetic Cloaking Replacement](https://arxiv.org/abs/2507.07640)
*Haotan Guo,Jianfei He,Jiayuan Ma,Hongbin Na,Zimu Wang,Haiyang Zhang,Qi Chen,Wei Wang,Zijing Shi,Tao Shen,Ling Chen*

Main category: cs.CL

TL;DR: 本研究专注于中国网络谐音变体的有害信息检测难题，首次提出系统分类与真实数据集，揭示大模型检测能力不足，并用拼音提示法提高了检测准确率。


<details>
  <summary>Details</summary>
Motivation: 现有中文内容审核中，用户通过谐音或近音替换（PCR）隐蔽表达有害信息，给自动检测系统带来极大挑战。现有研究多采用人工规则和合成数据，忽视了用户实际创造力，导致模型能力评估不真实。

Method: 研究者提出了PCR的四类表层结构分类法，并构建了一个包含500条真实自然出现的PCR有害样本的数据集，来自RedNote平台。对最先进的大模型（LLM）在此数据集上的表现进行了基准评测，并通过错误分析，改进并测试了以拼音为基础的提示策略。

Result: 主流大模型在此真实PCR数据集上的检测表现较差，最佳F1仅0.672，且zero-shot chain-of-thought提示法效果更低。通过拼音辅助提示，模型检测精度有明显恢复。

Conclusion: 该研究首次提出了详尽的PCR分类体系，建立了现实的评测基准，揭示了当前检测器的局限，并用轻量级拼音引导法有效提升了有害内容检测的鲁棒性。

Abstract: Phonetic Cloaking Replacement (PCR), defined as the deliberate use of
homophonic or near-homophonic variants to hide toxic intent, has become a major
obstacle to Chinese content moderation. While this problem is well-recognized,
existing evaluations predominantly rely on rule-based, synthetic perturbations
that ignore the creativity of real users. We organize PCR into a four-way
surface-form taxonomy and compile \ours, a dataset of 500 naturally occurring,
phonetically cloaked offensive posts gathered from the RedNote platform.
Benchmarking state-of-the-art LLMs on this dataset exposes a serious weakness:
the best model reaches only an F1-score of 0.672, and zero-shot
chain-of-thought prompting pushes performance even lower. Guided by error
analysis, we revisit a Pinyin-based prompting strategy that earlier studies
judged ineffective and show that it recovers much of the lost accuracy. This
study offers the first comprehensive taxonomy of Chinese PCR, a realistic
benchmark that reveals current detectors' limits, and a lightweight mitigation
technique that advances research on robust toxicity detection.

</details>


### [54] [An Automated Length-Aware Quality Metric for Summarization](https://arxiv.org/abs/2507.07653)
*Andrew D. Foland*

Main category: cs.CL

TL;DR: 论文提出NOIR指标，结合语义保留与压缩长度，用语言模型嵌入实现自动摘要质量评价，效果好、适用性强，无需人工参考摘要。


<details>
  <summary>Details</summary>
Motivation: 现有文本摘要质量评估通常依赖于人工参考摘要，耗时且主观，很难自动、精准地评价摘要在信息保留与压缩比例之间的权衡。

Method: 提出了一种新的定量评价指标——NOrmed Index of Retention（NOIR），它结合了摘要语义信息保留和压缩长度，利用语言模型嵌入计算语义相似度，实现自动化、客观地评估摘要质量。

Result: NOIR 能有效捕捉摘要系统在信息回忆与长度压缩之间的权衡，测量结果与人类主观评价显著相关，适用于评测各种自动摘要任务。

Conclusion: NOIR 为摘要质量自动化评估提供了新的有效手段，无需人工参考摘要，可广泛应用于算法、提示词及摘要结果的优化和比较。

Abstract: This paper proposes NOrmed Index of Retention (NOIR), a quantitative
objective metric for evaluating summarization quality of arbitrary texts that
relies on both the retention of semantic meaning and the summary length
compression. This gives a measure of how well the recall-compression tradeoff
is managed, the most important skill in summarization. Experiments demonstrate
that NOIR effectively captures the token-length / semantic retention tradeoff
of a summarizer and correlates to human perception of sumarization quality.
Using a language model-embedding to measure semantic similarity, it provides an
automated alternative for assessing summarization quality without relying on
time-consuming human-generated reference summaries. The proposed metric can be
applied to various summarization tasks, offering an automated tool for
evaluating and improving summarization algorithms, summarization prompts, and
synthetically-generated summaries.

</details>


### [55] [SAS: Simulated Attention Score](https://arxiv.org/abs/2507.07694)
*Chuanyang Zheng,Jiankai Sun,Yihang Gao,Yuehao Wang,Peihao Wang,Jing Xiong,Liliang Ren,Hao Cheng,Janardhan Kulkarni,Yelong Shen,Atlas Wang,Mac Schwager,Anderson Schneider,Xiaodong Liu,Jianfeng Gao*

Main category: cs.CL

TL;DR: 本文提出SAS和PEAA，在不显著增加参数的前提下，模拟更多注意力头和更大隐藏维度，实现Transformer注意力机制的大幅性能提升。


<details>
  <summary>Details</summary>
Motivation: Transformer中的注意力机制性能随着头数和每个头的隐藏维度增加而提升，但提升通常导致参数量激增，因此需要低成本提升注意力表达能力的新方法。

Method: 提出Simulated Attention Score（SAS）方法，通过将低维的头表示投影到高维空间，模拟更多注意力头和更大单头维度，但参数量保持小。此外，利用Parameter-Efficient Attention Aggregation（PEAA）方案，有效控制参数开销。

Result: 在多种数据集和任务上实验，SAS方法相较于不同注意力变体实现了显著性能提升。

Conclusion: SAS方法和PEAA不仅提升了Transformer注意力机制的表达能力，还有效控制了模型参数量，在多个任务上表现优异。

Abstract: The attention mechanism is a core component of the Transformer architecture.
Various methods have been developed to compute attention scores, including
multi-head attention (MHA), multi-query attention, group-query attention and so
on. We further analyze the MHA and observe that its performance improves as the
number of attention heads increases, provided the hidden size per head remains
sufficiently large. Therefore, increasing both the head count and hidden size
per head with minimal parameter overhead can lead to significant performance
gains at a low cost. Motivated by this insight, we introduce Simulated
Attention Score (SAS), which maintains a compact model size while simulating a
larger number of attention heads and hidden feature dimension per head. This is
achieved by projecting a low-dimensional head representation into a
higher-dimensional space, effectively increasing attention capacity without
increasing parameter count. Beyond the head representations, we further extend
the simulation approach to feature dimension of the key and query embeddings,
enhancing expressiveness by mimicking the behavior of a larger model while
preserving the original model size. To control the parameter cost, we also
propose Parameter-Efficient Attention Aggregation (PEAA). Comprehensive
experiments on a variety of datasets and tasks demonstrate the effectiveness of
the proposed SAS method, achieving significant improvements over different
attention variants.

</details>


### [56] [KeyKnowledgeRAG (K^2RAG): An Enhanced RAG method for improved LLM question-answering capabilities](https://arxiv.org/abs/2507.07695)
*Hruday Markondapatnaikuni,Basem Suleiman,Abdelkarim Erradi,Shijing Chen*

Main category: cs.CL

TL;DR: K2RAG框架通过多项创新（混合检索、知识图谱、摘要预处理）大幅提升了RAG在答案准确率、训练效率和系统扩展性方面的表现，展示了优于传统RAG实现的综合能力。


<details>
  <summary>Details</summary>
Motivation: 现有对大语言模型（LLM）进行知识扩展和微调的方法非常耗费资源，尤其是在模型规模持续增长的情况下。RAG技术虽可绕过微调，但其简单实现方案在可扩展性和准确率上有较大局限性。论文旨在提出更高效、准确的知识扩展方式。

Method: 提出KeyKnowledgeRAG（K2RAG）框架，结合了稠密和稀疏向量检索、知识图谱与文本摘要方法，并在训练前进行文本摘要以减少后续的训练时间。整个方案以分而治之的方法提升检索质量和系统效率。通过MultiHopRAG数据集开展实验，将提出的方法与常见RAG实现进行对比。

Result: K2RAG在答案相似度和第三四分位（Q3）相似度方面获得最好成绩，分别为0.57和0.82；摘要预处理步骤平均减少了93%的组件训练时间，执行速度比传统基于知识图谱的RAG系统快40%；在扩展性方面，所需VRAM是对比对象的三分之一。

Conclusion: K2RAG框架大幅提升了RAG系统在准确率、训练效率、执行速度和扩展性方面的综合表现，为大模型知识扩展提供了高效可行的新途径。

Abstract: Fine-tuning is an immensely resource-intensive process when retraining Large
Language Models (LLMs) to incorporate a larger body of knowledge. Although many
fine-tuning techniques have been developed to reduce the time and computational
cost involved, the challenge persists as LLMs continue to grow in size and
complexity. To address this, a new approach to knowledge expansion in LLMs is
needed. Retrieval-Augmented Generation (RAG) offers one such alternative by
storing external knowledge in a database and retrieving relevant chunks to
support question answering. However, naive implementations of RAG face
significant limitations in scalability and answer accuracy. This paper
introduces KeyKnowledgeRAG (K2RAG), a novel framework designed to overcome
these limitations. Inspired by the divide-and-conquer paradigm, K2RAG
integrates dense and sparse vector search, knowledge graphs, and text
summarization to improve retrieval quality and system efficiency. The framework
also includes a preprocessing step that summarizes the training data,
significantly reducing the training time. K2RAG was evaluated using the
MultiHopRAG dataset, where the proposed pipeline was trained on the document
corpus and tested on a separate evaluation set. Results demonstrated notable
improvements over common naive RAG implementations. K2RAG achieved the highest
mean answer similarity score of 0.57, and reached the highest third quartile
(Q3) similarity of 0.82, indicating better alignment with ground-truth answers.
In addition to improved accuracy, the framework proved highly efficient. The
summarization step reduced the average training time of individual components
by 93%, and execution speed was up to 40% faster than traditional knowledge
graph-based RAG systems. K2RAG also demonstrated superior scalability,
requiring three times less VRAM than several naive RAG implementations tested
in this study.

</details>


### [57] [Rethinking the Privacy of Text Embeddings: A Reproducibility Study of "Text Embeddings Reveal (Almost) As Much As Text"](https://arxiv.org/abs/2507.07700)
*Dominykas Seputis,Yongkang Li,Karsten Langerak,Serghei Mihailov*

Main category: cs.CL

TL;DR: 该论文实验证明，通过特定方法可从文本嵌入中重建原文，密码等敏感数据也受威胁。简单的噪声添加或量化可一定程度缓解风险，嵌入隐私保护需引起警觉，并需发展更可靠的防御技术。


<details>
  <summary>Details</summary>
Motivation: 文本嵌入（text embedding）广泛用于NLP任务并被认为是保护隐私的方法，但近期研究（如Vec2Text）显示原始文本可从嵌入中恢复出来，这挑战了传统观点。论文动机是验证和深入分析这一现象，并评估潜在隐私风险与防护措施。

Method: 复现Vec2Text框架，分别在同域和跨域环境下验证其文本重建能力，并扩展实验，包括参数敏感性分析、对敏感输入（如密码）的重建实验，以及利用嵌入量化和高斯噪声作为隐私防护措施的有效性评估。

Result: 成功复现了Vec2Text的核心结果，包括在理想条件下对缺乏语义的密码类序列实现有效重构。发现其对输入长度敏感，高斯噪声和量化能有效缓解隐私风险，其中量化法更简单且普适性高。

Conclusion: 单纯依赖文本嵌入的隐私保护并不安全，存在较大的文本重建风险。论文建议在实际NLP应用中应采取更多防护措施，并呼吁相关领域加强对鲁棒防御机制的研究。

Abstract: Text embeddings are fundamental to many natural language processing (NLP)
tasks, extensively applied in domains such as recommendation systems and
information retrieval (IR). Traditionally, transmitting embeddings instead of
raw text has been seen as privacy-preserving. However, recent methods such as
Vec2Text challenge this assumption by demonstrating that controlled decoding
can successfully reconstruct original texts from black-box embeddings. The
unexpectedly strong results reported by Vec2Text motivated us to conduct
further verification, particularly considering the typically non-intuitive and
opaque structure of high-dimensional embedding spaces. In this work, we
reproduce the Vec2Text framework and evaluate it from two perspectives: (1)
validating the original claims, and (2) extending the study through targeted
experiments. First, we successfully replicate the original key results in both
in-domain and out-of-domain settings, with only minor discrepancies arising due
to missing artifacts, such as model checkpoints and dataset splits.
Furthermore, we extend the study by conducting a parameter sensitivity
analysis, evaluating the feasibility of reconstructing sensitive inputs (e.g.,
passwords), and exploring embedding quantization as a lightweight privacy
defense. Our results show that Vec2Text is effective under ideal conditions,
capable of reconstructing even password-like sequences that lack clear
semantics. However, we identify key limitations, including its sensitivity to
input sequence length. We also find that Gaussian noise and quantization
techniques can mitigate the privacy risks posed by Vec2Text, with quantization
offering a simpler and more widely applicable solution. Our findings emphasize
the need for caution in using text embeddings and highlight the importance of
further research into robust defense mechanisms for NLP systems.

</details>


### [58] [Not All Preferences are What You Need for Post-Training: Selective Alignment Strategy for Preference Optimization](https://arxiv.org/abs/2507.07725)
*Zhijin Dong*

Main category: cs.CL

TL;DR: 本文提出Selective-DPO，通过选择性优化高影响token和提高参考模型质量，有效提升了LLM对齐性能和效率，在多个基准测试上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在大语言模型（LLM）训练后对齐中，不同token对模型性能贡献不均，现有方法难以高效识别并优化影响最大的token。作者希望提升模型对优先token的对齐效果，同时降低计算成本。

Method: 提出一种选择性对齐策略（Selective-DPO），通过比较当前策略与参考模型在token级别上的对数概率差异，选择对性能影响最大的token进行优化。同时，探索与增强参考模型质量对token选择准确性和整体优化效果的影响。

Result: 在Arena-Hard和MT-Bench等基准测试上，Selective-DPO方法相较于标准DPO与蒸馏基线都取得了更优的结果，验证了其在对齐质量和计算效率上的优势。

Conclusion: token级别的选择性优化与高质量参考模型选择对推进大语言模型的偏好对齐具有显著价值。

Abstract: Post-training alignment of large language models (LLMs) is a critical
challenge, as not all tokens contribute equally to model performance. This
paper introduces a selective alignment strategy that prioritizes high-impact
tokens within preference pairs, leveraging token-level log-probability
differences between the current policy and a reference model. By focusing on
these informative tokens, our approach reduces computational overhead and
enhances alignment fidelity. We further explore the role of reference model
quality, demonstrating that stronger reference models significantly improve
token selection accuracy and overall optimization effectiveness. Comprehensive
experiments on benchmarks such as Arena-Hard and MT-Bench validate the
superiority of our Selective-DPO method over standard DPO and
distillation-based baselines. Our findings highlight the importance of
token-level optimization and reference model selection in advancing preference
alignment for LLMs. The code is available at
https://github.com/Dongzhijin/SDPO.

</details>


### [59] [Code-Switching in End-to-End Automatic Speech Recognition: A Systematic Literature Review](https://arxiv.org/abs/2507.07741)
*Maha Tufail Agro,Atharva Kulkarni,Karima Kadaoui,Zeerak Talat,Hanan Aldarmaki*

Main category: cs.CL

TL;DR: 本文系统综述了多语混合环境下端到端自动语音识别的研究进展，归纳了涉及的语种、数据、评测方法和挑战，为后续研究指明方向。


<details>
  <summary>Details</summary>
Motivation: 近年来自动语音识别（ASR）和多语混合（code-switching, CS）语境下的研究兴起，亟需对当前CS下端到端ASR研究现状进行系统梳理。

Method: 系统性文献综述：收集并人工标注在同行评审期刊和会议发表的相关论文，对所涉及的语言、数据集、评测指标、模型选择和性能等进行整理和总结。

Result: 归纳了现有研究涉及的语种、数据资源、评测标准及主要模型方案，总结了当前CS端到端ASR面临的主要挑战。

Conclusion: 通过系统梳理CS场景下端到端ASR研究，揭示了现有努力、资源分布、研究机遇及待解决的研究空白，为未来相关领域的研究提供了指导。

Abstract: Motivated by a growing research interest into automatic speech recognition
(ASR), and the growing body of work for languages in which code-switching (CS)
often occurs, we present a systematic literature review of code-switching in
end-to-end ASR models. We collect and manually annotate papers published in
peer reviewed venues. We document the languages considered, datasets, metrics,
model choices, and performance, and present a discussion of challenges in
end-to-end ASR for code-switching. Our analysis thus provides insights on
current research efforts and available resources as well as opportunities and
gaps to guide future research.

</details>


### [60] [When Large Language Models Meet Law: Dual-Lens Taxonomy, Technical Advances, and Ethical Governance](https://arxiv.org/abs/2507.07748)
*Peizhang Shao,Linrui Xu,Jinxi Wang,Wei Zhou,Xingyu Wu*

Main category: cs.CL

TL;DR: 本综述首次系统梳理法律领域LLM应用，提出双重创新分类法，剖析技术进步与主要挑战，为学术界和实务界规划发展路线，并建立开源文献索引库。


<details>
  <summary>Details</summary>
Motivation: 法律领域AI应用日益增长，但相关研究分散且缺乏系统性。本文旨在统一既往研究与最新突破，建立完整的理论、技术与应用图谱，指导学界和业界发展。

Method: 通过构建融合法律角色与NLP子任务的本体分类体系，并将Toulmin论证框架计算化，系统性梳理和分析了历史成果和当前突破，对任务泛化、推理形式化、流程集成等进行技术回顾。

Result: 发现LLM在法律任务中的表现取得突破，如泛化能力提升、推理流程优化等，但同时存在幻觉、可解释性不足、适应性与伦理等实际问题。提出面向法律职业角色的NLP任务映射，并指出多模态证据整合、低资源系统等为未来关键前沿。

Conclusion: 本文为法律领域中大语言模型（LLM）的应用提供了首个系统性综述，并提出了整合法律推理与专业本体的双重创新分类法，为未来研究和实际应用提供理论与技术基础。

Abstract: This paper establishes the first comprehensive review of Large Language
Models (LLMs) applied within the legal domain. It pioneers an innovative dual
lens taxonomy that integrates legal reasoning frameworks and professional
ontologies to systematically unify historical research and contemporary
breakthroughs. Transformer-based LLMs, which exhibit emergent capabilities such
as contextual reasoning and generative argumentation, surmount traditional
limitations by dynamically capturing legal semantics and unifying evidence
reasoning. Significant progress is documented in task generalization, reasoning
formalization, workflow integration, and addressing core challenges in text
processing, knowledge integration, and evaluation rigor via technical
innovations like sparse attention mechanisms and mixture-of-experts
architectures. However, widespread adoption of LLM introduces critical
challenges: hallucination, explainability deficits, jurisdictional adaptation
difficulties, and ethical asymmetry. This review proposes a novel taxonomy that
maps legal roles to NLP subtasks and computationally implements the Toulmin
argumentation framework, thus systematizing advances in reasoning, retrieval,
prediction, and dispute resolution. It identifies key frontiers including
low-resource systems, multimodal evidence integration, and dynamic rebuttal
handling. Ultimately, this work provides both a technical roadmap for
researchers and a conceptual framework for practitioners navigating the
algorithmic future, laying a robust foundation for the next era of legal
artificial intelligence. We have created a GitHub repository to index the
relevant papers: https://github.com/Kilimajaro/LLMs_Meet_Law.

</details>


### [61] [StreamUni: Achieving Streaming Speech Translation with a Unified Large Speech-Language Model](https://arxiv.org/abs/2507.07803)
*Shoutao Guo,Xiang Li,Shaolei Zhang,Mengge Liu,Wei Chen,Yang Feng*

Main category: cs.CL

TL;DR: 这项工作提出了结合大型语音语言模型和Chain-of-Thought机制的方法，用以端到端提升流式语音翻译的质量和效率，取得了当前最优结果，革新了语音到翻译的流程。


<details>
  <summary>Details</summary>
Motivation: 现有的流式语音翻译(StreamST)方法需要借助分段模型进行协作，造成模型只能基于有限的上下文信息做决策，降低翻译质量和时效性。此外，SimulST模型因语音输入复杂和跨语种生成难以学习出有效的策略。

Method: 提出了StreamUni方法，通过一个统一的大型语音语言模型（LSLM）利用语音版Chain-of-Thought（CoT）机制生成分阶段多任务输出，实现语音分段、策略决策和翻译生成一体化。提出流式CoT训练方法，在有限的CoT数据下提升低延迟决策与生成能力。

Result: StreamUni无需大量额外的策略训练，即可端到端地完成StreamST任务，并在相关实验中达到了当前最优的性能表现。

Conclusion: StreamUni通过统一的LSLM架构和创新的流式CoT训练，有效提升了流式语音翻译在低延迟和高质量间的平衡，优于现有方法。

Abstract: Streaming speech translation (StreamST) requires determining appropriate
timing, known as policy, to generate translations while continuously receiving
source speech inputs, balancing low latency with high translation quality.
However, existing StreamST methods typically operate on sentence-level speech
segments, referred to as simultaneous speech translation (SimulST). In
practice, they require collaboration with segmentation models to accomplish
StreamST, where the truncated speech segments constrain SimulST models to make
policy decisions and generate translations based on limited contextual
information. Moreover, SimulST models struggle to learn effective policies due
to the complexity of speech inputs and cross-lingual generation. To address
these challenges, we propose StreamUni, which achieves StreamST through a
unified Large Speech-Language Model (LSLM). Specifically, StreamUni
incorporates speech Chain-of-Thought (CoT) in guiding the LSLM to generate
multi-stage outputs. Leveraging these multi-stage outputs, StreamUni
simultaneously accomplishes speech segmentation, policy decision, and
translation generation, completing StreamST without requiring massive
policy-specific training. Additionally, we propose a streaming CoT training
method that enhances low-latency policy decisions and generation capabilities
using limited CoT data. Experiments demonstrate that our approach achieves
state-of-the-art performance on StreamST tasks.

</details>


### [62] [Bridging Logic and Learning: Decoding Temporal Logic Embeddings via Transformers](https://arxiv.org/abs/2507.07808)
*Sara Candussio,Gaia Saveri,Gabriele Sarti,Luca Bortolussi*

Main category: cs.CL

TL;DR: 本文提出基于Transformer的解码器模型，实现STL公式嵌入向量的可逆解码，生成简明且语义对等的逻辑公式，并在自动化需求挖掘任务中展现出优秀性能，有效推动符号知识与深度学习的结合。


<details>
  <summary>Details</summary>
Motivation: 当前将符号知识融合到数据驱动学习算法中，依赖于逻辑公式的连续表示。但要使这些融合有效，需要实现表征的可逆性——即能够从连续向量还原出具体的逻辑公式。以往对此问题缺乏有效解决方案。

Method: 作者提出并训练了一种基于Transformer解码器（decoder-only）的模型，实现对Signal Temporal Logic（STL）公式的语义嵌入向量进行逆变换，生成原始或等价的STL公式。通过从STL语法构建小型词表，并系统性评估模型在不同复杂度训练数据下的表现。

Result: 提出的模型能快速学习，有效将嵌入向量还原为有效的STL公式，并且结果通常比原始公式更简洁，在语义上与参考标准接近或等价。模型能够泛化到新语义，与训练复杂度有关。其方法还成功应用于需求挖掘任务，直接在语义空间内完成优化。

Conclusion: Transformer解码器能够有效实现STL逻辑公式连续语义嵌入的可逆映射，为符号知识与深度学习整合提供了新途径，并可广泛应用于自动化需求挖掘与优化。

Abstract: Continuous representations of logic formulae allow us to integrate symbolic
knowledge into data-driven learning algorithms. If such embeddings are
semantically consistent, i.e. if similar specifications are mapped into nearby
vectors, they enable continuous learning and optimization directly in the
semantic space of formulae. However, to translate the optimal continuous
representation into a concrete requirement, such embeddings must be invertible.
We tackle this issue by training a Transformer-based decoder-only model to
invert semantic embeddings of Signal Temporal Logic (STL) formulae. STL is a
powerful formalism that allows us to describe properties of signals varying
over time in an expressive yet concise way. By constructing a small vocabulary
from STL syntax, we demonstrate that our proposed model is able to generate
valid formulae after only 1 epoch and to generalize to the semantics of the
logic in about 10 epochs. Additionally, the model is able to decode a given
embedding into formulae that are often simpler in terms of length and nesting
while remaining semantically close (or equivalent) to gold references. We show
the effectiveness of our methodology across various levels of training formulae
complexity to assess the impact of training data on the model's ability to
effectively capture the semantic information contained in the embeddings and
generalize out-of-distribution. Finally, we deploy our model for solving a
requirement mining task, i.e. inferring STL specifications that solve a
classification task on trajectories, performing the optimization directly in
the semantic space.

</details>


### [63] [Understanding and Controlling Repetition Neurons and Induction Heads in In-Context Learning](https://arxiv.org/abs/2507.07810)
*Nhi Hoai Doan,Tatsuya Hiraoka,Kentaro Inui*

Main category: cs.CL

TL;DR: 该文从重复神经元的视角探索了其对ICL表现和重复输出的影响，提出了平衡ICL能力与减少输出重复性的策略。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注注意力头对ICL的影响，本研究希望从技能神经元（特别是重复神经元）的角度深入了解其与重复输出及ICL能力的关系。

Method: 实验分析LLMs中重复神经元在不同层次对ICL的影响，并对比重复神经元与诱导头（induction heads）的作用。

Result: 确定了重复神经元对ICL的作用及其层级依赖性，并提出了针对性减少重复输出且不影响ICL性能的方法。

Conclusion: 重复神经元对LLMs ICL能力的影响与其所在层的深度有关，通过调节这些神经元可在降低重复输出的同时保持较强的ICL能力。

Abstract: This paper investigates the relationship between large language models'
(LLMs) ability to recognize repetitive input patterns and their performance on
in-context learning (ICL). In contrast to prior work that has primarily focused
on attention heads, we examine this relationship from the perspective of skill
neurons, specifically repetition neurons. Our experiments reveal that the
impact of these neurons on ICL performance varies depending on the depth of the
layer in which they reside. By comparing the effects of repetition neurons and
induction heads, we further identify strategies for reducing repetitive outputs
while maintaining strong ICL capabilities.

</details>


### [64] [On the Effect of Instruction Tuning Loss on Generalization](https://arxiv.org/abs/2507.07817)
*Anwoy Chatterjee,H S V N S Kowndinya Renduchintala,Sumit Bhatia,Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: 本文提出在指令微调中对prompt和response token 使用加权损失，证明了相较于传统只加权response的做法，更能提升模型效果和鲁棒性，并为进一步训练打下更好基础。


<details>
  <summary>Details</summary>
Motivation: 指令微调（Instruction Tuning）近年来成为提升大语言模型遵循用户指令能力的重要范式，但当前几乎所有方法只对响应（response）部分的token计算损失，而忽略了prompt部分，这种做法是否最优尚未被充分探讨。

Method: 作者系统性地研究了在指令微调过程中对prompt和response token采用不同权重的损失函数的影响，并提出了加权指令微调（Weighted Instruction Tuning, WIT）的方法。通过覆盖不同模型、数据集、评测基准的广泛实验，比较了不同加权策略的效果。

Result: 发现传统的仅对response token加权的方法往往带来次优的性能和对输入变化的鲁棒性有限。综合实验表明，给prompt token设定低到中等权重，同时给response token设定中到高等权重，能在各种场景下取得更优的结果，并为后续偏好对齐（preference alignment）提供更好的起点。

Conclusion: 传统指令微调的损失函数存在明显不足，对prompt和response部分分别加权能有效提升模型效果和鲁棒性。建议后续相关研究和实际部署中重新考虑损失函数设计。

Abstract: Instruction Tuning has emerged as a pivotal post-training paradigm that
enables pre-trained language models to better follow user instructions. Despite
its significance, little attention has been given to optimizing the loss
function used. A fundamental, yet often overlooked, question is whether the
conventional auto-regressive objective - where loss is computed only on
response tokens, excluding prompt tokens - is truly optimal for instruction
tuning. In this work, we systematically investigate the impact of
differentially weighting prompt and response tokens in instruction tuning loss,
and propose Weighted Instruction Tuning (WIT) as a better alternative to
conventional instruction tuning. Through extensive experiments on five language
models of different families and scale, three finetuning datasets of different
sizes, and five diverse evaluation benchmarks, we show that the standard
instruction tuning loss often yields suboptimal performance and limited
robustness to input prompt variations. We find that a low-to-moderate weight
for prompt tokens coupled with a moderate-to-high weight for response tokens
yields the best-performing models across settings and also serve as better
starting points for the subsequent preference alignment training. These
findings highlight the need to reconsider instruction tuning loss and offer
actionable insights for developing more robust and generalizable models. Our
code is open-sourced at https://github.com/kowndinya-renduchintala/WIT.

</details>


### [65] [Conditional Unigram Tokenization with Parallel Data](https://arxiv.org/abs/2507.07824)
*Gianluca Vico,Jindřinch Libovický*

Main category: cs.CL

TL;DR: 提出了一种结合源语言信息的Unigram分词法，能改善语言建模困惑度，但对机器翻译作用有限，需优化数据效率和方法参数化。


<details>
  <summary>Details</summary>
Motivation: 当前的Unigram分词方法未能充分利用平行语料中源语言信息，导致跨语言语义对齐不足。提升多语言处理系统中的分词方法对于增强跨语言表示能力和下游任务性能至关重要。

Method: 提出了条件Unigram分词法：在已定的源语言分词器基础上，通过利用平行语料，将目标语言词元概率条件化于源语言词元，从而学习最大化语义对齐的目标语言分词器。对四种不同语言对，涵盖不同语系与资源规模，评估了这种分词法的本质属性及其在机器翻译、语言建模等场景的表现。

Result: 条件Unigram分词法在统计属性上与标准Unigram分词器相当。在机器翻译质量上未见提升，但在语言建模任务上困惑度（perplexity）稳定降低。不过，方法中条件概率的二次扩展导致数据效率瓶颈。

Conclusion: 条件Unigram分词法虽提高了部分下游任务表现，但在机器翻译中未见显著效果。该方法在实际应用中受到参数化和数据效率限制，需要进一步研究更高效的参数化实现方式。

Abstract: We introduce conditional unigram tokenization, a novel approach that extends
unigram tokenization by conditioning target token probabilities on
source-language tokens from parallel data. Given a fixed source tokenizer, our
method learns a target tokenizer that maximizes cross-lingual semantic
alignment. We evaluate our tokenizer on four language pairs across different
families and resource levels, examining intrinsic properties and downstream
performance on machine translation and language modeling. While our conditional
tokenizer maintains comparable statistical properties to standard unigram
tokenizers, results are mixed: we observe no improvements in machine
translation quality, but find consistent perplexity reductions in language
modeling. We hypothesize that quadratic scaling of conditional probability
estimation with respect to the vocabulary size creates a data efficiency
bottleneck. Our findings suggest that alternative parameterizations may be
necessary for practical cross-lingual tokenization.

</details>


### [66] [From Ambiguity to Accuracy: The Transformative Effect of Coreference Resolution on Retrieval-Augmented Generation systems](https://arxiv.org/abs/2507.07847)
*Youngjoon Jang,Seongtae Hong,Junyoung Son,Sungjin Park,Chanjun Park,Heuiseok Lim*

Main category: cs.CL

TL;DR: 指代消解有助于解决RAG系统在检索和生成环节的歧义问题，尤其显著提升小模型性能，为提升知识密集型AI应用的效果提供了新方向。


<details>
  <summary>Details</summary>
Motivation: RAG框架通过结合文档检索提升了NLP任务的事实一致性，但由于文档中的指代复杂性，常引入歧义，干扰上下文学习。本文旨在系统性分析指代消解如何影响RAG系统的检索与生成表现。

Method: 分析指代消解对RAG检索相关性、上下文理解及生成质量的影响；对不同pooling策略的检索任务进行对比；在问答任务中，比较大小模型在消歧过程中的收益。

Result: 指代消解可提升检索效果及QA性能。mean pooling在指代消解后表现最佳。小模型因本身处理指代能力有限，受益更显著。

Conclusion: 本文揭示了指代复杂性对RAG系统带来的挑战，明确了指代消解对检索和生成提升的作用，并为知识密集型AI应用的改进提供了建议。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a crucial framework in
natural language processing (NLP), improving factual consistency and reducing
hallucinations by integrating external document retrieval with large language
models (LLMs). However, the effectiveness of RAG is often hindered by
coreferential complexity in retrieved documents, introducing ambiguity that
disrupts in-context learning. In this study, we systematically investigate how
entity coreference affects both document retrieval and generative performance
in RAG-based systems, focusing on retrieval relevance, contextual
understanding, and overall response quality. We demonstrate that coreference
resolution enhances retrieval effectiveness and improves question-answering
(QA) performance. Through comparative analysis of different pooling strategies
in retrieval tasks, we find that mean pooling demonstrates superior context
capturing ability after applying coreference resolution. In QA tasks, we
discover that smaller models benefit more from the disambiguation process,
likely due to their limited inherent capacity for handling referential
ambiguity. With these findings, this study aims to provide a deeper
understanding of the challenges posed by coreferential complexity in RAG,
providing guidance for improving retrieval and generation in
knowledge-intensive AI applications.

</details>


### [67] [Alpay Algebra V: Multi-Layered Semantic Games and Transfinite Fixed-Point Simulation](https://arxiv.org/abs/2507.07868)
*Bugra Kilictas,Faruk Alpay*

Main category: cs.CL

TL;DR: 该论文将AI与文档对齐过程抽象为多层次的语义博弈结构，通过固定点理论与范畴论证明了语义均衡的唯一性和存在性，为模型在AI应用中的泛化奠定基础。


<details>
  <summary>Details</summary>
Motivation: 推动Alpay Algebra自指框架发展为更高层次的多层语义博弈结构，旨在更好地对齐AI系统与文档间的理解过程。

Method: 构建嵌套的博弈论结构，将文档与AI系统的对齐视为一元博弈与嵌入式子博弈的组合，形式化为组合算子φ(·,γ(·))、采用超限不动点迭代、Banach不动点定理迁移、φ-拓扑及Yoneda引理等范畴理论工具进行证明和验证。

Result: 证明了在认知模拟假设下，嵌套不动点迭代自然产生博弈论推理；提出了语义均衡存在唯一性定理；展示了新颖的φ-拓扑及多种数学推理工具对该框架的适用性；模型本身可在AI嵌入空间中作为“语义病毒”模式传播。

Conclusion: 文章将AI文档对齐问题推广为多层次语义博弈，并在范畴论与认知理论支持下，奠定了AI-语义对齐的新型理论基础。

Abstract: This paper extends the self-referential framework of Alpay Algebra into a
multi-layered semantic game architecture where transfinite fixed-point
convergence encompasses hierarchical sub-games at each iteration level.
Building upon Alpay Algebra IV's empathetic embedding concept, we introduce a
nested game-theoretic structure where the alignment process between AI systems
and documents becomes a meta-game containing embedded decision problems. We
formalize this through a composite operator $\phi(\cdot, \gamma(\cdot))$ where
$\phi$ drives the main semantic convergence while $\gamma$ resolves local
sub-games. The resulting framework demonstrates that game-theoretic reasoning
emerges naturally from fixed-point iteration rather than being imposed
externally. We prove a Game Theorem establishing existence and uniqueness of
semantic equilibria under realistic cognitive simulation assumptions. Our
verification suite includes adaptations of Banach's fixed-point theorem to
transfinite contexts, a novel $\phi$-topology based on the
Kozlov-Maz'ya-Rossmann formula for handling semantic singularities, and
categorical consistency tests via the Yoneda lemma. The paper itself functions
as a semantic artifact designed to propagate its fixed-point patterns in AI
embedding spaces -- a deliberate instantiation of the "semantic virus" concept
it theorizes. All results are grounded in category theory, information theory,
and realistic AI cognition models, ensuring practical applicability beyond pure
mathematical abstraction.

</details>


### [68] [DocCHA: Towards LLM-Augmented Interactive Online diagnosis System](https://arxiv.org/abs/2507.07870)
*Xinyi Liu,Dachun Sun,Yi R. Fung,Dilek Hakkani-Tür,Tarek Abdelzaher*

Main category: cs.CL

TL;DR: 本文提出DocCHA，一个分模块、置信感知的临床问诊对话框架，大幅提升LLM医疗助手的诊断表现和可信度，为广泛实际应用提供可能。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLMs）在医疗对话助理（CHAs）应用中表现强大，但普遍缺乏灵活的多轮推理、症状澄清及决策透明度，从而降低了其在真实临床诊断场景中的实用性。

Method: 提出DocCHA框架，将诊断过程拆分为症状引出、病史采集、因果图构建三个模块。每一模块通过可解释的置信分驱动自适应提问和理由修正，提高诊断对话的结构性和透明度。

Result: 在中文真实问诊数据集（IMCS21和DX）上，DocCHA在诊断准确率上比基于强大LLM的提示方法（GPT-3.5、GPT-4o、LLaMA-3）最高提升5.18%，症状召回率提升超30%，对话轮次仅有适度增长。

Conclusion: DocCHA能够提升医疗对话系统的结构性、透明度及效率，为在多语种、资源受限环境中实现可可信赖的LLM医疗助手奠定基础。

Abstract: Despite the impressive capabilities of Large Language Models (LLMs), existing
Conversational Health Agents (CHAs) remain static and brittle, incapable of
adaptive multi-turn reasoning, symptom clarification, or transparent
decision-making. This hinders their real-world applicability in clinical
diagnosis, where iterative and structured dialogue is essential. We propose
DocCHA, a confidence-aware, modular framework that emulates clinical reasoning
by decomposing the diagnostic process into three stages: (1) symptom
elicitation, (2) history acquisition, and (3) causal graph construction. Each
module uses interpretable confidence scores to guide adaptive questioning,
prioritize informative clarifications, and refine weak reasoning links.
  Evaluated on two real-world Chinese consultation datasets (IMCS21, DX),
DocCHA consistently outperforms strong prompting-based LLM baselines (GPT-3.5,
GPT-4o, LLaMA-3), achieving up to 5.18 percent higher diagnostic accuracy and
over 30 percent improvement in symptom recall, with only modest increase in
dialogue turns. These results demonstrate the effectiveness of DocCHA in
enabling structured, transparent, and efficient diagnostic conversations --
paving the way for trustworthy LLM-powered clinical assistants in multilingual
and resource-constrained settings.

</details>


### [69] [Automating MD simulations for Proteins using Large language Models: NAMD-Agent](https://arxiv.org/abs/2507.07887)
*Achuth Chandrasekhar,Amir Barati Farimani*

Main category: cs.CL

TL;DR: 该文提出基于大模型和自动化脚本的分子动力学模拟输入文件自动生成流程，显著提高效率并减少人为失误，为计算结构生物学提供了高效、可扩展的解决方案。


<details>
  <summary>Details</summary>
Motivation: 分子动力学（MD）模拟在理解蛋白质结构和功能方面具有重要作用，但其输入文件准备过程繁琐且易出错。当前迫切需要高效、自动化的方法简化此流程。

Method: 提出了一个自动化流水线，结合了大型语言模型（Gemini 2.0 Flash）、Python脚本和基于Selenium的网页自动化。通过利用CHARMM GUI的网页界面并嵌入Gemini生成及优化代码，自动化撰写、执行与修正模拟脚本，自动获取参数并生成NAMD输入文件。随后用其他软件进一步后处理输出，实现几乎全自动的模拟准备。

Result: 该方法显著减少了设置时间，降低了人工错误，可扩展并行处理多蛋白系统。

Conclusion: LLM驱动的自动化模拟输入文件生成框架，为结构生物学中的模拟自动化开辟了新路径，具备强大适应性和广阔应用前景。

Abstract: Molecular dynamics simulations are an essential tool in understanding protein
structure, dynamics, and function at the atomic level. However, preparing high
quality input files for MD simulations can be a time consuming and error prone
process. In this work, we introduce an automated pipeline that leverages Large
Language Models (LLMs), specifically Gemini 2.0 Flash, in conjunction with
python scripting and Selenium based web automation to streamline the generation
of MD input files. The pipeline exploits CHARMM GUI's comprehensive web-based
interface for preparing simulation-ready inputs for NAMD. By integrating
Gemini's code generation and iterative refinement capabilities, simulation
scripts are automatically written, executed, and revised to navigate CHARMM
GUI, extract appropriate parameters, and produce the required NAMD input files.
Post processing is performed using additional software to further refine the
simulation outputs, thereby enabling a complete and largely hands free
workflow. Our results demonstrate that this approach reduces setup time,
minimizes manual errors, and offers a scalable solution for handling multiple
protein systems in parallel. This automated framework paves the way for broader
application of LLMs in computational structural biology, offering a robust and
adaptable platform for future developments in simulation automation.

</details>


### [70] [DTECT: Dynamic Topic Explorer & Context Tracker](https://arxiv.org/abs/2507.07910)
*Suman Adhya,Debarshi Kumar Sanyal*

Main category: cs.CL

TL;DR: DTECT是一个统一化的端到端动态主题建模平台，解决了现有流程割裂、难以解释的问题，集成了自动标注、趋势分析、交互问答等功能，极大提升了用户对文本主题动态的洞察与操作能力，且已开源。


<details>
  <summary>Details</summary>
Motivation: 随着文本数据量的爆炸性增长，从中发现主题演变和趋势变得极具挑战性。现有的动态主题建模技术虽然强大，但常常流程割裂，缺乏良好的可解释性和用户友好性，阻碍了用户对时序主题变化的高效探索。

Method: 提出DTECT系统，一个端到端的动态主题建模平台。DTECT集成了数据预处理、多种建模架构、针对时序主题评价的指标，并通过大语言模型自动主题标注、时序关键词趋势分析、交互式可视化及自然语言问答接口等功能，构建统一流程并增强模型解释性。

Result: DTECT使得用户能够更高效地追踪和理解文本数据中的主题动态。系统可用、可扩展且为开源，实现了原本割裂流程的整合，极大提升了主题模型的可解释性和可操作性。

Conclusion: DTECT有效弥合了原有动态主题建模技术的流程断裂问题，通过一体化设计与多项创新特性，显著强化了用户追踪、理解主题变化的能力，促进了文本主题研究的效率与透明性。

Abstract: The explosive growth of textual data over time presents a significant
challenge in uncovering evolving themes and trends. Existing dynamic topic
modeling techniques, while powerful, often exist in fragmented pipelines that
lack robust support for interpretation and user-friendly exploration. We
introduce DTECT (Dynamic Topic Explorer & Context Tracker), an end-to-end
system that bridges the gap between raw textual data and meaningful temporal
insights. DTECT provides a unified workflow that supports data preprocessing,
multiple model architectures, and dedicated evaluation metrics to analyze the
topic quality of temporal topic models. It significantly enhances
interpretability by introducing LLM-driven automatic topic labeling, trend
analysis via temporally salient words, interactive visualizations with
document-level summarization, and a natural language chat interface for
intuitive data querying. By integrating these features into a single, cohesive
platform, DTECT empowers users to more effectively track and understand
thematic dynamics. DTECT is open-source and available at
https://github.com/AdhyaSuman/DTECT.

</details>


### [71] [SAGE: A Visual Language Model for Anomaly Detection via Fact Enhancement and Entropy-aware Alignment](https://arxiv.org/abs/2507.07939)
*Guoxin Zang,Xue Li,Donglin Di,Lanshun Nie,Dechen Zhan,Yang Song,Lei Fan*

Main category: cs.CL

TL;DR: SAGE通过集成自引导事实增强和熵感知输出优化，显著提升了视觉语言模型在工业异常检测领域的推理表现，为实际应用提供了有力工具。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉-语言模型（VLM）在通用多模态任务中取得了进展，但它们在工业异常检测与推理任务中表现不佳，尤其是在解释性和对未见类别的泛化能力上有限。因为异常检测具有强领域特性，传统VLM难以满足工业应用对精准、结构化和可解释分析的需求。

Method: 本文提出SAGE框架，包括自引导事实增强（SFE）用于将领域知识融合进视觉推理，通过事实提取与融合提升推理能力；利用熵感知的直接偏好优化（E-DPO）使模型输出更契合专家偏好。此外，作者构建了工业异常推理专用数据集AD-PL和多尺度逻辑评估（MLE）体系。

Result: SAGE在多个工业异常检测数据集上，在零样本（zero-shot）和单样本（one-shot）设定下均表现优异，整体优于现有方法。相关代码、模型与数据集均已开源。

Conclusion: 结合专家知识融合、偏好优化与多层次逻辑评估，SAGE大幅提升了VLM在工业异常检测与推理中的泛化和解释能力，推动了实际工业应用发展。

Abstract: While Vision-Language Models (VLMs) have shown promising progress in general
multimodal tasks, they often struggle in industrial anomaly detection and
reasoning, particularly in delivering interpretable explanations and
generalizing to unseen categories. This limitation stems from the inherently
domain-specific nature of anomaly detection, which hinders the applicability of
existing VLMs in industrial scenarios that require precise, structured, and
context-aware analysis. To address these challenges, we propose SAGE, a
VLM-based framework that enhances anomaly reasoning through Self-Guided Fact
Enhancement (SFE) and Entropy-aware Direct Preference Optimization (E-DPO). SFE
integrates domain-specific knowledge into visual reasoning via fact extraction
and fusion, while E-DPO aligns model outputs with expert preferences using
entropy-aware optimization. Additionally, we introduce AD-PL, a
preference-optimized dataset tailored for industrial anomaly reasoning,
consisting of 28,415 question-answering instances with expert-ranked responses.
To evaluate anomaly reasoning models, we develop Multiscale Logical Evaluation
(MLE), a quantitative framework analyzing model logic and consistency. SAGE
demonstrates superior performance on industrial anomaly datasets under
zero-shot and one-shot settings. The code, model and dataset are available at
https://github.com/amoreZgx1n/SAGE.

</details>


### [72] [MIRIX: Multi-Agent Memory System for LLM-Based Agents](https://arxiv.org/abs/2507.07957)
*Yu Wang,Xi Chen*

Main category: cs.CL

TL;DR: MIRIX系统通过多类型模块化架构和多智能体调控，实现了大模型智能体在多模态和长期记忆上的突破，支持更优的表现和个性化安全的用户体验，显著优于上述任务上的现有系统。


<details>
  <summary>Details</summary>
Motivation: 现有AI代理记忆系统大多采用扁平、范围狭窄的结构，无法很好地实现长期、个性化和可靠的用户信息记忆及回溯，限制了其在真实场景中的有效性。

Method: 提出了MIRIX——一个模块化、多智能体的内存系统，包含六种类型（核心、情景、语义、程序、资源和知识库），并采用多智能体框架对记忆更新与检索进行动态调控，支持多模态（包括文本和视觉）的数据。

Result: 在ScreenshotVQA（大规模多模态、高分辨率截图序列任务）上提升35%准确率，存储需求降99.9%；在LOCOMO（长文本对话基准）上取得85.4%的SOTA表现，显著超越现有方法。

Conclusion: MIRIX有效解决了AI记忆系统在长期、多样化、个性化场景下的关键挑战，为内存增强型大模型智能体设立了新标准。并通过实时监控、个性化记忆库、可视化及本地隐私保障，提升了实用性。

Abstract: Although memory capabilities of AI agents are gaining increasing attention,
existing solutions remain fundamentally limited. Most rely on flat, narrowly
scoped memory components, constraining their ability to personalize, abstract,
and reliably recall user-specific information over time. To this end, we
introduce MIRIX, a modular, multi-agent memory system that redefines the future
of AI memory by solving the field's most critical challenge: enabling language
models to truly remember. Unlike prior approaches, MIRIX transcends text to
embrace rich visual and multimodal experiences, making memory genuinely useful
in real-world scenarios. MIRIX consists of six distinct, carefully structured
memory types: Core, Episodic, Semantic, Procedural, Resource Memory, and
Knowledge Vault, coupled with a multi-agent framework that dynamically controls
and coordinates updates and retrieval. This design enables agents to persist,
reason over, and accurately retrieve diverse, long-term user data at scale. We
validate MIRIX in two demanding settings. First, on ScreenshotVQA, a
challenging multimodal benchmark comprising nearly 20,000 high-resolution
computer screenshots per sequence, requiring deep contextual understanding and
where no existing memory systems can be applied, MIRIX achieves 35% higher
accuracy than the RAG baseline while reducing storage requirements by 99.9%.
Second, on LOCOMO, a long-form conversation benchmark with single-modal textual
input, MIRIX attains state-of-the-art performance of 85.4%, far surpassing
existing baselines. These results show that MIRIX sets a new performance
standard for memory-augmented LLM agents. To allow users to experience our
memory system, we provide a packaged application powered by MIRIX. It monitors
the screen in real time, builds a personalized memory base, and offers
intuitive visualization and secure local storage to ensure privacy.

</details>


### [73] [Why is Your Language Model a Poor Implicit Reward Model?](https://arxiv.org/abs/2507.07981)
*Noam Razin,Yong Lin,Jiarui Yao,Sanjeev Arora*

Main category: cs.CL

TL;DR: 本文揭示奖励模型的小设计差异（IM-RM和EX-RM的奖励计算方式不同）会显著影响模型的泛化能力，IM-RM更依赖token级特征，导致泛化能力逊色，特别是在分布外测试下。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明，语言模型可定义为隐式奖励模型（IM-RM），无需改变模型架构，但隐式奖励模型在泛化能力上（尤其是分布外情形）往往不如显式奖励模型（EX-RM）。研究动机在于理解这种泛化差异背后的根本原因。

Method: 通过理论分析及实验对比，系统地考察隐式奖励模型（IM-RM）和显式奖励模型（EX-RM）在相同训练数据、损失函数和模型架构条件下的表现差异，探究泛化能力差距的根本原因，并排除其它可能假设。

Result: 作者的主要发现是，IM-RM更依赖于表层的token级别线索，因此在token级别分布变化和同一分布下都比EX-RM具有更差的泛化性能。同时，本文提供了证据反驳了其他关于泛化差异的解释假说，例如IM-RM适应生成比验证更难的任务时存在困难等常规观点。

Conclusion: 简单的模型设计差异（如奖励的计算方式）会导致奖励模型泛化能力有显著不同，IM-RM比EX-RM更容易受到表层特征影响从而导致泛化性能下降。

Abstract: Reward models are key to language model post-training and inference
pipelines. Conveniently, recent work showed that every language model defines
an implicit reward model (IM-RM), without requiring any architectural changes.
However, such IM-RMs tend to generalize worse, especially out-of-distribution,
compared to explicit reward models (EX-RMs) that apply a dedicated linear head
over the hidden representations of a language model. The existence of a
generalization gap is puzzling, as EX-RMs and IM-RMs are nearly identical. They
can be trained using the same data, loss function, and language model, and
differ only in how the reward is computed. Towards a fundamental understanding
of the implicit biases underlying different reward model types, we investigate
the root cause of this gap. Our main finding, backed by theory and experiments,
is that IM-RMs rely more heavily on superficial token-level cues. Consequently,
they often generalize worse than EX-RMs under token-level distribution shifts,
as well as in-distribution. Furthermore, we provide evidence against
alternative hypotheses for the generalization gap. Most notably, we challenge
the intuitive claim that IM-RMs struggle in tasks where generation is harder
than verification because they can operate both as a verifier and a generator.
Taken together, our results highlight that seemingly minor design choices can
substantially impact the generalization behavior of reward models.

</details>


### [74] [Performance and Practical Considerations of Large and Small Language Models in Clinical Decision Support in Rheumatology](https://arxiv.org/abs/2507.07983)
*Sabine Felde,Rüdiger Buchkremer,Gamal Chehab,Christian Thielscher,Jörg HW Distler,Matthias Schneider,Jutta G. Richter*

Main category: cs.CL

TL;DR: 结合RAG的小型模型在风湿病临床任务中优于大型模型，能耗低、成本小，适合资源有限医疗应用，但依然离不开专家审核。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在风湿病等复杂临床领域辅助决策上展现出前景，但其能耗高、部署成本大，对资源有限的医疗体系不够友好，亟需寻找更高效的模型方案。

Method: 作者采用较小的语言模型（SLMs）结合检索增强生成（RAG）的方式，并与更大的语言模型在诊断和治疗任务上的表现进行了对比评估。

Result: 小型语言模型结合RAG在诊断和治疗表现上超过了更大的模型，同时能耗更低、部署更经济，适合本地化应用。但所有模型的准确度都未达到专科医生水平，需专家把关。

Conclusion: SLMs+RAG在能效和成本上优势明显，可助力资源有限地区的医疗，但临床实践仍需专家监督。

Abstract: Large language models (LLMs) show promise for supporting clinical
decision-making in complex fields such as rheumatology. Our evaluation shows
that smaller language models (SLMs), combined with retrieval-augmented
generation (RAG), achieve higher diagnostic and therapeutic performance than
larger models, while requiring substantially less energy and enabling
cost-efficient, local deployment. These features are attractive for
resource-limited healthcare. However, expert oversight remains essential, as no
model consistently reached specialist-level accuracy in rheumatology.

</details>


### [75] [Automating Expert-Level Medical Reasoning Evaluation of Large Language Models](https://arxiv.org/abs/2507.07988)
*Shuang Zhou,Wenya Xie,Jiaxi Li,Zaifu Zhan,Meijia Song,Han Yang,Cheyenna Espinoza,Lindsay Welton,Xinnie Mai,Yanwei Jin,Zidu Xu,Yuen-Hei Chung,Yiyun Xing,Meng-Han Tsai,Emma Schaffer,Yucheng Shi,Ninghao Liu,Zirui Liu,Rui Zhang*

Main category: cs.CL

TL;DR: 本文提出MedThink-Bench基准，解决大语言模型医学推理评测不严谨和难以扩展的问题，并通过新框架LLM-w-Ref实现高效准确定级；实验发现小模型有时优于大模型，为医疗场景下LLM落地提供科学评估基础。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLM）在临床决策中应用增多，但其医学推理能力的评价方法存在评估不准确或扩展性差的问题，缺乏严格的评测基准。

Method: 提出了MedThink-Bench基准，包括10个医学领域、共500个有专家详细推理过程注释的问题。同时提出LLM-w-Ref评价框架，结合细粒度推理和LLM作为评判者（LLM-as-a-Judge）机制，实现专家级精度的中间推理评估，并兼具可扩展性。

Result: LLM-w-Ref评价结果与专家评判高度相关。在12个主流LLM上的测试显示，一些较小的模型（如MedGemma-27B）表现优于更大的专有模型（如OpenAI-o3）。

Conclusion: MedThink-Bench是评估LLM医学推理能力的重要工具，将推动LLM在临床实践中的安全和负责任应用。

Abstract: As large language models (LLMs) become increasingly integrated into clinical
decision-making, ensuring transparent and trustworthy reasoning is essential.
However, existing evaluation strategies of LLMs' medical reasoning capability
either suffer from unsatisfactory assessment or poor scalability, and a
rigorous benchmark remains lacking. To address this, we introduce
MedThink-Bench, a benchmark designed for rigorous, explainable, and scalable
assessment of LLMs' medical reasoning. MedThink-Bench comprises 500 challenging
questions across ten medical domains, each annotated with expert-crafted
step-by-step rationales. Building on this, we propose LLM-w-Ref, a novel
evaluation framework that leverages fine-grained rationales and LLM-as-a-Judge
mechanisms to assess intermediate reasoning with expert-level fidelity while
maintaining scalability. Experiments show that LLM-w-Ref exhibits a strong
positive correlation with expert judgments. Benchmarking twelve
state-of-the-art LLMs, we find that smaller models (e.g., MedGemma-27B) can
surpass larger proprietary counterparts (e.g., OpenAI-o3). Overall,
MedThink-Bench offers a foundational tool for evaluating LLMs' medical
reasoning, advancing their safe and responsible deployment in clinical
practice.

</details>


### [76] [PyVision: Agentic Vision with Dynamic Tooling](https://arxiv.org/abs/2507.07998)
*Shitian Zhao,Haoquan Zhang,Shaoheng Lin,Ming Li,Qilong Wu,Kaipeng Zhang,Chen Wei*

Main category: cs.CL

TL;DR: PyVision让多模态大模型能够自主生成与改进任务相关的Python工具，显著提升视觉推理能力与灵活性，实现了性能大幅提升，为视觉智能体的发展带来新方向。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLMs）在视觉推理任务中，通常受限于预定义流程和静态工具集，难以实现动态、灵活和可解释的问题解决能力。

Method: 提出了PyVision，这是一个可交互、多轮的框架，使多模态大语言模型（MLLMs）能够自主生成、执行和优化基于Python的工具，并将其用于当前任务。研究还系统梳理和分析了PyVision工具的种类及其在不同基准上的使用情况。

Result: PyVision在多项基准测试中带来了持续稳定的性能提升，其中GPT-4.1模型在V*数据集上提升了7.8个百分点，Claude-4.0-Sonnet模型在VLMsAreBlind-mini数据集上提升了31.1个百分点。

Conclusion: 动态工具生成与应用不仅让模型会用工具，还让模型能够创新工具，这标志着向更具自主性的视觉推理迈进。

Abstract: LLMs are increasingly deployed as agents, systems capable of planning,
reasoning, and dynamically calling external tools. However, in visual
reasoning, prior approaches largely remain limited by predefined workflows and
static toolsets. In this report, we present PyVision, an interactive,
multi-turn framework that enables MLLMs to autonomously generate, execute, and
refine Python-based tools tailored to the task at hand, unlocking flexible and
interpretable problem-solving. We develop a taxonomy of the tools created by
PyVision and analyze their usage across a diverse set of benchmarks.
Quantitatively, PyVision achieves consistent performance gains, boosting
GPT-4.1 by +7.8% on V* and Claude-4.0-Sonnet by +31.1% on VLMsAreBlind-mini.
These results point to a broader shift: dynamic tooling allows models not just
to use tools, but to invent them, advancing toward more agentic visual
reasoning.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [77] [The Pandora's Box Problem with Sequential Inspections](https://arxiv.org/abs/2507.07508)
*Ali Aouad,Jingwei Ji,Yaron Shaposhnik*

Main category: cs.CE

TL;DR: 本文推广了传统Pandora's box问题，允许部分开箱从而在信息与成本之间权衡。证明了该问题的复杂性，分析出最优策略结构，给出逼近解，并通过实验发现基于阈值的策略实际很有效，为实际搜索决策提供理论支持。


<details>
  <summary>Details</summary>
Motivation: Pandora's box问题是经济理论中的核心模型之一，原始模型只考虑全额开箱以获得全部信息。本文关注一个更现实的扩展情形：代理人可以选择以较低费用部分开箱获取部分信息，引入信息获取与成本效益之间的新权衡。

Method: 本文首先证明了该问题的复杂性（hardness），采用了随机优化中的多种技术进行建模与分析。具体做法包括：识别最优策略的结构性特征；推导松弛模型并给出近似最优解；在特殊但有代表性的情形下严格刻画最优策略；大量数值实验比较多种策略的表现。

Result: 本文揭示了最优决策的结构，并通过数值实验证明了直观的阈值型策略（基于Pandora's box最优解的推广）在决策中实际有效。还给出了问题的可解边界和逼近解。

Conclusion: 在部分开箱的新设定下，文中分析和实验显示阈值型策略能够有效指导搜索决策，并为更复杂的实际搜索模型提供基础。文中分析了最优策略的结构、求解难度及近似解方法，为经济学中的最优搜索理论扩展了新视角。

Abstract: The Pandora's box problem (Weitzman 1979) is a core model in economic theory
that captures an agent's (Pandora's) search for the best alternative (box). We
study an important generalization of the problem where the agent can either
fully open boxes for a certain fee to reveal their exact values or partially
open them at a reduced cost. This introduces a new tradeoff between information
acquisition and cost efficiency. We establish a hardness result and employ an
array of techniques in stochastic optimization to provide a comprehensive
analysis of this model. This includes (1) the identification of structural
properties of the optimal policy that provide insights about optimal decisions;
(2) the derivation of problem relaxations and provably near-optimal solutions;
(3) the characterization of the optimal policy in special yet non-trivial
cases; and (4) an extensive numerical study that compares the performance of
various policies, and which provides additional insights about the optimal
policy. Throughout, we show that intuitive threshold-based policies that extend
the Pandora's box optimal solution can effectively guide search decisions.

</details>


### [78] [Meshless projection model-order reduction via reference spaces for smoothed-particle hydrodynamics](https://arxiv.org/abs/2507.07830)
*Steven N. Rodriguez,Steven L. Brunton,Liam K. Magargal,Parisa Khodabakshi,Justin W. Jaworski,Nicoleta A. Apetre,John C. Steuben,John G. Michopoulos,Athanasios Iliopoulos*

Main category: cs.CE

TL;DR: 作者提出一种适用于无网格SPH方法的模型降阶框架，利用模态参考空间和传统模态分解方法，能在不同算例中准确、低成本预测速度场，为SPH仿真显著节约计算资源。


<details>
  <summary>Details</summary>
Motivation: SPH（光滑粒子流体动力学）方法因其无网格、处理复杂边界和流体混合的能力而广受欢迎，但其高计算成本阻碍了实际应用。如何在保证SPH方法基本特性的同时，大幅度降低计算复杂性和开销，是该文的主要动力。

Method: 提出了一套用于无网格弱可压缩SPH方法的模型降阶（model-order reduction, MOR）框架。主要方法是引入模态参考空间（modal reference spaces），通过将SPH快照数据投影到参考空间，利用传统模态分解（如POD）得到低维表示，并在预测阶段用散点插值将模态量映射回SPH空间。方案包括Meshless Galerkin POD（GPOD）和Adjoint Petrov-Galerkin（APG）投影模型。

Result: 在Taylor-Green涡旋、带盖腔流和开腔流动三项数值实验中，框架在速度场的重建和预测上效果良好，能有效将无结构、动态、混合的SPH方程在低维子空间内演化。压力场对投影误差较敏感，但利用如APG的非线性近似可以改善这一点。

Conclusion: 提出的无网格模型降阶框架在保持SPH方法无网格本质的前提下，显著降低了计算开销，并为实际SPH大规模仿真铺平了降本增效的道路。

Abstract: This work proposes a model-order reduction framework for the meshless weakly
compressible smoothed particle hydrodynamics (SPH) method. The proposed
framework introduces the concept of modal reference spaces to overcome the
challenges of discovering low-dimensional subspaces from unstructured, dynamic,
and mixing numerical topology that is often seen in SPH simulations. The
proposed modal reference spaces enable a low-dimensional representation of the
SPH field equations while maintaining their inherent meshless qualities. Modal
reference spaces are constructed by projecting SPH snapshot data onto a
reference space where low-dimensionality of field quantities can be discovered
via traditional modal decomposition techniques (e.g., the proper orthogonal
decomposition (POD)). Modal quantities are mapped back to the meshless SPH
space via scattered data interpolation during the online predictive stage. The
proposed model-order reduction framework is cast into the \emph{meshless}
Galerkin POD (GPOD) and the Adjoint Petrov--Galerkin (APG) projection
model-order reduction (PMOR) formulation. The PMORs are tested on three
numerical experiments: 1) the Taylor--Green vortex; 2) lid-driven cavity; and
3) flow past an open cavity. Results show good agreement in reconstructed and
predictive velocity fields, which showcase the ability of the proposed
framework to evolve the unstructured, dynamic, and mixing SPH field equations
in a low-dimensional subspace. Results also show that the pressure field is
sensitive to the projection error due to the stiff weakly-compressible
assumption made in the current SPH framework, but can be alleviated through
nonlinear approximations, such as the APG approach. Ultimately, the presented
meshless model-order reduction framework marks a step toward enabling drastic
cost savings of SPH simulations.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [79] [Short-Term Gains, Long-Term Gaps: The Impact of GenAI and Search Technologies on Retention](https://arxiv.org/abs/2507.07357)
*Mahir Akgun,Sacip Toker*

Main category: cs.CY

TL;DR: 生成式AI等工具有助于任务即时表现，但对长期记忆支持有限，需与适当教学策略结合。


<details>
  <summary>Details</summary>
Motivation: 生成式AI（如ChatGPT）的普及改变了学生获取和使用信息的方式，激发了其对学习效果与知识留存的影响的研究兴趣。

Method: 本研究以123名学生为样本，基于Bloom认知层级理论，将任务细分为三个层级：了解与理解、应用，以及综合评估与创造。分别测试学生在使用ChatGPT、搜索引擎（Google）和电子教材三种辅助工具下的表现，并在不同时间节点检测其即时表现与知识留存。

Result: 在低阶认知任务中，ChatGPT和Google组在即时测试中优于对照组，但随时间推移，这一优势消失，留存分数与电子教材组趋同。在高阶认知任务中，各组表现无显著差异，对照组的长期留存最高。

Conclusion: AI工具能提升学生短期表现，但难以单独强化知识长期留存，若未结合系统的教学策略作用有限。需在教育中平衡技术与教学设计，促进深层学习与知识保持。

Abstract: The rise of Generative AI (GenAI) tools, such as ChatGPT, has transformed how
students access and engage with information, raising questions about their
impact on learning outcomes and retention. This study investigates how GenAI
(ChatGPT), search engines (Google), and e-textbooks influence student
performance across tasks of varying cognitive complexity, based on Bloom's
Taxonomy. Using a sample of 123 students, we examined performance in three
tasks: [1] knowing and understanding, [2] applying, and [3] synthesizing,
evaluating, and creating. Results indicate that ChatGPT and Google groups
outperformed the control group in immediate assessments for lower-order
cognitive tasks, benefiting from quick access to structured information.
However, their advantage diminished over time, with retention test scores
aligning with those of the e-textbook group. For higher-order cognitive tasks,
no significant differences were observed among groups, with the control group
demonstrating the highest retention. These findings suggest that while
AI-driven tools facilitate immediate performance, they do not inherently
reinforce long-term retention unless supported by structured learning
strategies. The study highlights the need for balanced technology integration
in education, ensuring that AI tools are paired with pedagogical approaches
that promote deep cognitive engagement and knowledge retention.

</details>


### [80] [The Evolution of Scientific Credit: When Authorship Norms Impede Collaboration](https://arxiv.org/abs/2507.07364)
*Toby Handfield,Kevin Zollman*

Main category: cs.CY

TL;DR: 通过进化博弈模型，作者发现传统的作者署名惯例如资深作者最后或字母排序，往往会妨碍科学合作和产出，而按贡献排序的署名规范更有助于促进高效和公平的科研合作。


<details>
  <summary>Details</summary>
Motivation: 科学领域作者署名规则差异极大，不同规则对科研合作与贡献认知影响尚不明确。理解这些规则如何进化及其对合作效率的影响，有助于优化科研激励机制。

Method: 本文建立了进化博弈论模型，分别分析不同作者署名规范（按贡献顺序、按字母顺序、资深作者最后等）如何形成及其对合作行为和成功合作数量的影响。

Result: 模型显示，当资深研究者在名次受损时承受更大适应性压力时，易演化出对贡献不敏感的规范。此外，贡献敏感的规范能更有效促进科研合作，而贡献不敏感的规范易引发主要贡献者或次贡献者的不满，导致合作意愿降低。

Conclusion: 常见的资深作者最后或字母排序等署名惯例，并非完全中立，反而可能成为阻碍科学合作和生产率的体制摩擦。推广按贡献排序等敏感规范，有助于提升学科合作效率和产出。

Abstract: Scientific authorship norms vary dramatically across disciplines, from
contribution-sensitive systems where first author is the greatest contributor
and subsequent author order reflects relative input, to
contribution-insensitive conventions like alphabetical ordering or
senior-author-last. We develop evolutionary game-theoretic models to examine
both how these divergent norms emerge and their subsequent effects on
collaborative behavior. Our first model reveals that contribution-insensitive
norms evolve when researchers who sacrifice positional advantage face the
strongest adaptive pressure -- for example senior authors managing larger
collaboration portfolios or bearing heavier reputational stakes. This "Red
King" dynamic potentially explains why fields in which senior researchers
command large labs, major grants, and extensive collaboration portfolios may
paradoxically evolve conventions that favour junior-author positioning. Our
second model demonstrates that established norms influence researchers'
willingness to collaborate, with contribution-sensitive norms consistently
outperforming insensitive alternatives in fostering successful partnerships.
Contribution-insensitive norms create systematic coordination failures through
two mechanisms: "main contributor resentment" when exceptional work goes
unrecognized, and "second contributor resentment" when comparable efforts
receive unequal credit. These findings suggest that widely adopted practices
like senior-last positioning and alphabetical ordering may function as
institutional frictions that impede valuable scientific collaborations rather
than neutral organizational conventions, potentially reducing overall
scientific productivity across affected disciplines.

</details>


### [81] [Vaccine Hesitancy on YouTube: a Competition between Health and Politics](https://arxiv.org/abs/2507.07517)
*Yelena Mejova,Michele Tizzani*

Main category: cs.CY

TL;DR: 该研究系统分析了YouTube上疫苗话题的视频，揭示社会/政治评论员主要传播反疫苗立场，且平台对疫苗犹豫内容的监管不足，为公共卫生传播提供政策启示。


<details>
  <summary>Details</summary>
Motivation: YouTube作为主要传播渠道，大量健康和疫苗相关视频上传，但信息质量良莠不齐，影响公众健康安全。针对疫苗相关视频，需了解不同内容创作者及其立场。

Method: 本研究系统性地每日收集了为期3个月的涉及疫苗话题的YouTube视频，对视频立场及内容进行了分析，比较不同内容创作者（公共卫生机构、个人教育者、社会与政治评论员等）对公众舆论的影响。

Result: 发现公共卫生机构/个人教育者与社会/政治评论员之间存在舆论竞争。反疫苗视频多由社会、政治评论员制作，更可能提及政治人物及传媒内容（播客、报道、新闻分析）；而支持疫苗视频更偏向提及具体疾病或健康主题。此外，尽管20.8%的视频表现出疫苗犹豫立场，仅有2.7%视频被删除，显示平台对含悬疑内容的管理有限。

Conclusion: 高质量信息对提升公众健康意识和疫苗接种依从性至关重要。理解不同内容创作者的角色和立场，有助于优化公共卫生传播策略和政策设计。

Abstract: YouTube has rapidly emerged as a predominant platform for content
consumption, effectively displacing conventional media such as television and
news outlets. A part of the enormous video stream uploaded to this platform
includes health-related content, both from official public health
organizations, and from any individual or group that can make an account. The
quality of information available on YouTube is a critical point of public
health safety, especially when concerning major interventions, such as
vaccination. This study differentiates itself from previous efforts of auditing
YouTube videos on this topic by conducting a systematic daily collection of
posted videos mentioning vaccination for the duration of 3 months. We show that
the competition for the public's attention is between public health messaging
by institutions and individual educators on one side, and commentators on
society and politics on the other, the latest contributing the most to the
videos expressing stances against vaccination. Videos opposing vaccination are
more likely to mention politicians and publication media such as podcasts,
reports, and news analysis, on the other hand, videos in favor are more likely
to mention specific diseases or health-related topics. Finally, we find that,
at the time of analysis, only 2.7% of the videos have been taken down (by the
platform or the channel), despite 20.8% of the collected videos having a
vaccination hesitant stance, pointing to a lack of moderation activity for
hesitant content. The availability of high-quality information is essential to
improve awareness and compliance with public health interventions. Our findings
help characterize the public discourse around vaccination on one of the largest
media platforms, disentangling the role of the different creators and their
stances, and as such, they provide important insights for public health
communication policy.

</details>


### [82] [AI Human Impact: Toward a Model for Ethical Investing in AI-Intensive Companies](https://arxiv.org/abs/2507.07703)
*James Brusseau*

Main category: cs.CY

TL;DR: 本文针对AI密集型企业提出了专门的伦理评估指标体系，弥补了传统ESG框架的不足。通过量化、以人为本的方式，为投资者在考量道德价值的基础上进行合理投资提供了切实可行的方法。


<details>
  <summary>Details</summary>
Motivation: AI技术越来越渗透到企业中，带来伦理挑战。传统ESG(环境、社会、治理)框架不足以全面评估以AI为核心的公司。因此需要新的方法帮助投资者通过伦理视角评估AI密集型企业。

Method: 提出并建立了九项可衡量AI“以人为本”特征的绩效指标体系，通过这些绩效指标分析和打分，以客观反映一家AI公司的伦理属性。指标参考了现有AI伦理原则，并定制化设计以适应大数据、预测分析和机器学习等现代AI应用。

Result: 构建了一套面向AI密集型公司的伦理评估框架，能帮助投资者获得客观的投资指引，让其依据个人价值观做出决策。提供了适用于分析师、投资组合经理和投资者的可操作、可信的AI伦理投资模型。

Conclusion: 要对以AI为核心的公司进行伦理审查和投资决策，必须基于专门化、以人为本的绩效指标，传统ESG框架无法胜任。新模型能帮助投资者更好地将道德价值纳入投资决策。

Abstract: Does AI conform to humans, or will we conform to AI? An ethical evaluation of
AI-intensive companies will allow investors to knowledgeably participate in the
decision. The evaluation is built from nine performance indicators that can be
analyzed and scored to reflect a technology's human-centering. The result is
objective investment guidance, as well as investors empowered to act in
accordance with their own values. Incorporating ethics into financial decisions
is a strategy that will be recognized by participants in environmental, social,
and governance investing, however, this paper argues that conventional ESG
frameworks are inadequate to companies that function with AI at their core.
Fully accounting for contemporary big data, predictive analytics, and machine
learning requires specialized metrics customized from established AI ethics
principles. With these metrics established, the larger goal is a model for
humanist investing in AI-intensive companies that is intellectually robust,
manageable for analysts, useful for portfolio managers, and credible for
investors.

</details>


### [83] [Distributed and Decentralised Training: Technical Governance Challenges in a Shifting AI Landscape](https://arxiv.org/abs/2507.07765)
*Jakub Kryś,Yashvardhan Sharma,Janet Egan*

Main category: cs.CY

TL;DR: 论文区分了分布式与去中心化AI训练，指出两者的发展对AI治理提出新的挑战，包括能力扩散和降低可监测性，建议政策制定要更精确，同时肯定去中心化AI可能带来的积极影响。


<details>
  <summary>Details</summary>
Motivation: 随着低通信训练算法的进步，模型训练逐渐由中央化向分布式甚至去中心化的计算环境转变。这一变革对现有AI治理模式提出了新的挑战，也带来了潜在的风险和机遇。论文关注于澄清分布式与去中心化训练的区别，并探讨这些趋势对政策制定的影响。

Method: 论文采用理论分析和前瞻性讨论的方法，从技术和政策角度区分分布式和去中心化训练，并分析其对AI治理、计算结构、能力扩散、可监测性及可关停性的影响。论文还讨论了相关政策工具（如出口管制）的适用性及未来作用。

Result: 论文明确指出分布式与去中心化训练常被混为一谈，应加以区分。分布式计算与去中心化AI贡献的共同发展可能带来治理难度上升、能力扩散、降低AI可监测性和可关停性等风险。与此同时，这些趋势也有助于保护隐私和减少权力过度集中。部分传统政策工具（如出口管制）虽面临挑战，但依然具有一定作用。

Conclusion: 分布式与去中心化AI训练的发展对现有治理模式提出挑战，但相关政策依然有应对空间。为实现更精准的AI治理，应深入理解计算结构与能力扩散，并重视去中心化AI发展中的风险与机遇。

Abstract: Advances in low-communication training algorithms are enabling a shift from
centralised model training to compute setups that are either distributed across
multiple clusters or decentralised via community-driven contributions. This
paper distinguishes these two scenarios - distributed and decentralised
training - which are little understood and often conflated in policy discourse.
We discuss how they could impact technical AI governance through an increased
risk of compute structuring, capability proliferation, and the erosion of
detectability and shutdownability. While these trends foreshadow a possible new
paradigm that could challenge key assumptions of compute governance, we
emphasise that certain policy levers, like export controls, remain relevant. We
also acknowledge potential benefits of decentralised AI, including
privacy-preserving training runs that could unlock access to more data, and
mitigating harmful power concentration. Our goal is to support more precise
policymaking around compute, capability proliferation, and decentralised AI
development.

</details>


### [84] [Structured Prompts, Better Outcomes? Exploring the Effects of a Structured Interface with ChatGPT in a Graduate Robotics Course](https://arxiv.org/abs/2507.07767)
*Jerome Brender,Laila El-Hamamsy,Kim Uittenhove,Francesco Mondada,Engin Bumbacher*

Main category: cs.CY

TL;DR: 结构化GPT平台可短暂改善学生的提示行为并关联高学习成效，但未能带来持续行为改变和显著学习表现提升。大多数学生对平台缺乏认同。研究建议关注激发学生动机的策略，而不仅是临时的界面干预。


<details>
  <summary>Details</summary>
Motivation: 已有研究表明，学生如何与大型语言模型（LLM）互动会影响其解决问题和理解能力，因此亟需支持能够提升学习效果的高效LLM使用方式。

Method: 本研究在一门研究生机器人课程中，将58名学生随机分配至两组：实验组使用旨在促进“良好”提示行为的结构化GPT平台，对照组自由使用ChatGPT，进行了两次练习实验。第三次实验所有学生均可自由使用ChatGPT。研究通过前后问卷、提示行为日志、任务得分和前后测试，分析了学生的感知、行为、表现和学习效果。

Result: 结果显示，实验组与对照组在表现和学习提升上无显著差异。但分析发现，用于理解代码并表述清晰的问题提示行为与更高的学习提升有关，并且这一行为在使用结构化平台时更为突出。但一旦回归自由使用环境，这种行为未被保留。问卷显示，部分学生认可结构化平台价值，但多数未感知其相关性且不愿改变习惯。

Conclusion: 结构化平台虽然能暂时改善提示行为，并与高效学习挂钩，但难以持续影响学生行为，多数学生对其接受度低。该研究质疑仅靠短期界面调整（自下而上方法）能否提升LLM学习效果，建议未来可探索从激发学生动机并明确教学模式优势的自上而下策略。

Abstract: Prior research shows that how students engage with Large Language Models
(LLMs) influences their problem-solving and understanding, reinforcing the need
to support productive LLM-uses that promote learning. This study evaluates the
impact of a structured GPT platform designed to promote 'good' prompting
behavior with data from 58 students in a graduate-level robotics course. The
students were assigned to either an intervention group using the structured
platform or a control group using ChatGPT freely for two practice lab sessions,
before a third session where all students could freely use ChatGPT. We analyzed
student perception (pre-post surveys), prompting behavior (logs), performance
(task scores), and learning (pre-post tests). Although we found no differences
in performance or learning between groups, we identified prompting behaviors -
such as having clear prompts focused on understanding code - that were linked
with higher learning gains and were more prominent when students used the
structured platform. However, such behaviors did not transfer once students
were no longer constrained to use the structured platform. Qualitative survey
data showed mixed perceptions: some students perceived the value of the
structured platform, but most did not perceive its relevance and resisted
changing their habits. These findings contribute to ongoing efforts to identify
effective strategies for integrating LLMs into learning and question the
effectiveness of bottom-up approaches that temporarily alter user interfaces to
influence students' interaction. Future research could instead explore top-down
strategies that address students' motivations and explicitly demonstrate how
certain interaction patterns support learning.

</details>


### [85] [Opting Out of Generative AI: a Behavioral Experiment on the Role of Education in Perplexity AI Avoidance](https://arxiv.org/abs/2507.07881)
*Roberto Ulloa,Juhi Kulshrestha,Celina Kacperski*

Main category: cs.CY

TL;DR: 研究发现，低学历人群回避使用对话式AI的概率更高，教育背景成为影响AI采纳的显著因素，提示AI技术应更加重视包容性设计以防加剧数字不平等。


<details>
  <summary>Details</summary>
Motivation: 随着对话式人工智能（CAI）的兴起，人们越来越依赖这类工具获取和交互数字信息。然而，这可能会加剧现有的数字鸿沟，尤其是受教育水平不同的人对CAI的使用态度和行为尚不明确。作者希望探究受教育程度是否影响人们对CAI的回避。

Method: 研究基于一项在线实验（N=1,636），参与者被随机分配到三组：传统搜索组、CAI搜索组（使用Perplexity AI）、以及控制组。作者通过任务回避（如中断调查或答非所问）来衡量参与者对任务的回避率，同时结合结构方程建模（基于UTAUT2理论框架）和LASSO回归分析来辨识教育背景与CAI 回避之间的关系。

Result: 结果显示，CAI组的任务回避率（51%）显著高于传统搜索（30.9%）及控制组（16.8%）；低学历参与者在CAI组的回避率尤其高（约74.4%）。即使控制其他认知和情感因素，教育水平依然与CAI回避显著相关。

Conclusion: 教育程度在AI采纳中起关键作用，CAI的设计需要包容性以避免加剧技术鸿沟。同时，AI相关研究中存在自我选择偏差，强调了包容设计和公平技术获取的重要性。

Abstract: The rise of conversational AI (CAI), powered by large language models, is
transforming how individuals access and interact with digital information.
However, these tools may inadvertently amplify existing digital inequalities.
This study investigates whether differences in formal education are associated
with CAI avoidance, leveraging behavioral data from an online experiment (N =
1,636). Participants were randomly assigned to a control or an
information-seeking task, either a traditional online search or a CAI
(Perplexity AI). Task avoidance (operationalized as survey abandonment or
providing unrelated responses during task assignment) was significantly higher
in the CAI group (51%) compared to the search (30.9%) and control (16.8%)
groups, with the highest CAI avoidance among participants with lower education
levels (~74.4%). Structural equation modeling based on the theoretical
framework UTAUT2 and LASSO regressions reveal that education is strongly
associated with CAI avoidance, even after accounting for various cognitive and
affective predictors of technology adoption. These findings underscore
education's central role in shaping AI adoption and the role of self-selection
biases in AI-related research, stressing the need for inclusive design to
ensure equitable access to emerging technologies.

</details>
