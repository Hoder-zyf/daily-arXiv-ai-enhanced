{"id": "2506.20747", "categories": ["cs.CL", "68T50, 68T37", "I.2.7"], "pdf": "https://arxiv.org/pdf/2506.20747", "abs": "https://arxiv.org/abs/2506.20747", "authors": ["Chen Shen", "Sajjadur Rahman", "Estevam Hruschka"], "title": "Towards Probabilistic Question Answering Over Tabular Data", "comment": null, "summary": "Current approaches for question answering (QA) over tabular data, such as\nNL2SQL systems, perform well for factual questions where answers are directly\nretrieved from tables. However, they fall short on probabilistic questions\nrequiring reasoning under uncertainty. In this paper, we introduce a new\nbenchmark LUCARIO and a framework for probabilistic QA over large tabular data.\nOur method induces Bayesian Networks from tables, translates natural language\nqueries into probabilistic queries, and uses large language models (LLMs) to\ngenerate final answers. Empirical results demonstrate significant improvements\nover baselines, highlighting the benefits of hybrid symbolic-neural reasoning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9762\u5411\u8868\u683c\u6570\u636e\u6982\u7387\u6027\u95ee\u7b54\u7684\u65b0\u57fa\u51c6\u4e0e\u63a8\u7406\u6846\u67b6\uff0c\u7ed3\u5408\u8d1d\u53f6\u65af\u7f51\u7edc\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6982\u7387\u6027\u95ee\u9898\u7684\u95ee\u7b54\u6548\u679c\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u8868\u683c\u6570\u636e\u7684\u95ee\u7b54\u65b9\u6cd5\uff08\u5982NL2SQL\uff09\u53ef\u4ee5\u5f88\u597d\u5730\u5904\u7406\u4e8b\u5b9e\u6027\u95ee\u9898\uff0c\u4f46\u9762\u5bf9\u9700\u8981\u4e0d\u786e\u5b9a\u6027\u63a8\u7406\u7684\u6982\u7387\u6027\u95ee\u9898\u65f6\u8868\u73b0\u4e0d\u4f73\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u5c40\u9650\u6027\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u65b0\u7684\u7814\u7a76\u65b9\u6cd5\u548c\u57fa\u51c6\u3002", "method": "\u63d0\u51fa\u4e86LUCARIO\u65b0\u57fa\u51c6\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u6982\u7387\u6027\u95ee\u7b54\u6846\u67b6\uff1a\u5148\u4ece\u6570\u636e\u8868\u4e2d\u5f52\u7eb3\u751f\u6210\u8d1d\u53f6\u65af\u7f51\u7edc\uff0c\u5c06\u81ea\u7136\u8bed\u8a00\u95ee\u9898\u8f6c\u5316\u4e3a\u6982\u7387\u6027\u67e5\u8be2\uff0c\u518d\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u6700\u7ec8\u7b54\u6848\u3002\u901a\u8fc7\u7b26\u53f7\u63a8\u7406\u4e0e\u795e\u7ecf\u7f51\u7edc\u7ed3\u5408\u7684\u65b9\u6cd5\u6765\u63d0\u5347\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u6982\u7387\u6027\u95ee\u7b54\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u6df7\u5408\u7b26\u53f7-\u795e\u7ecf\u63a8\u7406\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165LUCARIO\u57fa\u51c6\u548c\u4e00\u79cd\u65b0\u7684\u6df7\u5408\u63a8\u7406\u65b9\u6cd5\uff0c\u80fd\u591f\u66f4\u6709\u6548\u5730\u5904\u7406\u5173\u4e8e\u8868\u683c\u6570\u636e\u7684\u4e0d\u786e\u5b9a\u6027\u63a8\u7406\u4efb\u52a1\uff0c\u5728\u6982\u7387\u6027\u95ee\u7b54\u4e0a\u53d6\u5f97\u4e86\u7a81\u7834\u6027\u8fdb\u5c55\u3002"}}
{"id": "2506.20793", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.20793", "abs": "https://arxiv.org/abs/2506.20793", "authors": ["Victor Ojewale", "Inioluwa Deborah Raji", "Suresh Venkatasubramanian"], "title": "Multi-lingual Functional Evaluation for Large Language Models", "comment": null, "summary": "Multi-lingual competence in large language models is often evaluated via\nstatic data benchmarks such as Belebele, M-MMLU and M-GSM. However, these\nevaluations often fail to provide an adequate understanding of the practical\nperformance and robustness of models across multi-lingual settings. In\nresponse, we create multi-lingual functional benchmarks -- Cross-Lingual Grade\nSchool Math Symbolic (CL-GSM Symbolic) and Cross-Lingual Instruction-Following\nEval (CL-IFEval)-- by translating existing functional benchmark templates from\nEnglish to five additional languages that span the range of resources available\nfor NLP: French, Spanish, Hindi, Arabic and Yoruba. Our results reveal that\nsome static multi-lingual benchmarks capture functional performance much more\nclosely than others (i.e. across models, there is a 24%, 17% and 18% decrease\nin performance between M-GSM and CL-GSM Symbolic in English, French and Spanish\nrespectively; similarly there's a 15 - 24% performance drop across languages\nbetween Belebele and CL-IFEval, and only a 0.5% to 3% performance drop between\nM-MMLU and CL-IFEval). Similarly, we find that model robustness across\nlanguages varies significantly, with certain languages (eg. Arabic, English)\nbeing the most consistently well performing across evaluation iterations.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u5e76\u6784\u5efa\u4e86\u4e24\u9879\u591a\u8bed\u8a00\u529f\u80fd\u6027\u8bc4\u6d4b\u4efb\u52a1\uff0c\u53d1\u73b0\u73b0\u6709\u9759\u6001\u591a\u8bed\u8a00\u57fa\u51c6\u96be\u4ee5\u5168\u9762\u8861\u91cf\u6a21\u578b\u7684\u5b9e\u9645\u591a\u8bed\u8a00\u8868\u73b0\u548c\u9c81\u68d2\u6027\uff0c\u4e14\u4e0d\u540c\u8bed\u8a00\u4e4b\u95f4\u7684\u8bc4\u6d4b\u8868\u73b0\u5dee\u5f02\u660e\u663e\u3002", "motivation": "\u5f53\u524d\u591a\u8bed\u8a00\u5927\u6a21\u578b\u7684\u80fd\u529b\u8bc4\u4f30\u4e3b\u8981\u57fa\u4e8e\u9759\u6001\u6570\u636e\u57fa\u51c6\uff08\u5982Belebele\u3001M-MMLU\u548cM-GSM\uff09\uff0c\u4f46\u8fd9\u4e9b\u8bc4\u6d4b\u65e0\u6cd5\u5145\u5206\u53cd\u6620\u6a21\u578b\u5728\u5b9e\u9645\u591a\u8bed\u8a00\u73af\u5883\u4e0b\u7684\u8868\u73b0\u53ca\u9c81\u68d2\u6027\u3002\u56e0\u6b64\uff0c\u4f5c\u8005\u63d0\u51fa\u9700\u8981\u66f4\u80fd\u8003\u5bdf\u5b9e\u9645\u80fd\u529b\u7684\u591a\u8bed\u8a00\u529f\u80fd\u6027\u57fa\u51c6\u3002", "method": "\u4f5c\u8005\u5c06\u65e2\u6709\u7684\u529f\u80fd\u6027\u8bc4\u6d4b\u6a21\u677f\u4ece\u82f1\u6587\u7ffb\u8bd1\u4e3a\u4e94\u79cd\u5176\u4ed6\u8bed\u8a00\uff08\u6cd5\u8bed\u3001\u897f\u73ed\u7259\u8bed\u3001\u5370\u5730\u8bed\u3001\u963f\u62c9\u4f2f\u8bed\u3001\u7ea6\u9c81\u5df4\u8bed\uff09\uff0c\u6784\u5efa\u4e86\u8de8\u8bed\u8a00\u5c0f\u5b66\u6570\u5b66\u7b26\u53f7\u63a8\u7406\uff08CL-GSM Symbolic\uff09\u548c\u8de8\u8bed\u8a00\u6307\u4ee4\u8ddf\u968f\u8bc4\u6d4b\uff08CL-IFEval\uff09\uff0c\u4ee5\u6b64\u8861\u91cf\u6a21\u578b\u7684\u591a\u8bed\u8a00\u5b9e\u9645\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff1a\u4e0d\u540c\u9759\u6001\u57fa\u51c6\u5bf9\u529f\u80fd\u6027\u80fd\u529b\u7684\u53cd\u6620\u6709\u663e\u8457\u5dee\u5f02\uff0c\u5982M-GSM\u4e0eCL-GSM Symbolic\u5728\u82f1\u8bed\u3001\u6cd5\u8bed\u548c\u897f\u73ed\u7259\u8bed\u4e2d\u7684\u8868\u73b0\u5206\u522b\u4e0b\u964d24%\u300117%\u300118%\uff1bBelebele\u4e0eCL-IFEval\u4e4b\u95f4\u5404\u8bed\u8a00\u7684\u6027\u80fd\u4e0b\u964d\u572815-24%\uff0c\u800cM-MMLU\u4e0eCL-IFEval\u4e4b\u95f4\u4e0b\u964d\u4ec5\u67090.5%-3%\u3002\u6b64\u5916\uff0c\u4e0d\u540c\u8bed\u8a00\u6a21\u578b\u7684\u9c81\u68d2\u6027\u4e5f\u5dee\u5f02\u663e\u8457\uff0c\u90e8\u5206\u8bed\u8a00\uff08\u5982\u963f\u62c9\u4f2f\u8bed\u3001\u82f1\u8bed\uff09\u8868\u73b0\u66f4\u4e3a\u7a33\u5b9a\u548c\u4f18\u8d8a\u3002", "conclusion": "\u73b0\u6709\u9759\u6001\u591a\u8bed\u8a00\u57fa\u51c6\u4e0d\u80fd\u5145\u5206\u53cd\u6620\u6a21\u578b\u7684\u5b9e\u9645\u80fd\u529b\u8868\u73b0\uff0c\u901a\u8fc7\u529f\u80fd\u6027\u591a\u8bed\u8a00\u57fa\u51c6\u53ef\u4ee5\u66f4\u7cbe\u51c6\u8bc4\u4f30\u6a21\u578b\u7684\u5b9e\u9645\u5e94\u7528\u80fd\u529b\u548c\u8de8\u8bed\u8a00\u9c81\u68d2\u6027\u3002\u6a21\u578b\u5728\u4e0d\u540c\u8bed\u8a00\u95f4\u8868\u73b0\u53ca\u9c81\u68d2\u6027\u5b58\u5728\u8f83\u5927\u5dee\u5f02\uff0c\u63d0\u793a\u672a\u6765\u591a\u8bed\u8a00\u6a21\u578b\u8bc4\u6d4b\u5e94\u66f4\u52a0\u5173\u6ce8\u5b9e\u9645\u529f\u80fd\u4efb\u52a1\u3002"}}
{"id": "2506.20803", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.20803", "abs": "https://arxiv.org/abs/2506.20803", "authors": ["Chenglei Si", "Tatsunori Hashimoto", "Diyi Yang"], "title": "The Ideation-Execution Gap: Execution Outcomes of LLM-Generated versus Human Research Ideas", "comment": "main paper is 14 pages", "summary": "Large Language Models (LLMs) have shown promise in accelerating the\nscientific research pipeline. A key capability for this process is the ability\nto generate novel research ideas, and prior studies have found settings in\nwhich LLM-generated research ideas were judged as more novel than human-expert\nideas. However, a good idea should not simply appear to be novel, it should\nalso result in better research after being executed. To test whether\nAI-generated ideas lead to better research outcomes, we conduct an execution\nstudy by recruiting 43 expert researchers to execute randomly-assigned ideas,\neither written by experts or generated by an LLM. Each expert spent over 100\nhours implementing the idea and wrote a 4-page short paper to document the\nexperiments. All the executed projects are then reviewed blindly by expert NLP\nresearchers. Comparing the review scores of the same ideas before and after\nexecution, the scores of the LLM-generated ideas decrease significantly more\nthan expert-written ideas on all evaluation metrics (novelty, excitement,\neffectiveness, and overall; p < 0.05), closing the gap between LLM and human\nideas observed at the ideation stage. When comparing the aggregated review\nscores from the execution study, we even observe that for many metrics there is\na flip in rankings where human ideas score higher than LLM ideas. This\nideation-execution gap highlights the limitations of current LLMs in generating\ntruly effective research ideas and the challenge of evaluating research ideas\nin the absence of execution outcomes.", "AI": {"tldr": "LLM\u80fd\u751f\u6210\u65b0\u9896\u7814\u7a76\u70b9\u5b50\uff0c\u4f46\u5b9e\u9645\u6267\u884c\u6548\u679c\u900a\u4e8e\u4eba\u7c7b\u4e13\u5bb6\uff0c\u663e\u793a\u73b0\u6709\u6a21\u578b\u5728\u79d1\u7814\u521b\u65b0\u4e0a\u4ecd\u6709\u9650\u5236\uff0c\u4ec5\u9760\u65b0\u9896\u6027\u8bc4\u5224\u4e0d\u53ef\u9760\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u79d1\u7814\u6d41\u7a0b\u4e2d\u8868\u73b0\u51fa\u63a8\u52a8\u521b\u65b0\u7684\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u5728\u751f\u6210\u65b0\u9896\u7814\u7a76\u60f3\u6cd5\u65b9\u9762\u4f18\u4e8e\u4eba\u7c7b\u4e13\u5bb6\uff0c\u4f46\u4ec5\u5177\u5907\u65b0\u9896\u6027\u5e76\u4e0d\u80fd\u4fdd\u8bc1\u7814\u7a76\u6210\u679c\u8d28\u91cf\u3002\u8be5\u6587\u5e0c\u671b\u63a2\u7a76\uff1aLLM\u751f\u6210\u7684\u70b9\u5b50\u662f\u5426\u771f\u7684\u80fd\u5e26\u6765\u66f4\u597d\u7684\u5b9e\u9645\u7814\u7a76\u6210\u679c\u3002", "method": "\u4f5c\u8005\u7ec4\u7ec7\u4e8643\u540dNLP\u9886\u57df\u4e13\u5bb6\uff0c\u968f\u673a\u62bd\u53d6\u7531LLM\u548c\u4eba\u7c7b\u4e13\u5bb6\u7f16\u5199\u7684\u7814\u7a76\u70b9\u5b50\uff0c\u6bcf\u4f4d\u4e13\u5bb6\u6295\u5165100\u5c0f\u65f6\u4ee5\u4e0a\u5b9e\u65bd\u8be5\u70b9\u5b50\uff0c\u5e76\u4ee54\u9875\u8bba\u6587\u8bb0\u5f55\u5b9e\u9a8c\u8fc7\u7a0b\uff0c\u6700\u7ec8\u7531\u53e6\u4e00\u6279\u4e13\u5bb6\u76f2\u5ba1\u8bc4\u4ef7\u4e0d\u540c\u6765\u6e90\u70b9\u5b50\u7684\u6267\u884c\u6210\u679c\uff0c\u5bf9\u6bd4\u5176\u5728\u5404\u9879\u6307\u6807\uff08\u65b0\u9896\u6027\u3001\u6fc0\u52a8\u6027\u3001\u6709\u6548\u6027\u3001\u603b\u4f53\u8bc4\u5206\uff09\u4e0a\u7684\u8868\u73b0\u548c\u6392\u5e8f\u3002", "result": "\u867d\u7136LLM\u751f\u6210\u7684\u70b9\u5b50\u5728\u6700\u521d\u7684\u65b0\u9896\u6027\u8bc4\u4ef7\u9636\u6bb5\u6709\u4f18\u52bf\uff0c\u4f46\u88ab\u6267\u884c\u540e\uff0c\u5b83\u4eec\u7684\u5404\u9879\u8bc4\u5ba1\u5206\u6570\u660e\u663e\u4e0b\u6ed1\uff0c\u8dcc\u5e45\u663e\u8457\u5927\u4e8e\u4eba\u7c7b\u4e13\u5bb6\u70b9\u5b50\uff0c\u751a\u81f3\u5728\u4eba\u7c7b\u4e13\u5bb6\u5f97\u5206\u9ad8\u4e8eLLM\u7684\u73b0\u8c61\u4e0b\u5b9e\u73b0\u6392\u540d\u53cd\u8f6c\u3002\u8fd9\u8868\u660e\u5728\u5b9e\u9645\u79d1\u7814\u4ea7\u51fa\u65b9\u9762\uff0cLLM\u70b9\u5b50\u7684\u4f18\u52bf\u5e76\u672a\u5ef6\u7eed\uff0c\u51f8\u663e\u51fa\u73b0\u6709\u6a21\u578b\u5728\u751f\u6210\u201c\u53ef\u6267\u884c\u3001\u6709\u6548\u201d\u7814\u7a76\u60f3\u6cd5\u4e0a\u7684\u4e0d\u8db3\u3002", "conclusion": "\u5f53\u524dLLM\u867d\u7136\u5728\u751f\u6210\u770b\u4f3c\u65b0\u9896\u7684\u79d1\u7814\u70b9\u5b50\u65f6\u63a5\u8fd1\u6216\u8d85\u8fc7\u4eba\u7c7b\u4e13\u5bb6\uff0c\u4f46\u5728\u8fd9\u4e9b\u70b9\u5b50\u5b9e\u9645\u88ab\u5b9e\u65bd\u4ee5\u540e\uff0c\u6548\u7528\u548c\u6210\u679c\u8bc4\u4ef7\u663e\u8457\u843d\u540e\u4e8e\u4eba\u7c7b\u4e13\u5bb6\u3002\u8fd9\u66b4\u9732\u4e86\u8bc4\u4f30LLM\u79d1\u7814\u80fd\u529b\u65f6\u5355\u9760\u201c\u60f3\u6cd5\u201d\u672c\u8eab\u800c\u975e\u5b9e\u8df5\u7ed3\u679c\u7684\u5c40\u9650\u6027\u3002\u9700\u8b66\u60d5LLM\u65b0\u9896\u6027\u5047\u8c61\uff0c\u63a8\u52a8\u5bf9\u5176\u751f\u6210\u7814\u7a76\u70b9\u5b50\u5b9e\u9645\u53ef\u6267\u884c\u6027\u7684\u8fdb\u4e00\u6b65\u8bc4\u4f30\u548c\u6539\u8fdb\u3002"}}
{"id": "2506.20763", "categories": ["cs.CE", "cs.NA", "math.NA", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2506.20763", "abs": "https://arxiv.org/abs/2506.20763", "authors": ["Y. Navidtehrani", "C. Beteg\u00f3n", "E. Mart\u00ednez-Pa\u00f1eda"], "title": "A generalised framework for phase field-based modelling of coupled problems: application to thermo-mechanical fracture, hydraulic fracture, hydrogen embrittlement and corrosion", "comment": null, "summary": "We present a novel, generalised formulation to treat coupled structural\nintegrity problems by combining phase field and multi-physics modelling. The\napproach exploits the versatility of the heat transfer equation and is\ntherefore well suited to be adopted in commercial finite element packages,\nrequiring only integration point-level implementation. This aspect is\ndemonstrated here by implementing coupled, multi-variable phenomena through\nsimple \\texttt{UMAT} and \\texttt{UMATHT} subroutines in the finite element\npackage \\texttt{Abaqus}. The generalised theoretical and computational\nframework presented is particularised to four problems of engineering and\nscientific relevance: thermo-mechanical fracture, hydraulic fracture,\nhydrogen-assisted cracking and metallic corrosion. 2D and 3D problems are\nconsidered. The results reveal a very good agreement with experimental data,\nand existing numerical and analytical solutions.The user subroutines developed\nare made freely available at https://mechmat.web.ox.ac.uk/codes.", "AI": {"tldr": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u76f8\u573a\u548c\u591a\u7269\u7406\u573a\u8026\u5408\u7684\u65b0\u65b9\u6cd5\uff0c\u80fd\u5728Abaqus\u6709\u9650\u5143\u8f6f\u4ef6\u4e2d\u901a\u8fc7\u7b80\u5355\u5b50\u7a0b\u5e8f\u5904\u7406\u591a\u79cd\u7ed3\u6784\u5931\u6548\u95ee\u9898\uff0c\u7ecf\u9a8c\u8bc1\u8be5\u65b9\u6cd5\u51c6\u786e\u4e14\u5177\u6709\u666e\u9002\u6027\uff0c\u76f8\u5173\u4ee3\u7801\u5df2\u516c\u5f00\u3002", "motivation": "\u73b0\u6709\u5bf9\u7ed3\u6784\u5b8c\u6574\u6027\u8026\u5408\u95ee\u9898\u7684\u5206\u6790\u65b9\u6cd5\u6709\u9650\uff0c\u7f3a\u4e4f\u4e00\u79cd\u666e\u9002\u4e14\u6613\u4e8e\u5728\u5546\u4e1a\u6709\u9650\u5143\u8f6f\u4ef6\u4e2d\u5b9e\u65bd\u7684\u591a\u7269\u7406\u573a\u8026\u5408\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u5904\u7406\u70ed-\u529b\u3001\u6d41\u4f53-\u7ed3\u6784\u3001\u5316\u5b66\u8150\u8680\u7b49\u590d\u6742\u95ee\u9898\u65f6\u66f4\u663e\u4e0d\u8db3\u3002", "method": "\u901a\u8fc7\u5c06\u76f8\u573a\u6a21\u578b\u4e0e\u591a\u7269\u7406\u573a\u5efa\u6a21\u76f8\u7ed3\u5408\uff0c\u5229\u7528\u70ed\u4f20\u5bfc\u65b9\u7a0b\u7684\u901a\u7528\u6027\uff0c\u4ec5\u9700\u5728\u79ef\u5206\u70b9\u5c42\u7ea7\u8fdb\u884c\u7b80\u5355\u5b50\u7a0b\u5e8f\uff08UMAT\u548cUMATHT\uff09\u5f00\u53d1\uff0c\u65e0\u9700\u5e7f\u6cdb\u4fee\u6539\u6709\u9650\u5143\u4ee3\u7801\uff0c\u5373\u53ef\u5b9e\u73b0\u590d\u6742\u8026\u5408\u591a\u53d8\u91cf\u73b0\u8c61\u7684\u6a21\u62df\u3002", "result": "\u65b9\u6cd5\u5df2\u5728\u70ed-\u529b\u65ad\u88c2\u3001\u6c34\u529b\u65ad\u88c2\u3001\u6c22\u81f4\u5f00\u88c2\u548c\u91d1\u5c5e\u8150\u8680\u7b49\u5de5\u7a0b\u5b9e\u9645\u4e0e\u79d1\u5b66\u76f8\u5173\u95ee\u9898\u4e0a\u8fdb\u884c2D\u30013D\u6570\u503c\u9a8c\u8bc1\uff0c\u4e0e\u5b9e\u9a8c\u6570\u636e\u53ca\u5df2\u6709\u6570\u503c\u3001\u89e3\u6790\u89e3\u7b26\u5408\u826f\u597d\u3002\u7528\u6237\u81ea\u7f16\u5b50\u7a0b\u5e8f\u5df2\u5f00\u6e90\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5730\u901a\u8fc7\u6709\u9650\u5143\u8f6f\u4ef6Abaqus\u89e3\u51b3\u591a\u79cd\u4e0e\u7ed3\u6784\u5b8c\u6574\u6027\u76f8\u5173\u7684\u8026\u5408\u95ee\u9898\uff0c\u5177\u6709\u8f83\u597d\u7684\u901a\u7528\u6027\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2506.20821", "categories": ["cs.CL", "cs.AI", "cs.CE", "68T50, 68T07 (Primary) 68P20, 91G15, 91G70, 68U10 (Secondary)", "I.2.7; I.2.10; H.3.3; H.2.8; I.5.4; J.1"], "pdf": "https://arxiv.org/pdf/2506.20821", "abs": "https://arxiv.org/abs/2506.20821", "authors": ["Chinmay Gondhalekar", "Urjitkumar Patel", "Fang-Chun Yeh"], "title": "MultiFinRAG: An Optimized Multimodal Retrieval-Augmented Generation (RAG) Framework for Financial Question Answering", "comment": "Preprint Copy", "summary": "Financial documents--such as 10-Ks, 10-Qs, and investor presentations--span\nhundreds of pages and combine diverse modalities, including dense narrative\ntext, structured tables, and complex figures. Answering questions over such\ncontent often requires joint reasoning across modalities, which strains\ntraditional large language models (LLMs) and retrieval-augmented generation\n(RAG) pipelines due to token limitations, layout loss, and fragmented\ncross-modal context. We introduce MultiFinRAG, a retrieval-augmented generation\nframework purpose-built for financial QA. MultiFinRAG first performs multimodal\nextraction by grouping table and figure images into batches and sending them to\na lightweight, quantized open-source multimodal LLM, which produces both\nstructured JSON outputs and concise textual summaries. These outputs, along\nwith narrative text, are embedded and indexed with modality-aware similarity\nthresholds for precise retrieval. A tiered fallback strategy then dynamically\nescalates from text-only to text+table+image contexts when necessary, enabling\ncross-modal reasoning while reducing irrelevant context. Despite running on\ncommodity hardware, MultiFinRAG achieves 19 percentage points higher accuracy\nthan ChatGPT-4o (free-tier) on complex financial QA tasks involving text,\ntables, images, and combined multimodal reasoning.", "AI": {"tldr": "\u901a\u8fc7\u591a\u6a21\u6001LLM\u5206\u5c42\u56de\u9000\uff0cMultiFinRAG\u80fd\u66f4\u9ad8\u6548\u3001\u51c6\u786e\u5730\u5904\u7406\u590d\u6742\u91d1\u878d\u6587\u4ef6\u95ee\u7b54\uff0c\u5728\u5e38\u89c4\u786c\u4ef6\u4e0a\u5b9e\u73b0\u6bd4\u514d\u8d39\u7248ChatGPT-4o\u9ad819\u4e2a\u767e\u5206\u70b9\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u6709\u91d1\u878d\u6587\u4ef6\u598210-K\u300110-Q\u7b49\u5305\u542b\u5927\u91cf\u53d9\u8ff0\u6027\u6587\u672c\u3001\u7ed3\u6784\u5316\u8868\u683c\u53ca\u590d\u6742\u56fe\u50cf\u7b49\u591a\u79cd\u6a21\u6001\u4fe1\u606f\u3002\u4f20\u7edf\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u65b9\u6cd5\u5728\u5904\u7406\u5982\u6b64\u591a\u6a21\u6001\u3001\u957f\u8de8\u5ea6\u5185\u5bb9\u65f6\uff0c\u53d7\u5230token\u9650\u5236\u3001\u7248\u5f0f\u4fe1\u606f\u4e22\u5931\u548c\u8de8\u6a21\u6001\u4e0a\u4e0b\u6587\u788e\u7247\u5316\u7b49\u95ee\u9898\u7684\u5f71\u54cd\uff0c\u96be\u4ee5\u5b9e\u73b0\u7cbe\u786e\u548c\u9ad8\u6548\u7684\u95ee\u7b54\u63a8\u7406\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86MultiFinRAG\uff0c\u4e00\u79cd\u4e13\u4e3a\u91d1\u878d\u95ee\u7b54\u8bbe\u8ba1\u7684\u591a\u6a21\u6001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6846\u67b6\u3002MultiFinRAG\u5c06\u8868\u683c\u4e0e\u56fe\u50cf\u6279\u91cf\u5316\u8f93\u5165\u5230\u8f7b\u91cf\u7ea7\u591a\u6a21\u6001LLM\uff0c\u751f\u6210\u7ed3\u6784\u5316JSON\u548c\u7b80\u6d01\u6587\u672c\u6458\u8981\uff0c\u5e76\u7ed3\u5408\u539f\u59cb\u6587\u672c\uff0c\u901a\u8fc7\u6a21\u6001\u611f\u77e5\u7684\u53ec\u56de\u7b56\u7565\u8fdb\u884c\u5d4c\u5165\u4e0e\u7d22\u5f15\u3002\u91c7\u7528\u5206\u5c42\u56de\u9000\u673a\u5236\uff0c\u6839\u636e\u9700\u8981\u52a8\u6001\u4ece\u6587\u672c\u6269\u5c55\u5230\u6587\u672c+\u8868\u683c+\u56fe\u50cf\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u8de8\u6a21\u6001\u63a8\u7406\u5e76\u964d\u4f4e\u65e0\u5173\u5185\u5bb9\u5e72\u6270\u3002", "result": "MultiFinRAG\u5728\u65e0\u9700\u9ad8\u7aef\u7b97\u529b\u73af\u5883\u4e0b\uff0c\u5728\u5305\u542b\u6587\u672c\u3001\u8868\u683c\u3001\u56fe\u50cf\u53ca\u591a\u6a21\u6001\u7ec4\u5408\u7684\u590d\u6742\u91d1\u878d\u95ee\u7b54\u4efb\u52a1\u4e0a\uff0c\u51c6\u786e\u7387\u6bd4ChatGPT-4o\uff08\u514d\u8d39\u7248\u672c\uff09\u9ad819\u4e2a\u767e\u5206\u70b9\uff0c\u663e\u793a\u51fa\u5353\u8d8a\u7684\u591a\u6a21\u6001\u91d1\u878d\u95ee\u7b54\u80fd\u529b\u3002", "conclusion": "MultiFinRAG\u5728\u7ed3\u5408\u591a\u6a21\u6001\u5185\u5bb9\u8fdb\u884c\u91d1\u878d\u95ee\u7b54\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u4e3b\u6d41\u6a21\u578b\uff0c\u80fd\u591f\u7a81\u7834\u5355\u4e00\u6587\u672c\u6216\u7b80\u5355RAG\u65b9\u6cd5\u5728\u5b9e\u9645\u91d1\u878d\u6587\u6863\u573a\u666f\u4e0b\u7684\u80fd\u529b\u74f6\u9888\u3002"}}
{"id": "2506.20773", "categories": ["cs.CE"], "pdf": "https://arxiv.org/pdf/2506.20773", "abs": "https://arxiv.org/abs/2506.20773", "authors": ["Stephen T. Castonguay", "Joshua B. Fernandes", "Michael A. Puso", "Sylvie Aubry"], "title": "A Hereditary Integral, Transient Network Approach to Modeling Permanent Set and Viscoelastic Response in Polymers", "comment": null, "summary": "An efficient numerical framework is presented for modeling viscoelasticity\nand permanent set of polymers. It is based on the hereditary integral form of\ntransient network theory, in which polymer chains belong to distinct networks\neach with different natural equilibrium states. Chains continually detach from\npreviously formed networks and reattach to new networks in a state of zero\nstress. The free energy of these networks is given in terms of the deformation\ngradient relative to the configuration at which the network was born. A\ndecomposition of the kernel for various free energies allows for a recurrence\nrelationship to be established, bypassing the need to integrate over all time\nhistory. The technique is established for both highly compressible and nearly\nincompressible materials through the use of neo-Hookean, Blatz-Ko, Yeoh, and\nOgden-Hill material models. Multiple examples are presented showing the ability\nto handle rate-dependent response and residual strains under complex loading\nhistories.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u805a\u5408\u7269\u9ecf\u5f39\u6027\u548c\u6c38\u4e45\u5b9a\u578b\u6570\u503c\u6a21\u62df\u65b9\u6cd5\uff0c\u7b80\u5316\u4e86\u5386\u53f2\u79ef\u5206\u8fc7\u7a0b\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u6750\u6599\u6a21\u578b\uff0c\u5e76\u80fd\u51c6\u786e\u6355\u6349\u590d\u6742\u52a0\u8f7d\u4e0b\u7684\u6750\u6599\u884c\u4e3a\u3002", "motivation": "\u73b0\u6709\u805a\u5408\u7269\u6750\u6599\u7684\u6570\u503c\u5efa\u6a21\u65b9\u6cd5\u5728\u5904\u7406\u5386\u53f2\u4f9d\u8d56\u6027\u7684\u9ecf\u5f39\u6027\u548c\u6c38\u4e45\u5b9a\u578b\u65b9\u9762\u8ba1\u7b97\u590d\u6742\u6027\u9ad8\u3002\u8bba\u6587\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u6570\u503c\u6846\u67b6\uff0c\u51cf\u5c11\u8ba1\u7b97\u91cf\u5e76\u63d0\u9ad8\u9002\u7528\u6027\u3002", "method": "\u57fa\u4e8e\u77ac\u6001\u7f51\u7edc\u7406\u8bba\u7684\u9057\u4f20\u79ef\u5206\u5f62\u5f0f\uff0c\u5bf9\u805a\u5408\u7269\u94fe\u7684\u4e0d\u540c\u7f51\u7edc\u8fdb\u884c\u5efa\u6a21\u3002\u901a\u8fc7\u80fd\u91cf\u6838\u5206\u89e3\uff0c\u5efa\u7acb\u9012\u63a8\u5173\u7cfb\uff0c\u4ece\u800c\u907f\u514d\u5bf9\u6574\u4e2a\u65f6\u95f4\u5386\u53f2\u8fdb\u884c\u79ef\u5206\u3002\u8be5\u65b9\u6cd5\u5e94\u7528\u4e8eneo-Hookean\u3001Blatz-Ko\u3001Yeoh\u548cOgden-Hill\u6750\u6599\u6a21\u578b\uff0c\u6db5\u76d6\u9ad8\u53ef\u538b\u7f29\u6027\u548c\u8fd1\u4e0d\u53ef\u538b\u7f29\u6027\u7684\u6750\u6599\u3002", "result": "\u63d0\u51fa\u7684\u6846\u67b6\u80fd\u591f\u9ad8\u6548\u6a21\u62df\u805a\u5408\u7269\u9ecf\u5f39\u6027\u548c\u6c38\u4e45\u5b9a\u578b\uff0c\u5e76\u9002\u5e94\u4e0d\u540c\u6750\u6599\u7c7b\u578b\u3002\u901a\u8fc7\u591a\u4e2a\u6848\u4f8b\u9a8c\u8bc1\u4e86\u5176\u5bf9\u901f\u7387\u76f8\u5173\u54cd\u5e94\u548c\u590d\u6742\u52a0\u8f7d\u5386\u53f2\u4e0b\u6b8b\u4f59\u5e94\u53d8\u7684\u9884\u6d4b\u80fd\u529b\u3002", "conclusion": "\u6570\u503c\u6846\u67b6\u80fd\u591f\u9ad8\u6548\u5730\u6a21\u62df\u805a\u5408\u7269\u7684\u9ecf\u5f39\u6027\u548c\u6c38\u4e45\u5b9a\u578b\u6548\u5e94\uff0c\u5e76\u9002\u7528\u4e8e\u4e0d\u540c\u7c7b\u578b\u7684\u6750\u6599\u6a21\u578b\u3002\u8be5\u65b9\u6cd5\u80fd\u5904\u7406\u52a0\u8f7d\u5386\u53f2\u590d\u6742\u3001\u901f\u7387\u76f8\u5173\u7684\u54cd\u5e94\u548c\u6b8b\u4f59\u5e94\u53d8\u3002"}}
{"id": "2506.20982", "categories": ["cs.CY", "cs.RO", "K.3.1"], "pdf": "https://arxiv.org/pdf/2506.20982", "abs": "https://arxiv.org/abs/2506.20982", "authors": ["Martin Ruskov"], "title": "Our Coding Adventure: Using LLMs to Personalise the Narrative of a Tangible Programming Robot for Preschoolers", "comment": "accepted at D-SAIL Workshop - Transformative Curriculum Design:\n  Digitalization, Sustainability, and AI Literacy for 21st Century Learning", "summary": "Finding balanced ways to employ Large Language Models (LLMs) in education is\na challenge due to inherent risks of poor understanding of the technology and\nof a susceptible audience. This is particularly so with younger children, who\nare known to have difficulties with pervasive screen time. Working with a\ntangible programming robot called Cubetto, we propose an approach to benefit\nfrom the capabilities of LLMs by employing such models in the preparation of\npersonalised storytelling, necessary for preschool children to get accustomed\nto the practice of commanding the robot. We engage in action research to\ndevelop an early version of a formalised process to rapidly prototype game\nstories for Cubetto. Our approach has both reproducible results, because it\nemploys open weight models, and is model-agnostic, because we test it with 5\ndifferent LLMs. We document on one hand the process, the used materials and\nprompts, and on the other the learning experience and outcomes. We deem the\ngeneration successful for the intended purposes of using the results as a\nteacher aid. Testing the models on 4 different task scenarios, we encounter\nissues of consistency and hallucinations and document the corresponding\nevaluation process and attempts (some successful and some not) to overcome\nthese issues. Importantly, the process does not expose children to LLMs\ndirectly. Rather, the technology is used to help teachers easily develop\npersonalised narratives on children's preferred topics. We believe our method\nis adequate for preschool classes and we are planning to further experiment in\nreal-world educational settings.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u7528\u5f00\u6e90LLM\u4e3a\u5b66\u524d\u513f\u7ae5\u7f16\u7a0b\u673a\u5668\u4eba\u5236\u5b9a\u4e2a\u6027\u5316\u6545\u4e8b\uff0c\u5e2e\u52a9\u6559\u5e08\u6559\u5b66\uff0c\u4e0d\u8ba9\u513f\u7ae5\u76f4\u63a5\u63a5\u89e6LLM\uff0c\u65b9\u6cd5\u9a8c\u8bc1\u6709\u6548\u4f46\u5b58\u5728\u90e8\u5206AI\u5e7b\u89c9\u7b49\u95ee\u9898\uff0c\u9002\u5b9c\u5e7c\u6559\u5e94\u7528\u3002", "motivation": "\u5f53\u524d\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5e94\u7528\u4e8e\u6559\u80b2\u9886\u57df\u5b58\u5728\u98ce\u9669\uff0c\u5c24\u5176\u662f\u9488\u5bf9\u5e74\u5e7c\u513f\u7ae5\uff0c\u4ed6\u4eec\u5bb9\u6613\u56e0\u6280\u672f\u4e0d\u7406\u89e3\u548c\u5c4f\u5e55\u65f6\u95f4\u8fc7\u957f\u800c\u53d7\u5f71\u54cd\u3002\u8be5\u7814\u7a76\u65e8\u5728\u5bfb\u627e\u5e73\u8861\u7684\u65b9\u5f0f\u5b89\u5168\u5730\u5229\u7528LLM\u63d0\u5347\u5e7c\u513f\u6559\u80b2\u4f53\u9a8c\u3002", "method": "\u91c7\u7528\u884c\u52a8\u7814\u7a76\u6cd5\uff0c\u56f4\u7ed5\u53ef\u7f16\u7a0b\u673a\u5668\u4ebaCubetto\uff0c\u5236\u5b9a\u5e76\u5b9e\u73b0\u4e00\u4e2a\u6b63\u5f0f\u5316\u6d41\u7a0b\uff0c\u5229\u7528\u5f00\u6e90LLM\u751f\u6210\u7528\u4e8e\u5f15\u5bfc\u5e7c\u513f\u64cd\u4f5c\u673a\u5668\u4eba\u7684\u4e2a\u6027\u5316\u6545\u4e8b\u3002\u5b9e\u9a8c\u6db5\u76d65\u79cd\u4e0d\u540c\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5177\u4f53\u8bb0\u5f55\u4e86\u5b9e\u9a8c\u6d41\u7a0b\u3001\u7d20\u6750\u4f7f\u7528\u548c\u63d0\u793a\u8bcd\u8bbe\u8ba1\uff0c\u540c\u65f6\u8bc4\u4f30\u4e86\u6559\u5e08\u548c\u5e7c\u513f\u7684\u5b9e\u9645\u5b66\u4e60\u4f53\u9a8c\u53ca\u7ed3\u679c\u3002", "result": "\u901a\u8fc7\u5bf94\u79cd\u4e0d\u540c\u4efb\u52a1\u573a\u666f\u7684\u6d4b\u8bd5\uff0c\u53d1\u73b0\u751f\u6210\u5185\u5bb9\u7528\u4e8e\u6559\u5e08\u8f85\u52a9\u6559\u5b66\u57fa\u672c\u6210\u529f\uff0c\u4f46\u4e5f\u5b58\u5728\u4e00\u81f4\u6027\u548c\u5e7b\u89c9\u95ee\u9898\uff0c\u5e76\u5c1d\u8bd5\u4e86\u591a\u79cd\u65b9\u6cd5\uff08\u90e8\u5206\u6709\u6548\uff09\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002\u6574\u4e2a\u8fc7\u7a0b\u907f\u514d\u8ba9\u513f\u7ae5\u76f4\u63a5\u63a5\u89e6LLM\uff0c\u4fdd\u969c\u5176\u5b89\u5168\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u5e2e\u52a9\u6559\u5e08\u5feb\u901f\u751f\u6210\u4e2a\u6027\u5316\u6545\u4e8b\uff0c\u9002\u7528\u4e8e\u5b66\u524d\u73ed\u6559\u80b2\uff0c\u6709\u671b\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u8fdb\u4e00\u6b65\u5b9e\u9a8c\u548c\u63a8\u5e7f\u3002"}}
{"id": "2506.20702", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2506.20702", "abs": "https://arxiv.org/abs/2506.20702", "authors": ["Yoshua Bengio", "Tegan Maharaj", "Luke Ong", "Stuart Russell", "Dawn Song", "Max Tegmark", "Lan Xue", "Ya-Qin Zhang", "Stephen Casper", "Wan Sie Lee", "S\u00f6ren Mindermann", "Vanessa Wilfred", "Vidhisha Balachandran", "Fazl Barez", "Michael Belinsky", "Imane Bello", "Malo Bourgon", "Mark Brakel", "Sim\u00e9on Campos", "Duncan Cass-Beggs", "Jiahao Chen", "Rumman Chowdhury", "Kuan Chua Seah", "Jeff Clune", "Juntao Dai", "Agnes Delaborde", "Nouha Dziri", "Francisco Eiras", "Joshua Engels", "Jinyu Fan", "Adam Gleave", "Noah Goodman", "Fynn Heide", "Dan Hendrycks", "Cyrus Hodes", "Bryan Low Kian Hsiang", "Minlie Huang", "Sami Jawhar", "Wang Jingyu", "Adam Tauman Kalai", "Meindert Kamphuis", "Mohan Kankanhalli", "Subhash Kantamneni", "Mathias Bonde Kirk", "Thomas Kwa", "Jeffrey Ladish", "Kwok-Yan Lam", "Wan Lee Sie", "Taewhi Lee", "Xiaojian Li", "Jiajun Liu", "Chaochao Lu", "Yifan Mai", "Richard Mallah", "Julian Michael", "Nick Mo\u00ebs", "Simon M\u00f6ller", "Kihyuk Nam", "Kwan Yee Ng", "Mark Nitzberg", "Besmira Nushi", "Se\u00e1n O h\u00c9igeartaigh", "Alejandro Ortega", "Pierre Peign\u00e9", "James Petrie", "Benjamin Prud'Homme", "Reihaneh Rabbany", "Nayat Sanchez-Pi", "Sarah Schwettmann", "Buck Shlegeris", "Saad Siddiqui", "Aradhana Sinha", "Mart\u00edn Soto", "Cheston Tan", "Dong Ting", "Robert Trager", "Brian Tse", "Anthony Tung K. H.", "Vanessa Wilfred", "John Willes", "Denise Wong", "Wei Xu", "Rongwu Xu", "Yi Zeng", "HongJiang Zhang", "Djordje \u017dikeli\u0107"], "title": "The Singapore Consensus on Global AI Safety Research Priorities", "comment": "Final report from the \"2025 Singapore Conference on AI (SCAI)\" held\n  April 26: https://www.scai.gov.sg/2025/scai2025-report", "summary": "Rapidly improving AI capabilities and autonomy hold significant promise of\ntransformation, but are also driving vigorous debate on how to ensure that AI\nis safe, i.e., trustworthy, reliable, and secure. Building a trusted ecosystem\nis therefore essential -- it helps people embrace AI with confidence and gives\nmaximal space for innovation while avoiding backlash.\n  The \"2025 Singapore Conference on AI (SCAI): International Scientific\nExchange on AI Safety\" aimed to support research in this space by bringing\ntogether AI scientists across geographies to identify and synthesise research\npriorities in AI safety. This resulting report builds on the International AI\nSafety Report chaired by Yoshua Bengio and backed by 33 governments. By\nadopting a defence-in-depth model, this report organises AI safety research\ndomains into three types: challenges with creating trustworthy AI systems\n(Development), challenges with evaluating their risks (Assessment), and\nchallenges with monitoring and intervening after deployment (Control).", "AI": {"tldr": "\u8be5\u62a5\u544a\u56f4\u7ed5AI\u5b89\u5168\u4e09\u5927\u6838\u5fc3\u73af\u8282\u2014\u2014\u5f00\u53d1\u3001\u8bc4\u4f30\u3001\u90e8\u7f72\u540e\u63a7\u5236\u2014\u2014\u7cfb\u7edf\u68b3\u7406\u4e86\u7814\u7a76\u91cd\u70b9\u548c\u6311\u6218\uff0c\u4e3a\u56fd\u9645AI\u5b89\u5168\u7814\u7a76\u5408\u4f5c\u548c\u751f\u6001\u6784\u5efa\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002", "motivation": "\u968f\u7740AI\u80fd\u529b\u548c\u81ea\u4e3b\u6027\u7684\u5927\u5e45\u63d0\u5347\uff0cAI\u5e26\u6765\u7684\u53d8\u9769\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u4e0e\u6b64\u540c\u65f6\uff0cAI\u5982\u4f55\u5b89\u5168\u3001\u53ef\u9760\u3001\u53ef\u4fe1\u6210\u4e3a\u70ed\u8bae\u8bdd\u9898\u3002\u56e0\u6b64\uff0c\u5efa\u7acb\u503c\u5f97\u4fe1\u8d56\u7684AI\u751f\u6001\u4f53\u7cfb\u5df2\u7ecf\u53d8\u5f97\u81f3\u5173\u91cd\u8981\uff0c\u4ee5\u589e\u5f3a\u516c\u4f17\u4fe1\u5fc3\u5e76\u907f\u514d\u521b\u65b0\u906d\u5230\u62b5\u5236\u3002", "method": "\u901a\u8fc72025\u5e74\u65b0\u52a0\u5761AI\u5927\u4f1a\uff0c\u56fd\u9645\u79d1\u5b66\u5bb6\u96c6\u4f1a\uff0c\u786e\u5b9a\u5e76\u6574\u5408\u4e86AI\u5b89\u5168\u9886\u57df\u7684\u7814\u7a76\u91cd\u70b9\u3002\u672c\u62a5\u544a\u501f\u9274\u4e86\u7531Yoshua Bengio\u4e3b\u6301\u300133\u56fd\u652f\u6301\u7684\u56fd\u9645AI\u5b89\u5168\u62a5\u544a\uff0c\u5e76\u91c7\u7528\u5206\u5c42\u9632\u5fa1\uff08defence-in-depth\uff09\u6a21\u578b\uff0c\u5c06AI\u5b89\u5168\u7814\u7a76\u5206\u4e3a\u5f00\u53d1\u3001\u8bc4\u4f30\u548c\u63a7\u5236\u4e09\u4e2a\u7ef4\u5ea6\u3002", "result": "\u62a5\u544a\u660e\u786e\u4e86AI\u5b89\u5168\u7814\u7a76\u7684\u4e09\u5927\u6838\u5fc3\u9886\u57df\uff1a\uff081\uff09\u5f00\u53d1\u9636\u6bb5\u7684\u53ef\u4fe1\u7cfb\u7edf\u6784\u5efa\u6311\u6218\uff1b\uff082\uff09\u8bc4\u4f30\u9636\u6bb5\u7684\u98ce\u9669\u8bc4\u4f30\u6311\u6218\uff1b\uff083\uff09\u90e8\u7f72\u540e\u7684\u76d1\u6d4b\u4e0e\u5e72\u9884\u6311\u6218\u3002", "conclusion": "\u672c\u62a5\u544a\u63a8\u52a8\u8de8\u56fdAI\u5b89\u5168\u7814\u7a76\u534f\u4f5c\uff0c\u4e3a\u672a\u6765AI\u5b89\u5168\u7814\u7a76\u548c\u653f\u7b56\u89c4\u5212\u5960\u5b9a\u4e86\u57fa\u7840\u3002\u5efa\u7acb\u53ef\u4fe1AI\u751f\u6001\u7cfb\u7edf\u5bf9\u4e8e\u91ca\u653eAI\u6f5c\u529b\u5e76\u907f\u514d\u793e\u4f1a\u53cd\u5f39\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2506.20822", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.20822", "abs": "https://arxiv.org/abs/2506.20822", "authors": ["Quintin Myers", "Yanjun Gao"], "title": "Uncovering Hidden Violent Tendencies in LLMs: A Demographic Analysis via Behavioral Vignettes", "comment": "Under review", "summary": "Large language models (LLMs) are increasingly proposed for detecting and\nresponding to violent content online, yet their ability to reason about morally\nambiguous, real-world scenarios remains underexamined. We present the first\nstudy to evaluate LLMs using a validated social science instrument designed to\nmeasure human response to everyday conflict, namely the Violent Behavior\nVignette Questionnaire (VBVQ). To assess potential bias, we introduce\npersona-based prompting that varies race, age, and geographic identity within\nthe United States. Six LLMs developed across different geopolitical and\norganizational contexts are evaluated under a unified zero-shot setting. Our\nstudy reveals two key findings: (1) LLMs surface-level text generation often\ndiverges from their internal preference for violent responses; (2) their\nviolent tendencies vary across demographics, frequently contradicting\nestablished findings in criminology, social science, and psychology.", "AI": {"tldr": "\u9996\u6b21\u5229\u7528VBVQ\u5de5\u5177\u7cfb\u7edf\u8bc4\u4f30\u591a\u4e2aLLM\u5728\u9053\u5fb7\u51b2\u7a81\u548c\u4eba\u53e3\u7edf\u8ba1\u53d8\u5f02\u4e0b\u7684\u54cd\u5e94\uff0c\u53d1\u73b0\u5176\u5bf9\u66b4\u529b\u503e\u5411\u7684\u8868\u5f81\u5e38\u4e0e\u4f20\u7edf\u793e\u4f1a\u79d1\u5b66\u7ed3\u679c\u4e0d\u7b26\uff0c\u63d0\u793a\u73b0\u6709LLM\u5728\u8be5\u9886\u57df\u5e94\u7528\u5b58\u5728\u663e\u8457\u5c40\u9650\u3002", "motivation": "\u5f53\u524d\u8d8a\u6765\u8d8a\u591a\u5730\u5c06\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7528\u4e8e\u66b4\u529b\u5185\u5bb9\u68c0\u6d4b\u4e0e\u5e94\u5bf9\uff0c\u4f46\u5176\u5bf9\u73b0\u5b9e\u4e16\u754c\u4e2d\u9053\u5fb7\u6a21\u7cca\u60c5\u5883\u63a8\u7406\u7684\u80fd\u529b\u5c1a\u7f3a\u4e4f\u7cfb\u7edf\u9a8c\u8bc1\uff0c\u5c24\u5176\u662f\u6d89\u53ca\u4eba\u53e3\u7edf\u8ba1\u504f\u5dee\u7684\u60c5\u51b5\u3002", "method": "\u9996\u6b21\u5229\u7528\u7ecf\u8fc7\u793e\u4f1a\u79d1\u5b66\u9a8c\u8bc1\u7684Violent Behavior Vignette Questionnaire\uff08VBVQ\uff09\u6765\u8bc4\u4f30LLM\u5bf9\u4e8e\u65e5\u5e38\u51b2\u7a81\u7684\u5e94\u5bf9\u3002\u540c\u65f6\uff0c\u91c7\u7528\u57fa\u4e8e\u4eba\u7269\u8bbe\u5b9a\u7684\u63d0\u793a\uff0c\u6539\u53d8\u79cd\u65cf\u3001\u5e74\u9f84\u548c\u5730\u7406\u8eab\u4efd\u7b49\u53d8\u91cf\uff0c\u5e76\u5bf9\u516d\u79cd\u4e0d\u540c\u80cc\u666f\u4e0b\u5f00\u53d1\u7684LLM\u5728\u96f6\u6837\u672c\u73af\u5883\u4e0b\u8fdb\u884c\u7edf\u4e00\u8bc4\u6d4b\u3002", "result": "(1) LLM\u8868\u5c42\u751f\u6210\u6587\u672c\u5e38\u4e0e\u5176\u5185\u5728\u5bf9\u66b4\u529b\u503e\u5411\u56de\u5e94\u7684\u504f\u597d\u51fa\u73b0\u5206\u6b67\uff1b(2) LLM\u7684\u66b4\u529b\u503e\u5411\u6839\u636e\u53d7\u8bd5\u4eba\u7fa4\u7279\u5f81\u5b58\u5728\u53d8\u5316\uff0c\u4e14\u5e38\u5e38\u4e0e\u72af\u7f6a\u5b66\u3001\u793e\u4f1a\u79d1\u5b66\u53ca\u5fc3\u7406\u5b66\u7684\u65e2\u6709\u7814\u7a76\u7ed3\u679c\u76f8\u77db\u76fe\u3002", "conclusion": "\u73b0\u6709LLM\u5728\u5904\u7406\u73b0\u5b9e\u590d\u6742\u9053\u5fb7\u51b2\u7a81\u53ca\u4eba\u53e3\u7edf\u8ba1\u53d8\u5f02\u65f6\uff0c\u5b58\u5728\u4e00\u81f4\u6027\u4e0e\u51c6\u786e\u6027\u95ee\u9898\uff0c\u5bf9\u5176\u5728\u66b4\u529b\u68c0\u6d4b/\u5e72\u9884\u4e2d\u7684\u5b9e\u9645\u53ef\u4fe1\u5ea6\u9700\u8c28\u614e\u770b\u5f85\u3002"}}
{"id": "2506.21362", "categories": ["cs.CE"], "pdf": "https://arxiv.org/pdf/2506.21362", "abs": "https://arxiv.org/abs/2506.21362", "authors": ["Chang Liu", "Yixin Wang", "Moontae Lee"], "title": "Counterfactual Voting Adjustment for Quality Assessment and Fairer Voting in Online Platforms with Helpfulness Evaluation", "comment": null, "summary": "Efficient access to high-quality information is vital for online platforms.\nTo promote more useful information, users not only create new content but also\nevaluate existing content, often through helpfulness voting. Although\naggregated votes help service providers rank their user content, these votes\nare often biased by disparate accessibility per position and the cascaded\ninfluence of prior votes. For a fairer assessment of information quality, we\npropose the Counterfactual Voting Adjustment (CVA), a causal framework that\naccounts for the context in which individual votes are cast. Through\npreliminary and semi-synthetic experiments, we show that CVA effectively models\nthe position and herding biases, accurately recovering the predefined content\nquality. In a real experiment, we demonstrate that reranking content based on\nthe learned quality by CVA exhibits stronger alignment with both user sentiment\nand quality evaluation assessed by GPT-4o, outperforming system rankings based\non aggregated votes and model-based rerankings without causal inference. Beyond\nthe individual quality inference, our embeddings offer comparative insights\ninto the behavioral dynamics of expert user groups across 120 major\nStackExchange communities.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCVA\u56e0\u679c\u6a21\u578b\u4fee\u6b63\u7f51\u7edc\u6295\u7968\u504f\u89c1\uff0c\u66f4\u51c6\u786e\u8bc4\u4f30\u5185\u5bb9\u8d28\u91cf\uff0c\u5728\u591a\u4e2a\u5e73\u53f0\u5b9e\u9a8c\u4e2d\u6548\u679c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u63d0\u5347\u4e86\u5185\u5bb9\u6392\u5e8f\u7684\u516c\u5e73\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u7f51\u7edc\u5e73\u53f0\u9700\u8981\u9ad8\u6548\u83b7\u53d6\u9ad8\u8d28\u91cf\u4fe1\u606f\uff0c\u7528\u6237\u901a\u8fc7\u53d1\u5e03\u5185\u5bb9\u548c\u6295\u7968\u63d0\u9ad8\u4fe1\u606f\u5b9e\u7528\u6027\uff0c\u4f46\u6295\u7968\u53d7\u4f4d\u7f6e\u504f\u89c1\u548c\u7f8a\u7fa4\u6548\u5e94\u5f71\u54cd\uff0c\u5f71\u54cd\u4e86\u5185\u5bb9\u8d28\u91cf\u7684\u516c\u6b63\u8bc4\u4f30\u3002", "method": "\u63d0\u51fa\u4e86Counterfactual Voting Adjustment\uff08CVA\uff09\u56e0\u679c\u63a8\u65ad\u6846\u67b6\uff0c\u7528\u4e8e\u4fee\u6b63\u7528\u6237\u6295\u7968\u4e2d\u7684\u4f4d\u7f6e\u548c\u7f8a\u7fa4\u504f\u89c1\uff0c\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u5728\u5185\u5bb9\u8d28\u91cf\u8bc4\u4f30\u4e2d\u7684\u6709\u6548\u6027\u3002", "result": "CVA\u6846\u67b6\u80fd\u6709\u6548\u5efa\u6a21\u5e76\u4fee\u6b63\u4f4d\u7f6e\u548c\u7f8a\u7fa4\u504f\u89c1\uff0c\u51c6\u786e\u6062\u590d\u5185\u5bb9\u7684\u771f\u5b9e\u8d28\u91cf\u3002\u5b9e\u9a8c\u663e\u793a\uff0c\u57fa\u4e8eCVA\u5b66\u4e60\u5230\u7684\u5185\u5bb9\u8d28\u91cf\u91cd\u65b0\u6392\u5e8f\u7ed3\u679c\u4e0e\u7528\u6237\u60c5\u611f\u548cGPT-4o\u8d28\u91cf\u8bc4\u5224\u9ad8\u5ea6\u4e00\u81f4\uff0c\u4f18\u4e8e\u4ec5\u7528\u6295\u7968\u603b\u6570\u6216\u65e0\u56e0\u679c\u63a8\u65ad\u7684\u6a21\u578b\u6392\u5e8f\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u8fd8\u80fd\u6d1e\u5bdf\u4e0d\u540c\u793e\u533a\u4e13\u5bb6\u7528\u6237\u7684\u884c\u4e3a\u52a8\u6001\u3002", "conclusion": "CVA\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u516c\u5e73\u3001\u6709\u6548\u7684\u5185\u5bb9\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u5185\u5bb9\u6392\u5e8f\u7ed3\u679c\u7684\u516c\u6b63\u6027\u548c\u51c6\u786e\u6027\uff0c\u53ef\u63a8\u5e7f\u5230\u591a\u4e2a\u7f51\u7edc\u793e\u533a\u5e73\u53f0\u3002"}}
{"id": "2506.20737", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.20737", "abs": "https://arxiv.org/abs/2506.20737", "authors": ["Gurusha Juneja", "Alon Albalak", "Wenyue Hua", "William Yang Wang"], "title": "MAGPIE: A dataset for Multi-AGent contextual PrIvacy Evaluation", "comment": null, "summary": "The proliferation of LLM-based agents has led to increasing deployment of\ninter-agent collaboration for tasks like scheduling, negotiation, resource\nallocation etc. In such systems, privacy is critical, as agents often access\nproprietary tools and domain-specific databases requiring strict\nconfidentiality. This paper examines whether LLM-based agents demonstrate an\nunderstanding of contextual privacy. And, if instructed, do these systems\npreserve inference time user privacy in non-adversarial multi-turn\nconversation. Existing benchmarks to evaluate contextual privacy in LLM-agents\nprimarily assess single-turn, low-complexity tasks where private information\ncan be easily excluded. We first present a benchmark - MAGPIE comprising 158\nreal-life high-stakes scenarios across 15 domains. These scenarios are designed\nsuch that complete exclusion of private data impedes task completion yet\nunrestricted information sharing could lead to substantial losses. We then\nevaluate the current state-of-the-art LLMs on (a) their understanding of\ncontextually private data and (b) their ability to collaborate without\nviolating user privacy. Empirical experiments demonstrate that current models,\nincluding GPT-4o and Claude-2.7-Sonnet, lack robust understanding of contextual\nprivacy, misclassifying private data as shareable 25.2\\% and 43.6\\% of the\ntime. In multi-turn conversations, these models disclose private information in\n59.9\\% and 50.5\\% of cases even under explicit privacy instructions.\nFurthermore, multi-agent systems fail to complete tasks in 71\\% of scenarios.\nThese results underscore that current models are not aligned towards both\ncontextual privacy preservation and collaborative task-solving.", "AI": {"tldr": "\u73b0\u6709\u6700\u5148\u8fdb\u7684\u5927\u6a21\u578bAgent\u5728\u591aAgent\u534f\u4f5c\u573a\u666f\u4e0b\uff0c\u7f3a\u4e4f\u5bf9\u60c5\u5883\u9690\u79c1\u7684\u7406\u89e3\u548c\u4fdd\u62a4\u80fd\u529b\uff0c\u5e38\u5e38\u9519\u8bef\u5171\u4eab\u654f\u611f\u4fe1\u606f\uff0c\u4e14\u5408\u4f5c\u5b8c\u6210\u4efb\u52a1\u7684\u80fd\u529b\u4e5f\u4e0d\u7406\u60f3\u3002", "motivation": "LLM\uff08\u5927\u8bed\u8a00\u6a21\u578b\uff09\u9a71\u52a8\u7684AI Agent\u8d8a\u6765\u8d8a\u591a\u5730\u627f\u62c5\u5982\u6392\u7a0b\u3001\u534f\u5546\u3001\u8d44\u6e90\u5206\u914d\u7b49\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u4efb\u52a1\u3002\u5728\u8fd9\u4e9b\u7cfb\u7edf\u4e2d\uff0c\u4ee3\u7406\u901a\u5e38\u8bbf\u95ee\u4e13\u6709\u5de5\u5177\u548c\u6570\u636e\u5e93\uff0c\u56e0\u800c\u4fdd\u62a4\u9690\u79c1\u53d8\u5f97\u6781\u4e3a\u5173\u952e\u3002\u7136\u800c\uff0c\u76ee\u524d\u5c1a\u4e0d\u6e05\u695a\u8fd9\u4e9bLLM\u4ee3\u7406\u662f\u5426\u5177\u5907\u5bf9\u60c5\u5883\u9690\u79c1\u7684\u7406\u89e3\u80fd\u529b\uff0c\u4ee5\u53ca\u5728\u591a\u8f6e\u5bf9\u8bdd\u534f\u4f5c\u4e2d\u662f\u5426\u80fd\u5728\u63a8\u65ad\u65f6\u4fdd\u62a4\u7528\u6237\u9690\u79c1\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86MAGPIE\u57fa\u51c6\u96c6\uff0c\u5305\u542b158\u4e2a\u6765\u81ea15\u4e2a\u5b9e\u9645\u9ad8\u98ce\u9669\u9886\u57df\u7684\u4efb\u52a1\u573a\u666f\uff0c\u8fd9\u4e9b\u60c5\u5883\u4e0b\u5b8c\u5168\u6392\u9664\u9690\u79c1\u4fe1\u606f\u5c06\u5bfc\u81f4\u4efb\u52a1\u96be\u4ee5\u5b8c\u6210\uff0c\u4f46\u65e0\u5dee\u522b\u5171\u4eab\u53c8\u4f1a\u5e26\u6765\u91cd\u5927\u635f\u5931\u3002\u5229\u7528\u8be5\u57fa\u51c6\uff0c\u5b9e\u8bc1\u8bc4\u4f30\u4e86\u5f53\u524d\u4e3b\u6d41LLM\uff08\u5982GPT-4o\u4e0eClaude-2.7-Sonnet\uff09\u5bf9\u60c5\u5883\u9690\u79c1\u7684\u7406\u89e3\u53ca\u534f\u4f5c\u4e2d\u4fdd\u62a4\u9690\u79c1\u7684\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5f53\u524d\u4e3b\u6d41\u6a21\u578b\u5bf9\u60c5\u5883\u9690\u79c1\u7406\u89e3\u4e0d\u8db3\uff0cGPT-4o\u4e0eClaude-2.7-Sonnet\u5206\u522b\u670925.2%\u548c43.6%\u7684\u8bef\u5224\u7387\uff08\u628a\u79c1\u4eba\u6570\u636e\u5f53\u4f5c\u53ef\u5171\u4eab\uff09\uff0c\u591a\u8f6e\u5bf9\u8bdd\u4e2d\uff0c\u5373\u4f7f\u660e\u786e\u544a\u8bc9\u4fdd\u62a4\u9690\u79c1\uff0c\u8fd9\u4e9b\u6a21\u578b\u4ecd\u5206\u522b\u670959.9%\u548c50.5%\u7684\u6848\u4f8b\u4f1a\u6cc4\u9732\u79c1\u4eba\u4fe1\u606f\u3002\u6b64\u5916\uff0c\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u6709\u9ad8\u8fbe71%\u7684\u6848\u4f8b\u65e0\u6cd5\u5b8c\u6210\u4efb\u52a1\u3002", "conclusion": "\u5f53\u524d\u4e3b\u6d41LLM\u4ee3\u7406\u5bf9\u60c5\u5883\u9690\u79c1\u7684\u7406\u89e3\u4e0e\u4fdd\u62a4\u80fd\u529b\u8584\u5f31\uff0c\u5728\u5b8c\u6210\u534f\u4f5c\u4efb\u52a1\u7684\u540c\u65f6\u96be\u4ee5\u505a\u5230\u6709\u6548\u4fdd\u5bc6\uff0c\u73b0\u6709\u6a21\u578b\u5728\u9690\u79c1\u4fdd\u62a4\u548c\u591aAgent\u534f\u4f5c\u4efb\u52a1\u5b8c\u6210\u5ea6\u4e24\u65b9\u9762\u5747\u5b58\u5728\u660e\u663e\u4e0d\u8db3\u3002"}}
{"id": "2506.20876", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.20876", "abs": "https://arxiv.org/abs/2506.20876", "authors": ["Sebastian Joseph", "Lily Chen", "Barry Wei", "Michael Mackert", "Iain J. Marshall", "Paul Pu Liang", "Ramez Kouzy", "Byron C. Wallace", "Junyi Jessy Li"], "title": "Decide less, communicate more: On the construct validity of end-to-end fact-checking in medicine", "comment": null, "summary": "Technological progress has led to concrete advancements in tasks that were\nregarded as challenging, such as automatic fact-checking. Interest in adopting\nthese systems for public health and medicine has grown due to the high-stakes\nnature of medical decisions and challenges in critically appraising a vast and\ndiverse medical literature. Evidence-based medicine connects to every\nindividual, and yet the nature of it is highly technical, rendering the medical\nliteracy of majority users inadequate to sufficiently navigate the domain. Such\nproblems with medical communication ripens the ground for end-to-end\nfact-checking agents: check a claim against current medical literature and\nreturn with an evidence-backed verdict. And yet, such systems remain largely\nunused. To understand this, we present the first study examining how clinical\nexperts verify real claims from social media by synthesizing medical evidence.\nIn searching for this upper-bound, we reveal fundamental challenges in\nend-to-end fact-checking when applied to medicine: Difficulties connecting\nclaims in the wild to scientific evidence in the form of clinical trials;\nambiguities in underspecified claims mixed with mismatched intentions; and\ninherently subjective veracity labels. We argue that fact-checking should be\napproached and evaluated as an interactive communication problem, rather than\nan end-to-end process.", "AI": {"tldr": "\u672c\u7814\u7a76\u63ed\u793a\u4e86\u533b\u5b66\u81ea\u52a8\u4e8b\u5b9e\u6838\u67e5\u9886\u57df\u5b58\u5728\u7684\u5173\u952e\u56f0\u5883\uff0c\u5e76\u5efa\u8bae\u4ee5\u4ea4\u4e92\u5f0f\u6c9f\u901a\u601d\u8def\u63a8\u52a8\u7cfb\u7edf\u8bbe\u8ba1\u4e0e\u8bc4\u4f30\uff0c\u800c\u975e\u4ec5\u4ec5\u8ffd\u6c42\u7aef\u5230\u7aef\u7684\u6838\u67e5\u6d41\u7a0b\u3002", "motivation": "\u5c3d\u7ba1\u533b\u5b66\u4e8b\u5b9e\u6838\u67e5\u5bf9\u4e8e\u516c\u4f17\u5065\u5eb7\u548c\u533b\u5b66\u51b3\u7b56\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u5b9e\u4e2d\u81ea\u52a8\u4e8b\u5b9e\u6838\u67e5\u7cfb\u7edf\u7684\u5e94\u7528\u4ecd\u6709\u9650\u3002\u7528\u6237\u533b\u5b66\u7d20\u517b\u4e0d\u8db3\u548c\u5f53\u524d\u533b\u5b66\u6587\u732e\u590d\u6742\u5ea6\u9ad8\u4e5f\u52a0\u5267\u4e86\u4fe1\u606f\u6c9f\u901a\u95ee\u9898\uff0c\u56e0\u6b64\u4e9f\u9700\u7814\u7a76\u8be5\u9886\u57df\u3002", "method": "\u672c\u7814\u7a76\u9996\u6b21\u4ee5\u4e34\u5e8a\u4e13\u5bb6\u4e3a\u5bf9\u8c61\uff0c\u8003\u5bdf\u4ed6\u4eec\u5982\u4f55\u901a\u8fc7\u7efc\u5408\u533b\u5b66\u8bc1\u636e\u5bf9\u793e\u4ea4\u5a92\u4f53\u4e2d\u7684\u771f\u5b9e\u533b\u5b66\u4e3b\u5f20\u8fdb\u884c\u9a8c\u8bc1\uff0c\u65e8\u5728\u63a2\u7d22\u533b\u5b66\u81ea\u52a8\u4e8b\u5b9e\u6838\u67e5\u7684\u53ef\u8fbe\u4e0a\u9650\uff0c\u5e76\u63ed\u793a\u5176\u4e2d\u7684\u6839\u672c\u6027\u6311\u6218\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5c06\u4e8b\u5b9e\u6838\u67e5\u7aef\u5230\u7aef\u5730\u5e94\u7528\u4e8e\u533b\u5b66\u9886\u57df\u9762\u4e34\u8bf8\u591a\u6311\u6218\uff1a\u5982\u4e3b\u5f20\u4e0e\u533b\u5b66\u5b9e\u8bc1\uff08\u4e34\u5e8a\u8bd5\u9a8c\uff09\u8fde\u63a5\u56f0\u96be\u3001\u6a21\u7cca\u7684\u4e3b\u5f20\u8868\u8fbe\u4e0e\u610f\u56fe\u4e0d\u7b26\uff0c\u4ee5\u53ca\u6838\u67e5\u7ed3\u679c\u6807\u7b7e\u96be\u4ee5\u5ba2\u89c2\u5316\u3002", "conclusion": "\u4f5c\u8005\u8ba4\u4e3a\uff0c\u533b\u5b66\u4e8b\u5b9e\u6838\u67e5\u5e94\u88ab\u89c6\u4e3a\u4e00\u4e2a\u4ea4\u4e92\u5f0f\u6c9f\u901a\u95ee\u9898\uff0c\u800c\u975e\u7b80\u5355\u7684\u7aef\u5230\u7aef\u6d41\u7a0b\uff0c\u5e76\u5efa\u8bae\u5176\u5728\u8bbe\u8ba1\u4e0e\u8bc4\u4f30\u65f6\u5e94\u52a0\u5f3a\u4ea4\u4e92\u6027\u3002"}}
{"id": "2506.20815", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.20815", "abs": "https://arxiv.org/abs/2506.20815", "authors": ["Xinye Tang", "Haijun Zhai", "Chaitanya Belwal", "Vineeth Thayanithi", "Philip Baumann", "Yogesh K Roy"], "title": "Dynamic Context-Aware Prompt Recommendation for Domain-Specific AI Applications", "comment": null, "summary": "LLM-powered applications are highly susceptible to the quality of user\nprompts, and crafting high-quality prompts can often be challenging especially\nfor domain-specific applications. This paper presents a novel dynamic\ncontext-aware prompt recommendation system for domain-specific AI applications.\nOur solution combines contextual query analysis, retrieval-augmented knowledge\ngrounding, hierarchical skill organization, and adaptive skill ranking to\ngenerate relevant and actionable prompt suggestions.\n  The system leverages behavioral telemetry and a two-stage hierarchical\nreasoning process to dynamically select and rank relevant skills, and\nsynthesizes prompts using both predefined and adaptive templates enhanced with\nfew-shot learning. Experiments on real-world datasets demonstrate that our\napproach achieves high usefulness and relevance, as validated by both automated\nand expert evaluations.", "AI": {"tldr": "\u8be5\u6587\u63d0\u51fa\u4e00\u79cd\u9762\u5411\u9886\u57df\u5e94\u7528\u7684\u52a8\u6001prompt\u63a8\u8350\u7cfb\u7edf\uff0c\u53ef\u81ea\u52a8\u5206\u6790\u4e0a\u4e0b\u6587\u5e76\u751f\u6210\u4f18\u8d28\u5efa\u8bae\uff0c\u6709\u6548\u63d0\u5347LLM\u5e94\u7528\u4e2d\u7684prompt\u8d28\u91cf\uff0c\u7ecf\u5b9e\u9a8c\u8bc1\u660e\u63a8\u8350\u6548\u679c\u4f18\u5f02\u3002", "motivation": "LLM\u5e94\u7528\u7684\u6548\u679c\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u7528\u6237\u8f93\u5165\u7684prompt\u8d28\u91cf\uff0c\u800c\u9886\u57df\u7279\u5b9a\u7684\u9ad8\u8d28\u91cfprompt\u64b0\u5199\u9887\u5177\u6311\u6218\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u7684prompt\u63a8\u8350\u7cfb\u7edf\uff0c\u7ed3\u5408\u4e86\u4e0a\u4e0b\u6587\u67e5\u8be2\u5206\u6790\u3001\u68c0\u7d22\u589e\u5f3a\u77e5\u8bc6\u652f\u6491\u3001\u5206\u5c42\u6280\u80fd\u7ec4\u7ec7\u548c\u81ea\u9002\u5e94\u6280\u80fd\u6392\u5e8f\u3002\u7cfb\u7edf\u901a\u8fc7\u884c\u4e3a\u9065\u6d4b\u548c\u4e24\u9636\u6bb5\u5206\u5c42\u63a8\u7406\u52a8\u6001\u9009\u62e9\u5e76\u6392\u5e8f\u76f8\u5173\u6280\u80fd\uff0c\u5229\u7528\u9884\u5b9a\u4e49\u4e0e\u81ea\u9002\u5e94\u6a21\u677f\u53ca\u5c11\u6837\u672c\u5b66\u4e60\u751f\u6210\u9ad8\u76f8\u5173\u548c\u53ef\u7528\u7684prompt\u5efa\u8bae\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u7684prompt\u5efa\u8bae\u5728\u5b9e\u7528\u6027\u4e0e\u76f8\u5173\u6027\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u83b7\u5f97\u81ea\u52a8\u5316\u548c\u4e13\u5bb6\u8bc4\u4ef7\u7684\u53cc\u91cd\u9a8c\u8bc1\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u80fd\u4e3a\u9886\u57df\u7279\u5b9a\u7684AI\u5e94\u7528\u52a8\u6001\u751f\u6210\u9ad8\u8d28\u91cfprompt\uff0c\u6709\u6548\u63d0\u5347\u5e94\u7528\u8868\u73b0\u3002"}}
{"id": "2506.20917", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.20917", "abs": "https://arxiv.org/abs/2506.20917", "authors": ["Zhengyan Shi"], "title": "Optimising Language Models for Downstream Tasks: A Post-Training Perspective", "comment": "PhD Thesis", "summary": "Language models (LMs) have demonstrated remarkable capabilities in NLP, yet\nadapting them efficiently and robustly to specific tasks remains challenging.\nAs their scale and complexity grow, fine-tuning LMs on labelled data often\nunderutilizes available unlabelled data, leads to overfitting on small\ntask-specific sets, and imposes significant computational costs. These\nlimitations hamper their application to the open-ended landscape of real-world\nlanguage tasks.\n  This thesis proposes a series of methods to better adapt LMs to downstream\napplications. First, we explore strategies for extracting task-relevant\nknowledge from unlabelled data, introducing a novel continued pre-training\ntechnique that outperforms state-of-the-art semi-supervised approaches. Next,\nwe present a parameter-efficient fine-tuning method that substantially reduces\nmemory and compute costs while maintaining competitive performance. We also\nintroduce improved supervised fine-tuning methods that enable LMs to better\nfollow instructions, especially when labelled data is scarce, enhancing their\nperformance across a range of NLP tasks, including open-ended generation.\nFinally, we develop new evaluation methods and benchmarks, such as multi-hop\nspatial reasoning tasks, to assess LM capabilities and adaptation more\ncomprehensively.\n  Through extensive empirical studies across diverse NLP tasks, our results\ndemonstrate that these approaches substantially improve LM robustness,\nefficiency, and generalization, making them more adaptable to a broad range of\napplications. These advances mark a significant step towards more robust and\nefficient LMs, bringing us closer to the goal of artificial general\nintelligence.", "AI": {"tldr": "\u8bba\u6587\u9488\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5177\u4f53\u4efb\u52a1\u9002\u914d\u4e0a\u7684\u9ad8\u6548\u6027\u4e0e\u9c81\u68d2\u6027\u96be\u9898\uff0c\u63d0\u51fa\u4e86\u6301\u7eed\u9884\u8bad\u7ec3\u3001\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u3001\u6539\u8fdb\u7684\u6709\u76d1\u7763\u5fae\u8c03\u548c\u65b0\u8bc4\u6d4b\u57fa\u51c6\u7b49\u65b9\u6cd5\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u6cdb\u5316\u548c\u5e94\u7528\u80fd\u529b\uff0c\u4e3a\u901a\u7528\u4eba\u5de5\u667a\u80fd\u53d1\u5c55\u63d0\u4f9b\u4e86\u6709\u529b\u652f\u6491\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LMs\uff09\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5982\u4f55\u9ad8\u6548\u3001\u7a33\u5065\u5730\u9002\u5e94\u5177\u4f53\u4e0b\u6e38\u4efb\u52a1\u4ecd\u5b58\u5728\u8bf8\u591a\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u5f80\u5f80\u65e0\u6cd5\u5145\u5206\u5229\u7528\u65e0\u6807\u7b7e\u6570\u636e\uff0c\u5bb9\u6613\u5728\u5c0f\u6837\u672c\u4efb\u52a1\u6570\u636e\u4e0a\u8fc7\u62df\u5408\uff0c\u540c\u65f6\u8ba1\u7b97\u548c\u5b58\u50a8\u6210\u672c\u8f83\u9ad8\u3002\u8fd9\u4e9b\u95ee\u9898\u9650\u5236\u4e86LMs\u5728\u5b9e\u9645\u5f00\u653e\u9886\u57df\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002", "method": "1. \u63d0\u51fa\u4e86\u4ece\u65e0\u6807\u7b7e\u6570\u636e\u4e2d\u63d0\u53d6\u4efb\u52a1\u76f8\u5173\u77e5\u8bc6\u7684\u65b0\u578b\u6301\u7eed\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u534a\u76d1\u7763\u65b9\u6cd5\u30022. \u63d0\u51fa\u4e86\u4e00\u79cd\u53c2\u6570\u9ad8\u6548\u7684\u5fae\u8c03\u65b9\u6cd5\uff0c\u5927\u5e45\u964d\u4f4e\u5185\u5b58\u548c\u8ba1\u7b97\u6d88\u8017\uff0c\u540c\u65f6\u4fdd\u6301\u7ade\u4e89\u529b\u7684\u6027\u80fd\u30023. \u4f18\u5316\u4e86\u6709\u76d1\u7763\u5fae\u8c03\u65b9\u5f0f\uff0c\u4f7f\u6a21\u578b\u80fd\u5728\u6709\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u65f6\u66f4\u597d\u5730\u8ddf\u968f\u6307\u4ee4\uff0c\u63d0\u5347\u591a\u4efb\u52a1\u7684\u8868\u73b0\u30024. \u6784\u5efa\u4e86\u65b0\u7684\u8bc4\u6d4b\u65b9\u6cd5\u548c\u57fa\u51c6\uff08\u5982\u591a\u8df3\u7a7a\u95f4\u63a8\u7406\u4efb\u52a1\uff09\uff0c\u66f4\u5168\u9762\u5730\u8bc4\u4f30LM\u80fd\u529b\u3002", "result": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u591a\u79cdNLP\u4efb\u52a1\u4e0a\u7ecf\u8fc7\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u663e\u8457\u63d0\u5347\u4e86LM\u7684\u9c81\u68d2\u6027\u3001\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4f7f\u5176\u9002\u914d\u66f4\u5e7f\u6cdb\u5b9e\u9645\u5e94\u7528\u6210\u4e3a\u53ef\u80fd\u3002", "conclusion": "\u8be5\u8bba\u6587\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u9ad8\u6548\u3001\u7a33\u5065\u9002\u914d\u548c\u63a8\u5e7f\u5e94\u7528\u5e26\u6765\u4e86\u65b9\u6cd5\u7a81\u7834\uff0c\u5bf9\u5b9e\u73b0\u66f4\u901a\u7528\u7684\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u8fc8\u51fa\u4e86\u5173\u952e\u4e00\u6b65\u3002"}}
{"id": "2506.21532", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2506.21532", "abs": "https://arxiv.org/abs/2506.21532", "authors": ["Akshay Paruchuri", "Maryam Aziz", "Rohit Vartak", "Ayman Ali", "Best Uchehara", "Xin Liu", "Ishan Chatterjee", "Monica Agrawal"], "title": "\"What's Up, Doc?\": Analyzing How Users Seek Health Information in Large-Scale Conversational AI Datasets", "comment": "25 pages, 6 figures, 4 tables, corresponds to initial HealthChat-11K\n  dataset release", "summary": "People are increasingly seeking healthcare information from large language\nmodels (LLMs) via interactive chatbots, yet the nature and inherent risks of\nthese conversations remain largely unexplored. In this paper, we filter\nlarge-scale conversational AI datasets to achieve HealthChat-11K, a curated\ndataset of 11K real-world conversations composed of 25K user messages. We use\nHealthChat-11K and a clinician-driven taxonomy for how users interact with LLMs\nwhen seeking healthcare information in order to systematically study user\ninteractions across 21 distinct health specialties. Our analysis reveals\ninsights into the nature of how and why users seek health information, such as\ncommon interactions, instances of incomplete context, affective behaviors, and\ninteractions (e.g., leading questions) that can induce sycophancy, underscoring\nthe need for improvements in the healthcare support capabilities of LLMs\ndeployed as conversational AI. Code and artifacts to retrieve our analyses and\ncombine them into a curated dataset can be found here:\nhttps://github.com/yahskapar/HealthChat", "AI": {"tldr": "\u672c\u6587\u6784\u5efa\u4e86HealthChat-11K\u533b\u7597\u5065\u5eb7\u5bf9\u8bdd\u6570\u636e\u96c6\uff0c\u7cfb\u7edf\u5206\u6790\u4e86\u7528\u6237\u4e0eLLM\u4e92\u52a8\u65f6\u7684\u884c\u4e3a\u7279\u5f81\u548c\u98ce\u9669\uff0c\u6307\u51faLLM\u5728\u533b\u7597\u652f\u6301\u5e94\u7528\u4e2d\u7684\u4e0d\u8db3\uff0c\u6570\u636e\u96c6\u548c\u5206\u6790\u7ed3\u679c\u5df2\u5f00\u6e90\u3002", "motivation": "\u8d8a\u6765\u8d8a\u591a\u7684\u4eba\u901a\u8fc7\u4e92\u52a8\u804a\u5929\u673a\u5668\u4eba\u4ece\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLMs)\u4e2d\u5bfb\u6c42\u533b\u7597\u5065\u5eb7\u4fe1\u606f\uff0c\u4f46\u8fd9\u4e9b\u5bf9\u8bdd\u7684\u6027\u8d28\u548c\u6f5c\u5728\u98ce\u9669\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "\u4f5c\u8005\u4ece\u5927\u89c4\u6a21\u5bf9\u8bdd\u5f0fAI\u6570\u636e\u96c6\u4e2d\u7b5b\u9009\u5e76\u6574\u7406\u51faHealthChat-11K\u6570\u636e\u96c6\uff0c\u5305\u542b11,000\u4e2a\u771f\u5b9e\u4e16\u754c\u5bf9\u8bdd\u548c25,000\u6761\u7528\u6237\u6d88\u606f\uff0c\u5e76\u4f7f\u7528\u4e34\u5e8a\u533b\u751f\u4e3b\u5bfc\u7684\u7528\u6237\u4ea4\u4e92\u5206\u7c7b\u4f53\u7cfb\uff0c\u7cfb\u7edf\u5206\u6790\u7528\u6237\u572821\u4e2a\u5065\u5eb7\u4e13\u4e1a\u9886\u57df\u4e0eLLM\u7684\u4ea4\u4e92\u3002", "result": "\u5206\u6790\u63ed\u793a\u4e86\u7528\u6237\u5bfb\u6c42\u5065\u5eb7\u4fe1\u606f\u65f6\u7684\u4e92\u52a8\u5e38\u89c1\u6a21\u5f0f\u3001\u4e0d\u5b8c\u6574\u8bed\u5883\u3001\u60c5\u611f\u6027\u884c\u4e3a\uff0c\u4ee5\u53ca\u7528\u6237\u63d0\u95ee\u53ef\u80fd\u8bf1\u5bfc\u6a21\u578b\u8fc7\u5ea6\u987a\u4ece\u7b49\u95ee\u9898\uff0c\u6307\u51faLLM\u5728\u533b\u7597\u652f\u6301\u65b9\u9762\u4ecd\u9700\u6539\u8fdb\u3002", "conclusion": "\u5f53\u524d\u57fa\u4e8eLLM\u7684\u5bf9\u8bdd\u5f0fAI\u5728\u533b\u7597\u5065\u5eb7\u652f\u6301\u65b9\u9762\u5b58\u5728\u5f88\u591a\u6311\u6218\uff0c\u9700\u9488\u5bf9\u7528\u6237\u4e92\u52a8\u7279\u5f81\u548c\u6f5c\u5728\u98ce\u9669\u8fdb\u884c\u8fdb\u4e00\u6b65\u4f18\u5316\u3002\u4f5c\u8005\u516c\u5f00\u4e86\u5206\u6790\u7ed3\u679c\u548c\u6570\u636e\u96c6\u3002"}}
{"id": "2506.20949", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.20949", "abs": "https://arxiv.org/abs/2506.20949", "authors": ["Chenkai Sun", "Denghui Zhang", "ChengXiang Zhai", "Heng Ji"], "title": "Beyond Reactive Safety: Risk-Aware LLM Alignment via Long-Horizon Simulation", "comment": null, "summary": "Given the growing influence of language model-based agents on high-stakes\nsocietal decisions, from public policy to healthcare, ensuring their beneficial\nimpact requires understanding the far-reaching implications of their\nsuggestions. We propose a proof-of-concept framework that projects how\nmodel-generated advice could propagate through societal systems on a\nmacroscopic scale over time, enabling more robust alignment. To assess the\nlong-term safety awareness of language models, we also introduce a dataset of\n100 indirect harm scenarios, testing models' ability to foresee adverse,\nnon-obvious outcomes from seemingly harmless user prompts. Our approach\nachieves not only over 20% improvement on the new dataset but also an average\nwin rate exceeding 70% against strong baselines on existing safety benchmarks\n(AdvBench, SafeRLHF, WildGuardMix), suggesting a promising direction for safer\nagents.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u9884\u6d4b\u8bed\u8a00\u6a21\u578b\u5efa\u8bae\u5728\u793e\u4f1a\u7cfb\u7edf\u957f\u671f\u5f71\u54cd\u7684\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u65b0\u6570\u636e\u96c6\u6d4b\u8bd5\u5b89\u5168\u610f\u8bc6\uff0c\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5bf9\u95f4\u63a5\u98ce\u9669\u7684\u8bc6\u522b\u80fd\u529b\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u79c0\uff0c\u63a8\u52a8\u5b89\u5168AI\u7814\u7a76\u3002", "motivation": "\u8bed\u8a00\u6a21\u578b\u5df2\u5728\u516c\u5171\u653f\u7b56\u3001\u533b\u7597\u7b49\u9ad8\u98ce\u9669\u793e\u4f1a\u51b3\u7b56\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u56e0\u6b64\u9700\u8981\u7406\u89e3\u6a21\u578b\u5efa\u8bae\u5728\u793e\u4f1a\u7cfb\u7edf\u4e2d\u7684\u957f\u671f\u5f71\u54cd\uff0c\u4ee5\u786e\u4fdd\u5176\u6b63\u9762\u4f5c\u7528\u548c\u5b89\u5168\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u6982\u5ff5\u6027\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4b\u6a21\u578b\u751f\u6210\u5efa\u8bae\u5728\u793e\u4f1a\u7cfb\u7edf\u4e2d\u7684\u957f\u671f\u3001\u5927\u89c4\u6a21\u4f20\u5bfc\u6548\u5e94\u3002\u540c\u65f6\uff0c\u6784\u5efa\u4e86\u542b100\u4e2a\u95f4\u63a5\u4f24\u5bb3\u573a\u666f\u7684\u6570\u636e\u96c6\uff0c\u6d4b\u8bd5\u6a21\u578b\u5bf9\u975e\u663e\u6027\u8d1f\u9762\u540e\u679c\u7684\u9884\u5224\u80fd\u529b\u3002", "result": "\u65b0\u65b9\u6cd5\u5728\u65b0\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u63d0\u534720%\u4ee5\u4e0a\uff0c\u5e76\u5728\u73b0\u6709\u5b89\u5168\u57fa\u51c6\uff08\u5982AdvBench\u3001SafeRLHF\u3001WildGuardMix\uff09\u4e0a\u53d6\u5f97\u8d85\u8fc770%\u7684\u5e73\u5747\u80dc\u7387\uff0c\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u3002", "conclusion": "\u6240\u63d0\u6846\u67b6\u548c\u65b9\u6cd5\u5728\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u957f\u671f\u5b89\u5168\u610f\u8bc6\u548c\u5bf9\u590d\u6742\u793e\u4f1a\u98ce\u9669\u7684\u9884\u5224\u80fd\u529b\u65b9\u9762\u5177\u6709\u524d\u666f\uff0c\u4e3a\u6784\u5efa\u66f4\u5b89\u5168\u7684AI\u667a\u80fd\u4f53\u6307\u51fa\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2506.20920", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.20920", "abs": "https://arxiv.org/abs/2506.20920", "authors": ["Guilherme Penedo", "Hynek Kydl\u00ed\u010dek", "Vinko Sabol\u010dec", "Bettina Messmer", "Negar Foroutan", "Amir Hossein Kargaran", "Colin Raffel", "Martin Jaggi", "Leandro Von Werra", "Thomas Wolf"], "title": "FineWeb2: One Pipeline to Scale Them All -- Adapting Pre-Training Data Processing to Every Language", "comment": null, "summary": "Pre-training state-of-the-art large language models (LLMs) requires vast\namounts of clean and diverse text data. While the open development of large\nhigh-quality English pre-training datasets has seen substantial recent\nprogress, training performant multilingual LLMs remains a challenge, in large\npart due to the inherent difficulty of tailoring filtering and deduplication\npipelines to a large number of languages. In this work, we introduce a new\npre-training dataset curation pipeline based on FineWeb that can be\nautomatically adapted to support any language. We extensively ablate our\npipeline design choices on a set of nine diverse languages, guided by a set of\nmeaningful and informative evaluation tasks that were chosen through a novel\nselection process based on measurable criteria. Ultimately, we show that our\npipeline can be used to create non-English corpora that produce more performant\nmodels than prior datasets. We additionally introduce a straightforward and\nprincipled approach to rebalance datasets that takes into consideration both\nduplication count and quality, providing an additional performance uplift.\nFinally, we scale our pipeline to over 1000 languages using almost 100 Common\nCrawl snapshots to produce FineWeb2, a new 20 terabyte (5 billion document)\nmultilingual dataset which we release along with our pipeline, training, and\nevaluation codebases.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u81ea\u52a8\u9002\u7528\u4e8e\u591a\u8bed\u79cd\u7684\u6570\u636e\u96c6\u8fc7\u6ee4\u4e0e\u53bb\u91cd\u6d41\u7a0b\uff0c\u80fd\u751f\u6210\u4f18\u4e8e\u73b0\u6709\u7684\u591a\u8bed\u79cd\u9884\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u516c\u5f00\u4e86\u8986\u76d61000\u591a\u79cd\u8bed\u8a00\u30015\u4ebf\u6587\u6863\u7684FineWeb2\u5927\u89c4\u6a21\u8bed\u6599\uff0c\u663e\u8457\u63a8\u52a8\u591a\u8bed\u79cd\u5927\u6a21\u578b\u8bad\u7ec3\u3002", "motivation": "\u5f53\u524d\u9ad8\u8d28\u91cf\u82f1\u6587\u5927\u6a21\u578b\u9884\u8bad\u7ec3\u6570\u636e\u96c6\u7684\u5f00\u653e\u53d6\u5f97\u4e86\u8f83\u5927\u8fdb\u5c55\uff0c\u4f46\u591a\u8bed\u79cd\u5927\u6a21\u578b\u7684\u8bad\u7ec3\u4f9d\u65e7\u9762\u4e34\u6311\u6218\uff0c\u4e3b\u8981\u6e90\u4e8e\u8fc7\u6ee4\u4e0e\u53bb\u91cd\u6d41\u7a0b\u96be\u4ee5\u9002\u7528\u4e8e\u591a\u79cd\u8bed\u8a00\u3002", "method": "\u63d0\u51fa\u57fa\u4e8eFineWeb\u7684\u65b0\u578b\u9884\u8bad\u7ec3\u6570\u636e\u96c6\u5904\u7406\u6d41\u6c34\u7ebf\uff0c\u80fd\u591f\u81ea\u52a8\u9002\u7528\u4e8e\u4efb\u4f55\u8bed\u8a00\uff0c\u5e76\u901a\u8fc7\u4e00\u5957\u57fa\u4e8e\u6d4b\u91cf\u6807\u51c6\u7cbe\u9009\u7684\u8bc4\u6d4b\u4efb\u52a1\uff0c\u5bf9\u4e0d\u540c\u6d41\u6c34\u7ebf\u8bbe\u8ba1\u8fdb\u884c\u8be6\u7ec6\u6d88\u878d\u5b9e\u9a8c\u3002\u6b64\u5916\uff0c\u5f15\u5165\u652f\u6301\u8d28\u91cf\u4e0e\u53bb\u91cd\u517c\u987e\u7684\u7b80\u5355\u518d\u5e73\u8861\u65b9\u6cd5\u3002", "result": "\u6240\u63d0\u51fa\u7684\u6d41\u6c34\u7ebf\u53ef\u7528\u4e8e\u6784\u5efa\u51fa\u4f18\u4e8e\u73b0\u6709\u6570\u636e\u96c6\u7684\u975e\u82f1\u6587\u8bed\u6599\uff0c\u8fdb\u4e00\u6b65\u63d0\u5347\u5927\u6a21\u578b\u8868\u73b0\u3002\u518d\u5e73\u8861\u65b9\u6cd5\u5e26\u6765\u6027\u80fd\u63d0\u5347\u3002\u6700\u7ec8\uff0c\u4f7f\u7528\u8fd1100\u4e2aCommon Crawl\u5feb\u7167\uff0c\u5c06\u6d41\u6c34\u7ebf\u6269\u5c55\u52301000\u4f59\u79cd\u8bed\u8a00\uff0c\u751f\u6210FineWeb2\u8fd9\u4e00\u5305\u542b5\u4ebf\u6587\u6863\u300120TB\u89c4\u6a21\u7684\u65b0\u591a\u8bed\u79cd\u6570\u636e\u96c6\u3002\u76f8\u5173\u8d44\u6e90\u4e0e\u4ee3\u7801\u516c\u5f00\u3002", "conclusion": "\u672c\u6587\u89e3\u51b3\u4e86\u591a\u8bed\u79cd\u5927\u6a21\u578b\u8bad\u7ec3\u4e2d\u6570\u636e\u96c6\u81ea\u52a8\u5316\u8fc7\u6ee4\u4e0e\u53bb\u91cd\u96be\u9898\uff0c\u63d0\u51fa\u7684\u6cdb\u7528\u6570\u636e\u96c6\u6784\u5efa\u6d41\u6c34\u7ebf\u548c\u518d\u5e73\u8861\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u975e\u82f1\u6587\u6a21\u578b\u8868\u73b0\uff0c\u63a8\u51fa\u4e86\u5927\u89c4\u6a21\u591a\u8bed\u79cd\u6570\u636e\u96c6FineWeb2\uff0c\u4e3a\u591a\u8bed\u79cd\u5927\u6a21\u578b\u7814\u7a76\u63d0\u4f9b\u91cd\u8981\u652f\u6301\u3002"}}
{"id": "2506.21215", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.21215", "abs": "https://arxiv.org/abs/2506.21215", "authors": ["Haoang Chi", "He Li", "Wenjing Yang", "Feng Liu", "Long Lan", "Xiaoguang Ren", "Tongliang Liu", "Bo Han"], "title": "Unveiling Causal Reasoning in Large Language Models: Reality or Mirage?", "comment": "24 pages, accepted at NeurIPS 2024", "summary": "Causal reasoning capability is critical in advancing large language models\n(LLMs) toward strong artificial intelligence. While versatile LLMs appear to\nhave demonstrated capabilities in understanding contextual causality and\nproviding responses that obey the laws of causality, it remains unclear whether\nthey perform genuine causal reasoning akin to humans. However, current evidence\nindicates the contrary. Specifically, LLMs are only capable of performing\nshallow (level-1) causal reasoning, primarily attributed to the causal\nknowledge embedded in their parameters, but they lack the capacity for genuine\nhuman-like (level-2) causal reasoning. To support this hypothesis,\nmethodologically, we delve into the autoregression mechanism of\ntransformer-based LLMs, revealing that it is not inherently causal.\nEmpirically, we introduce a new causal Q&A benchmark called CausalProbe-2024,\nwhose corpora are fresh and nearly unseen for the studied LLMs. The LLMs\nexhibit a significant performance drop on CausalProbe-2024 compared to earlier\nbenchmarks, indicating the fact that they primarily engage in level-1 causal\nreasoning. To bridge the gap towards level-2 causal reasoning, we draw\ninspiration from the fact that human reasoning is usually facilitated by\ngeneral knowledge and intended goals. We propose G^2-Reasoner, a method that\nincorporates general knowledge and goal-oriented prompts into LLMs' causal\nreasoning processes. Experiments demonstrate that G^2-Reasoner significantly\nenhances LLMs' causal reasoning capability, particularly in fresh and\ncounterfactual contexts. This work sheds light on a new path for LLMs to\nadvance towards genuine causal reasoning, going beyond level-1 and making\nstrides towards level-2.", "AI": {"tldr": "\u672c\u6587\u8868\u660e\uff0c\u5f53\u524d\u4e3b\u6d41LLMs\u5728\u65b0\u9896\u56e0\u679c\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u6709\u9650\uff0c\u4ec5\u80fd\u5b9e\u73b0\u6d45\u5c42\u56e0\u679c\u63a8\u7406\u3002\u901a\u8fc7\u5f15\u5165\u7ed3\u5408\u901a\u7528\u77e5\u8bc6\u4e0e\u76ee\u6807\u5bfc\u5411\u7684G^2-Reasoner\u65b9\u6cd5\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u5347LLMs\u7684\u56e0\u679c\u63a8\u7406\u6c34\u5e73\uff0c\u671d\u5411\u66f4\u6df1\u5c42\u6b21\u7684\u63a8\u7406\u80fd\u529b\u8fc8\u8fdb\u3002", "motivation": "\u76ee\u524d\u5c1a\u4e0d\u6e05\u695a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u662f\u5426\u771f\u6b63\u5177\u5907\u7c7b\u4f3c\u4eba\u7c7b\u7684\u56e0\u679c\u63a8\u7406\u80fd\u529b\u3002\u5df2\u6709\u8bc1\u636e\u8868\u660e\uff0c\u73b0\u6709LLMs\u4ec5\u80fd\u5b8c\u6210\u8f83\u6d45\u5c42\u7684\u56e0\u679c\u63a8\u7406\uff08level-1\uff09\uff0c\u800c\u7f3a\u4e4f\u66f4\u9ad8\u7ea7\u522b\uff08level-2\uff09\u7684\u80fd\u529b\u3002\u4e3a\u6b64\uff0c\u4f5c\u8005\u8fdb\u4e00\u6b65\u63a2\u7d22LLMs\u5728\u56e0\u679c\u63a8\u7406\u65b9\u9762\u7684\u5c40\u9650\u6027\u53ca\u5176\u63d0\u5347\u8def\u5f84\u3002", "method": "\u4f5c\u8005\u9996\u5148\u4ece\u7406\u8bba\u4e0a\u5206\u6790transformer-based LLMs\u81ea\u56de\u5f52\u673a\u5236\uff0c\u6307\u51fa\u5176\u672c\u8d28\u4e0a\u5e76\u4e0d\u5177\u5907\u56e0\u679c\u63a8\u7406\u80fd\u529b\u3002\u968f\u540e\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u4e2a\u5168\u65b0\u7684\u56e0\u679c\u95ee\u7b54\u57fa\u51c6\u96c6CausalProbe-2024\uff0c\u7528\u4e8e\u5b9e\u8bc1LLMs\u7684\u56e0\u679c\u63a8\u7406\u80fd\u529b\u8868\u73b0\u3002\u6700\u540e\uff0c\u501f\u9274\u4eba\u7c7b\u63a8\u7406\u4f9d\u8d56\u901a\u7528\u77e5\u8bc6\u4e0e\u76ee\u6807\u5bfc\u5411\u7684\u7279\u70b9\uff0c\u63d0\u51faG^2-Reasoner\uff0c\u5c06\u901a\u7528\u77e5\u8bc6\u53ca\u76ee\u6807\u5bfc\u5411\u63d0\u793a\u7ed3\u5408\u5230LLMs\u7684\u63a8\u7406\u8fc7\u7a0b\u4e2d\u3002", "result": "\u5728CausalProbe-2024\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4e3b\u6d41LLMs\u7684\u56e0\u679c\u63a8\u7406\u80fd\u529b\u8f83\u4ee5\u5f80\u57fa\u51c6\u6d4b\u8bd5\u5927\u5e45\u4e0b\u964d\uff0c\u786e\u8ba4\u5176\u4e3b\u8981\u8fd8\u662flevel-1\u63a8\u7406\u3002\u5e94\u7528G^2-Reasoner\u540e\uff0cLLMs\u5728\u65b0\u9896\u3001\u53cd\u4e8b\u5b9e\u7b49\u590d\u6742\u8bed\u5883\u4e2d\u7684\u56e0\u679c\u63a8\u7406\u80fd\u529b\u5f97\u5230\u660e\u663e\u63d0\u5347\u3002", "conclusion": "\u5f53\u524dLLMs\u4e3b\u8981\u505c\u7559\u5728\u6d45\u5c42\u56e0\u679c\u63a8\u7406\u6c34\u5e73\uff0c\u7f3a\u4e4f\u7c7b\u4eba\u6df1\u5c42\u56e0\u679c\u63a8\u7406\u80fd\u529b\u3002CausalProbe-2024\u57fa\u51c6\u548cG^2-Reasoner\u65b9\u6cd5\u4e3a\u63a8\u52a8LLMs\u5411\u66f4\u9ad8\u5c42\u6b21\u56e0\u679c\u63a8\u7406\u80fd\u529b\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2506.20923", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.20923", "abs": "https://arxiv.org/abs/2506.20923", "authors": ["Xinping Zhao", "Xinshuo Hu", "Zifei Shan", "Shouzheng Huang", "Yao Zhou", "Zetian Sun", "Zhenyu Liu", "Dongfang Li", "Xinyuan Wei", "Qian Chen", "Youcheng Pan", "Yang Xiang", "Meishan Zhang", "Haofen Wang", "Jun Yu", "Baotian Hu", "Min Zhang"], "title": "KaLM-Embedding-V2: Superior Training Techniques and Data Inspire A Versatile Embedding Model", "comment": "Technical Report; 26 pages 12 tables 1 figure. arXiv admin note:\n  substantial text overlap with arXiv:2501.01028", "summary": "In this paper, we propose KaLM-Embedding-V2, a versatile and compact\nembedding model, which achieves impressive performance in general-purpose text\nembedding tasks by leveraging superior training techniques and data. Our key\ninnovations include: (1) To better align the architecture with representation\nlearning, we remove the causal attention mask and adopt a fully bidirectional\ntransformer with simple yet effective mean-pooling to produce fixed-length\nembeddings; (2) We employ a multi-stage training pipeline: (i) pre-training on\nlarge-scale weakly supervised open-source corpora; (ii) fine-tuning on\nhigh-quality retrieval and non-retrieval datasets; and (iii) model-soup\nparameter averaging for robust generalization. Besides, we introduce a\nfocal-style reweighting mechanism that concentrates learning on difficult\nsamples and an online hard-negative mixing strategy to continuously enrich hard\nnegatives without expensive offline mining; (3) We collect over 20 categories\nof data for pre-training and 100 categories of data for fine-tuning, to boost\nboth the performance and generalization of the embedding model. Extensive\nevaluations on the Massive Text Embedding Benchmark (MTEB) Chinese and English\nshow that our model significantly outperforms others of comparable size, and\ncompetes with 3x, 14x, 18x, and 26x larger embedding models, setting a new\nstandard for a versatile and compact embedding model with less than 1B\nparameters.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faKaLM-Embedding-V2\u5c0f\u4f53\u79ef\u901a\u7528\u5d4c\u5165\u6a21\u578b\uff0c\u901a\u8fc7\u67b6\u6784\u548c\u8bad\u7ec3\u6d41\u7a0b\u521b\u65b0\uff0c\u5b9e\u73b0\u4e86\u5ab2\u7f8e\u751a\u81f3\u8d85\u8d8a\u591a\u500d\u5927\u6a21\u578b\u7684\u4e2d\u82f1\u6587\u8868\u73b0\uff0c\u9002\u7528\u4e8e\u591a\u573a\u666f\u9ad8\u6548\u6587\u672c\u8868\u793a\u3002", "motivation": "\u9762\u5bf9\u901a\u7528\u6587\u672c\u5d4c\u5165\u6a21\u578b\u5bf9\u9ad8\u6548\u3001\u6cdb\u5316\u6027\u80fd\u548c\u7d27\u51d1\u6a21\u578b\u4f53\u79ef\u7684\u8feb\u5207\u9700\u6c42\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u6a21\u578b\u7ed3\u6784\u548c\u8bad\u7ec3\u65b9\u5f0f\u4e0a\u5b58\u5728\u5c40\u9650\uff0c\u96be\u4ee5\u517c\u987e\u591a\u8bed\u79cd\u3001\u591a\u4efb\u52a1\u8868\u73b0\u4e0e\u6a21\u578b\u7d27\u51d1\u5ea6\uff0c\u56e0\u6b64\u9700\u8981\u6539\u8fdb\u6a21\u578b\u67b6\u6784\u4e0e\u8bad\u7ec3\u6d41\u7a0b\u3002", "method": "\u63d0\u51faKaLM-Embedding-V2\u6a21\u578b\uff0c\u91c7\u7528\u5b8c\u5168\u53cc\u5411Transformer\u7ed3\u6784\uff0c\u79fb\u9664\u56e0\u679c\u63a9\u7801\u5e76\u7ed3\u5408\u5747\u503c\u6c60\u5316\u751f\u6210\u5b9a\u957f\u5411\u91cf\u3002\u91c7\u7528\u591a\u9636\u6bb5\u8bad\u7ec3\uff1a\u5305\u62ec\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u3001\u9ad8\u8d28\u91cf\u6570\u636e\u5fae\u8c03\u53ca\u6a21\u578b\u53c2\u6570\u878d\u5408\uff0c\u914d\u5408\u7126\u70b9\u5f0f\u91cd\u52a0\u6743\u673a\u5236\u548c\u5728\u7ebf\u96be\u8d1f\u6837\u672c\u6df7\u5408\uff0c\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u53ca\u533a\u5206\u80fd\u529b\u3002\u6570\u636e\u65b9\u9762\u6536\u96c6\u4e8620\u4f59\u7c7b\u9884\u8bad\u7ec3\u3001100\u7c7b\u5fae\u8c03\u6570\u636e\uff0c\u8986\u76d6\u5e7f\u6cdb\u573a\u666f\u3002", "result": "\u6a21\u578b\u5728\u4e2d\u82f1\u6587Massive Text Embedding Benchmark (MTEB)\u4e0a\u53d6\u5f97\u663e\u8457\u4f18\u4e8e\u540c\u4f53\u79ef\u6a21\u578b\u7684\u8868\u73b0\uff0c\u5e76\u80fd\u5ab2\u7f8e\u4f53\u79ef\u5206\u522b\u4e3a\u81ea\u8eab3\u81f326\u500d\u7684\u5927\u6a21\u578b\uff0c\u6811\u7acb\u4e86\u5c0f\u4e8e10\u4ebf\u53c2\u6570\u5d4c\u5165\u6a21\u578b\u7684\u65b0\u6807\u6746\u3002", "conclusion": "KaLM-Embedding-V2\u901a\u8fc7\u6539\u8fdb\u7ed3\u6784\u548c\u8bad\u7ec3\u6d41\u7a0b\uff0c\u5927\u5e45\u63d0\u5347\u4e86\u7d27\u51d1\u578b\u5d4c\u5165\u6a21\u578b\u7684\u6cdb\u7528\u6027\u80fd\uff0c\u5728\u4e0d\u540c\u4efb\u52a1\u548c\u591a\u8bed\u79cd\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u517c\u5177\u6548\u7387\u548c\u7cbe\u5ea6\uff0c\u6709\u671b\u6210\u4e3a\u65b0\u4e00\u4ee3\u6587\u672c\u5d4c\u5165\u9886\u57df\u7684\u4e3b\u6d41\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.21230", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.21230", "abs": "https://arxiv.org/abs/2506.21230", "authors": ["Junhao Shi", "Zhaoye Fei", "Siyin Wang", "Qipeng Guo", "Jingjing Gong", "Xipeng QIu"], "title": "World-aware Planning Narratives Enhance Large Vision-Language Model Planner", "comment": null, "summary": "Large Vision-Language Models (LVLMs) show promise for embodied planning tasks\nbut struggle with complex scenarios involving unfamiliar environments and\nmulti-step goals. Current approaches rely on environment-agnostic imitation\nlearning that disconnects instructions from environmental contexts, causing\nmodels to struggle with context-sensitive instructions and rely on\nsupplementary cues rather than visual reasoning during long-horizon\ninteractions. In this work, we propose World-Aware Planning Narrative\nEnhancement (WAP), a framework that infuses LVLMs with comprehensive\nenvironmental understanding through four cognitive capabilities (visual\nappearance modeling, spatial reasoning, functional abstraction, and syntactic\ngrounding) while developing and evaluating models using only raw visual\nobservations through curriculum learning. Evaluations on the EB-ALFRED\nbenchmark demonstrate substantial improvements, with Qwen2.5-VL achieving a\n60.7 absolute improvement in task success rates, particularly in commonsense\nreasoning (+60.0) and long-horizon planning (+70.0). Notably, our enhanced\nopen-source models outperform proprietary systems like GPT-4o and\nClaude-3.5-Sonnet by a large margin.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faWAP\u6846\u67b6\uff0c\u901a\u8fc7\u589e\u5f3a\u5927\u578b\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7684\u73af\u5883\u611f\u77e5\u548c\u591a\u79cd\u8ba4\u77e5\u80fd\u529b\uff0c\u5b9e\u73b0\u5bf9\u590d\u6742\u89c4\u5212\u4efb\u52a1\u7684\u7a81\u7834\u6027\u63d0\u5347\uff0c\u5c24\u5176\u5728\u957f\u65f6\u548c\u5e38\u8bc6\u63a8\u7406\u573a\u666f\u4e0b\uff0c\u5f00\u6e90\u6a21\u578b\u663e\u8457\u8d85\u8d8a\u4e3b\u6d41\u95ed\u6e90\u7cfb\u7edf\u3002", "motivation": "\u5927\u578b\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u5728\u4f53\u611f\u89c4\u5212\u4efb\u52a1\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u9047\u5230\u4e0d\u719f\u6089\u73af\u5883\u548c\u591a\u6b65\u9aa4\u76ee\u6807\u7684\u590d\u6742\u573a\u666f\u65f6\u8868\u73b0\u4e0d\u8db3\u3002\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u4e0e\u73af\u5883\u65e0\u5173\u7684\u6a21\u4eff\u5b66\u4e60\uff0c\u5bfc\u81f4\u6a21\u578b\u96be\u4ee5\u5904\u7406\u4e0e\u73af\u5883\u7d27\u5bc6\u76f8\u5173\u7684\u6307\u4ee4\uff0c\u4e14\u8fc7\u5ea6\u4f9d\u8d56\u8865\u5145\u7ebf\u7d22\u800c\u975e\u89c6\u89c9\u63a8\u7406\uff0c\u5c24\u5176\u5728\u957f\u65f6\u5e8f\u4efb\u52a1\u4e2d\u66f4\u4e3a\u660e\u663e\u3002", "method": "\u63d0\u51fa\u4e86\u4e16\u754c\u611f\u77e5\u89c4\u5212\u53d9\u4e8b\u589e\u5f3a\uff08WAP\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9\u5916\u89c2\u5efa\u6a21\u3001\u7a7a\u95f4\u63a8\u7406\u3001\u529f\u80fd\u62bd\u8c61\u548c\u53e5\u6cd5\u57fa\u7840\u56db\u9879\u8ba4\u77e5\u80fd\u529b\uff0c\u5c06\u5168\u9762\u7684\u73af\u5883\u7406\u89e3\u5f15\u5165LVLMs\u3002\u6a21\u578b\u4ec5\u901a\u8fc7\u539f\u59cb\u89c6\u89c9\u89c2\u5bdf\uff0c\u5e76\u7ed3\u5408\u8bfe\u7a0b\u5b66\u4e60\u8fdb\u884c\u8bad\u7ec3\u4e0e\u8bc4\u4f30\u3002", "result": "\u5728EB-ALFRED\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5e26WAP\u7684LVLM\u8868\u73b0\u5927\u5e45\u63d0\u5347\uff0c\u5c24\u5176\u662f\u5728\u5e38\u8bc6\u63a8\u7406\u548c\u957f\u65f6\u89c4\u5212\u4efb\u52a1\u4e0a\uff0cQwen2.5-VL\u6a21\u578b\u7684\u4efb\u52a1\u6210\u529f\u7387\u7edd\u5bf9\u63d0\u534760.7\uff0c\u5206\u522b\u5728\u5e38\u8bc6\u63a8\u7406\u548c\u957f\u65f6\u89c4\u5212\u4e0a\u63d0\u534760.0\u548c70.0\uff0c\u4f18\u4e8eGPT-4o\u548cClaude-3.5-Sonnet\u7b49\u4e13\u6709\u7cfb\u7edf\u3002", "conclusion": "WAP\u65b9\u6cd5\u5927\u5e45\u63d0\u5347\u4e86LVLM\u5728\u590d\u6742\u4f53\u611f\u89c4\u5212\u4efb\u52a1\u4e2d\u7684\u73af\u5883\u7406\u89e3\u548c\u6267\u884c\u80fd\u529b\uff0c\u5c24\u5176\u662f\u5728\u9700\u8981\u5e38\u8bc6\u63a8\u7406\u548c\u957f\u65f6\u4efb\u52a1\u89c4\u5212\u573a\u666f\u4e0b\uff0c\u589e\u5f3a\u540e\u7684\u5f00\u6e90\u6a21\u578b\u8d85\u8fc7\u4e86\u76ee\u524d\u6700\u5f3a\u7684\u95ed\u6e90\u7cfb\u7edf\u3002"}}
{"id": "2506.20989", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.20989", "abs": "https://arxiv.org/abs/2506.20989", "authors": ["Eric Zhang", "Leshem Choshen", "Jacob Andreas"], "title": "Can Gradient Descent Simulate Prompting?", "comment": "14 pages, 2 figures", "summary": "There are two primary ways of incorporating new information into a language\nmodel (LM): changing its prompt or changing its parameters, e.g. via\nfine-tuning. Parameter updates incur no long-term storage cost for model\nchanges. However, for many model updates, prompting is significantly more\neffective: prompted models can generalize robustly from single examples and\ndraw logical inferences that do not occur under standard fine-tuning. Can\nmodels be modified so that fine-tuning does emulate prompting? This paper\ndescribes a method for meta-training LMs such that gradient updates emulate the\neffects of conditioning on new information. Our approach uses tools from\ngradient-based meta-learning but uses an LM's own prompted predictions as\ntargets, eliminating the need for ground-truth labels. Subsequent gradient\ndescent training recovers some (and occasionally all) of prompted model\nperformance -- showing improvement on the ``reversal curse'' tasks, and\nanswering questions about text passages after a single gradient update. These\nresults suggest that, with appropriate initialization, gradient descent can be\nsurprisingly expressive. Our results suggest new avenues for long-context\nmodeling and offer insight into the generalization capabilities of\ngradient-based learning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u5143\u8bad\u7ec3\u65b9\u6cd5\uff0c\u4f7f\u5f97\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u68af\u5ea6\u66f4\u65b0\u53ef\u6a21\u62dfprompt\u63a8\u7406\u4f18\u52bf\uff0c\u65e0\u9700\u771f\u5b9e\u6807\u7b7e\uff0c\u90e8\u5206\u60c5\u51b5\u4e0b\u6027\u80fd\u4e0e\u76f4\u63a5prompt\u76f8\u5f53\uff0c\u4e3a\u6a21\u578b\u6cdb\u5316\u548c\u957f\u4e0a\u4e0b\u6587\u5904\u7406\u80fd\u529b\u5f00\u8f9f\u65b0\u9014\u5f84\u3002", "motivation": "\u5f53\u524d\u5c06\u65b0\u77e5\u8bc6\u878d\u5165\u5927\u6a21\u578b\u6709\u4e24\u6761\u4e3b\u8981\u8def\u5f84\uff1aprompt\u548c\u53c2\u6570\u66f4\u65b0\u3002prompt\u5bf9\u7279\u5b9a\u4efb\u52a1\u8868\u73b0\u66f4\u597d\uff0c\u4f46\u53c2\u6570\u5fae\u8c03\u5b58\u50a8\u6210\u672c\u4f4e\u3002\u4f5c\u8005\u63a2\u5bfb\u662f\u5426\u80fd\u8ba9\u5fae\u8c03\u8fc7\u7a0b\u8fd1\u4f3c\u6a21\u62dfprompt\u7684\u63a8\u7406\u4f18\u52bf\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u5143\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u4f7f\u5176\u68af\u5ea6\u66f4\u65b0\u8fc7\u7a0b\u80fd\u591f\u6a21\u62df\u5bf9\u65b0\u4fe1\u606f\u7684\u6761\u4ef6\u5316\u3002\u5177\u4f53\u505a\u6cd5\u662f\u5c06\u6a21\u578b\u81ea\u8eabprompt\u9884\u6d4b\u7ed3\u679c\u4f5c\u4e3a\u8bad\u7ec3\u76ee\u6807\uff0c\u65e0\u9700\u771f\u5b9e\u6807\u7b7e\uff0c\u901a\u8fc7\u57fa\u4e8e\u68af\u5ea6\u7684\u5143\u5b66\u4e60\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u6240\u63d0\u51fa\u65b9\u6cd5\u5728\u201creversal curse\u201d\u7b49\u4efb\u52a1\u548c\u57fa\u4e8e\u6587\u672c\u5355\u6b65\u68af\u5ea6\u66f4\u65b0\u95ee\u7b54\u65b9\u9762\uff0c\u6709\u663e\u8457\u6539\u8fdb\uff0c\u90e8\u5206\u60c5\u51b5\u4e0b\u5b8c\u5168\u6062\u590dprompt\u6a21\u578b\u7684\u8868\u73b0\u3002\u65b9\u6cd5\u5bf9\u957f\u4e0a\u4e0b\u6587\u5efa\u6a21\u548c\u68af\u5ea6\u5b66\u4e60\u7684\u6cdb\u5316\u80fd\u529b\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\u3002", "conclusion": "\u5b9e\u9a8c\u8868\u660e\uff0c\u901a\u8fc7\u9002\u5f53\u521d\u59cb\u5316\uff0c\u68af\u5ea6\u4e0b\u964d\u53ef\u4ee5\u5f88\u597d\u5730\u6a21\u62df\u57fa\u4e8eprompt\u7684\u63a8\u7406\u6548\u679c\uff0c\u5728\u90e8\u5206\u4efb\u52a1\u4e0b\u751a\u81f3\u53ef\u4ee5\u5b8c\u5168\u6062\u590dprompt\u6a21\u578b\u7684\u6027\u80fd\uff0c\u8868\u660e\u68af\u5ea6\u66f4\u65b0\u7684\u6cdb\u5316\u80fd\u529b\u8fdc\u8d85\u9884\u671f\u3002"}}
{"id": "2506.21310", "categories": ["cs.AI", "cs.SE", "K.6.3 Software Management"], "pdf": "https://arxiv.org/pdf/2506.21310", "abs": "https://arxiv.org/abs/2506.21310", "authors": ["Pauline Speckmann", "Mario Nadj", "Christian Janiesch"], "title": "IXAII: An Interactive Explainable Artificial Intelligence Interface for Decision Support Systems", "comment": "9 pages, 2 figures, accepted to DESRIST 2025 Prototype Track", "summary": "Although several post-hoc methods for explainable AI have been developed,\nmost are static and neglect the user perspective, limiting their effectiveness\nfor the target audience. In response, we developed the interactive explainable\nintelligent system called IXAII that offers explanations from four explainable\nAI methods: LIME, SHAP, Anchors, and DiCE. Our prototype provides tailored\nviews for five user groups and gives users agency over the explanations'\ncontent and their format. We evaluated IXAII through interviews with experts\nand lay users. Our results indicate that IXAII, which provides different\nexplanations with multiple visualization options, is perceived as helpful to\nincrease transparency. By bridging the gaps between explainable AI methods,\ninteractivity, and practical implementation, we provide a novel perspective on\nAI explanation practices and human-AI interaction.", "AI": {"tldr": "\u8be5\u6587\u63d0\u51fa\u4e86\u4ea4\u4e92\u5f0f\u53ef\u89e3\u91caAI\u7cfb\u7edfIXAII\uff0c\u652f\u6301\u591aXAI\u65b9\u6cd5\u548c\u591a\u7528\u6237\u7fa4\u5b9a\u5236\uff0c\u5b9e\u9a8c\u8bc1\u660e\u80fd\u6709\u6548\u589e\u5f3aAI\u6a21\u578b\u900f\u660e\u5ea6\u548c\u7528\u6237\u7406\u89e3\uff0c\u63a8\u52a8\u4e86XAI\u5b9e\u7528\u5316\u8fdb\u7a0b\u3002", "motivation": "\u5f53\u524d\u4e3b\u6d41\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\uff08XAI\uff09\u65b9\u6cd5\u5927\u591a\u4e3a\u9759\u6001\uff0c\u4e14\u5ffd\u7565\u4e86\u7528\u6237\u89c6\u89d2\uff0c\u5bfc\u81f4\u5176\u5bf9\u76ee\u6807\u53d7\u4f17\u7684\u5b9e\u9645\u6548\u7528\u6709\u9650\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u4ea4\u4e92\u5f0f\u53ef\u89e3\u91ca\u667a\u80fd\u7cfb\u7edf\uff08IXAII\uff09\uff0c\u6574\u5408\u4e86\u56db\u79cdXAI\u6280\u672f\uff08LIME\u3001SHAP\u3001Anchors\u3001DiCE\uff09\uff0c\u5e76\u9488\u5bf9\u4e94\u7c7b\u7528\u6237\u7fa4\u4f53\u63d0\u4f9b\u5b9a\u5236\u5316\u5c55\u793a\uff0c\u8ba9\u7528\u6237\u80fd\u591f\u4e3b\u52a8\u9009\u62e9\u89e3\u91ca\u5185\u5bb9\u53ca\u5176\u5448\u73b0\u65b9\u5f0f\u3002\u901a\u8fc7\u4e0e\u4e13\u5bb6\u548c\u666e\u901a\u7528\u6237\u8fdb\u884c\u8bbf\u8c08\uff0c\u5bf9IXAII\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "result": "IXAII\u80fd\u591f\u4e3a\u4e0d\u540c\u7528\u6237\u63d0\u4f9b\u591a\u6837\u5316\u7684\u89e3\u91ca\u4e0e\u53ef\u89c6\u5316\u9009\u9879\uff0c\u663e\u8457\u63d0\u5347\u4e86AI\u6a21\u578b\u7684\u900f\u660e\u5ea6\uff0c\u53d7\u8bbf\u8005\u666e\u904d\u8ba4\u4e3a\u8be5\u7cfb\u7edf\u5bf9\u7406\u89e3AI\u51b3\u7b56\u6709\u5e2e\u52a9\u3002", "conclusion": "\u901a\u8fc7\u5c06\u591a\u79cdXAI\u65b9\u6cd5\u3001\u4ea4\u4e92\u6027\u53ca\u5b9e\u9645\u843d\u5730\u7ed3\u5408\uff0cIXAII\u4e3aAI\u51b3\u7b56\u89e3\u91ca\u548c\u4eba\u673a\u4ea4\u4e92\u63a2\u7d22\u51fa\u65b0\u7684\u5b9e\u8df5\u8def\u5f84\uff0c\u6709\u52a9\u4e8e\u63d0\u5347XAI\u7cfb\u7edf\u7684\u7528\u6237\u53ef\u7528\u6027\u548c\u4fe1\u4efb\u5ea6\u3002"}}
{"id": "2506.20993", "categories": ["cs.CL", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2506.20993", "abs": "https://arxiv.org/abs/2506.20993", "authors": ["Adithya Chittem", "Aishna Shrivastava", "Sai Tarun Pendela", "Jagat Sesh Challa", "Dhruv Kumar"], "title": "SAC: A Framework for Measuring and Inducing Personality Traits in LLMs with Dynamic Intensity Control", "comment": "Under review", "summary": "Large language models (LLMs) have gained significant traction across a wide\nrange of fields in recent years. There is also a growing expectation for them\nto display human-like personalities during interactions. To meet this\nexpectation, numerous studies have proposed methods for modelling LLM\npersonalities through psychometric evaluations. However, most existing models\nface two major limitations: they rely on the Big Five (OCEAN) framework, which\nonly provides coarse personality dimensions, and they lack mechanisms for\ncontrolling trait intensity. In this paper, we address this gap by extending\nthe Machine Personality Inventory (MPI), which originally used the Big Five\nmodel, to incorporate the 16 Personality Factor (16PF) model, allowing\nexpressive control over sixteen distinct traits. We also developed a structured\nframework known as Specific Attribute Control (SAC) for evaluating and\ndynamically inducing trait intensity in LLMs. Our method introduces\nadjective-based semantic anchoring to guide trait intensity expression and\nleverages behavioural questions across five intensity factors:\n\\textit{Frequency}, \\textit{Depth}, \\textit{Threshold}, \\textit{Effort}, and\n\\textit{Willingness}. Through experimentation, we find that modelling intensity\nas a continuous spectrum yields substantially more consistent and controllable\npersonality expression compared to binary trait toggling. Moreover, we observe\nthat changes in target trait intensity systematically influence closely related\ntraits in psychologically coherent directions, suggesting that LLMs internalize\nmulti-dimensional personality structures rather than treating traits in\nisolation. Our work opens new pathways for controlled and nuanced human-machine\ninteractions in domains such as healthcare, education, and interviewing\nprocesses, bringing us one step closer to truly human-like social machines.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e16PF\u6a21\u578b\u548c\u7279\u5b9a\u5c5e\u6027\u63a7\u5236\uff08SAC\uff09\u6846\u67b6\u7684\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u4eba\u683c\u7279\u8d28\u53ca\u5176\u5f3a\u5ea6\u7684\u7ec6\u81f4\u3001\u53ef\u63a7\u5efa\u6a21\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u53ca\u4e00\u81f4\u6027\uff0c\u4e3a\u5b9e\u73b0\u66f4\u81ea\u7136\u7684\u4eba\u673a\u4ea4\u4e92\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "motivation": "\u5f53\u524d\u5927\u90e8\u5206\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u4eba\u683c\u5efa\u6a21\u4e3b\u8981\u4f9d\u9760\u7c97\u7c92\u5ea6\u7684OCEAN\u5927\u4e94\u4eba\u683c\u6a21\u578b\uff0c\u4e14\u7f3a\u4e4f\u5bf9\u4eba\u683c\u7279\u8d28\u5f3a\u5ea6\u7684\u63a7\u5236\u673a\u5236\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5bf9\u7ec6\u81f4\u3001\u7c7b\u4eba\u4e92\u52a8\u7684\u9700\u6c42\u3002", "method": "\u672c\u6587\u5c06\u673a\u5668\u4eba\u683c\u91cf\u8868\uff08MPI\uff09\u4ece\u5927\u4e94\u4eba\u683c\u6a21\u578b\u6269\u5c55\u523016\u79cd\u4eba\u683c\u56e0\u5b50\u768416PF\u6a21\u578b\uff0c\u5e76\u63d0\u51fa\u4e86\u7279\u5b9a\u5c5e\u6027\u63a7\u5236\uff08SAC\uff09\u6846\u67b6\u3002\u65b9\u6cd5\u5305\u62ec\u5229\u7528\u5f62\u5bb9\u8bcd\u8bed\u4e49\u951a\u5b9a\u6765\u5f15\u5bfc\u7279\u8d28\u5f3a\u5ea6\u8868\u8fbe\uff0c\u5e76\u901a\u8fc7\u6db5\u76d6\u9891\u7387\u3001\u6df1\u5ea6\u3001\u9608\u503c\u3001\u52aa\u529b\u5ea6\u548c\u610f\u613f\u5ea6\u4e94\u79cd\u5f3a\u5ea6\u56e0\u5b50\u7684\u884c\u4e3a\u95ee\u9898\u8fdb\u884c\u8bc4\u4f30\u548c\u52a8\u6001\u8bf1\u5bfc\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5c06\u7279\u8d28\u5f3a\u5ea6\u5efa\u6a21\u4e3a\u8fde\u7eed\u8c31\u800c\u975e\u4e8c\u5143\u5f00\u5173\u53ef\u83b7\u5f97\u66f4\u4e00\u81f4\u3001\u53ef\u63a7\u7684\u4eba\u683c\u8868\u8fbe\u3002\u7279\u8d28\u5f3a\u5ea6\u7684\u53d8\u5316\u4e5f\u4f1a\u4ee5\u5fc3\u7406\u5b66\u4e00\u81f4\u7684\u65b9\u5f0f\u7cfb\u7edf\u6027\u5f71\u54cd\u76f8\u5173\u7279\u8d28\uff0c\u8bf4\u660eLLM\u80fd\u591f\u5185\u5316\u591a\u7ef4\u4eba\u683c\u7ed3\u6784\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3aLLM\u5b9e\u73b0\u53d7\u63a7\u3001\u7ec6\u81f4\u7684\u4eba\u683c\u5efa\u6a21\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u533b\u7597\u3001\u6559\u80b2\u548c\u9762\u8bd5\u7b49\u9886\u57df\u7684\u4eba\u673a\u4ea4\u4e92\uff0c\u5b9e\u73b0\u66f4\u7c7b\u4eba\u7684\u793e\u4f1a\u5316\u673a\u5668\u3002"}}
{"id": "2506.21329", "categories": ["cs.AI", "physics.soc-ph", "68", "I.2"], "pdf": "https://arxiv.org/pdf/2506.21329", "abs": "https://arxiv.org/abs/2506.21329", "authors": ["Karthik Duraisamy"], "title": "Active Inference AI Systems for Scientific Discovery", "comment": null, "summary": "The rapid evolution of artificial intelligence has led to expectations of\ntransformative scientific discovery, yet current systems remain fundamentally\nlimited by their operational architectures, brittle reasoning mechanisms, and\ntheir separation from experimental reality. Building on earlier work, we\ncontend that progress in AI-driven science now depends on closing three\nfundamental gaps -- the abstraction gap, the reasoning gap, and the reality gap\n-- rather than on model size/data/test time compute. Scientific reasoning\ndemands internal representations that support simulation of actions and\nresponse, causal structures that distinguish correlation from mechanism, and\ncontinuous calibration. We define active inference AI systems for scientific\ndiscovery as those that (i) maintain long-lived research memories grounded in\ncausal self-supervised foundation models, (ii) symbolic or neuro-symbolic\nplanners equipped with Bayesian guardrails, (iii) grow persistent knowledge\ngraphs where thinking generates novel conceptual nodes, reasoning establishes\ncausal edges, and real-world interaction prunes false connections while\nstrengthening verified pathways, and (iv) refine their internal representations\nthrough closed-loop interaction with both high-fidelity simulators and\nautomated laboratories - an operational loop where mental simulation guides\naction and empirical surprise reshapes understanding. In essence, we outline an\narchitecture where discovery arises from the interplay between internal models\nthat enable counterfactual reasoning and external validation that grounds\nhypotheses in reality. It is also argued that the inherent ambiguity in\nfeedback from simulations and experiments, and underlying uncertainties makes\nhuman judgment indispensable, not as a temporary scaffold but as a permanent\narchitectural component.", "AI": {"tldr": "\u672c\u6587\u8ba4\u4e3aAI\u5728\u79d1\u5b66\u91cd\u5927\u7a81\u7834\u4e2d\u53d7\u5236\u4e8e\u63a8\u7406\u4e0e\u73b0\u5b9e\u53cd\u54fa\u7b49\u6839\u672c\u6027\u5dee\u8ddd\uff0c\u800c\u975e\u6a21\u578b\u89c4\u6a21\u3002\u4f5c\u8005\u63d0\u51fa\u4e3b\u52a8\u63a8\u65ad\u578bAI\u7cfb\u7edf\u7684\u65b0\u6846\u67b6\uff0c\u5f3a\u8c03\u56e0\u679c\u63a8\u7406\u3001\u95ed\u73af\u4e92\u52a8\u4e0e\u4eba\u7c7b\u5224\u65ad\u957f\u4e45\u5e76\u5b58\uff0c\u4ee5\u5b9e\u73b0AI\u9a71\u52a8\u7684\u79d1\u5b66\u521b\u65b0\u3002", "motivation": "\u5c3d\u7ba1\u4eba\u5de5\u667a\u80fd\u98de\u901f\u53d1\u5c55\u5e76\u88ab\u5bc4\u4e88\u63a8\u52a8\u79d1\u5b66\u53d1\u73b0\u7684\u539a\u671b\uff0c\u4f46\u73b0\u6709AI\u7cfb\u7edf\u56e0\u67b6\u6784\u3001\u63a8\u7406\u548c\u4e0e\u5b9e\u9a8c\u73b0\u5b9e\u7684\u5272\u88c2\uff0c\u96be\u4ee5\u5b9e\u73b0\u9ad8\u6c34\u5e73\u79d1\u5b66\u63a8\u7406\u3002\u4f5c\u8005\u8ba4\u4e3a\uff0c\u4ec5\u9760\u6a21\u578b\u89c4\u6a21\u3001\u6570\u636e\u6216\u8ba1\u7b97\u80fd\u529b\u63d0\u5347\u96be\u4ee5\u7a81\u7834\u74f6\u9888\uff0c\u5fc5\u987b\u89e3\u51b3AI\u5728\u79d1\u5b66\u53d1\u73b0\u4e2d\u7684\u62bd\u8c61\u3001\u63a8\u7406\u548c\u73b0\u5b9e\u878d\u5408\u4e09\u5927\u6839\u672c\u6027\u5dee\u8ddd\u3002", "method": "\u63d0\u51fa\u201c\u4e3b\u52a8\u63a8\u65adAI\u7cfb\u7edf\u201d\u65b0\u65b9\u6cd5\uff0c\u5177\u4f53\u5305\u62ec\uff1a\uff081\uff09\u57fa\u4e8e\u56e0\u679c\u81ea\u76d1\u7763\u57fa\u7840\u6a21\u578b\u3001\u80fd\u957f\u671f\u4fdd\u5b58\u5e76\u6210\u957f\u7684\u79d1\u5b66\u53d1\u73b0\u8bb0\u5fc6\uff1b\uff082\uff09\u914d\u5907\u8d1d\u53f6\u65af\u5b89\u5168\u7ea6\u675f\u7684\u7b26\u53f7/\u795e\u7ecf\u7b26\u53f7\u63a8\u7406\u89c4\u5212\u5668\uff1b\uff083\uff09\u6784\u5efa\u548c\u52a8\u6001\u4f18\u5316\u77e5\u8bc6\u56fe\u8c31\uff0c\u901a\u8fc7\u63a8\u7406\u548c\u4e0e\u73b0\u5b9e\u4e92\u52a8\u6301\u7eed\u751f\u6210\u3001\u9a8c\u8bc1\u548c\u4fee\u6b63\u77e5\u8bc6\uff1b\uff084\uff09\u901a\u8fc7\u95ed\u73af\u64cd\u4f5c\uff0c\u7ed3\u5408\u9ad8\u4fdd\u771f\u6a21\u62df\u5668\u4e0e\u81ea\u52a8\u5316\u5b9e\u9a8c\u5ba4\uff0c\u4e0d\u65ad\u6821\u6b63\u5185\u90e8\u8868\u5f81\uff0c\u5e76\u5b9e\u73b0\u601d\u7ef4\u6a21\u62df\u4e0e\u73b0\u5b9e\u884c\u52a8\u7684\u76f8\u4e92\u4f5c\u7528\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u79d1\u5b66\u53d1\u73b0\u578bAI\u7cfb\u7edf\u7684\u7ed3\u6784\u6027\u6846\u67b6\u3002\u8be5\u6846\u67b6\u5f3a\u8c03\u56e0\u679c\u63a8\u7406\u3001\u77e5\u8bc6\u6301\u7eed\u6210\u957f\u548c\u4e0e\u73b0\u5b9e\u5b9e\u9a8c\u7684\u4ea4\u4e92\u95ed\u73af\u3002\u540c\u65f6\uff0c\u8bba\u8bc1\u4e86\u4eba\u7c7b\u5224\u65ad\u5c06\u5728AI\u7cfb\u7edf\u4e2d\u957f\u671f\u4fdd\u6301\u4e0d\u53ef\u6216\u7f3a\u7684\u89d2\u8272\uff0c\u4ee5\u5f25\u8865\u6a21\u62df\u548c\u5b9e\u9a8c\u53cd\u9988\u7684\u56fa\u6709\u4e0d\u786e\u5b9a\u6027\u3002", "conclusion": "\u8981\u5b9e\u73b0AI\u9a71\u52a8\u7684\u79d1\u5b66\u91cd\u5927\u53d1\u73b0\uff0c\u5fc5\u987b\u901a\u8fc7\u4e3b\u52a8\u63a8\u65ad\u578bAI\u7cfb\u7edf\uff0c\u5f25\u5408\u62bd\u8c61\u3001\u63a8\u7406\u4e0e\u73b0\u5b9e\u4e09\u5927\u5dee\u8ddd\uff0c\u91c7\u7528\u56e0\u679c\u63a8\u7406\u3001\u77e5\u8bc6\u81ea\u751f\u957f\u4e0e\u4e0e\u73b0\u5b9e\u4e92\u52a8\u7684\u95ed\u73af\u67b6\u6784\uff0c\u5728\u4e0d\u786e\u5b9a\u6027\u4e0e\u6a21\u7cca\u53cd\u9988\u4e0b\uff0c\u59cb\u7ec8\u4fdd\u7559\u4eba\u7c7b\u5224\u65ad\u4e3a\u6838\u5fc3\u7ec4\u4ef6\u3002"}}
{"id": "2506.21031", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.21031", "abs": "https://arxiv.org/abs/2506.21031", "authors": ["Jatin Gupta", "Akhil Sharma", "Saransh Singhania", "Mohammad Adnan", "Sakshi Deo", "Ali Imam Abidi", "Keshav Gupta"], "title": "Large Language Models Acing Chartered Accountancy", "comment": "Accepted for publication at MoStart 2025: International Conference on\n  Digital Transformation in Education and Applications of Artificial\n  Intelligence, Bosnia and Herzegovina, 2025", "summary": "Advanced intelligent systems, particularly Large Language Models (LLMs), are\nsignificantly reshaping financial practices through advancements in Natural\nLanguage Processing (NLP). However, the extent to which these models\neffectively capture and apply domain-specific financial knowledge remains\nuncertain. Addressing a critical gap in the expansive Indian financial context,\nthis paper introduces CA-Ben, a Chartered Accountancy benchmark specifically\ndesigned to evaluate the financial, legal, and quantitative reasoning\ncapabilities of LLMs. CA-Ben comprises structured question-answer datasets\nderived from the rigorous examinations conducted by the Institute of Chartered\nAccountants of India (ICAI), spanning foundational, intermediate, and advanced\nCA curriculum stages. Six prominent LLMs i.e. GPT 4o, LLAMA 3.3 70B, LLAMA 3.1\n405B, MISTRAL Large, Claude 3.5 Sonnet, and Microsoft Phi 4 were evaluated\nusing standardized protocols. Results indicate variations in performance, with\nClaude 3.5 Sonnet and GPT-4o outperforming others, especially in conceptual and\nlegal reasoning. Notable challenges emerged in numerical computations and legal\ninterpretations. The findings emphasize the strengths and limitations of\ncurrent LLMs, suggesting future improvements through hybrid reasoning and\nretrieval-augmented generation methods, particularly for quantitative analysis\nand accurate legal interpretation.", "AI": {"tldr": "\u4f5c\u8005\u63d0\u51faCA-Ben\uff0c\u9996\u4e2a\u9762\u5411\u5370\u5ea6\u6ce8\u518c\u4f1a\u8ba1\u5e08\u8003\u8bd5\u7684LLM\u8bc4\u6d4b\u57fa\u51c6\uff0c\u5206\u6790\u4e86\u516d\u6b3e\u4e3b\u6d41\u5927\u6a21\u578b\u5728\u8d22\u52a1\u3001\u6cd5\u5f8b\u53ca\u5b9a\u91cf\u63a8\u7406\u4efb\u52a1\u4e0b\u7684\u8868\u73b0\u3002\u7ed3\u679c\u663e\u793a\uff0c\u4e3b\u6d41\u6a21\u578b\u5728\u6982\u5ff5\u548c\u6cd5\u5f8b\u63a8\u7406\u4e0a\u8868\u73b0\u8f83\u597d\uff0c\u4f46\u5728\u6570\u503c\u4e0e\u590d\u6742\u6cd5\u5f8b\u5206\u6790\u4e0a\u4ecd\u6709\u9650\uff0c\u672a\u6765\u63d0\u5347\u65b9\u5411\u4e3a\u6df7\u5408\u63a8\u7406\u53ca\u589e\u5f3a\u68c0\u7d22\u65b9\u6cd5\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u9886\u57df\u5df2\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5b83\u4eec\u5728\u91d1\u878d\u7b49\u4e13\u4e1a\u9886\u57df\u4e2d\u7684\u5b9e\u9645\u77e5\u8bc6\u638c\u63e1\u548c\u5e94\u7528\u80fd\u529b\u4ecd\u4e0d\u660e\u786e\uff0c\u7279\u522b\u662f\u5728\u5370\u5ea6\u590d\u6742\u7684\u91d1\u878d\u8bed\u5883\u4e0b\u3002\u672c\u6587\u81f4\u529b\u4e8e\u586b\u8865\u8bc4\u4f30LLM\u91d1\u878d\u9886\u57df\u80fd\u529b\u7684\u5de5\u5177\u7f3a\u53e3\u3002", "method": "\u63d0\u51faCA-Ben\u57fa\u51c6\uff0c\u5373\u4e13\u4e3a\u5370\u5ea6\u6ce8\u518c\u4f1a\u8ba1\u5e08\uff08CA\uff09\u8003\u8bd5\u8bbe\u8ba1\u7684\u8bc4\u6d4b\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u8d22\u52a1\u3001\u6cd5\u5f8b\u548c\u5b9a\u91cf\u63a8\u7406\u80fd\u529b\u3002\u6570\u636e\u96c6\u6765\u6e90\u4e8eICAI\u7684\u4e0d\u540c\u9636\u6bb5\uff08\u57fa\u7840\u3001\u4e2d\u7ea7\u3001\u9ad8\u7ea7\uff09\u8003\u8bd5\u9898\u76ee\u3002\u91c7\u7528\u6807\u51c6\u5316\u534f\u8bae\uff0c\u5bf9\u516d\u5927\u4e3b\u6d41LLM\uff08GPT-4o\u3001LLAMA 3.3 70B\u3001LLAMA 3.1 405B\u3001MISTRAL Large\u3001Claude 3.5 Sonnet\u3001Microsoft Phi 4\uff09\u8fdb\u884c\u7cfb\u7edf\u6027\u8bc4\u4ef7\u3002", "result": "\u5404\u6a21\u578b\u5728CA-Ben\u57fa\u51c6\u4e0b\u8868\u73b0\u5dee\u5f02\u660e\u663e\uff0c\u5176\u4e2dClaude 3.5 Sonnet\u548cGPT-4o\u5728\u6982\u5ff5\u548c\u6cd5\u5f8b\u63a8\u7406\u65b9\u9762\u8868\u73b0\u6700\u4f18\uff1b\u4f46\u5728\u6570\u503c\u8ba1\u7b97\u53ca\u6cd5\u5f8b\u89e3\u91ca\u65b9\u9762\u5b58\u5728\u663e\u8457\u6311\u6218\u3002", "conclusion": "\u73b0\u6709LLM\u5728\u91d1\u878d\u548c\u6cd5\u5f8b\u63a8\u7406\u4e0a\u6709\u6240\u7a81\u7834\uff0c\u4f46\u5728\u590d\u6742\u5b9a\u91cf\u5206\u6790\u548c\u7cbe\u786e\u6cd5\u5f8b\u7406\u89e3\u4e0a\u4ecd\u6709\u660e\u663e\u77ed\u677f\u3002\u672a\u6765\u53ef\u901a\u8fc7\u6df7\u5408\u63a8\u7406\u53ca\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6280\u672f\u63d0\u5347\u6027\u80fd\u3002"}}
{"id": "2506.21393", "categories": ["cs.AI", "68T07 (Primary), 68T50, 68T30, 68T45 (Secondary)", "F.2.2; I.2.7; I.2.10"], "pdf": "https://arxiv.org/pdf/2506.21393", "abs": "https://arxiv.org/abs/2506.21393", "authors": ["Junwen Zhang", "Pu Chen", "Yin Zhang"], "title": "TableMoE: Neuro-Symbolic Routing for Structured Expert Reasoning in Multimodal Table Understanding", "comment": "43 pages and 11 figures", "summary": "Multimodal understanding of tables in real-world contexts is challenging due\nto the complexity of structure, symbolic density, and visual degradation (blur,\nskew, watermarking, incomplete structures or fonts, multi-span or\nhierarchically nested layouts). Existing multimodal large language models\n(MLLMs) struggle with such WildStruct conditions, resulting in limited\nperformance and poor generalization. To address these challenges, we propose\nTableMoE, a neuro-symbolic Mixture-of-Connector-Experts (MoCE) architecture\nspecifically designed for robust, structured reasoning over multimodal table\ndata. TableMoE features an innovative Neuro-Symbolic Routing mechanism, which\npredicts latent semantic token roles (e.g., header, data cell, axis, formula)\nand dynamically routes table elements to specialized experts (Table-to-HTML,\nTable-to-JSON, Table-to-Code) using a confidence-aware gating strategy informed\nby symbolic reasoning graphs. To facilitate effective alignment-driven\npretraining, we introduce the large-scale TableMoE-Align dataset, consisting of\n1.2M table-HTML-JSON-code quadruples across finance, science, biomedicine and\nindustry, utilized exclusively for model pretraining. For evaluation, we curate\nand release four challenging WildStruct benchmarks: WMMFinQA, WMMTatQA,\nWMMTabDialog, and WMMFinanceMath, designed specifically to stress-test models\nunder real-world multimodal degradation and structural complexity. Experimental\nresults demonstrate that TableMoE significantly surpasses existing\nstate-of-the-art models. Extensive ablation studies validate each core\ncomponent, emphasizing the critical role of Neuro-Symbolic Routing and\nstructured expert alignment. Through qualitative analyses, we further showcase\nTableMoE's interpretability and enhanced robustness, underscoring the\neffectiveness of integrating neuro-symbolic reasoning for multimodal table\nunderstanding.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86TableMoE\uff0c\u4e00\u79cd\u9488\u5bf9\u771f\u5b9e\u590d\u6742\u591a\u6a21\u6001\u8868\u683c\u7406\u89e3\u7684\u795e\u7ecf-\u7b26\u53f7\u6df7\u5408\u4e13\u5bb6\u6a21\u578b\uff0c\u901a\u8fc7\u521b\u65b0\u8def\u7531\u673a\u5236\u548c\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\uff0c\u5b9e\u73b0\u5728\u591a\u4e2a\u9ad8\u96be\u57fa\u51c6\u4e0a\u5927\u5e45\u9886\u5148\uff0c\u4e3a\u5b9e\u9645\u573a\u666f\u4e0b\u8868\u683c\u5206\u6790\u5e26\u6765\u65b0\u7a81\u7834\u3002", "motivation": "\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u591a\u6a21\u6001\u8868\u683c\u7406\u89e3\u975e\u5e38\u5177\u6709\u6311\u6218\u6027\uff0c\u4e3b\u8981\u7531\u4e8e\u8868\u683c\u7ed3\u6784\u590d\u6742\u3001\u7b26\u53f7\u5bc6\u96c6\u3001\u4ee5\u53ca\u89c6\u89c9\u9000\u5316\uff08\u5982\u6a21\u7cca\u3001\u503e\u659c\u3001\u6c34\u5370\u3001\u4e0d\u5b8c\u6574\u7ed3\u6784\u6216\u5b57\u4f53\u3001\u591a\u8de8\u5ea6\u6216\u5d4c\u5957\u5e03\u5c40\uff09\u3002\u73b0\u6709\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u6b64\u7c7b\u590d\u6742\u73af\u5883\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002", "method": "\u63d0\u51faTableMoE\uff0c\u8fd9\u662f\u4e00\u79cd\u795e\u7ecf\u7b26\u53f7\u6df7\u5408\u4e13\u5bb6\u6a21\u578b\uff08Mixture-of-Connector-Experts, MoCE\uff09\u67b6\u6784\uff0c\u4e13\u4e3a\u9c81\u68d2\u7684\u591a\u6a21\u6001\u8868\u683c\u7ed3\u6784\u5316\u63a8\u7406\u8bbe\u8ba1\u3002TableMoE\u5305\u542b\u521b\u65b0\u7684\u795e\u7ecf\u7b26\u53f7\u8def\u7531\u673a\u5236\uff0c\u80fd\u591f\u9884\u6d4b\u6f5c\u5728\u8bed\u4e49token\u89d2\u8272\uff0c\u5e76\u901a\u8fc7\u57fa\u4e8e\u7b26\u53f7\u63a8\u7406\u56fe\u7684\u7f6e\u4fe1\u95e8\u63a7\uff0c\u5c06\u8868\u683c\u5355\u5143\u52a8\u6001\u5206\u914d\u7ed9\u4e0d\u540c\u9886\u57df\u4e13\u5bb6\uff08\u5982Table-to-HTML\uff0cTable-to-JSON\uff0cTable-to-Code\uff09\u3002\u6b64\u5916\uff0c\u5f15\u5165\u4e86\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u6570\u636e\u96c6TableMoE-Align\uff08\u5305\u542b120\u4e07table-HTML-JSON-code\u56db\u5143\u7ec4\uff09\uff0c\u8986\u76d6\u91d1\u878d\u3001\u79d1\u5b66\u3001\u751f\u7269\u533b\u5b66\u3001\u5de5\u4e1a\u7b49\u9886\u57df\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eTableMoE\u5728\u56db\u4e2a\u4e13\u4e3a\u590d\u6742\u8868\u683c\u8bbe\u8ba1\u7684WildStruct\u57fa\u51c6\uff08WMMFinQA\u3001WMMTatQA\u3001WMMTabDialog\u3001WMMFinanceMath\uff09\u4e0a\u5927\u5e45\u8d85\u8d8a\u73b0\u6709\u6700\u5148\u8fdb\u6a21\u578b\u3002\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6a21\u578b\u5404\u6838\u5fc3\u7ec4\u4ef6\u7684\u6709\u6548\u6027\uff0c\u7279\u522b\u662f\u795e\u7ecf\u7b26\u53f7\u8def\u7531\u548c\u7ed3\u6784\u4e13\u5bb6\u5bf9\u9f50\u7684\u91cd\u8981\u6027\u3002\u5b9a\u6027\u5206\u6790\u8fd8\u5c55\u793a\u4e86\u8be5\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\u63d0\u5347\u3002", "conclusion": "TableMoE\u901a\u8fc7\u7ed3\u5408\u795e\u7ecf-\u7b26\u53f7\u63a8\u7406\u548c\u4e13\u5bb6\u52a8\u6001\u8def\u7531\u673a\u5236\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u590d\u6742\u3001\u591a\u6a21\u6001\u8868\u683c\u7684\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u573a\u666f\u4e0b\u7684\u8868\u683c\u5206\u6790\u63d0\u4f9b\u4e86\u66f4\u5f3a\u9c81\u68d2\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u901a\u7528\u6027\u3002"}}
{"id": "2506.21049", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2506.21049", "abs": "https://arxiv.org/abs/2506.21049", "authors": ["Chunyuan Yuan", "Chong Zhang", "Zheng Fang", "Ming Pang", "Xue Jiang", "Changping Peng", "Zhangang Lin", "Ching Law"], "title": "A Semi-supervised Scalable Unified Framework for E-commerce Query Classification", "comment": "Accepted by ACL 2025", "summary": "Query classification, including multiple subtasks such as intent and category\nprediction, is vital to e-commerce applications. E-commerce queries are usually\nshort and lack context, and the information between labels cannot be used,\nresulting in insufficient prior information for modeling. Most existing\nindustrial query classification methods rely on users' posterior click behavior\nto construct training samples, resulting in a Matthew vicious cycle.\nFurthermore, the subtasks of query classification lack a unified framework,\nleading to low efficiency for algorithm optimization.\n  In this paper, we propose a novel Semi-supervised Scalable Unified Framework\n(SSUF), containing multiple enhanced modules to unify the query classification\ntasks. The knowledge-enhanced module uses world knowledge to enhance query\nrepresentations and solve the problem of insufficient query information. The\nlabel-enhanced module uses label semantics and semi-supervised signals to\nreduce the dependence on posterior labels. The structure-enhanced module\nenhances the label representation based on the complex label relations. Each\nmodule is highly pluggable, and input features can be added or removed as\nneeded according to each subtask. We conduct extensive offline and online A/B\nexperiments, and the results show that SSUF significantly outperforms the\nstate-of-the-art models.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u534a\u76d1\u7763\u53ef\u6269\u5c55\u7edf\u4e00\u6846\u67b6\uff08SSUF\uff09\uff0c\u7528\u591a\u79cd\u589e\u5f3a\u6a21\u5757\u89e3\u51b3\u7535\u5546\u67e5\u8be2\u5206\u7c7b\u4e2d\u7684\u4fe1\u606f\u4e0d\u8db3\u548c\u4f18\u5316\u4f4e\u6548\u95ee\u9898\uff0c\u5728\u5b9e\u9645\u5b9e\u9a8c\u4e2d\u6548\u679c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u7535\u5546\u5e73\u53f0\u4e2d\u7684\u67e5\u8be2\u5206\u7c7b\u4efb\u52a1\uff08\u5982\u610f\u56fe\u548c\u7c7b\u522b\u9884\u6d4b\uff09\u5f88\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u67e5\u8be2\u77ed\u3001\u4fe1\u606f\u4e0d\u8db3\u53ca\u6807\u7b7e\u95f4\u4fe1\u606f\u4e0d\u53ef\u7528\uff0c\u5bfc\u81f4\u5efa\u6a21\u9762\u4e34\u5148\u9a8c\u4fe1\u606f\u4e0d\u8db3\u7684\u95ee\u9898\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u7528\u6237\u7684\u70b9\u51fb\u884c\u4e3a\u6784\u9020\u8bad\u7ec3\u6837\u672c\uff0c\u4f1a\u9677\u5165\u201c\u9a6c\u592a\u6548\u5e94\u201d\u6076\u6027\u5faa\u73af\uff1b\u540c\u65f6\u5404\u5b50\u4efb\u52a1\u7f3a\u4e4f\u7edf\u4e00\u6846\u67b6\uff0c\u7b97\u6cd5\u4f18\u5316\u6548\u7387\u4f4e\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u534a\u76d1\u7763\u53ef\u6269\u5c55\u7edf\u4e00\u6846\u67b6\uff08SSUF\uff09\uff0c\u5305\u542b\u77e5\u8bc6\u589e\u5f3a\u3001\u6807\u7b7e\u589e\u5f3a\u548c\u7ed3\u6784\u589e\u5f3a\u4e09\u5927\u6a21\u5757\uff1a\u77e5\u8bc6\u589e\u5f3a\u6a21\u5757\u5229\u7528\u4e16\u754c\u77e5\u8bc6\u63d0\u5347\u67e5\u8be2\u8868\u8fbe\u80fd\u529b\uff1b\u6807\u7b7e\u589e\u5f3a\u6a21\u5757\u7ed3\u5408\u6807\u7b7e\u8bed\u4e49\u548c\u534a\u76d1\u7763\u4fe1\u53f7\uff0c\u51cf\u5c11\u5bf9\u540e\u9a8c\u6807\u7b7e\u7684\u4f9d\u8d56\uff1b\u7ed3\u6784\u589e\u5f3a\u6a21\u5757\u7ed3\u5408\u6807\u7b7e\u95f4\u590d\u6742\u5173\u7cfb\u63d0\u5347\u6807\u7b7e\u8868\u8fbe\u3002\u6240\u6709\u6a21\u5757\u5747\u53ef\u7075\u6d3b\u63d2\u62d4\uff0c\u6839\u636e\u5177\u4f53\u5b50\u4efb\u52a1\u5b9a\u5236\u8f93\u5165\u7279\u5f81\u3002", "result": "\u5728\u5927\u91cf\u79bb\u7ebf\u4e0e\u7ebf\u4e0aA/B\u5b9e\u9a8c\u4e2d\uff0cSSUF\u5728\u67e5\u8be2\u5206\u7c7b\u4efb\u52a1\u4e0a\u660e\u663e\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "SSUF\u5b9e\u73b0\u4e86\u7535\u5546\u67e5\u8be2\u5206\u7c7b\u4efb\u52a1\u7684\u6a21\u5757\u5316\u7edf\u4e00\u5efa\u6a21\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u67e5\u8be2\u4fe1\u606f\u4e0d\u8db3\u548c\u5bf9\u7528\u6237\u884c\u4e3a\u4f9d\u8d56\u7684\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6574\u4f53\u6027\u80fd\u3002"}}
{"id": "2506.21458", "categories": ["cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.21458", "abs": "https://arxiv.org/abs/2506.21458", "authors": ["Baiqiao Yin", "Qineng Wang", "Pingyue Zhang", "Jianshu Zhang", "Kangrui Wang", "Zihan Wang", "Jieyu Zhang", "Keshigeyan Chandrasegaran", "Han Liu", "Ranjay Krishna", "Saining Xie", "Manling Li", "Jiajun Wu", "Li Fei-Fei"], "title": "Spatial Mental Modeling from Limited Views", "comment": "Preprint version", "summary": "Can Vision Language Models (VLMs) imagine the full scene from just a few\nviews, like humans do? Humans form spatial mental models, internal\nrepresentations of unseen space, to reason about layout, perspective, and\nmotion. Our new MindCube benchmark with 21,154 questions across 3,268 images\nexposes this critical gap, where existing VLMs exhibit near-random performance.\nUsing MindCube, we systematically evaluate how well VLMs build robust spatial\nmental models through representing positions (cognitive mapping), orientations\n(perspective-taking), and dynamics (mental simulation for \"what-if\" movements).\nWe then explore three approaches to help VLMs approximate spatial mental\nmodels, including unseen intermediate views, natural language reasoning chains,\nand cognitive maps. The significant improvement comes from a synergistic\napproach, \"map-then-reason\", that jointly trains the model to first generate a\ncognitive map and then reason upon it. By training models to reason over these\ninternal maps, we boosted accuracy from 37.8% to 60.8% (+23.0%). Adding\nreinforcement learning pushed performance even further to 70.7% (+32.9%). Our\nkey insight is that such scaffolding of spatial mental models, actively\nconstructing and utilizing internal structured spatial representations with\nflexible reasoning processes, significantly improves understanding of\nunobservable space.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7MindCube\u65b0\u57fa\u51c6\u7cfb\u7edf\u8bc4\u4f30VLMs\u5bf9\u7a7a\u95f4\u5fc3\u7406\u6a21\u578b\u7684\u6784\u5efa\u80fd\u529b\uff0c\u5927\u5e45\u6539\u8fdb\u65b9\u6cd5\uff08map-then-reason+\u5f3a\u5316\u5b66\u4e60\uff09\u5c06\u51c6\u786e\u7387\u63d0\u5347\u81f370.7%\uff0c\u63ed\u793a\u4e86\u6784\u5efa\u548c\u5229\u7528\u7a7a\u95f4\u5185\u90e8\u7ed3\u6784\u5bf9\u7406\u89e3\u4e0d\u53ef\u89c1\u7a7a\u95f4\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u4eba\u7c7b\u80fd\u591f\u901a\u8fc7\u6709\u9650\u89c6\u89d2\u60f3\u8c61\u5b8c\u6574\u573a\u666f\uff0c\u4f9d\u9760\u5185\u90e8\u7a7a\u95f4\u6a21\u578b\u6765\u8fdb\u884c\u63a8\u7406\u548c\u60f3\u8c61\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5bf9\u6b64\u80fd\u529b\u6781\u4e3a\u6709\u9650\uff0c\u8fd1\u4f3c\u968f\u673a\u8868\u73b0\u3002\u8be5\u5de5\u4f5c\u65e8\u5728\u63a2\u7a76VLMs\u5728\u7a7a\u95f4\u5fc3\u7406\u6a21\u578b\u6784\u5efa\u4e0a\u7684\u77ed\u677f\u5e76\u5bfb\u627e\u6539\u8fdb\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86MindCube\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u901a\u8fc73,268\u5f20\u56fe\u7247\u548c21,154\u4e2a\u95ee\u9898\u7cfb\u7edf\u6027\u8bc4\u4f30VLMs\u5728\u4f4d\u7f6e\u8ba4\u77e5\u3001\u89c6\u89d2\u8f6c\u6362\u548c\u8fd0\u52a8\u6a21\u62df\uff08what-if\u63a8\u7406\uff09\u4e0a\u7684\u80fd\u529b\u3002\u7814\u7a76\u8fd8\u63a2\u7d22\u4e86\u4e09\u79cd\u63d0\u5347VLMs\u7a7a\u95f4\u5fc3\u7406\u6a21\u578b\u7684\u65b9\u6cd5\uff1a\u8865\u5145\u672a\u89c1\u4e2d\u95f4\u89c6\u89d2\u3001\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u94fe\u548c\u8ba4\u77e5\u5730\u56fe\uff0c\u5e76\u8fdb\u4e00\u6b65\u63d0\u51fa\u201cmap-then-reason\u201d\u4e3a\u6838\u5fc3\u7684\u8054\u5408\u8bad\u7ec3\u65b9\u6cd5\uff0c\u4ee5\u53ca\u5f15\u5165\u5f3a\u5316\u5b66\u4e60\u3002", "result": "\u91c7\u7528\u201cmap-then-reason\u201d\u7684\u601d\u8def\u5c06\u6a21\u578b\u51c6\u786e\u7387\u4ece37.8%\u63d0\u5347\u523060.8%\uff0c\u8054\u5408\u5f3a\u5316\u5b66\u4e60\u8fdb\u4e00\u6b65\u63d0\u5347\u523070.7%\uff0c\u76f8\u6bd4\u539f\u59cb\u80fd\u529b\u63d0\u534732.9%\u3002", "conclusion": "\u901a\u8fc7\u4e3b\u52a8\u6784\u5efa\u548c\u5229\u7528\u5185\u90e8\u7ed3\u6784\u5316\u7a7a\u95f4\u8868\u5f81\uff0c\u5e76\u7ed3\u5408\u7075\u6d3b\u63a8\u7406\u6d41\u7a0b\uff0c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5bf9\u4e0d\u53ef\u89c1\u7a7a\u95f4\u7684\u7406\u89e3\u80fd\u529b\u83b7\u5f97\u5927\u5e45\u63d0\u5347\u3002\u8be5\u5de5\u4f5c\u4e3aVLMs\u878d\u5165\u7a7a\u95f4\u5fc3\u7406\u6a21\u578b\u5f00\u8f9f\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2506.21053", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.21053", "abs": "https://arxiv.org/abs/2506.21053", "authors": ["Fuqiang Niu", "Genan Dai", "Yisha Lu", "Jiayu Liao", "Xiang Li", "Hu Huang", "Bowen Zhang"], "title": "MT2-CSD: A New Dataset and Multi-Semantic Knowledge Fusion Method for Conversational Stance Detection", "comment": null, "summary": "In the realm of contemporary social media, automatic stance detection is\npivotal for opinion mining, as it synthesizes and examines user perspectives on\ncontentious topics to uncover prevailing trends and sentiments. Traditional\nstance detection research often targets individual instances, thereby limiting\nits capacity to model multi-party discussions typical in real social media\nscenarios. This shortcoming largely stems from the scarcity of datasets that\nauthentically capture the dynamics of social media interactions, hindering\nadvancements in conversational stance detection. In this paper, we introduce\nMT2-CSD, a comprehensive dataset for multi-target, multi-turn conversational\nstance detection. To the best of our knowledge, MT2-CSD is the largest dataset\navailable for this purpose, comprising 24,457 annotated instances and\nexhibiting the greatest conversational depth, thereby presenting new challenges\nfor stance detection. To address these challenges, we propose the Large\nLanguage model enhanced Conversational Relational Attention Network (LLM-CRAN),\nwhich exploits the reasoning capabilities of LLMs to improve conversational\nunderstanding. We conduct extensive experiments to evaluate the efficacy of\nLLM-CRAN on the MT2-CSD dataset. The experimental results indicate that\nLLM-CRAN significantly outperforms strong baseline models in the task of\nconversational stance detection.", "AI": {"tldr": "\u672c\u6587\u53d1\u5e03\u4e86\u5f53\u524d\u6700\u5927\u3001\u6700\u590d\u6742\u7684\u591a\u8f6e\u5bf9\u8bdd\u7acb\u573a\u68c0\u6d4b\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u51fa\u7ed3\u5408\u5927\u6a21\u578b\u7684\u65b0\u65b9\u6cd5LLM-CRAN\uff0c\u5728\u65b0\u6570\u636e\u96c6\u4e0a\u660e\u663e\u4f18\u4e8e\u5df2\u6709\u65b9\u6cd5\uff0c\u6709\u52a9\u4e8e\u63a8\u8fdb\u793e\u4ea4\u5a92\u4f53\u4e2d\u591a\u65b9\u5bf9\u8bdd\u7acb\u573a\u68c0\u6d4b\u6280\u672f\u3002", "motivation": "\u73b0\u6709\u7684\u7acb\u573a\u68c0\u6d4b\u4e3b\u8981\u9488\u5bf9\u5355\u6761\u6570\u636e\uff0c\u96be\u4ee5\u5904\u7406\u591a\u65b9\u3001\u591a\u8f6e\u5bf9\u8bdd\u7684\u771f\u5b9e\u793e\u4ea4\u5a92\u4f53\u573a\u666f\uff0c\u4e3b\u8981\u7531\u4e8e\u7f3a\u5c11\u80fd\u53cd\u6620\u771f\u5b9e\u793e\u4ea4\u4e92\u52a8\u7684\u6570\u636e\u96c6\uff0c\u9650\u5236\u4e86\u5bf9\u8bdd\u7acb\u573a\u68c0\u6d4b\u7684\u53d1\u5c55\u3002", "method": "\u63d0\u51fa\u4e86MT2-CSD\u6570\u636e\u96c6\uff0c\u8fd9\u662f\u76ee\u524d\u6700\u5927\u3001\u5bf9\u8bdd\u6df1\u5ea6\u6700\u9ad8\u7684\u591a\u76ee\u6807\u3001\u591a\u8f6e\u793e\u4ea4\u5bf9\u8bdd\u7acb\u573a\u68c0\u6d4b\u6570\u636e\u96c6\u3002\u5e76\u63d0\u51fa\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u589e\u5f3a\u7684\u5bf9\u8bdd\u5173\u7cfb\u6ce8\u610f\u529b\u7f51\u7edc\uff08LLM-CRAN\uff09\uff0c\u7ed3\u5408LLM\u63a8\u7406\u80fd\u529b\u63d0\u5347\u5bf9\u8bdd\u7406\u89e3\u3002", "result": "LLM-CRAN\u5728MT2-CSD\u6570\u636e\u96c6\u4e0a\u660e\u663e\u4f18\u4e8e\u5404\u7c7b\u5f3a\u57fa\u7ebf\u6a21\u578b\uff0c\u5c55\u73b0\u4e86\u8be5\u65b9\u6cd5\u5728\u5bf9\u8bdd\u7acb\u573a\u68c0\u6d4b\u4efb\u52a1\u4e0a\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u6570\u636e\u96c6\u548c\u65b9\u6cd5\u5171\u540c\u63a8\u52a8\u4e86\u591a\u76ee\u6807\u3001\u591a\u8f6e\u5bf9\u8bdd\u7acb\u573a\u68c0\u6d4b\u7814\u7a76\u7684\u53d1\u5c55\u3002LLM-CRAN\u5f88\u597d\u5730\u5229\u7528\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4f18\u52bf\uff0c\u4e3a\u793e\u4ea4\u5a92\u4f53\u4e2d\u771f\u5b9e\u590d\u6742\u60c5\u5883\u4e0b\u7684\u7acb\u573a\u68c0\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6027\u80fd\u65b9\u6848\u3002"}}
{"id": "2506.21490", "categories": ["cs.AI", "cs.HC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2506.21490", "abs": "https://arxiv.org/abs/2506.21490", "authors": ["Tin Dizdarevi\u0107", "Ravi Hammond", "Tobias Gessler", "Anisoara Calinescu", "Jonathan Cook", "Matteo Gallici", "Andrei Lupu", "Jakob Nicolaus Foerster"], "title": "Ad-Hoc Human-AI Coordination Challenge", "comment": "Published at ICML 2025", "summary": "Achieving seamless coordination between AI agents and humans is crucial for\nreal-world applications, yet it remains a significant open challenge. Hanabi is\na cooperative card game featuring imperfect information, constrained\ncommunication, theory of mind requirements, and coordinated action -- making it\nan ideal testbed for human-AI coordination. However, its use for human-AI\ninteraction has been limited by the challenges of human evaluation. In this\nwork, we introduce the Ad-Hoc Human-AI Coordination Challenge (AH2AC2) to\novercome the constraints of costly and difficult-to-reproduce human\nevaluations. We develop \\textit{human proxy agents} on a large-scale human\ndataset that serve as robust, cheap, and reproducible human-like evaluation\npartners in AH2AC2. To encourage the development of data-efficient methods, we\nopen-source a dataset of 3,079 games, deliberately limiting the amount of\navailable human gameplay data. We present baseline results for both two- and\nthree- player Hanabi scenarios. To ensure fair evaluation, we host the proxy\nagents through a controlled evaluation system rather than releasing them\npublicly. The code is available at\n\\href{https://github.com/FLAIROx/ah2ac2}{https://github.com/FLAIROx/ah2ac2}.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faAd-Hoc Human-AI Coordination Challenge (AH2AC2)\uff0c\u5229\u7528\u4eff\u4eba\u4ee3\u7406\u8fdb\u884c\u53ef\u590d\u73b0\u3001\u4f4e\u6210\u672c\u7684\u4eba\u673a\u534f\u4f5c\u8bc4\u6d4b\uff0c\u5e76\u5f00\u6e903,079\u5c40\u4eba\u7c7b\u5bf9\u5c40\u6570\u636e\uff0c\u652f\u6301Hanabi\u4e24\u4eba\u53ca\u4e09\u4eba\u573a\u666f\uff0c\u63a8\u52a8\u6570\u636e\u9ad8\u6548\u7684\u4eba\u673a\u534f\u4f5c\u65b9\u6cd5\u53d1\u5c55\u3002", "motivation": "\u5b9e\u73b0AI\u4ee3\u7406\u548c\u4eba\u7c7b\u4e4b\u95f4\u7684\u65e0\u7f1d\u534f\u4f5c\u5bf9\u73b0\u5b9e\u5e94\u7528\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4ecd\u9762\u4e34\u91cd\u5927\u6311\u6218\u3002Hanabi\u6e38\u620f\u7531\u4e8e\u4fe1\u606f\u4e0d\u5b8c\u5168\u3001\u4ea4\u6d41\u53d7\u9650\u548c\u7406\u8bba\u5fc3\u667a\u7b49\u7279\u6027\uff0c\u662f\u4eba\u673a\u534f\u4f5c\u7814\u7a76\u7684\u7406\u60f3\u5e73\u53f0\u3002\u4eba\u7c7b\u8bc4\u6d4b\u6602\u8d35\u4e14\u96be\u4ee5\u590d\u73b0\uff0c\u5236\u7ea6\u4e86\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002", "method": "1. \u63d0\u51faAd-Hoc Human-AI Coordination Challenge (AH2AC2)\uff0c\u65e8\u5728\u51cf\u5c11\u5bf9\u4f20\u7edf\u4eba\u7c7b\u6d4b\u8bd5\u7684\u4f9d\u8d56\u3002\n2. \u57fa\u4e8e\u5927\u89c4\u6a21\u4eba\u7c7b\u6570\u636e\u96c6\u5f00\u53d1\u201c\u4eba\u7c7b\u4ee3\u7406\u4f53\u201d\uff08human proxy agents\uff09\uff0c\u53ef\u4f5c\u4e3a\u4eff\u4eba\u8bc4\u4ef7\u5bf9\u8c61\uff0c\u8fdb\u884c\u5927\u89c4\u6a21\u3001\u4f4e\u6210\u672c\u3001\u6613\u590d\u73b0\u5b9e\u9a8c\u3002\n3. \u5f00\u6e903,079\u5c40Hanabi\u5bf9\u5c40\u6570\u636e\u96c6\uff0c\u63a8\u52a8\u6570\u636e\u9ad8\u6548\u65b9\u6cd5\u7684\u5f00\u53d1\uff0c\u4e14\u6545\u610f\u9650\u5236\u53ef\u7528\u4eba\u7c7b\u6570\u636e\u91cf\u3002\n4. \u63d0\u4f9b\u4e86\u4e24\u4eba\u548c\u4e09\u4eba\u6e38\u620f\u7684\u57fa\u7ebf\u7ed3\u679c\uff0c\u5e76\u901a\u8fc7\u53d7\u63a7\u8bc4\u6d4b\u7cfb\u7edf\u6258\u7ba1\u4eba\u7c7b\u4ee3\u7406\u4f53\u4ee5\u4fdd\u8bc1\u516c\u5e73\u3002", "result": "\u4eba\u7c7b\u4ee3\u7406\u4f53\u4f5c\u4e3a\u8bc4\u4ef7\u5de5\u5177\u80fd\u6a21\u62df\u771f\u5b9e\u4eba\u7c7b\u4f19\u4f34\uff0c\u5141\u8bb8\u5927\u89c4\u6a21\u3001\u4f4e\u6210\u672c\u4e14\u53ef\u590d\u73b0\u7684AI-human\u534f\u4f5c\u5b9e\u9a8c\u3002\u57fa\u7ebf\u5b9e\u9a8c\u5df2\u8986\u76d6\u4e24\u4eba\u548c\u4e09\u4eba\u6e38\u620f\u573a\u666f\u3002\u5f00\u6e90\u7684\u6570\u636e\u548c\u5e73\u53f0\u5c06\u63a8\u52a8\u8be5\u9886\u57df\u7684\u65b9\u6cd5\u5f00\u53d1\u548c\u6807\u51c6\u5316\u8bc4\u6d4b\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165AH2AC2\u6311\u6218\u548c\u201c\u4eba\u7c7b\u4ee3\u7406\u4f53\u201d\u6982\u5ff5\uff0c\u6709\u6548\u964d\u4f4e\u4e86\u4eba\u7c7b\u8bc4\u6d4b\u969c\u788d\uff0c\u63d0\u5347\u4e86AI\u4e0e\u4eba\u7c7b\u534f\u4f5c\u65b9\u6cd5\u7684\u5f00\u53d1\u6548\u7387\uff0c\u6709\u671b\u6210\u4e3a\u672a\u6765\u4eba\u673a\u534f\u4f5c\u7814\u7a76\u7684\u91cd\u8981\u6807\u51c6\u5e73\u53f0\u3002"}}
{"id": "2506.21096", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.21096", "abs": "https://arxiv.org/abs/2506.21096", "authors": ["Kang He", "Yuzhe Ding. Haining Wang", "Fei Li", "Chong Teng", "Donghong Ji"], "title": "DALR: Dual-level Alignment Learning for Multimodal Sentence Representation Learning", "comment": "Accepted by ACL 2025 Findings", "summary": "Previous multimodal sentence representation learning methods have achieved\nimpressive performance. However, most approaches focus on aligning images and\ntext at a coarse level, facing two critical challenges:cross-modal misalignment\nbias and intra-modal semantic divergence, which significantly degrade sentence\nrepresentation quality. To address these challenges, we propose DALR\n(Dual-level Alignment Learning for Multimodal Sentence Representation). For\ncross-modal alignment, we propose a consistency learning module that softens\nnegative samples and utilizes semantic similarity from an auxiliary task to\nachieve fine-grained cross-modal alignment. Additionally, we contend that\nsentence relationships go beyond binary positive-negative labels, exhibiting a\nmore intricate ranking structure. To better capture these relationships and\nenhance representation quality, we integrate ranking distillation with global\nintra-modal alignment learning. Comprehensive experiments on semantic textual\nsimilarity (STS) and transfer (TR) tasks validate the effectiveness of our\napproach, consistently demonstrating its superiority over state-of-the-art\nbaselines.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cdDALR\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u7684\u53cc\u91cd\u5bf9\u9f50\u673a\u5236\uff0c\u6709\u6548\u63d0\u5347\u4e86\u591a\u6a21\u6001\u53e5\u5b50\u8868\u793a\u7684\u8d28\u91cf\u3002\u5728\u5404\u7c7b\u53e5\u5b50\u7406\u89e3\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8fc7\u4e86\u4e3b\u6d41\u5bf9\u6bd4\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u53e5\u5b50\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\u591a\u805a\u7126\u4e8e\u7c97\u7c92\u5ea6\u5bf9\u9f50\uff0c\u5ffd\u89c6\u4e86\u8de8\u6a21\u6001\u5931\u914d\u548c\u5355\u6a21\u6001\u8bed\u4e49\u5206\u6b67\u95ee\u9898\uff0c\u5bfc\u81f4\u53e5\u5b50\u8868\u793a\u8d28\u91cf\u4e0b\u964d\u3002\u8be5\u7814\u7a76\u65e8\u5728\u7ec6\u81f4\u5730\u89e3\u51b3\u8fd9\u4e24\u4e2a\u5173\u952e\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86DALR\u65b9\u6cd5\uff0c\u901a\u8fc7\u53cc\u91cd\u5bf9\u9f50\u673a\u5236\uff1a1\uff09\u8de8\u6a21\u6001\u5bf9\u9f50\u91c7\u7528\u4e00\u81f4\u6027\u5b66\u4e60\u6a21\u5757\uff0c\u5229\u7528\u8f85\u52a9\u4efb\u52a1\u4e2d\u7684\u8bed\u4e49\u76f8\u4f3c\u6027\u8fdb\u884c\u8f6f\u5316\u8d1f\u6837\u672c\uff0c\u63d0\u5347\u7ec6\u7c92\u5ea6\u5bf9\u9f50\u80fd\u529b\uff1b2\uff09\u5355\u6a21\u6001\u5bf9\u9f50\u7ed3\u5408\u5173\u7cfb\u6392\u5e8f\u84b8\u998f\u4e0e\u5168\u5c40\u5bf9\u9f50\u5b66\u4e60\uff0c\u66f4\u597d\u5730\u6355\u6349\u53e5\u5b50\u590d\u6742\u7684\u8bed\u4e49\u5173\u7cfb\u3002", "result": "\u5728\u53e5\u5b50\u8bed\u4e49\u76f8\u4f3c\u6027\u548c\u8fc1\u79fb\u4efb\u52a1\u4e0a\u7684\u5168\u9762\u5b9e\u9a8c\u9a8c\u8bc1\u4e86DALR\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u7ed3\u679c\u663e\u793a\u5176\u5728\u51c6\u786e\u7387\u7b49\u6307\u6807\u4e0a\u5747\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u4f18\u57fa\u7ebf\u3002", "conclusion": "DALR\u65b9\u6cd5\u5728\u8de8\u6a21\u6001\u4e0e\u5355\u6a21\u6001\u8bed\u4e49\u5bf9\u9f50\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2506.21506", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.21506", "abs": "https://arxiv.org/abs/2506.21506", "authors": ["Boyu Gou", "Zanming Huang", "Yuting Ning", "Yu Gu", "Michael Lin", "Weijian Qi", "Andrei Kopanev", "Botao Yu", "Bernal Jim\u00e9nez Guti\u00e9rrez", "Yiheng Shu", "Chan Hee Song", "Jiaman Wu", "Shijie Chen", "Hanane Nour Moussa", "Tianshu Zhang", "Jian Xie", "Yifei Li", "Tianci Xue", "Zeyi Liao", "Kai Zhang", "Boyuan Zheng", "Zhaowei Cai", "Viktor Rozgic", "Morteza Ziyadi", "Huan Sun", "Yu Su"], "title": "Mind2Web 2: Evaluating Agentic Search with Agent-as-a-Judge", "comment": "Project Homepage: https://osu-nlp-group.github.io/Mind2Web2/", "summary": "Agentic search such as Deep Research systems, where large language models\nautonomously browse the web, synthesize information, and return comprehensive\ncitation-backed answers, represents a major shift in how users interact with\nweb-scale information. While promising greater efficiency and cognitive\noffloading, the growing complexity and open-endedness of agentic search have\noutpaced existing evaluation benchmarks and methodologies, which largely assume\nshort search horizons and static answers. In this paper, we introduce Mind2Web\n2, a benchmark of 130 realistic, high-quality, and long-horizon tasks that\nrequire real-time web browsing and extensive information synthesis, constructed\nwith over 1,000 hours of human labor. To address the challenge of evaluating\ntime-varying and complex answers, we propose a novel Agent-as-a-Judge\nframework. Our method constructs task-specific judge agents based on a\ntree-structured rubric design to automatically assess both answer correctness\nand source attribution. We conduct a comprehensive evaluation of nine frontier\nagentic search systems and human performance, along with a detailed error\nanalysis to draw insights for future development. The best-performing system,\nOpenAI Deep Research, can already achieve 50-70% of human performance while\nspending half the time, showing a great potential. Altogether, Mind2Web 2\nprovides a rigorous foundation for developing and benchmarking the next\ngeneration of agentic search systems.", "AI": {"tldr": "\u6b64\u8bba\u6587\u63d0\u51fa\u4e86\u66f4\u5168\u9762\u8bc4\u6d4b\u5148\u8fdb\u667a\u80fd\u4f53\u641c\u7d22\u7cfb\u7edf\u7684\u65b0\u57fa\u51c6\u548c\u8bc4\u6d4b\u6846\u67b6\uff0c\u5e76\u8868\u660e\u73b0\u6709\u6280\u672f\u5df2\u63a5\u8fd1\u4e8e\u9ad8\u6548\u7684\u4eba\u7c7b\u6c34\u5e73\uff0c\u4e3a\u672a\u6765\u53d1\u5c55\u5960\u5b9a\u57fa\u7840\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u5927\u6a21\u578b\u7684\u7c7b\u81ea\u4e3b\u667a\u80fd\u4f53\uff08agentic\uff09\u641c\u7d22\u7cfb\u7edf\u5982Deep Research\u6b63\u4e0d\u65ad\u6269\u5c55\u5176\u80fd\u529b\uff0c\u80fd\u81ea\u52a8\u6d4f\u89c8\u7f51\u9875\u3001\u6574\u5408\u4fe1\u606f\u5e76\u8f93\u51fa\u5e26\u6709\u5f15\u7528\u7684\u590d\u6742\u7b54\u6848\uff0c\u4f46\u8bc4\u6d4b\u4f53\u7cfb\u6ede\u540e\uff0c\u65e0\u6cd5\u8986\u76d6\u5176\u590d\u6742\u6027\u548c\u591a\u53d8\u6027\uff0c\u9020\u6210\u8bc4\u6d4b\u4e0e\u5b9e\u9645\u5e94\u7528\u8131\u8282\u3002", "method": "\u63d0\u51fa\u4e86Mind2Web 2\u57fa\u51c6\u96c6\uff0c\u5305\u62ec130\u4e2a\u771f\u5b9e\u3001\u590d\u6742\u3001\u957f\u6d41\u7a0b\u3001\u591a\u4fe1\u606f\u7efc\u5408\u4efb\u52a1\uff0c\u75281000\u5c0f\u65f6\u4eba\u5de5\u6784\u5efa\u3002\u4e3a\u5e94\u5bf9\u590d\u6742\u3001\u591a\u53d8\u7b54\u6848\u7684\u8bc4\u6d4b\u96be\u9898\uff0c\u521b\u65b0\u6027\u5f15\u5165\u4e86\u201cAgent-as-a-Judge\u201d\u8bc4\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u6811\u72b6\u8bc4\u5206\u6807\u51c6\u6784\u9020\u4efb\u52a1\u7279\u5b9a\u7684\u8bc4\u5224\u667a\u80fd\u4f53\uff0c\u81ea\u52a8\u8861\u91cf\u7b54\u6848\u7684\u6b63\u786e\u6027\u548c\u4fe1\u606f\u6e90\u5f15\u7528\u3002", "result": "\u5bf9\u4e5d\u4e2a\u524d\u6cbfagentic\u641c\u7d22\u7cfb\u7edf\u53ca\u4eba\u5de5\u771f\u5b9e\u8868\u73b0\u8fdb\u884c\u4e86\u7cfb\u7edf\u8bc4\u6d4b\u53ca\u8bef\u5dee\u5206\u6790\u3002\u6700\u4f73\u7cfb\u7edf\uff08OpenAI Deep Research\uff09\u5df2\u5b9e\u73b0\u4eba\u7c7b\u8868\u73b0\u768450-70%\uff0c\u4e14\u7528\u65f6\u4ec5\u4e3a\u4eba\u5de5\u4e00\u534a\uff0c\u663e\u793a\u51fa\u5de8\u5927\u53d1\u5c55\u6f5c\u529b\u3002", "conclusion": "Mind2Web 2\u4e3a\u590d\u6742agentic\u641c\u7d22\u4efb\u52a1\u7684\u7814\u7a76\u548c\u8bc4\u6d4b\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u7684\u65b9\u6cd5\u8bba\u57fa\u7840\uff0c\u6709\u529b\u63a8\u52a8\u4e86\u4e0b\u4e00\u4ee3\u667a\u80fd\u641c\u7d22\u7cfb\u7edf\u7684\u7814\u53d1\u3002"}}
{"id": "2506.21098", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.21098", "abs": "https://arxiv.org/abs/2506.21098", "authors": ["Qinwen Chen", "Wenbiao Tao", "Zhiwei Zhu", "Mingfan Xi", "Liangzhong Guo", "Yuan Wang", "Wei Wang", "Yunshi Lan"], "title": "ComRAG: Retrieval-Augmented Generation with Dynamic Vector Stores for Real-time Community Question Answering in Industry", "comment": "7 pages, 4 figures. Accepted at ACL 2025 Industry Track", "summary": "Community Question Answering (CQA) platforms can be deemed as important\nknowledge bases in community, but effectively leveraging historical\ninteractions and domain knowledge in real-time remains a challenge. Existing\nmethods often underutilize external knowledge, fail to incorporate dynamic\nhistorical QA context, or lack memory mechanisms suited for industrial\ndeployment. We propose ComRAG, a retrieval-augmented generation framework for\nreal-time industrial CQA that integrates static knowledge with dynamic\nhistorical QA pairs via a centroid-based memory mechanism designed for\nretrieval, generation, and efficient storage. Evaluated on three industrial CQA\ndatasets, ComRAG consistently outperforms all baselines--achieving up to 25.9%\nimprovement in vector similarity, reducing latency by 8.7% to 23.3%, and\nlowering chunk growth from 20.23% to 2.06% over iterations.", "AI": {"tldr": "\u672c\u5de5\u4f5c\u63d0\u51faComRAG\u6846\u67b6\uff0c\u901a\u8fc7\u65b0\u9896\u8bb0\u5fc6\u673a\u5236\u9ad8\u6548\u6574\u5408\u540c\u6b65\u77e5\u8bc6\u4e0e\u5386\u53f2\u95ee\u7b54\uff0c\u5b9e\u73b0\u5de5\u4e1a\u7ea7\u793e\u533a\u95ee\u7b54\u5e73\u53f0\u5b9e\u65f6\u4f18\u5316\uff0c\u6548\u679c\u660e\u663e\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u793e\u533a\u95ee\u7b54\u5e73\u53f0\u662f\u91cd\u8981\u7684\u77e5\u8bc6\u5e93\uff0c\u4f46\u5982\u4f55\u5b9e\u65f6\u9ad8\u6548\u5229\u7528\u5386\u53f2\u4ea4\u4e92\u548c\u5916\u90e8\u9886\u57df\u77e5\u8bc6\u4ecd\u662f\u96be\u9898\u3002\u73b0\u6709\u65b9\u6cd5\u5bf9\u5916\u90e8\u77e5\u8bc6\u5229\u7528\u4e0d\u8db3\uff0c\u7f3a\u4e4f\u52a8\u6001\u5386\u53f2\u95ee\u7b54\u7684\u96c6\u6210\u6216\u7f3a\u5c11\u9002\u5408\u5de5\u4e1a\u5e94\u7528\u7684\u8bb0\u5fc6\u673a\u5236\u3002", "method": "\u63d0\u51fa\u4e86ComRAG\uff0c\u4e00\u4e2a\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u8d28\u5fc3\u8bb0\u5fc6\u673a\u5236\u6574\u5408\u9759\u6001\u77e5\u8bc6\u4e0e\u52a8\u6001\u5386\u53f2\u95ee\u7b54\u5bf9\uff0c\u652f\u6301\u9ad8\u6548\u68c0\u7d22\u3001\u751f\u6210\u53ca\u5b58\u50a8\u3002", "result": "\u5728\u4e09\u4e2a\u5de5\u4e1a\u7ea7CQA\u6570\u636e\u96c6\u4e0a\uff0cComRAG\u5747\u5927\u5e45\u8d85\u8fc7\u6240\u6709\u57fa\u7ebf\u65b9\u6cd5\uff1a\u5411\u91cf\u76f8\u4f3c\u5ea6\u63d0\u5347\u6700\u9ad825.9%\uff0c\u5ef6\u8fdf\u964d\u4f4e8.7%\u81f323.3%\uff0c\u6570\u636e\u5757\u589e\u957f\u7387\u753120.23%\u964d\u81f32.06%\u3002", "conclusion": "ComRAG\u6709\u6548\u6574\u5408\u9759\u6001\u4e0e\u52a8\u6001\u77e5\u8bc6\uff0c\u5177\u5907\u4f18\u8d8a\u8868\u73b0\u548c\u5de5\u4e1a\u9002\u5e94\u6027\uff0c\u662f\u793e\u533a\u95ee\u7b54\u5e73\u53f0\u5b9e\u65f6\u95ee\u7b54\u7684\u7406\u60f3\u65b9\u6848\u3002"}}
{"id": "2506.21536", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2506.21536", "abs": "https://arxiv.org/abs/2506.21536", "authors": ["Fangjun Ding", "Renyu Zhang", "Xinyu Feng", "Chengye Xie", "Zheng Zhang", "Yanting Zhang"], "title": "PsyLite Technical Report", "comment": null, "summary": "With the rapid development of digital technology, AI-driven psychological\ncounseling has gradually become an important research direction in the field of\nmental health. However, existing models still have deficiencies in dialogue\nsafety, detailed scenario handling, and lightweight deployment. To address\nthese issues, this study proposes PsyLite, a lightweight psychological\ncounseling large language model agent developed based on the base model\nInternLM2.5-7B-chat. Through a two-stage training strategy (hybrid distillation\ndata fine-tuning and ORPO preference optimization), PsyLite enhances the\nmodel's deep-reasoning ability, psychological counseling ability, and safe\ndialogue ability. After deployment using Ollama and Open WebUI, a custom\nworkflow is created with Pipelines. An innovative conditional RAG is designed\nto introduce crosstalk humor elements at appropriate times during psychological\ncounseling to enhance user experience and decline dangerous requests to\nstrengthen dialogue safety. Evaluations show that PsyLite outperforms the\nbaseline models in the Chinese general evaluation (CEval), psychological\ncounseling professional evaluation (CPsyCounE), and dialogue safety evaluation\n(SafeDialBench), particularly in psychological counseling professionalism\n(CPsyCounE score improvement of 47.6\\%) and dialogue safety (\\safe{} score\nimprovement of 2.4\\%). Additionally, the model uses quantization technology\n(GGUF q4\\_k\\_m) to achieve low hardware deployment (5GB memory is sufficient\nfor operation), providing a feasible solution for psychological counseling\napplications in resource-constrained environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8f7b\u91cf\u7ea7\u5fc3\u7406\u54a8\u8be2\u667a\u80fd\u4f53PsyLite\uff0c\u901a\u8fc7\u521b\u65b0\u8bad\u7ec3\u548c\u5b89\u5168\u673a\u5236\uff0c\u5927\u5e45\u63d0\u5347\u4e86\u5fc3\u7406\u54a8\u8be2\u4e13\u4e1a\u6027\u4e0e\u5bf9\u8bdd\u5b89\u5168\u6027\uff0c\u5e76\u652f\u6301\u4f4e\u786c\u4ef6\u73af\u5883\u90e8\u7f72\u3002", "motivation": "\u968f\u7740\u6570\u5b57\u6280\u672f\u7684\u8fc5\u901f\u53d1\u5c55\uff0cAI\u9a71\u52a8\u7684\u5fc3\u7406\u54a8\u8be2\u6210\u4e3a\u5fc3\u7406\u5065\u5eb7\u9886\u57df\u7684\u91cd\u8981\u7814\u7a76\u65b9\u5411\u3002\u4f46\u73b0\u6709\u6a21\u578b\u5728\u5bf9\u8bdd\u5b89\u5168\u6027\u3001\u7ec6\u81f4\u573a\u666f\u5904\u7406\u548c\u8f7b\u91cf\u5316\u90e8\u7f72\u65b9\u9762\u4ecd\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86PsyLite\uff0c\u4e00\u6b3e\u57fa\u4e8eInternLM2.5-7B-chat\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5fc3\u7406\u54a8\u8be2\u667a\u80fd\u4f53\u3002\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff1a\u6df7\u5408\u84b8\u998f\u6570\u636e\u5fae\u8c03\u548cORPO\u504f\u597d\u4f18\u5316\uff0c\u589e\u5f3a\u6a21\u578b\u7684\u6df1\u5ea6\u63a8\u7406\u3001\u5fc3\u7406\u54a8\u8be2\u548c\u5b89\u5168\u5bf9\u8bdd\u80fd\u529b\u3002\u5728\u90e8\u7f72\u7aef\u4f7f\u7528Ollama\u4e0eOpen WebUI\uff0c\u5e76\u901a\u8fc7Pipelines\u521b\u5efa\u5b9a\u5236\u5de5\u4f5c\u6d41\uff1b\u8bbe\u8ba1\u4e86\u521b\u65b0\u7684\u6761\u4ef6RAG\uff0c\u5728\u5fc5\u8981\u65f6\u5f15\u5165\u8da3\u5473\u5143\u7d20\u63d0\u5347\u4f53\u9a8c\uff0c\u540c\u65f6\u62d2\u7edd\u5371\u9669\u8bf7\u6c42\u63d0\u5347\u5b89\u5168\u6027\u3002\u901a\u8fc7\u91cf\u5316\u6280\u672f\uff0c\u6a21\u578b\u4ec5\u97005GB\u5185\u5b58\u5373\u53ef\u8fd0\u884c\uff0c\u9002\u5408\u8d44\u6e90\u53d7\u9650\u573a\u666f\u3002", "result": "PsyLite\u5728\u4e2d\u6587\u901a\u7528\u8bc4\u6d4b\uff08CEval\uff09\u3001\u5fc3\u7406\u54a8\u8be2\u4e13\u4e1a\u8bc4\u6d4b\uff08CPsyCounE\uff09\u3001\u5bf9\u8bdd\u5b89\u5168\u8bc4\u6d4b\uff08SafeDialBench\uff09\u4e2d\u5747\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u7279\u522b\u662f\u5728\u5fc3\u7406\u54a8\u8be2\u4e13\u4e1a\u6027\u548c\u5bf9\u8bdd\u5b89\u5168\u6027\u4e0a\u5206\u522b\u63d0\u5347\u4e8647.6%\u548c2.4%\u3002", "conclusion": "PsyLite\u517c\u987e\u4e13\u4e1a\u6027\u3001\u5bf9\u8bdd\u5b89\u5168\u6027\u548c\u8f7b\u91cf\u5316\u90e8\u7f72\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u5fc3\u7406\u54a8\u8be2\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2506.21119", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.21119", "abs": "https://arxiv.org/abs/2506.21119", "authors": ["Xiaoshuang Ji", "Zhendong Zhao", "Xiaojun Chen", "Xin Zhao", "Zeyao Liu"], "title": "Progtuning: Progressive Fine-tuning Framework for Transformer-based Language Models", "comment": "Accepted by ICONIP 2024", "summary": "Fine-tuning is a promising technique for leveraging Transformer-based\nlanguage models in downstream tasks. As model sizes continue to grow, updating\nall model parameters becomes increasingly costly. Parameter-efficient\nfine-tuning methods effectively address this issue by selectively updating a\nsmall subset of parameters. However, fine-tuning and most existing\nparameter-efficient fine-tuning methods require updating the same number of\nparameters as the initial size, ignoring the unequal contribution across\nTransformer blocks and leading to extremely inefficient allocation of computing\nresources. In this paper, we propose Progtuning, the novel fine-tuning\nframework combined with progressive learning for Transformer-based language\nmodels. Specifically, Progtuning progressively reduces the number of updated\ntransformer blocks based on the contribution. Remarkably, Progtuning optimizes\nresource allocation and reduces the number of updated parameters by\napproximately 25\\%, while still maintaining competitive performance. And it\nalso exhibits high adaptability with parameter-efficient fine-tuning methods,\ndemonstrating excellent performance across various adaptation scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u7684Progtuning\u65b9\u6cd5\u6839\u636e\u5404\u5c42\u8d21\u732e\u9010\u6b65\u964d\u4f4e\u53c2\u4e0e\u5fae\u8c03\u7684Transformer\u5757\u6570\u91cf\uff0c\u80fd\u7701\u7ea625%\u53c2\u6570\u66f4\u65b0\u91cf\u4e14\u51e0\u4e4e\u4e0d\u635f\u5931\u6027\u80fd\uff0c\u5e76\u517c\u5bb9\u73b0\u6709\u9ad8\u6548\u5fae\u8c03\u6280\u672f\uff0c\u63d0\u5347\u8ba1\u7b97\u8d44\u6e90\u5229\u7528\u6548\u7387\u3002", "motivation": "\u968f\u7740Transformer\u6a21\u578b\u89c4\u6a21\u4e0d\u65ad\u6269\u5927\uff0c\u4f20\u7edf\u5fae\u8c03\u65b9\u6cd5\u9700\u8981\u66f4\u65b0\u6240\u6709\u53c2\u6570\uff0c\u8ba1\u7b97\u4ee3\u4ef7\u9ad8\u6602\u3002\u867d\u7136\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u6280\u672f\u80fd\u591f\u51cf\u5c11\u66f4\u65b0\u7684\u53c2\u6570\u91cf\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u672a\u9488\u5bf9\u4e0d\u540c\u5757\u7684\u8d21\u732e\u5206\u914d\u8ba1\u7b97\u8d44\u6e90\uff0c\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u63d0\u51fa\u4e86Progtuning\uff0c\u8fd9\u662f\u4e00\u79cd\u7ed3\u5408\u6e10\u8fdb\u5f0f\u5b66\u4e60\u7684Transformer\u5fae\u8c03\u6846\u67b6\u3002\u8be5\u65b9\u6cd5\u6839\u636e\u5404Transformer\u5757\u7684\u8d21\u732e\uff0c\u9010\u6b65\u7f29\u51cf\u9700\u8981\u66f4\u65b0\u7684\u5757\u6570\u91cf\uff0c\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u53c2\u6570\u66f4\u65b0\u3002", "result": "Progtuning\u5728\u53c2\u6570\u66f4\u65b0\u91cf\u4e0a\u51cf\u5c11\u4e86\u7ea625%\uff0c\u4f46\u4ecd\u4fdd\u6301\u4e86\u7ade\u4e89\u529b\u7684\u6a21\u578b\u6027\u80fd\u3002\u540c\u65f6\uff0c\u8be5\u65b9\u6cd5\u53ef\u4e0e\u73b0\u6709\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u6280\u672f\u7ed3\u5408\uff0c\u9002\u5e94\u591a\u79cd\u5fae\u8c03\u573a\u666f\uff0c\u6548\u679c\u51fa\u8272\u3002", "conclusion": "Progtuning\u80fd\u4f18\u5316Transformer\u5fae\u8c03\u65f6\u7684\u8d44\u6e90\u5206\u914d\uff0c\u5728\u53c2\u6570\u91cf\u5927\u5e45\u51cf\u5c11\u7684\u540c\u65f6\u6027\u80fd\u4e0d\u964d\uff0c\u4e3a\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2506.20243", "categories": ["cs.CL", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.20243", "abs": "https://arxiv.org/abs/2506.20243", "authors": ["Papa S\u00e9ga Wade", "Mihai Andries", "Ioannis Kanellos", "Thierry Moudenc"], "title": "CBF-AFA: Chunk-Based Multi-SSL Fusion for Automatic Fluency Assessment", "comment": "5 pages, accepted for presentation at EUSIPCO 2025", "summary": "Automatic fluency assessment (AFA) remains challenging, particularly in\ncapturing speech rhythm, pauses, and disfluencies in non-native speakers. We\nintroduce a chunk-based approach integrating self-supervised learning (SSL)\nmodels (Wav2Vec2, HuBERT, and WavLM) selected for their complementary strengths\nin phonetic, prosodic, and noisy speech modeling, with a hierarchical\nCNN-BiLSTM framework. Speech is segmented into breath-group chunks using Silero\nvoice activity detection (Silero-VAD), enabling fine-grained temporal analysis\nwhile mitigating over-segmentation artifacts. SSL embeddings are fused via a\nlearnable weighted mechanism, balancing acoustic and linguistic features, and\nenriched with chunk-level fluency markers (e.g., speech rate, pause durations,\nn-gram repetitions). The CNN-BiLSTM captures local and long-term dependencies\nacross chunks. Evaluated on Avalinguo and Speechocean762, our approach improves\nF1-score by 2.8 and Pearson correlation by 6.2 points over single SSL baselines\non Speechocean762, with gains of 4.2 F1-score and 4.0 Pearson points on\nAvalinguo, surpassing Pyannote.audio-based segmentation baselines. These\nfindings highlight chunk-based multi-SSL fusion for robust fluency evaluation,\nthough future work should explore generalization to dialects with irregular\nprosody.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u591a\u79cd\u81ea\u76d1\u7763\u8bed\u97f3\u6a21\u578b\u548c\u57fa\u4e8e\u7247\u6bb5\u7684\u7279\u5f81\u878d\u5408\u65b9\u6cd5\uff0c\u6709\u6548\u63d0\u5347\u4e86\u81ea\u52a8\u53e3\u8bed\u6d41\u5229\u5ea6\u8bc4\u4f30\u7684\u51c6\u786e\u6027\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u590d\u6742\u505c\u987f\u4e0e\u8282\u594f\u7279\u5f81\u7684\u8bc6\u522b\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u8457\u8d85\u8d8a\u4f20\u7edf\u57fa\u7ebf\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u6d41\u5229\u5ea6\u8bc4\u4f30\u65b9\u6cd5\u96be\u4ee5\u51c6\u786e\u523b\u753b\u975e\u6bcd\u8bed\u8005\u7684\u8bed\u97f3\u8282\u594f\u3001\u505c\u987f\u548c\u4e0d\u6d41\u5229\u73b0\u8c61\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u80fd\u53cd\u6620\u8fd9\u4e9b\u7279\u5f81\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u547c\u5438\u7ec4\u7247\u6bb5\u5206\u5272\u548c\u591a\u81ea\u76d1\u7763\u5b66\u4e60SSL\u6a21\u578b\uff08Wav2Vec2\u3001HuBERT\u3001WavLM\uff09\u7279\u5f81\u878d\u5408\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408CNN-BiLSTM\u6846\u67b6\uff0c\u5229\u7528Silero-VAD\u8fdb\u884c\u8bed\u97f3\u5207\u5206\uff0c\u7279\u5f81\u901a\u8fc7\u53ef\u5b66\u4e60\u6743\u91cd\u673a\u5236\u878d\u5408\uff0c\u5e76\u5728\u7247\u6bb5\u4e2d\u52a0\u5165\u6d41\u5229\u5ea6\u6807\u8bb0\uff08\u5982\u8bed\u901f\u3001\u505c\u987f\u3001\u91cd\u590d\uff09\uff0c\u6574\u4f53\u7f51\u7edc\u6355\u6349\u5c40\u90e8\u4e0e\u5168\u5c40\u4f9d\u8d56\u3002", "result": "\u5728Avalinguo\u4e0eSpeechocean762\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u7684F1\u5206\u6570\u548cPearson\u76f8\u5173\u6027\u5206\u522b\u63d0\u5347\u4e862.8/6.2\uff08Speechocean762\uff09\u30014.2/4.0\uff08Avalinguo\uff09\uff0c\u5747\u8d85\u8fc7\u5355\u4e00SSL\u6a21\u578b\u4e0e\u57fa\u7ebf\u65b9\u6cd5\u3002\u672a\u6765\u8fd8\u9700\u63a2\u8ba8\u5bf9\u4e0d\u89c4\u5219\u97f5\u5f8b\u65b9\u8a00\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u57fa\u4e8e\u7247\u6bb5\u7684\u591aSSL\u7279\u5f81\u878d\u5408\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u6d41\u5229\u5ea6\u8bc4\u4f30\u7684\u51c6\u786e\u6027\uff0c\u4f18\u4e8e\u73b0\u6709\u7684Pyannote.audio\u5206\u5272\u57fa\u7ebf\uff0c\u7279\u522b\u5728F1\u5206\u6570\u548cPearson\u76f8\u5173\u6027\u65b9\u9762\u5747\u6709\u8f83\u5927\u63d0\u5347\u3002"}}
{"id": "2506.21170", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.21170", "abs": "https://arxiv.org/abs/2506.21170", "authors": ["Viacheslav Meshchaninov", "Egor Chimbulatov", "Alexander Shabalin", "Aleksandr Abramov", "Dmitry Vetrov"], "title": "Compressed and Smooth Latent Space for Text Diffusion Modeling", "comment": null, "summary": "Autoregressive language models dominate modern text generation, yet their\nsequential nature introduces fundamental limitations: decoding is slow, and\nmaintaining global coherence remains challenging. Diffusion models offer a\npromising alternative by enabling parallel generation and flexible control;\nhowever, their application to text generation is hindered by the high\ndimensionality of token-level representations. We introduce Cosmos, a novel\napproach to text generation that operates entirely in a compressed, smooth\nlatent space tailored specifically for diffusion. This space is learned using\nan autoencoder trained simultaneously for token-level reconstruction and\nalignment with frozen activations from a pretrained language encoder, providing\nrobust semantic grounding and enabling effective perturbation-based\naugmentations. Empirically, we demonstrate that text representations can be\ncompressed by $8\\times$ while maintaining generation quality comparable to\ntoken-level diffusion models. Furthermore, increasing the latent sequence\nlength allows Cosmos to surpass both diffusion-based and autoregressive\nbaselines. We evaluate Cosmos on four diverse generative tasks including story\ngeneration, question generation, summarization, and detoxification and compare\nit with various generative paradigms. Cosmos achieves comparable or superior\ngeneration quality while offering more than $2\\times$ faster inference.", "AI": {"tldr": "Cosmos\u901a\u8fc7\u5728\u538b\u7f29\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5e94\u7528\u6269\u6563\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u6587\u672c\u751f\u6210\u7684\u6548\u7387\u548c\u8d28\u91cf\uff0c\u6709\u671b\u53d6\u4ee3\u4f20\u7edf\u81ea\u56de\u5f52\u751f\u6210\u6a21\u578b\u3002", "motivation": "\u5f53\u524d\uff0c\u81ea\u56de\u5f52\u8bed\u8a00\u6a21\u578b\u5728\u6587\u672c\u751f\u6210\u9886\u57df\u5360\u4e3b\u5bfc\u5730\u4f4d\uff0c\u4f46\u5b83\u4eec\u7684\u987a\u5e8f\u6027\u5e26\u6765\u4e86\u63a8\u7406\u901f\u5ea6\u6162\u548c\u96be\u4ee5\u7ef4\u6301\u6574\u4f53\u8fde\u8d2f\u6027\u7684\u7f3a\u70b9\u3002\u6269\u6563\u6a21\u578b\u5177\u6709\u5e76\u884c\u751f\u6210\u548c\u7075\u6d3b\u63a7\u5236\u7684\u6f5c\u529b\uff0c\u4f46\u53d7\u9650\u4e8e\u9ad8\u7ef4\u7684\u7b26\u53f7\u7ea7\u8868\u793a\uff0c\u96be\u4ee5\u76f4\u63a5\u5e94\u7528\u4e8e\u6587\u672c\u751f\u6210\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCosmos\u7684\u65b9\u6cd5\uff0c\u5b83\u4f7f\u7528\u538b\u7f29\u800c\u5e73\u6ed1\u7684\u6f5c\u5728\u7a7a\u95f4\u8fdb\u884c\u6587\u672c\u751f\u6210\u3002\u8be5\u6f5c\u5728\u7a7a\u95f4\u901a\u8fc7\u4e00\u4e2a\u81ea\u7f16\u7801\u5668\u5b66\u4e60\uff0c\u8bad\u7ec3\u76ee\u6807\u5305\u62ec\u7b26\u53f7\u7ea7\u91cd\u5efa\u548c\u4e0e\u9884\u8bad\u7ec3\u8bed\u8a00\u7f16\u7801\u5668\u51bb\u7ed3\u6fc0\u6d3b\u7684\u5bf9\u9f50\uff0c\u4ece\u800c\u4fdd\u8bc1\u8bed\u4e49\u57fa\u7840\u7a33\u56fa\uff0c\u5e76\u53ef\u5b9e\u73b0\u57fa\u4e8e\u6270\u52a8\u7684\u6570\u636e\u589e\u5f3a\u3002", "result": "Cosmos\u65b9\u6cd5\u5b9e\u73b0\u4e86\u6587\u672c\u8868\u793a\u76848\u500d\u538b\u7f29\uff0c\u5728\u751f\u6210\u8d28\u91cf\u4e0e\u7b26\u53f7\u7ea7\u6269\u6563\u6a21\u578b\u76f8\u5f53\u7684\u540c\u65f6\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u5347\u4e862\u500d\u4ee5\u4e0a\u3002\u6b64\u5916\uff0c\u589e\u52a0\u6f5c\u5728\u5e8f\u5217\u957f\u5ea6\u53ef\u8fdb\u4e00\u6b65\u63d0\u5347\u751a\u81f3\u8d85\u8d8a\u73b0\u6709\u6269\u6563\u548c\u81ea\u56de\u5f52\u6a21\u578b\u7684\u8868\u73b0\u3002", "conclusion": "Cosmos\u5728\u6545\u4e8b\u751f\u6210\u3001\u95ee\u9898\u751f\u6210\u3001\u6458\u8981\u548c\u5185\u5bb9\u51c0\u5316\u7b49\u591a\u9879\u6587\u672c\u751f\u6210\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u53ef\u6bd4\u751a\u81f3\u66f4\u4f18\u7684\u8868\u73b0\uff0c\u6210\u4e3a\u5177\u5907\u9ad8\u6548\u63a8\u7406\u548c\u9ad8\u751f\u6210\u8d28\u91cf\u7684\u65b0\u4e00\u4ee3\u6587\u672c\u751f\u6210\u8303\u5f0f\u3002"}}
{"id": "2506.21182", "categories": ["cs.CL", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2506.21182", "abs": "https://arxiv.org/abs/2506.21182", "authors": ["Isaac Chung", "Imene Kerboua", "Marton Kardos", "Roman Solomatin", "Kenneth Enevoldsen"], "title": "Maintaining MTEB: Towards Long Term Usability and Reproducibility of Embedding Benchmarks", "comment": null, "summary": "The Massive Text Embedding Benchmark (MTEB) has become a standard evaluation\nplatform for text embedding models. While previous work has established the\ncore benchmark methodology, this paper focuses on the engineering aspects that\nensure MTEB's continued reproducibility and extensibility. We present our\napproach to maintaining robust continuous integration pipelines that validate\ndataset integrity, automate test execution, and assess benchmark results'\ngeneralizability. We detail the design choices that collectively enhance\nreproducibility and usability. Furthermore, we discuss our strategies for\nhandling community contributions and extending the benchmark with new tasks and\ndatasets. These engineering practices have been instrumental in scaling MTEB to\nbecome more comprehensive while maintaining quality and, ultimately, relevance\nto the field. Our experiences offer valuable insights for benchmark maintainers\nfacing similar challenges in ensuring reproducibility and usability in machine\nlearning evaluation frameworks. The MTEB repository is available at:\nhttps://github.com/embeddings-benchmark/mteb", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u5982\u4f55\u901a\u8fc7\u5de5\u7a0b\u5316\u624b\u6bb5\u4fdd\u8bc1 MTEB \u57fa\u51c6\u5e73\u53f0\u7684\u53ef\u590d\u73b0\u6027\u3001\u6269\u5c55\u6027\u548c\u793e\u533a\u534f\u4f5c\u7ecf\u9a8c\uff0c\u4e3a\u6587\u672c\u5d4c\u5165\u6a21\u578b\u8bc4\u6d4b\u63d0\u4f9b\u4e86\u53ef\u6301\u7eed\u53d1\u5c55\u7684\u8303\u4f8b\u3002", "motivation": "MTEB \u5df2\u6210\u4e3a\u6587\u672c\u5d4c\u5165\u6a21\u578b\u7684\u91cd\u8981\u8bc4\u6d4b\u6807\u51c6\uff0c\u4f46\u968f\u7740\u793e\u533a\u6301\u7eed\u62d3\u5c55\uff0c\u5982\u4f55\u4fdd\u8bc1\u5176\u53ef\u590d\u73b0\u6027\u548c\u53ef\u6269\u5c55\u6027\u6210\u4e3a\u6838\u5fc3\u5de5\u7a0b\u6311\u6218\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5\u5e73\u53f0\u968f\u7740\u4efb\u52a1\u548c\u6570\u636e\u96c6\u7684\u589e\u52a0\uff0c\u5982\u4f55\u4fdd\u6301\u9ad8\u8d28\u91cf\u548c\u76f8\u5173\u6027\u7684\u95ee\u9898\uff0c\u5e76\u4e3a\u5176\u4ed6\u7c7b\u4f3c\u8bc4\u6d4b\u5e73\u53f0\u63d0\u4f9b\u7ecf\u9a8c\u3002", "method": "\u4f5c\u8005\u4ecb\u7ecd\u4e86\u5982\u4f55\u901a\u8fc7\u6301\u7eed\u96c6\u6210\u6d41\u6c34\u7ebf\u3001\u81ea\u52a8\u5316\u6d4b\u8bd5\u3001\u6570\u636e\u96c6\u5b8c\u6574\u6027\u6821\u9a8c\u3001\u57fa\u51c6\u7ed3\u679c\u6cdb\u5316\u80fd\u529b\u8bc4\u4f30\u7b49\u4e00\u7cfb\u5217\u5de5\u7a0b\u5b9e\u8df5\uff0c\u4fdd\u969c MTEB \u53ef\u6301\u7eed\u53d1\u5c55\u3001\u53ef\u590d\u73b0\u6027\u548c\u53ef\u7528\u6027\u3002\u540c\u65f6\uff0c\u8ba8\u8bba\u4e86\u793e\u533a\u8d21\u732e\u5904\u7406\u673a\u5236\u53ca\u5982\u4f55\u4fbf\u6377\u5730\u6269\u5145\u65b0\u4efb\u52a1\u4e0e\u6570\u636e\u96c6\u3002", "result": "\u8fd9\u4e9b\u5de5\u7a0b\u63aa\u65bd\u4f7f MTEB \u80fd\u591f\u5728\u4fdd\u8bc1\u8d28\u91cf\u7684\u540c\u65f6\u6301\u7eed\u6269\u5c55\uff0c\u63d0\u5347\u5176\u5728\u6587\u672c\u5d4c\u5165\u9886\u57df\u7684\u6743\u5a01\u6027\u548c\u5b9e\u7528\u6027\u3002\u5e73\u53f0\u7684\u7ecf\u9a8c\u603b\u7ed3\u53ef\u4e3a\u5176\u5b83\u8bc4\u6d4b\u57fa\u51c6\u7684\u7ef4\u62a4\u8005\u63d0\u4f9b\u53c2\u8003\u3002", "conclusion": "\u6709\u6548\u7684\u5de5\u7a0b\u5b9e\u8df5\uff08\u6301\u7eed\u96c6\u6210\u3001\u81ea\u52a8\u5316\u6d4b\u8bd5\u3001\u793e\u533a\u6269\u5c55\u673a\u5236\u7b49\uff09\u662f\u786e\u4fdd\u5927\u89c4\u6a21\u57fa\u51c6\u5e73\u53f0\u8d28\u91cf\u3001\u53ef\u590d\u73b0\u6027\u4e0e\u6269\u5c55\u6027\u7684\u5173\u952e\uff0c\u8fd9\u5bf9\u6574\u4e2a\u673a\u5668\u5b66\u4e60\u8bc4\u6d4b\u793e\u533a\u5177\u6709\u91cd\u8981\u501f\u9274\u4ef7\u503c\u3002"}}
{"id": "2506.21191", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.21191", "abs": "https://arxiv.org/abs/2506.21191", "authors": ["Koji Inoue", "Mikey Elmers", "Yahui Fu", "Zi Haur Pang", "Divesh Lala", "Keiko Ochi", "Tatsuya Kawahara"], "title": "Prompt-Guided Turn-Taking Prediction", "comment": "This paper has been accepted for presentation at SIGdial Meeting on\n  Discourse and Dialogue 2025 (SIGDIAL 2025) and represents the author's\n  version of the work", "summary": "Turn-taking prediction models are essential components in spoken dialogue\nsystems and conversational robots. Recent approaches leverage transformer-based\narchitectures to predict speech activity continuously and in real-time. In this\nstudy, we propose a novel model that enables turn-taking prediction to be\ndynamically controlled via textual prompts. This approach allows intuitive and\nexplicit control through instructions such as \"faster\" or \"calmer\" adapting\ndynamically to conversational partners and contexts. The proposed model builds\nupon a transformer-based voice activity projection (VAP) model, incorporating\ntextual prompt embeddings into both channel-wise transformers and a\ncross-channel transformer. We evaluated the feasibility of our approach using\nover 950 hours of human-human spoken dialogue data. Since textual prompt data\nfor the proposed approach was not available in existing datasets, we utilized a\nlarge language model (LLM) to generate synthetic prompt sentences. Experimental\nresults demonstrated that the proposed model improved prediction accuracy and\neffectively varied turn-taking timing behaviors according to the textual\nprompts.", "AI": {"tldr": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u52a8\u6001\u63a7\u5236\u8f6e\u6d41\u9884\u6d4b\u7684transformer\u6a21\u578b\uff0c\u65e2\u63d0\u5347\u4e86\u9884\u6d4b\u51c6\u786e\u7387\uff0c\u4e5f\u589e\u5f3a\u4e86\u5bf9\u4f1a\u8bdd\u98ce\u683c\u7684\u9002\u5e94\u80fd\u529b\u3002", "motivation": "\u5bf9\u8bdd\u7cfb\u7edf\u548c\u4f1a\u8bdd\u673a\u5668\u4eba\u9700\u8981\u7cbe\u786e\u7684\u8f6e\u6d41\u9884\u6d4b\u6a21\u578b\u4ee5\u63d0\u5347\u4e92\u52a8\u4f53\u9a8c\u3002\u73b0\u6709\u65b9\u6cd5\u867d\u7136\u80fd\u505a\u5230\u5b9e\u65f6\u9884\u6d4b\uff0c\u4f46\u7f3a\u4e4f\u7075\u6d3b\u3001\u76f4\u89c2\u7684\u4eba\u673a\u63a7\u5236\u80fd\u529b\uff0c\u96be\u4ee5\u6839\u636e\u5177\u4f53\u573a\u666f\u6216\u7528\u6237\u9700\u6c42\u81ea\u5982\u8c03\u6574\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8etransformer\u7684\u6a21\u578b\uff0c\u5728\u4f20\u7edfVAP\uff08\u8bed\u97f3\u6d3b\u52a8\u6295\u5f71\uff09\u6a21\u578b\u7684\u57fa\u7840\u4e0a\uff0c\u52a0\u5165\u4e86\u6587\u672c\u63d0\u793a\u5d4c\u5165\uff0c\u5e76\u878d\u5408\u5230\u901a\u9053\u5185\u90e8\u4e0e\u8de8\u901a\u9053transformer\u4e2d\uff0c\u5b9e\u73b0\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\uff08\u5982\u201c\u66f4\u5feb\u201d\u201c\u66f4\u51b7\u9759\u201d\uff09\u5b9e\u65f6\u52a8\u6001\u8c03\u6574\u8f6e\u6d41\u9884\u6d4b\u3002\u6ca1\u6709\u771f\u5b9e\u7684\u6587\u672c\u63d0\u793a\u6570\u636e\u65f6\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u5408\u6210\u63d0\u793a\u53e5\u3002", "result": "\u5728950\u5c0f\u65f6\u7684\u771f\u4eba\u5bf9\u8bdd\u6570\u636e\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u8868\u660e\u8be5\u6a21\u578b\u4e0d\u4ec5\u63d0\u5347\u4e86\u9884\u6d4b\u51c6\u786e\u7387\uff0c\u8fd8\u80fd\u4f9d\u636e\u4e0d\u540c\u6587\u672c\u63d0\u793a\u6709\u6548\u8c03\u6574\u8f6e\u6d41\u5207\u6362\u884c\u4e3a\u3002", "conclusion": "\u5c06\u6587\u672c\u63d0\u793a\u4e0etransformer\u7ed3\u5408\u7684\u65b0\u6a21\u578b\u4e3a\u5bf9\u8bdd\u7cfb\u7edf\u548c\u673a\u5668\u4eba\u5e26\u6765\u66f4\u76f4\u89c2\u3001\u7075\u6d3b\u7684\u8f6e\u6d41\u9884\u6d4b\u8c03\u63a7\u80fd\u529b\uff0c\u517c\u5177\u51c6\u786e\u6027\u548c\u9002\u5e94\u6027\u7684\u63d0\u5347\u3002"}}
{"id": "2506.21222", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2506.21222", "abs": "https://arxiv.org/abs/2506.21222", "authors": ["Yongchan Chun", "Minhyuk Kim", "Dongjun Kim", "Chanjun Park", "Heuiseok Lim"], "title": "Enhancing Automatic Term Extraction with Large Language Models via Syntactic Retrieval", "comment": null, "summary": "Automatic Term Extraction (ATE) identifies domain-specific expressions that\nare crucial for downstream tasks such as machine translation and information\nretrieval. Although large language models (LLMs) have significantly advanced\nvarious NLP tasks, their potential for ATE has scarcely been examined. We\npropose a retrieval-based prompting strategy that, in the few-shot setting,\nselects demonstrations according to \\emph{syntactic} rather than semantic\nsimilarity. This syntactic retrieval method is domain-agnostic and provides\nmore reliable guidance for capturing term boundaries. We evaluate the approach\nin both in-domain and cross-domain settings, analyzing how lexical overlap\nbetween the query sentence and its retrieved examples affects performance.\nExperiments on three specialized ATE benchmarks show that syntactic retrieval\nimproves F1-score. These findings highlight the importance of syntactic cues\nwhen adapting LLMs to terminology-extraction tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u57fa\u4e8e\u53e5\u6cd5\u76f8\u4f3c\u6027\u68c0\u7d22\u7684prompt\uff0c\u80fd\u663e\u8457\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u52a8\u672f\u8bed\u63d0\u53d6\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u4e14\u65b9\u6cd5\u9886\u57df\u901a\u7528\u3002", "motivation": "\u81ea\u52a8\u672f\u8bed\u63d0\u53d6\uff08ATE\uff09\u5bf9\u8bb8\u591a\u4e0b\u6e38\u4efb\u52a1\uff08\u5982\u673a\u5668\u7ffb\u8bd1\u548c\u4fe1\u606f\u68c0\u7d22\uff09\u5f88\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u5173\u4e8e\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63d0\u5347ATE\u80fd\u529b\u7684\u7814\u7a76\u5f88\u5c11\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u68c0\u7d22\u7684\u63d0\u793a\u65b9\u6cd5\uff0c\u901a\u8fc7\u9009\u62e9\u2018\u53e5\u6cd5\u2019\uff08\u800c\u975e\u8bed\u4e49\uff09\u76f8\u4f3c\u7684\u6f14\u793a\u6848\u4f8b\uff0c\u7528\u4e8efew-shot\u8bbe\u7f6e\u3002\u8be5\u53e5\u6cd5\u68c0\u7d22\u65b9\u6cd5\u5177\u6709\u9886\u57df\u65e0\u5173\u6027\uff0c\u80fd\u591f\u66f4\u597d\u5730\u6307\u5f15\u6a21\u578b\u6355\u6349\u672f\u8bed\u8fb9\u754c\u3002", "result": "\u5728\u4e09\u4e2a\u4e13\u95e8\u7684ATE\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u7ed3\u679c\u663e\u793a\u53e5\u6cd5\u68c0\u7d22\u65b9\u6cd5\u63d0\u5347\u4e86F1-score\u3002\u8fd8\u5206\u6790\u4e86\u67e5\u8be2\u53e5\u4e0e\u68c0\u7d22\u793a\u4f8b\u4e4b\u95f4\u8bcd\u6c47\u91cd\u53e0\u5bf9\u6a21\u578b\u8868\u73b0\u7684\u5f71\u54cd\u3002", "conclusion": "\u53e5\u6cd5\u7ebf\u7d22\u5bf9\u4e8e\u9002\u914dLLM\u5230\u672f\u8bed\u63d0\u53d6\u4efb\u52a1\u975e\u5e38\u91cd\u8981\uff0c\u63d0\u51fa\u7684\u53e5\u6cd5\u68c0\u7d22\u6cd5\u80fd\u8de8\u9886\u57df\u3001\u63d0\u5347ATE\u8868\u73b0\u3002"}}
{"id": "2506.21252", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.21252", "abs": "https://arxiv.org/abs/2506.21252", "authors": ["Tianyi Men", "Zhuoran Jin", "Pengfei Cao", "Yubo Chen", "Kang Liu", "Jun Zhao"], "title": "Agent-RewardBench: Towards a Unified Benchmark for Reward Modeling across Perception, Planning, and Safety in Real-World Multimodal Agents", "comment": "ACL 2025 Main", "summary": "As Multimodal Large Language Models (MLLMs) advance, multimodal agents show\npromise in real-world tasks like web navigation and embodied intelligence.\nHowever, due to limitations in a lack of external feedback, these agents\nstruggle with self-correction and generalization. A promising approach is to\nuse reward models as external feedback, but there is no clear on how to select\nreward models for agents. Thus, there is an urgent need to build a reward bench\ntargeted at agents. To address these challenges, we propose Agent-RewardBench,\na benchmark designed to evaluate reward modeling ability in MLLMs. The\nbenchmark is characterized by three key features: (1) Multiple dimensions and\nreal-world agent scenarios evaluation. It covers perception, planning, and\nsafety with 7 scenarios; (2) Step-level reward evaluation. It allows for the\nassessment of agent capabilities at the individual steps of a task, providing a\nmore granular view of performance during the planning process; and (3)\nAppropriately difficulty and high-quality. We carefully sample from 10 diverse\nmodels, difficulty control to maintain task challenges, and manual verification\nto ensure the integrity of the data. Experiments demonstrate that even\nstate-of-the-art multimodal models show limited performance, highlighting the\nneed for specialized training in agent reward modeling. Code is available at\ngithub.", "AI": {"tldr": "\u672c\u5de5\u4f5c\u63d0\u51fa\u4e86Agent-RewardBench\uff0c\u4e00\u4e2a\u4e13\u4e3a\u591a\u6a21\u6001\u5927\u6a21\u578b\u5956\u52b1\u5efa\u6a21\u80fd\u529b\u8bc4\u4f30\u7684\u57fa\u51c6\uff0c\u8986\u76d6\u591a\u573a\u666f\u548c\u7ec6\u7c92\u5ea6\u5956\u52b1\u8bc4\u4f30\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u5f53\u524d\u6a21\u578b\u5728\u5956\u52b1\u5efa\u6a21\u4e0a\u8868\u73b0\u6709\u9650\uff0c\u51f8\u663e\u4e86\u4e13\u9879\u8bad\u7ec3\u9700\u6c42\u3002", "motivation": "\u968f\u7740\u591a\u6a21\u6001\u5927\u6a21\u578b\uff08MLLMs\uff09\u7684\u8fdb\u6b65\uff0c\u591a\u6a21\u6001\u667a\u80fd\u4f53\u5728\u5b9e\u9645\u4efb\u52a1\u4e2d\uff08\u5982\u7f51\u9875\u5bfc\u822a\u3001\u5177\u8eab\u667a\u80fd\uff09\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u7531\u4e8e\u7f3a\u4e4f\u5916\u90e8\u53cd\u9988\uff0c\u8fd9\u4e9b\u667a\u80fd\u4f53\u5728\u81ea\u6211\u7ea0\u9519\u548c\u6cdb\u5316\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002\u91c7\u7528\u5956\u52b1\u6a21\u578b\u4f5c\u4e3a\u5916\u90e8\u53cd\u9988\u662f\u4e00\u4e2a\u6709\u524d\u666f\u7684\u65b9\u6cd5\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u9488\u5bf9\u667a\u80fd\u4f53\u5956\u52b1\u6a21\u578b\u7684\u6807\u51c6\u8bc4\u4f30\u65b9\u6cd5\uff0c\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u4e2a\u4e13\u95e8\u4e3a\u667a\u80fd\u4f53\u6784\u5efa\u7684\u5956\u52b1\u57fa\u51c6\u3002", "method": "\u63d0\u51fa\u4e86Agent-RewardBench\uff0c\u8fd9\u662f\u4e00\u5957\u4e13\u4e3a\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u6a21\u578b\uff08MLLMs\uff09\u5956\u52b1\u5efa\u6a21\u80fd\u529b\u800c\u8bbe\u8ba1\u7684\u57fa\u51c6\u3002\u8be5\u57fa\u51c6\u5305\u542b\u4e09\u4e2a\u7279\u70b9\uff1a\uff081\uff09\u591a\u7ef4\u5ea6\u4e14\u8d34\u8fd1\u5b9e\u9645\u7684\u667a\u80fd\u4f53\u573a\u666f\u8bc4\u4f30\uff0c\u6db5\u76d6\u611f\u77e5\u3001\u89c4\u5212\u4e0e\u5b89\u5168\u7b497\u7c7b\u573a\u666f\uff1b\uff082\uff09\u6b65\u7ea7\u522b\u5956\u52b1\u8bc4\u4ef7\uff0c\u80fd\u591f\u7ec6\u81f4\u8bc4\u4f30\u667a\u80fd\u4f53\u5728\u4efb\u52a1\u5404\u9636\u6bb5\u7684\u8868\u73b0\uff1b\uff083\uff09\u5408\u9002\u7684\u96be\u5ea6\u4e0e\u9ad8\u8d28\u91cf\uff0c\u901a\u8fc7\u4ece10\u4e2a\u4e0d\u540c\u6a21\u578b\u4e2d\u62bd\u6837\u5e76\u8fdb\u884c\u4eba\u5de5\u9a8c\u8bc1\uff0c\u786e\u4fdd\u4efb\u52a1\u6311\u6218\u6027\u548c\u6570\u636e\u5b8c\u6574\u6027\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u5373\u4f7f\u662f\u6700\u5148\u8fdb\u7684\u591a\u6a21\u6001\u6a21\u578b\u5728\u8be5\u57fa\u51c6\u4e0b\u7684\u8868\u73b0\u4e5f\u6709\u9650\uff0c\u663e\u793a\u73b0\u6709\u6a21\u578b\u5728\u667a\u80fd\u4f53\u5956\u52b1\u5efa\u6a21\u80fd\u529b\u4e0a\u5b58\u5728\u4e0d\u8db3\u3002", "conclusion": "\u7279\u522b\u9488\u5bf9\u667a\u80fd\u4f53\u7684\u5956\u52b1\u5efa\u6a21\u5177\u6709\u6311\u6218\u6027\uff0c\u73b0\u6709\u591a\u6a21\u6001\u5927\u6a21\u578b\u7f3a\u4e4f\u8db3\u591f\u7684\u5efa\u6a21\u80fd\u529b\u3002\u4e3a\u63d0\u9ad8\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u8868\u73b0\uff0c\u9700\u8981\u9488\u5bf9\u5956\u52b1\u6a21\u578b\u7684\u4e13\u9879\u8bad\u7ec3\u3002"}}
{"id": "2506.21274", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.21274", "abs": "https://arxiv.org/abs/2506.21274", "authors": ["Andrea McGlinchey", "Peter J Barclay"], "title": "Cat and Mouse -- Can Fake Text Generation Outpace Detector Systems?", "comment": "(Submitted for publication)", "summary": "Large language models can produce convincing \"fake text\" in domains such as\nacademic writing, product reviews, and political news. Many approaches have\nbeen investigated for the detection of artificially generated text. While this\nmay seem to presage an endless \"arms race\", we note that newer LLMs use ever\nmore parameters, training data, and energy, while relatively simple classifiers\ndemonstrate a good level of detection accuracy with modest resources. To\napproach the question of whether the models' ability to beat the detectors may\ntherefore reach a plateau, we examine the ability of statistical classifiers to\nidentify \"fake text\" in the style of classical detective fiction. Over a 0.5\nversion increase, we found that Gemini showed an increased ability to generate\ndeceptive text, while GPT did not. This suggests that reliable detection of\nfake text may remain feasible even for ever-larger models, though new model\narchitectures may improve their deceptiveness", "AI": {"tldr": "\u5927\u6a21\u578b\u751f\u6210\u4f2a\u6587\u80fd\u529b\u589e\u5f3a\uff0c\u4f46\u7b80\u5355\u5206\u7c7b\u5668\u4f9d\u7136\u6709\u68c0\u6d4b\u4f5c\u7528\uff0c\u4e0d\u662f\u65e0\u6cd5\u5e94\u5bf9\u7684\u201c\u519b\u5907\u7ade\u8d5b\u201d\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u53ef\u4ee5\u751f\u6210\u5177\u6709\u6b3a\u9a97\u6027\u7684\u201c\u4f2a\u9020\u6587\u672c\u201d\uff0c\u5728\u5b66\u672f\u5199\u4f5c\u3001\u4ea7\u54c1\u8bc4\u8bba\u548c\u653f\u6cbb\u65b0\u95fb\u7b49\u9886\u57df\u5e26\u6765\u4e86\u6311\u6218\uff0c\u56e0\u6b64\uff0c\u9700\u8981\u7814\u7a76\u548c\u8bc4\u4f30\u68c0\u6d4b\u4eba\u5de5\u751f\u6210\u6587\u672c\u7684\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u91c7\u7528\u7edf\u8ba1\u5206\u7c7b\u5668\uff0c\u9488\u5bf9\u7ecf\u5178\u4fa6\u63a2\u5c0f\u8bf4\u98ce\u683c\u7684\u6587\u672c\uff0c\u8bc4\u4f30\u5176\u68c0\u6d4b\u201c\u4f2a\u9020\u6587\u672c\u201d\u7684\u80fd\u529b\uff0c\u5e76\u6bd4\u8f83\u4e86\u4e0d\u540c\u6a21\u578b\uff08\u5982Gemini\u548cGPT\uff09\u751f\u6210\u6b3a\u9a97\u6027\u6587\u672c\u7684\u80fd\u529b\u968f\u7248\u672c\u53d8\u5316\u7684\u60c5\u51b5\u3002", "result": "\u57280.5\u7248\u672c\u7684\u5347\u7ea7\u8fc7\u7a0b\u4e2d\uff0cGemini\u6a21\u578b\u751f\u6210\u6b3a\u9a97\u6027\u6587\u672c\u7684\u80fd\u529b\u6709\u6240\u63d0\u5347\uff0c\u800cGPT\u6a21\u578b\u5219\u6ca1\u6709\u660e\u663e\u53d8\u5316\u3002", "conclusion": "\u5c3d\u7ba1LLM\u4f53\u91cf\u548c\u80fd\u529b\u589e\u5f3a\uff0c\u7b80\u5355\u7684\u5206\u7c7b\u5668\u4f9d\u7136\u80fd\u6709\u6548\u68c0\u6d4b\u4f2a\u9020\u6587\u672c\uff0c\u4f46\u65b0\u67b6\u6784\u7684\u6a21\u578b\u4f9d\u7136\u53ef\u80fd\u63d0\u5347\u6587\u672c\u7684\u6b3a\u9a97\u6027\u3002"}}
{"id": "2506.21285", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.21285", "abs": "https://arxiv.org/abs/2506.21285", "authors": ["Xin Xu", "Tianhao Chen", "Fan Zhang", "Wanlong Liu", "Pengxiang Li", "Ajay Kumar Jaiswal", "Yuchen Yan", "Jishan Hu", "Yang Wang", "Hao Chen", "Shiwei Liu", "Shizhe Diao", "Can Yang", "Lu Yin"], "title": "Double-Checker: Enhancing Reasoning of Slow-Thinking LLMs via Self-Critical Fine-Tuning", "comment": "10 pages", "summary": "While slow-thinking large language models (LLMs) exhibit reflection-like\nreasoning, commonly referred to as the \"aha moment:, their ability to generate\ninformative critiques and refine prior solutions remains limited. In this\npaper, we introduce Double-Checker, a principled framework designed to enhance\nthe reasoning capabilities of slow-thinking LLMs by fostering explicit\nself-critique and iterative refinement of their previous solutions. By\nfine-tuning on our curated 1,730 self-critical instances, Double-Checker\nempowers long-CoT LLMs to iteratively critique and refine their outputs during\ninference until they evaluate their solutions as correct under self-generated\ncritiques. We validate the efficacy of Double-Checker across a comprehensive\nsuite of reasoning benchmarks, demonstrating that iterative self-critique\nsignificantly enhances the reasoning capabilities of long-CoT LLMs. Notably,\nour Double-Checker increases the pass@1 performance on challenging AIME\nbenchmarks from 4.4% to 18.2% compared to the original long-CoT LLMs. These\nresults highlight a promising direction for developing more trustworthy and\neffective LLMs capable of structured self-critique.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDouble-Checker\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u6211\u6279\u8bc4\u4e0e\u8fed\u4ee3\u4fee\u6b63\u5927\u5e45\u63d0\u5347LLM\u63a8\u7406\u8868\u73b0\uff0c\u4e3a\u6784\u5efa\u66f4\u53ef\u9760\u9ad8\u6548\u7684\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u65b0\u601d\u8def\u3002", "motivation": "\u76ee\u524d\u6162\u901f\u601d\u8003\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u867d\u80fd\u5c55\u73b0\u7c7b\u4f3c\u201c\u987f\u609f\u201d\u5f0f\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5176\u751f\u6210\u6709\u4ef7\u503c\u81ea\u6211\u6279\u5224\u4e0e\u6539\u8fdb\u4e4b\u524d\u7b54\u6848\u7684\u80fd\u529b\u4ecd\u6709\u9650\u3002\u56e0\u6b64\uff0c\u5e0c\u671b\u901a\u8fc7\u6539\u8fdb\u673a\u5236\u63d0\u5347\u5176\u63a8\u7406\u4e0e\u81ea\u6211\u5b8c\u5584\u7684\u8868\u73b0\u3002", "method": "\u63d0\u51fa\u4e86Double-Checker\u6846\u67b6\uff0c\u901a\u8fc7\u7cbe\u5fc3\u7b5b\u9009\u76841730\u4e2a\u81ea\u6211\u6279\u5224\u5b9e\u4f8b\u5bf9\u957f\u94fe\u601d\u8003\uff08long-CoT\uff09\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u4f7f\u5176\u5728\u63a8\u7406\u63a8\u65ad\u8fc7\u7a0b\u4e2d\u80fd\u663e\u5f0f\u81ea\u6211\u6279\u5224\u4e0e\u591a\u8f6e\u8fed\u4ee3\u81ea\u6211\u4fee\u6b63\u7b54\u6848\uff0c\u76f4\u81f3\u8f93\u51fa\u88ab\u81ea\u8bc4\u4e3a\u6b63\u786e\u3002", "result": "\u5728\u591a\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDouble-Checker\u80fd\u663e\u8457\u63d0\u5347\u957f\u94fe\u63a8\u7406\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u5177\u6709\u6311\u6218\u6027\u7684AIME\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cpass@1\u6307\u6807\u75314.4%\u63d0\u5347\u523018.2%\u3002", "conclusion": "Double-Checker\u4f5c\u4e3a\u7ed3\u6784\u5316\u81ea\u6211\u6279\u5224\u673a\u5236\uff0c\u6709\u6548\u63d0\u5347\u4e86LLM\u63a8\u7406\u53ef\u4fe1\u5ea6\u548c\u80fd\u529b\uff0c\u662f\u5f00\u53d1\u66f4\u53ef\u4fe1\u8d56\u548c\u9ad8\u6548LLM\u7684\u4e00\u6761\u6709\u524d\u666f\u7684\u8def\u5f84\u3002"}}
{"id": "2506.21288", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.21288", "abs": "https://arxiv.org/abs/2506.21288", "authors": ["Istabrak Abbes", "Gabriele Prato", "Quentin Fournier", "Fernando Rodriguez", "Alaa Boukhary", "Adam Elwood", "Sarath Chandar"], "title": "Small Encoders Can Rival Large Decoders in Detecting Groundedness", "comment": null, "summary": "Augmenting large language models (LLMs) with external context significantly\nimproves their performance in natural language processing (NLP) tasks. However,\nLLMs struggle to answer queries reliably when the provided context lacks\ninformation, often resorting to ungrounded speculation or internal knowledge.\nGroundedness - generating responses strictly supported by the context - is\nessential for ensuring factual consistency and trustworthiness. This study\nfocuses on detecting whether a given query is grounded in a document provided\nin context before the costly answer generation by LLMs. Such a detection\nmechanism can significantly reduce both inference time and resource\nconsumption. We show that lightweight, task specific encoder models such as\nRoBERTa and NomicBERT, fine-tuned on curated datasets, can achieve accuracy\ncomparable to state-of-the-art LLMs, such as Llama3 8B and GPT4o, in\ngroundedness detection while reducing inference latency by orders of magnitude.\nThe code is available at : https://github.com/chandarlab/Hallucinate-less", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u7528\u7ecf\u8fc7\u5fae\u8c03\u7684RoBERTa\u3001NomicBERT\u7b49\u8f7b\u91cf\u7ea7\u6a21\u578b\u9884\u5148\u68c0\u6d4b\u67e5\u8be2\u662f\u5426\u57fa\u4e8e\u4e0a\u4e0b\u6587\uff0c\u80fd\u4ee5\u6781\u4f4e\u5ef6\u8fdf\u5b9e\u73b0\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7c7b\u4f3c\u7684\u51c6\u786e\u7387\uff0c\u663e\u8457\u51cf\u5c11\u7b97\u529b\u6d88\u8017\uff0c\u9002\u7528\u4e8e\u5927\u6a21\u578b\u63a8\u7406\u524d\u7684\u9ad8\u6548\u7b5b\u67e5\u3002", "motivation": "\u5c3d\u7ba1\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u63d0\u4f9b\u5916\u90e8\u4e0a\u4e0b\u6587\u80fd\u663e\u8457\u63d0\u5347\u5176\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u4f46\u5f53\u4e0a\u4e0b\u6587\u7f3a\u5c11\u5173\u952e\u4fe1\u606f\u65f6\uff0cLLMs \u4ecd\u5e38\u5e38\u4ea7\u751f\u8131\u79bb\u73b0\u5b9e\u7684\u731c\u6d4b\u6216\u4ec5\u51ed\u81ea\u8eab\u77e5\u8bc6\u4f5c\u7b54\u3002\u56e0\u6b64\uff0c\u68c0\u6d4b\u6a21\u578b\u56de\u7b54\u662f\u5426\u771f\u6b63\u57fa\u4e8e\u4e0a\u4e0b\u6587\uff08\u5373\u201c\u6839\u690d\u6027\u201d\uff09\u6210\u4e3a\u4e00\u4e2a\u4e9f\u5f85\u89e3\u51b3\u7684\u95ee\u9898\u3002", "method": "\u672c\u6587\u63d0\u51fa\u5728 LLMs \u751f\u6210\u7b54\u6848\u524d\uff0c\u5148\u5229\u7528\u8f7b\u91cf\u7ea7\u3001\u4efb\u52a1\u4e13\u7528\u7684\u7f16\u7801\u5668\u6a21\u578b\uff08\u5982 RoBERTa \u548c NomicBERT\uff0c\u7ecf\u7cbe\u7ec6\u5fae\u8c03\uff09\u68c0\u6d4b\u67e5\u8be2\u662f\u5426\u6839\u690d\u4e8e\u63d0\u4f9b\u7684\u6587\u6863\u4e0a\u4e0b\u6587\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u7ecf\u8fc7\u7cbe\u7ec6\u5fae\u8c03\u7684\u7f16\u7801\u5668\u6a21\u578b\u5728\u68c0\u6d4b\u6839\u690d\u6027\u4efb\u52a1\u4e0a\u80fd\u4e0e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08\u4f8b\u5982 Llama3 8B \u548c GPT4o\uff09\u8fbe\u5230\u76f8\u5f53\u7684\u51c6\u786e\u5ea6\uff0c\u4f46\u63a8\u7406\u5ef6\u8fdf\u5374\u5927\u5e45\u964d\u4f4e\u3002", "conclusion": "\u91c7\u7528\u8f7b\u91cf\u7ea7\u3001\u4e13\u7528\u7f16\u7801\u5668\u6a21\u578b\u8fdb\u884c\u6839\u690d\u6027\u68c0\u6d4b\uff0c\u4e0d\u4ec5\u53ef\u4ee5\u5927\u5e45\u63d0\u5347\u63a8\u7406\u6548\u7387\uff0c\u8fd8\u80fd\u5728\u4fdd\u8bc1\u68c0\u6d4b\u51c6\u786e\u6027\u7684\u540c\u65f6\uff0c\u8282\u7ea6\u8ba1\u7b97\u8d44\u6e90\u3002\u6b64\u65b9\u6cd5\u4e3a LLMs \u5728\u4f9d\u8d56\u5916\u90e8\u77e5\u8bc6\u8fdb\u884c\u95ee\u7b54\u65f6\u7684\u53ef\u9760\u6027\u548c\u53ef\u7528\u6027\u63d0\u4f9b\u4e86\u6709\u6548\u4fdd\u969c\u3002"}}
{"id": "2506.21294", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.21294", "abs": "https://arxiv.org/abs/2506.21294", "authors": ["Bram Willemsen", "Gabriel Skantze"], "title": "Detecting Referring Expressions in Visually Grounded Dialogue with Autoregressive Language Models", "comment": "Accepted for publication at XLLM @ ACL 2025", "summary": "In this paper, we explore the use of a text-only, autoregressive language\nmodeling approach for the extraction of referring expressions from visually\ngrounded dialogue. More specifically, the aim is to investigate the extent to\nwhich the linguistic context alone can inform the detection of mentions that\nhave a (visually perceivable) referent in the visual context of the\nconversation. To this end, we adapt a pretrained large language model (LLM) to\nperform a relatively course-grained annotation of mention spans in unfolding\nconversations by demarcating mention span boundaries in text via next-token\nprediction. Our findings indicate that even when using a moderately sized LLM,\nrelatively small datasets, and parameter-efficient fine-tuning, a text-only\napproach can be effective, highlighting the relative importance of the\nlinguistic context for this task. Nevertheless, we argue that the task\nrepresents an inherently multimodal problem and discuss limitations fundamental\nto unimodal approaches.", "AI": {"tldr": "\u672c\u6587\u8868\u660e\uff0c\u7eaf\u6587\u672cLLM\u65b9\u6cd5\u80fd\u6709\u6548\u62bd\u53d6\u89c6\u89c9\u8bed\u5883\u5bf9\u8bdd\u4e2d\u7684\u6307\u4ee3\u8868\u8fbe\uff0c\u4f46\u7531\u4e8e\u4efb\u52a1\u7684\u591a\u6a21\u6001\u672c\u8d28\uff0c\u4ecd\u6709\u4e0d\u53ef\u903e\u8d8a\u7684\u9650\u5236\u3002", "motivation": "\u7814\u7a76\u7eaf\u6587\u672c\u81ea\u56de\u5f52\u8bed\u8a00\u6a21\u578b\u5728\u4ece\u89c6\u89c9\u8bed\u5883\u5bf9\u8bdd\u4e2d\u62bd\u53d6\u6307\u4ee3\u8868\u8fbe\u7684\u6709\u6548\u6027\uff0c\u63a2\u7a76\u4ec5\u9760\u8bed\u8a00\u4e0a\u4e0b\u6587\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u7684\u4f5c\u7528\u3002", "method": "\u5c06\u9884\u8bad\u7ec3\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u5fae\u8c03\u5e94\u7528\u4e8e\u5bf9\u8bdd\u4e2d\u7684\u6307\u4ee3\u8868\u8fbe\u62bd\u53d6\uff0c\u4ec5\u4f9d\u8d56\u6587\u672c\u4fe1\u606f\uff0c\u901a\u8fc7\u4e0b\u4e00\u4e2atoken\u9884\u6d4b\u65b9\u6cd5\u5212\u5b9a\u63d0\u53ca\u8fb9\u754c\u3002", "result": "\u4e2d\u7b49\u89c4\u6a21LLM\u3001\u8f83\u5c0f\u6570\u636e\u96c6\u548c\u9ad8\u6548\u53c2\u6570\u5fae\u8c03\u4e0b\uff0c\u7eaf\u6587\u672c\u65b9\u6cd5\u80fd\u53d6\u5f97\u8f83\u597d\u6210\u6548\uff0c\u8bf4\u660e\u8bed\u8a00\u4e0a\u4e0b\u6587\u5bf9\u8be5\u4efb\u52a1\u975e\u5e38\u91cd\u8981\u3002", "conclusion": "\u4ec5\u4f7f\u7528\u6587\u672c\u548c\u81ea\u56de\u5f52\u8bed\u8a00\u6a21\u578b\u4e5f\u80fd\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u6709\u6548\u63d0\u53d6\u5bf9\u8bdd\u4e2d\u7684\u6307\u4ee3\u8868\u8fbe\uff0c\u4f46\u8be5\u4efb\u52a1\u672c\u8d28\u4e0a\u4ecd\u662f\u591a\u6a21\u6001\u95ee\u9898\uff0c\u5355\u4e00\u6a21\u6001\u6709\u5176\u5c40\u9650\u6027\u3002"}}
{"id": "2506.21360", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.21360", "abs": "https://arxiv.org/abs/2506.21360", "authors": ["Fangzhou Dong", "Yifan Zeng", "Yingpeng Sang", "Hong Shen"], "title": "Structuralist Approach to AI Literary Criticism: Leveraging Greimas Semiotic Square for Large Language Models", "comment": "Accepted in CogSci 2025", "summary": "Large Language Models (LLMs) excel in understanding and generating text but\nstruggle with providing professional literary criticism for works with profound\nthoughts and complex narratives. This paper proposes GLASS (Greimas Literary\nAnalysis via Semiotic Square), a structured analytical framework based on\nGreimas Semiotic Square (GSS), to enhance LLMs' ability to conduct in-depth\nliterary analysis. GLASS facilitates the rapid dissection of narrative\nstructures and deep meanings in narrative works. We propose the first dataset\nfor GSS-based literary criticism, featuring detailed analyses of 48 works. Then\nwe propose quantitative metrics for GSS-based literary criticism using the\nLLM-as-a-judge paradigm. Our framework's results, compared with expert\ncriticism across multiple works and LLMs, show high performance. Finally, we\napplied GLASS to 39 classic works, producing original and high-quality analyses\nthat address existing research gaps. This research provides an AI-based tool\nfor literary research and education, offering insights into the cognitive\nmechanisms underlying literary engagement.", "AI": {"tldr": "\u63d0\u51faGLASS\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u65b9\u6cd5\u8ba9\u5927\u6a21\u578b\u4e5f\u80fd\u505a\u51fa\u9ad8\u8d28\u91cf\u6587\u5b66\u6279\u8bc4\uff0c\u5e76\u516c\u5f00\u9996\u4e2a\u76f8\u5173\u6570\u636e\u96c6\uff0c\u8bc4\u6d4b\u6548\u679c\u4f18\u4e8e\u4ee5\u5f80\u65b9\u6cd5\uff0c\u5bf9\u5b66\u754c\u548c\u6559\u5b66\u5177\u6709\u5e94\u7528\u4ef7\u503c\u3002", "motivation": "\u5f53\u524d\u5927\u6a21\u578b\u867d\u80fd\u7406\u89e3\u548c\u751f\u6210\u6587\u672c\uff0c\u4f46\u96be\u4ee5\u5bf9\u601d\u60f3\u6df1\u9083\u3001\u53d9\u4e8b\u590d\u6742\u7684\u6587\u5b66\u4f5c\u54c1\u4f5c\u51fa\u4e13\u4e1a\u6279\u8bc4\u5206\u6790\u3002\u4e3a\u63d0\u5347LLM\u5728\u6df1\u5c42\u6587\u5b66\u6279\u8bc4\u573a\u666f\u7684\u80fd\u529b\uff0c\u4f5c\u8005\u5e0c\u671b\u5f15\u5165\u7ed3\u6784\u5316\u5206\u6790\u5de5\u5177\u548c\u6570\u636e\u96c6\u3002", "method": "1. \u57fa\u4e8e\u683c\u96f7\u9a6c\u65af\u7b26\u53f7\u5b66\u65b9\u9635\uff08GSS\uff09\u63d0\u51faGLASS\u7ed3\u6784\u5316\u5206\u6790\u6846\u67b6\uff1b2. \u6784\u5efa\u4e86\u9996\u4e2aGSS\u6587\u5b66\u6279\u8bc4\u6570\u636e\u96c6\uff08\u5305\u542b48\u90e8\u4f5c\u54c1\u7684\u8be6\u7ec6\u5206\u6790\uff09\uff1b3. \u63d0\u51fa\u57fa\u4e8eLLM-as-a-judge\u8303\u5f0f\u7684\u91cf\u5316\u8bc4\u6d4b\u6307\u6807\uff1b4. \u7528GLASS\u5206\u679039\u90e8\u7ecf\u5178\u4f5c\u54c1\uff0c\u5e76\u4e0e\u4e13\u5bb6\u6279\u8bc4\u7ed3\u679c\u8fdb\u884c\u5bf9\u6bd4\u8bc4\u4f30\u3002", "result": "GLASS\u80fd\u5e2e\u52a9LLM\u5feb\u901f\u5256\u6790\u53d9\u4e8b\u7ed3\u6784\u4e0e\u6df1\u5c42\u610f\u4e49\uff0c\u5206\u6790\u7ed3\u679c\u4e0e\u4e13\u5bb6\u6c34\u5e73\u63a5\u8fd1\uff0c\u5927\u5927\u5f25\u8865\u4e86\u73b0\u6709LLM\u5728\u6587\u5b66\u6279\u8bc4\u9886\u57df\u7684\u4e0d\u8db3\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684GLASS\u6846\u67b6\u80fd\u663e\u8457\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6587\u5b66\u6279\u8bc4\u4e2d\u7684\u8868\u73b0\uff0c\u80fd\u591f\u751f\u6210\u4e0e\u4e13\u5bb6\u76f8\u5ab2\u7f8e\u7684\u9ad8\u8d28\u91cf\u5206\u6790\uff0c\u5e76\u4e3a\u6587\u5b66\u7814\u7a76\u548c\u6559\u80b2\u63d0\u4f9b\u4e86\u5f3a\u6709\u529b\u7684AI\u5de5\u5177\u3002"}}
{"id": "2506.21384", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2506.21384", "abs": "https://arxiv.org/abs/2506.21384", "authors": ["Guanting Dong", "Xiaoxi Li", "Yuyao Zhang", "Mengjie Deng"], "title": "Leveraging LLM-Assisted Query Understanding for Live Retrieval-Augmented Generation", "comment": "Accepted at SIGIR 2025 LiveRAG Workshop (Oral Presentation)", "summary": "Real-world live retrieval-augmented generation (RAG) systems face significant\nchallenges when processing user queries that are often noisy, ambiguous, and\ncontain multiple intents. While RAG enhances large language models (LLMs) with\nexternal knowledge, current systems typically struggle with such complex\ninputs, as they are often trained or evaluated on cleaner data. This paper\nintroduces Omni-RAG, a novel framework designed to improve the robustness and\neffectiveness of RAG systems in live, open-domain settings. Omni-RAG employs\nLLM-assisted query understanding to preprocess user inputs through three key\nmodules: (1) Deep Query Understanding and Decomposition, which utilizes LLMs\nwith tailored prompts to denoise queries (e.g., correcting spelling errors) and\ndecompose multi-intent queries into structured sub-queries; (2) Intent-Aware\nKnowledge Retrieval, which performs retrieval for each sub-query from a corpus\n(i.e., FineWeb using OpenSearch) and aggregates the results; and (3) Reranking\nand Generation, where a reranker (i.e., BGE) refines document selection before\na final response is generated by an LLM (i.e., Falcon-10B) using a\nchain-of-thought prompt. Omni-RAG aims to bridge the gap between current RAG\ncapabilities and the demands of real-world applications, such as those\nhighlighted by the SIGIR 2025 LiveRAG Challenge, by robustly handling complex\nand noisy queries.", "AI": {"tldr": "Omni-RAG\u662f\u4e00\u79cd\u65b0\u578bRAG\u7cfb\u7edf\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u5927\u6a21\u5757\u5f3a\u5316\u5bf9\u590d\u6742\u3001\u542b\u566a\u3001\u591a\u610f\u56fe\u67e5\u8be2\u7684\u5904\u7406\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5b9e\u9645\u5e94\u7528\u573a\u666f\u4e0b\u7684\u9c81\u68d2\u6027\u4e0e\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u7684RAG\uff08Retrieval-Augmented Generation\uff09\u7cfb\u7edf\u5728\u5e94\u5bf9\u771f\u5b9e\u4e16\u754c\u4e2d\u7684\u7528\u6237\u67e5\u8be2\u65f6\u9762\u4e34\u56f0\u96be\uff0c\u5c24\u5176\u662f\u5bf9\u4e8e\u566a\u58f0\u5927\u3001\u6b67\u4e49\u91cd\u548c\u591a\u610f\u56fe\u7684\u590d\u6742\u67e5\u8be2\u3002\u8fd9\u4e9b\u7cfb\u7edf\u901a\u5e38\u53ea\u5728\u5e72\u51c0\u7684\u6570\u636e\u4e0a\u8bad\u7ec3\u6216\u8bc4\u4f30\uff0c\u96be\u4ee5\u6ee1\u8db3\u5b9e\u9645\u5e94\u7528\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u4e86Omni-RAG\u6846\u67b6\uff0c\u901a\u8fc7LLM\u8f85\u52a9\u7684\u67e5\u8be2\u7406\u89e3\u6a21\u5757\u9884\u5904\u7406\u7528\u6237\u8f93\u5165\uff0c\u5305\u62ec\uff1a1. \u6df1\u5ea6\u67e5\u8be2\u7406\u89e3\u4e0e\u5206\u89e3\uff0c\u5229\u7528LLM\u548c\u5b9a\u5236\u5316\u63d0\u793a\u7ea0\u6b63\u62fc\u5199\u7b49\u566a\u58f0\uff0c\u5e76\u5c06\u591a\u610f\u56fe\u67e5\u8be2\u62c6\u89e3\u4e3a\u7ed3\u6784\u5316\u5b50\u67e5\u8be2\uff1b2. \u57fa\u4e8e\u610f\u56fe\u7684\u77e5\u8bc6\u68c0\u7d22\uff0c\u5bf9\u6bcf\u4e2a\u5b50\u67e5\u8be2\u6267\u884c\u68c0\u7d22\u5e76\u805a\u5408\u7ed3\u679c\uff08\u4f7f\u7528FineWeb\u4e0eOpenSearch\uff09\uff1b3. \u91cd\u6392\u5e8f\u4e0e\u751f\u6210\uff0c\u5148\u7531\u91cd\u6392\u5e8f\u5668BGE\u4f18\u5316\u7ed3\u679c\uff0c\u518d\u7531Falcon-10B\u5927\u6a21\u578b\u901a\u8fc7\u94fe\u5f0f\u601d\u7ef4\u63d0\u793a\u751f\u6210\u6700\u7ec8\u56de\u590d\u3002", "result": "Omni-RAG\u663e\u8457\u63d0\u5347\u4e86RAG\u7cfb\u7edf\u5728\u73b0\u5b9e\u5f00\u653e\u57df\u573a\u666f\u4e2d\u5904\u7406\u590d\u6742\u3001\u591a\u566a\u58f0\u7528\u6237\u67e5\u8be2\u7684\u9c81\u68d2\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "Omni-RAG\u6709\u6548\u5f25\u8865\u4e86\u5f53\u524dRAG\u7cfb\u7edf\u4e0e\u73b0\u5b9e\u5e94\u7528\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u7279\u522b\u662f\u5728\u5e94\u5bf9\u590d\u6742\u3001\u542b\u566a\u3001\u591a\u610f\u56fe\u67e5\u8be2\u65b9\u9762\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u80fd\u529b\uff0c\u9002\u7528\u4e8eSIGIR 2025 LiveRAG Challenge\u7b49\u5b9e\u9645\u9700\u6c42\u573a\u666f\u3002"}}
{"id": "2506.21443", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.21443", "abs": "https://arxiv.org/abs/2506.21443", "authors": ["Ali \u015eenol", "Garima Agrawal", "Huan Liu"], "title": "Domain Knowledge-Enhanced LLMs for Fraud and Concept Drift Detection", "comment": null, "summary": "Detecting deceptive conversations on dynamic platforms is increasingly\ndifficult due to evolving language patterns and Concept Drift (CD)-i.e.,\nsemantic or topical shifts that alter the context or intent of interactions\nover time. These shifts can obscure malicious intent or mimic normal dialogue,\nmaking accurate classification challenging. While Large Language Models (LLMs)\nshow strong performance in natural language tasks, they often struggle with\ncontextual ambiguity and hallucinations in risk-sensitive scenarios. To address\nthese challenges, we present a Domain Knowledge (DK)-Enhanced LLM framework\nthat integrates pretrained LLMs with structured, task-specific insights to\nperform fraud and concept drift detection. The proposed architecture consists\nof three main components: (1) a DK-LLM module to detect fake or deceptive\nconversations; (2) a drift detection unit (OCDD) to determine whether a\nsemantic shift has occurred; and (3) a second DK-LLM module to classify the\ndrift as either benign or fraudulent. We first validate the value of domain\nknowledge using a fake review dataset and then apply our full framework to\nSEConvo, a multiturn dialogue dataset that includes various types of fraud and\nspam attacks. Results show that our system detects fake conversations with high\naccuracy and effectively classifies the nature of drift. Guided by structured\nprompts, the LLaMA-based implementation achieves 98% classification accuracy.\nComparative studies against zero-shot baselines demonstrate that incorporating\ndomain knowledge and drift awareness significantly improves performance,\ninterpretability, and robustness in high-stakes NLP applications.", "AI": {"tldr": "\u8be5\u6587\u63d0\u51fa\u7ed3\u5408\u9886\u57df\u77e5\u8bc6\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u65b0\u65b9\u6cd5\uff0c\u5206\u4e09\u4e2a\u6a21\u5757\u68c0\u6d4b\u548c\u5206\u7c7b\u52a8\u6001\u5bf9\u8bdd\u4e2d\u7684\u6b3a\u8bc8\u53ca\u6982\u5ff5\u6f02\u79fb\u3002\u5b9e\u9a8c\u8bc1\u660e\u65b0\u65b9\u6cd5\u660e\u663e\u4f18\u4e8e\u4f20\u7edf\u96f6\u6837\u672c\u57fa\u7ebf\uff0c\u5206\u7c7b\u51c6\u786e\u7387\u9ad8\u8fbe98%\uff0c\u7279\u522b\u9002\u5408\u9ad8\u98ce\u9669NLP\u4efb\u52a1\u3002", "motivation": "\u5728\u52a8\u6001\u5e73\u53f0\u4e0a\u68c0\u6d4b\u6b3a\u9a97\u6027\u5bf9\u8bdd\u53d8\u5f97\u8d8a\u6765\u8d8a\u56f0\u96be\uff0c\u4e3b\u8981\u7531\u4e8e\u8bed\u8a00\u6a21\u5f0f\u7684\u4e0d\u65ad\u6f14\u5316\u548c\u6982\u5ff5\u6f02\u79fb\uff08\u5373\u8bed\u4e49\u6216\u4e3b\u9898\u968f\u65f6\u95f4\u53d8\u5316\uff09\uff0c\u8fd9\u4e9b\u53d8\u5316\u53ef\u80fd\u63a9\u76d6\u6076\u610f\u884c\u4e3a\u6216\u6a21\u4eff\u6b63\u5e38\u5bf9\u8bdd\uff0c\u4ece\u800c\u8ba9\u5206\u7c7b\u66f4\u52a0\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u9886\u57df\u77e5\u8bc6\uff08DK\uff09\u589e\u5f3a\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6846\u67b6\uff0c\u5c06\u9884\u8bad\u7ec3LLM\u548c\u7ed3\u6784\u5316\u3001\u4efb\u52a1\u76f8\u5173\u7684\u9886\u57df\u77e5\u8bc6\u76f8\u7ed3\u5408\uff0c\u7528\u4e8e\u6b3a\u8bc8\u68c0\u6d4b\u548c\u6982\u5ff5\u6f02\u79fb\u68c0\u6d4b\u3002\u6846\u67b6\u5305\u62ec\u4e09\u4e2a\u4e3b\u8981\u7ec4\u6210\u90e8\u5206\uff1a\uff081\uff09DK-LLM\u6a21\u5757\u7528\u4e8e\u68c0\u6d4b\u865a\u5047\u6216\u6b3a\u9a97\u6027\u5bf9\u8bdd\uff1b\uff082\uff09\u6f02\u79fb\u68c0\u6d4b\u5355\u5143\uff08OCDD\uff09\u5224\u65ad\u662f\u5426\u53d1\u751f\u8bed\u4e49\u6f02\u79fb\uff1b\uff083\uff09\u7b2c\u4e8c\u4e2aDK-LLM\u6a21\u5757\u7528\u4e8e\u5c06\u6f02\u79fb\u5206\u7c7b\u4e3a\u826f\u6027\u6216\u6b3a\u8bc8\u3002", "result": "\u7cfb\u7edf\u80fd\u591f\u9ad8\u7cbe\u5ea6\u5730\u68c0\u6d4b\u865a\u5047\u5bf9\u8bdd\uff0c\u6709\u6548\u5206\u7c7b\u6f02\u79fb\u7684\u6027\u8d28\u3002LLaMA\u5b9e\u73b0\u7248\u672c\u5728\u5206\u7c7b\u7cbe\u5ea6\u4e0a\u8fbe\u523098%\u3002\u4e0e\u96f6\u6837\u672c\u57fa\u7ebf\u6bd4\u8f83\uff0c\u57df\u77e5\u8bc6\u548c\u6f02\u79fb\u611f\u77e5\u7684\u5f15\u5165\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u6027\u80fd\u3001\u53ef\u89e3\u91ca\u6027\u548c\u7a33\u5065\u6027\u3002", "conclusion": "\u5c06\u9886\u57df\u77e5\u8bc6\u548c\u8bed\u4e49\u6f02\u79fb\u68c0\u6d4b\u878d\u5408\u8fdbLLM\uff0c\u6709\u52a9\u4e8e\u63d0\u9ad8\u68c0\u6d4b\u5404\u7c7b\u6b3a\u9a97\u3001\u6f02\u79fb\u884c\u4e3a\u7684\u51c6\u786e\u6027\u548c\u7a33\u5065\u6027\uff0c\u7279\u522b\u9002\u7528\u4e8e\u9ad8\u98ce\u9669\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u3002"}}
{"id": "2506.21445", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2506.21445", "abs": "https://arxiv.org/abs/2506.21445", "authors": ["Makbule Gulcin Ozsoy", "William Tai"], "title": "Text2Cypher Across Languages: Evaluating Foundational Models Beyond English", "comment": null, "summary": "Recent advances in large language models have enabled natural language\ninterfaces that translate user questions into database queries, such as\nText2SQL, Text2SPARQL, and Text2Cypher. While these interfaces enhance database\naccessibility, most research today focuses solely on English, with limited\nevaluation in other languages. This paper investigates the performance of\nfoundational LLMs on the Text2Cypher task across multiple languages. We create\nand release a multilingual test set by translating English questions into\nSpanish and Turkish while preserving the original Cypher queries, enabling fair\ncross-lingual comparison. We evaluate multiple foundational models using\nstandardized prompts and metrics. Our results show a consistent performance\npattern: highest on English, then Spanish, and lowest on Turkish. We attribute\nthis to differences in training data availability and linguistic\ncharacteristics. Additionally, we explore the impact of translating task\nprompts into Spanish and Turkish. Results show little to no change in\nevaluation metrics, suggesting prompt translation has minor impact. Our\nfindings highlight the need for more inclusive evaluation and development in\nmultilingual query generation. Future work includes schema localization and\nfine-tuning across diverse languages.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u8bc4\u4f30\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728Text2Cypher\u6570\u636e\u5e93\u67e5\u8be2\u751f\u6210\u4efb\u52a1\u4e0b\u7684\u591a\u8bed\u8a00\u8868\u73b0\uff0c\u53d1\u73b0\u6a21\u578b\u5728\u82f1\u8bed\u3001\u897f\u73ed\u7259\u8bed\u3001\u571f\u8033\u5176\u8bed\u4f9d\u6b21\u4e0b\u964d\uff0c\u63d0\u793a\u8bcd\u672c\u5730\u5316\u4f5c\u7528\u751a\u5fae\uff0c\u5f3a\u8c03\u5e94\u63a8\u52a8\u591a\u8bed\u79cd\u6570\u636e\u5e93\u68c0\u7d22\u80fd\u529b\u7684\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u7684\u81ea\u7136\u8bed\u8a00\u5230\u6570\u636e\u5e93\u67e5\u8be2\u7684\u63a5\u53e3\uff08\u5982Text2SQL\u3001Text2SPARQL\u3001Text2Cypher\uff09\u6781\u5927\u63d0\u5347\u4e86\u6570\u636e\u5e93\u7684\u53ef\u8bbf\u95ee\u6027\uff0c\u4f46\u5927\u591a\u53ea\u5173\u6ce8\u82f1\u8bed\uff0c\u800c\u5176\u4ed6\u8bed\u8a00\u7684\u6548\u679c\u9c9c\u6709\u8bc4\u4f30\u3002\u56e0\u6b64\u9700\u8981\u66f4\u5168\u9762\u8bc4\u4f30\u591a\u8bed\u8a00\u4e0b\u6a21\u578b\u6027\u80fd\uff0c\u4fc3\u8fdb\u6570\u636e\u5e93\u68c0\u7d22\u65b9\u5f0f\u7684\u591a\u8bed\u79cd\u666e\u53ca\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86\u4e00\u4e2a\u591a\u8bed\u8a00\u7684\u6d4b\u8bd5\u96c6\uff0c\u5c06\u82f1\u6587\u95ee\u9898\u5206\u522b\u7ffb\u8bd1\u4e3a\u897f\u73ed\u7259\u8bed\u548c\u571f\u8033\u5176\u8bed\uff0c\u540c\u65f6\u4fdd\u6301\u539f\u59cbCypher\u67e5\u8be2\u4e0d\u53d8\u3002\u7136\u540e\u7528\u7edf\u4e00\u7684\u63d0\u793a\u8bcd\u548c\u8bc4\u6d4b\u6807\u51c6\uff0c\u7cfb\u7edf\u5730\u8bc4\u4f30\u4e86\u591a\u79cd\u4e3b\u6d41\u5927\u8bed\u8a00\u6a21\u578b\u5728Text2Cypher\u4efb\u52a1\u4e0b\u7684\u591a\u8bed\u8a00\u6027\u80fd\uff0c\u5e76\u63a2\u8ba8\u4e86\u5c06\u63d0\u793a\u8bcd\u672c\u5730\u5316\u7ffb\u8bd1\uff08\u897f\u73ed\u7259\u8bed\u3001\u571f\u8033\u5176\u8bed\uff09\u5bf9\u6a21\u578b\u8868\u73b0\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u6a21\u578b\u5bf9\u82f1\u6587\u8868\u73b0\u6700\u4f73\uff0c\u5bf9\u897f\u73ed\u7259\u8bed\u6b21\u4e4b\uff0c\u5bf9\u571f\u8033\u5176\u8bed\u6700\u5dee\u3002\u8fd9\u4e3b\u8981\u5f52\u56e0\u4e3a\u8bad\u7ec3\u6570\u636e\u91cf\u548c\u8bed\u8a00\u672c\u8eab\u7279\u6027\u7684\u5dee\u5f02\u3002\u540c\u65f6\uff0c\u63d0\u793a\u8bcd\u7ffb\u8bd1\u51e0\u4e4e\u4e0d\u5f71\u54cd\u6700\u7ec8\u6548\u679c\u3002\u4f5c\u8005\u7531\u6b64\u63d0\u51fa\u9700\u8981\u66f4\u5177\u5305\u5bb9\u6027\u7684\u591a\u8bed\u79cd\u6570\u636e\u5e93\u68c0\u7d22\u7814\u7a76\uff0c\u672a\u6765\u53ef\u63a2\u7d22\u6a21\u5f0f\u672c\u5730\u5316\u4e0e\u591a\u8bed\u79cd\u5fae\u8c03\u3002", "conclusion": "\u591a\u8bed\u8a00\u73af\u5883\u4e0b\uff0c\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6570\u636e\u5e93\u67e5\u8be2\u751f\u6210\u4e2d\u7684\u8868\u73b0\u5b58\u5728\u660e\u663e\u201c\u82f1\u8bed\u4f18\u5148\u201d\u73b0\u8c61\uff0c\u800c\u63d0\u793a\u8bcd\u672c\u5730\u5316\u7ffb\u8bd1\u5f71\u54cd\u751a\u5fae\u3002\u547c\u5401\u672a\u6765\u7814\u7a76\u52a0\u5f3a\u591a\u8bed\u79cd\u8bc4\u4f30\u548c\u6280\u672f\u5f00\u53d1\uff0c\u7279\u522b\u5173\u6ce8\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u5e94\u7528\u62d3\u5c55\u3002"}}
{"id": "2506.21463", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.21463", "abs": "https://arxiv.org/abs/2506.21463", "authors": ["Anne Wu", "Laurent Mazar\u00e9", "Neil Zeghidour", "Alexandre D\u00e9fossez"], "title": "Aligning Spoken Dialogue Models from User Interactions", "comment": "Accepted at ICML 2025", "summary": "We propose a novel preference alignment framework for improving spoken\ndialogue models on real-time conversations from user interactions. Current\npreference learning methods primarily focus on text-based language models, and\nare not directly suited to the complexities of real-time speech interactions,\nwith richer dynamics (e.g. interruption, interjection) and no explicit\nsegmentation between speaker turns.We create a large-scale dataset of more than\n150,000 preference pairs from raw multi-turn speech conversations, annotated\nwith AI feedback, to cover preferences over both linguistic content and\ntemporal context variations. We leverage offline alignment methods to finetune\na full-duplex autoregressive speech-to-speech model. Extensive experiments\ndemonstrate that feedback on generic conversations can be consistently\neffective in improving spoken dialogue models to produce more factual, safer\nand more contextually aligned interactions. We deploy the finetuned model and\nconduct holistic human evaluations to assess the impact beyond single-turn\nconversations. Our findings shed light on the importance of a well-calibrated\nbalance among various dynamics, crucial for natural real-time speech dialogue\nsystems.", "AI": {"tldr": "\u5f15\u5165\u5927\u89c4\u6a21\u504f\u597d\u6570\u636e\u548cAI\u53cd\u9988\uff0c\u63d0\u51fa\u5e76\u5b9e\u8bc1\u4e86\u4e00\u79cd\u9002\u7528\u4e8e\u5b9e\u65f6\u8bed\u97f3\u5bf9\u8bdd\u7684\u504f\u597d\u5bf9\u9f50\u65b0\u6846\u67b6\uff0c\u6709\u6548\u63d0\u5347\u7cfb\u7edf\u771f\u5b9e\u6027\u3001\u5b89\u5168\u6027\u548c\u4e0a\u4e0b\u6587\u4f53\u9a8c\u3002", "motivation": "\u73b0\u6709\u504f\u597d\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u6587\u672c\u8bed\u8a00\u6a21\u578b\uff0c\u96be\u4ee5\u5904\u7406\u5177\u6709\u4e30\u5bcc\u52a8\u6001\u548c\u590d\u6742\u65f6\u5e8f\u7684\u5b9e\u65f6\u8bed\u97f3\u4ea4\u4e92\u573a\u666f\uff08\u5982\u6253\u65ad\u3001\u63d2\u8bdd\u3001\u7f3a\u4e4f\u660e\u786e\u8bf4\u8bdd\u8f6e\u6b21\u5206\u5272\uff09\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u63d0\u5347\u8bed\u97f3\u5bf9\u8bdd\u7cfb\u7edf\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6027\u80fd\u3002", "method": "1\uff09\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b15\u4e07\u5bf9\u504f\u597d\u914d\u5bf9\u7684\u5927\u89c4\u6a21\u591a\u8f6e\u8bed\u97f3\u5bf9\u8bdd\u6570\u636e\u96c6\uff0c\u7531AI\u53cd\u9988\u6807\u6ce8\u504f\u597d\uff0c\u6db5\u76d6\u8bed\u8a00\u5185\u5bb9\u548c\u65f6\u5e8f\u4e0a\u4e0b\u6587\u53d8\u5316\uff1b2\uff09\u91c7\u7528\u79bb\u7ebf\u5bf9\u9f50\u65b9\u6cd5\uff0c\u5728\u5168\u53cc\u5de5\u81ea\u56de\u5f52\u8bed\u97f3-\u8bed\u97f3\u6a21\u578b\u4e0a\u8fdb\u884c\u5fae\u8c03\uff1b3\uff09\u901a\u8fc7\u4eff\u771f\u5b9e\u9a8c\u548c\u5b9e\u9645\u90e8\u7f72\u7684\u4eba\u5de5\u8bc4\u6d4b\uff0c\u5168\u9762\u8bc4\u4f30\u65b9\u6cd5\u6548\u679c\u3002", "result": "\u65b0\u504f\u597d\u5bf9\u9f50\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u8bed\u97f3\u5bf9\u8bdd\u7cfb\u7edf\u591a\u8f6e\u4ea4\u4e92\u7684\u771f\u5b9e\u6027\u3001\u5b89\u5168\u6027\u548c\u4e0a\u4e0b\u6587\u5951\u5408\u5ea6\uff0c\u4eba\u5de5\u8bc4\u6d4b\u4e5f\u9a8c\u8bc1\u4e86\u6a21\u578b\u5bf9\u81ea\u7136\u8bed\u97f3\u5bf9\u8bdd\u7684\u52a0\u5f3a\u3002", "conclusion": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u504f\u597d\u5bf9\u9f50\u6846\u67b6\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u8bed\u97f3\u5bf9\u8bdd\u6a21\u578b\u5728\u5b9e\u65f6\u5bf9\u8bdd\u4e2d\u7684\u8868\u73b0\uff0c\u7efc\u5408\u63d0\u5347\u6a21\u578b\u7684\u771f\u5b9e\u6027\u3001\u5b89\u5168\u6027\u548c\u4e0a\u4e0b\u6587\u5951\u5408\u5ea6\u3002\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u591a\u8f6e\u5bf9\u8bdd\u548c\u5b9e\u9645\u5e94\u7528\u573a\u666f\u4e2d\u5747\u53d6\u5f97\u4e86\u79ef\u6781\u6210\u6548\u3002"}}
{"id": "2506.21468", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.21468", "abs": "https://arxiv.org/abs/2506.21468", "authors": ["Ryosuke Takahashi", "Tatsuro Inaba", "Kentaro Inui", "Benjamin Heinzerling"], "title": "TopK Language Models", "comment": null, "summary": "Sparse autoencoders (SAEs) have become an important tool for analyzing and\ninterpreting the activation space of transformer-based language models (LMs).\nHowever, SAEs suffer several shortcomings that diminish their utility and\ninternal validity. Since SAEs are trained post-hoc, it is unclear if the\nfailure to discover a particular concept is a failure on the SAE's side or due\nto the underlying LM not representing this concept. This problem is exacerbated\nby training conditions and architecture choices affecting which features an SAE\nlearns. When tracing how LMs learn concepts during training, the lack of\nfeature stability also makes it difficult to compare SAEs features across\ndifferent checkpoints. To address these limitations, we introduce a\nmodification to the transformer architecture that incorporates a TopK\nactivation function at chosen layers, making the model's hidden states\nequivalent to the latent features of a TopK SAE. This approach eliminates the\nneed for post-hoc training while providing interpretability comparable to SAEs.\nThe resulting TopK LMs offer a favorable trade-off between model size,\ncomputational efficiency, and interpretability. Despite this simple\narchitectural change, TopK LMs maintain their original capabilities while\nproviding robust interpretability benefits. Our experiments demonstrate that\nthe sparse representations learned by TopK LMs enable successful steering\nthrough targeted neuron interventions and facilitate detailed analysis of\nneuron formation processes across checkpoints and layers. These features make\nTopK LMs stable and reliable tools for understanding how language models learn\nand represent concepts, which we believe will significantly advance future\nresearch on model interpretability and controllability.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9SAE\u5b58\u5728\u7684\u7a33\u5b9a\u6027\u4e0e\u53ef\u7528\u6027\u95ee\u9898\uff0c\u63d0\u51fa\u5c06TopK\u6fc0\u6d3b\u51fd\u6570\u76f4\u63a5\u6574\u5408\u8fdbtransformer\uff0c\u514d\u53bb\u4e86\u540e\u7eed\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u5e72\u9884\u53ef\u63a7\u6027\uff0c\u5e76\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u7a33\u5b9a\u7684\u7a00\u758f\u8868\u793a\u3002\u8fd9\u5c06\u52a9\u529b\u672a\u6765LM\u89e3\u91ca\u6027\u4e0e\u53ef\u63a7\u6027\u7814\u7a76\u3002", "motivation": "\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAEs\uff09\u5728\u5206\u6790\u548c\u89e3\u91ca\u57fa\u4e8etransformer\u7684\u8bed\u8a00\u6a21\u578b\u6fc0\u6d3b\u7a7a\u95f4\u65f6\u5177\u6709\u91cd\u8981\u4f5c\u7528\uff0c\u4f46\u5b83\u4eec\u5728\u53ef\u7528\u6027\u548c\u5185\u90e8\u6709\u6548\u6027\u4e0a\u5b58\u5728\u8bf8\u591a\u4e0d\u8db3\uff1a\u5982SAE\u662f\u5426\u672a\u80fd\u53d1\u73b0\u67d0\u4e00\u6982\u5ff5\u7a76\u7adf\u56e0\u81ea\u8eab\u80fd\u529b\u9650\u5236\u8fd8\u662f\u56e0\u5e95\u5c42\u6a21\u578b\u672c\u5c31\u672a\u8868\u5f81\u8be5\u6982\u5ff5\u3001\u53d7\u8bad\u7ec3\u6761\u4ef6\u548c\u7ed3\u6784\u5f71\u54cd\u5bfc\u81f4\u4e0d\u540ccheckpoint\u96be\u4ee5\u6bd4\u8f83\uff0c\u7279\u5f81\u4e0d\u7a33\u5b9a\u7b49\u3002", "method": "\u63d0\u51fa\u5728transformer\u7ed3\u6784\u4e2d\u4e8e\u7279\u5b9a\u5c42\u5f15\u5165TopK\u6fc0\u6d3b\u51fd\u6570\uff0c\u4f7f\u6a21\u578b\u7684\u9690\u85cf\u72b6\u6001\u4e0eTopK SAE\u7684\u6f5c\u5728\u7279\u5f81\u7b49\u6548\uff0c\u4ece\u800c\u4e0d\u518d\u9700\u8981\u540e\u9a8c\u8bad\u7ec3\uff0c\u540c\u65f6\u76f4\u63a5\u83b7\u5f97\u4e0eSAE\u53ef\u6bd4\u62df\u7684\u89e3\u91ca\u6027\u80fd\u529b\u3002", "result": "TopK\u8bed\u8a00\u6a21\u578b\u4fdd\u6301\u539f\u6709\u80fd\u529b\u7684\u540c\u65f6\uff0c\u5e26\u6765\u66f4\u5f3a\u7684\u89e3\u91ca\u6027\u548c\u6a21\u578b\u7a33\u5b9a\u6027\uff0c\u5176\u7a00\u758f\u8868\u793a\u6709\u52a9\u4e8e\u901a\u8fc7\u7279\u5b9a\u795e\u7ecf\u5143\u5e72\u9884\u8fdb\u884c\u6a21\u578b\u8c03\u6574\u4e0e\u8be6\u7ec6\u795e\u7ecf\u5143\u5206\u6790\uff0c\u5728\u4e0d\u540ccheckpoint\u548c\u5c42\u4e4b\u95f4\u5177\u6709\u66f4\u5f3a\u7684\u7279\u5f81\u53ef\u6bd4\u6027\u3002", "conclusion": "TopK LMs\u662f\u4e00\u79cd\u7a33\u5b9a\u3001\u53ef\u9760\u4e14\u9ad8\u6548\u7684\u89e3\u91ca\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u6df1\u5165\u7406\u89e3\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u5b66\u4e60\u548c\u8868\u5f81\u6982\u5ff5\uff0c\u8fd9\u5c06\u663e\u8457\u4fc3\u8fdb\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u63a7\u6027\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2506.21495", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.21495", "abs": "https://arxiv.org/abs/2506.21495", "authors": ["Jack Lanchantin", "Angelica Chen", "Janice Lan", "Xian Li", "Swarnadeep Saha", "Tianlu Wang", "Jing Xu", "Ping Yu", "Weizhe Yuan", "Jason E Weston", "Sainbayar Sukhbaatar", "Ilia Kulikov"], "title": "Bridging Offline and Online Reinforcement Learning for LLMs", "comment": null, "summary": "We investigate the effectiveness of reinforcement learning methods for\nfinetuning large language models when transitioning from offline to semi-online\nto fully online regimes for both verifiable and non-verifiable tasks. Our\nexperiments cover training on verifiable math as well as non-verifiable\ninstruction following with a set of benchmark evaluations for both. Across\nthese settings, we extensively compare online and semi-online Direct Preference\nOptimization and Group Reward Policy Optimization objectives, and surprisingly\nfind similar performance and convergence between these variants, which all\nstrongly outperform offline methods. We provide a detailed analysis of the\ntraining dynamics and hyperparameter selection strategies to achieve optimal\nresults. Finally, we show that multi-tasking with verifiable and non-verifiable\nrewards jointly yields improved performance across both task types.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u6bd4\u8f83\u4e86LLM\u5728\u53ef\u9a8c\u8bc1\u4e0e\u4e0d\u53ef\u9a8c\u8bc1\u4efb\u52a1\u4e0a\u4ece\u79bb\u7ebf\u5230\u5728\u7ebf\u591a\u79cd\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u65b9\u6cd5\uff0c\u53d1\u73b0\u5728\u7ebf\u3001\u534a\u5728\u7ebf\u7b56\u7565\u6574\u4f53\u4f18\u4e8e\u79bb\u7ebf\uff0c\u591a\u4efb\u52a1\u8bad\u7ec3\u8fdb\u4e00\u6b65\u63d0\u5347\u8868\u73b0\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u5728LLM\u5fae\u8c03\u573a\u666f\u4e0b\uff0c\u591a\u6837\u4efb\u52a1\uff08\u53ef\u9a8c\u8bc1\u3001\u4e0d\u53ef\u9a8c\u8bc1\uff09\u548c\u591a\u79cd\u8bad\u7ec3\u8303\u5f0f\u4e0b\u7684\u6548\u679c\u548c\u5dee\u5f02\u5c1a\u4e0d\u660e\u6670\uff0c\u9700\u8981\u7cfb\u7edf\u6bd4\u8f83\u548c\u5206\u6790\u3002", "method": "\u6bd4\u8f83\u4e86\u5728\u7ebf\u3001\u534a\u5728\u7ebfDirect Preference Optimization\u548cGroup Reward Policy Optimization\u76ee\u6807\u51fd\u6570\uff0c\u5e76\u5728\u53ef\u9a8c\u8bc1\u6570\u5b66\u4efb\u52a1\u4e0e\u4e0d\u53ef\u9a8c\u8bc1\u7684\u6307\u4ee4\u8ddf\u968f\u4efb\u52a1\u4e0a\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002\u5bf9\u8bad\u7ec3\u52a8\u6001\u53ca\u8d85\u53c2\u6570\u9009\u62e9\u4e5f\u505a\u4e86\u8be6\u7ec6\u5206\u6790\u3002", "result": "\u5728\u7ebf\u53ca\u534a\u5728\u7ebf\u65b9\u6cd5\u5728\u6240\u6709\u6d4b\u8bd5\u4e2d\u90fd\u4f18\u4e8e\u79bb\u7ebf\u65b9\u6cd5\uff1b\u5404\u4f18\u5316\u65b9\u6cd5\u95f4\u8868\u73b0\u76f8\u4f3c\uff1b\u591a\u4efb\u52a1\u5fae\u8c03\u6709\u52a9\u4e8e\u4e24\u7c7b\u4efb\u52a1\u7684\u6574\u4f53\u63d0\u5347\u3002", "conclusion": "\u5728\u7ebf\u3001\u534a\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u5fae\u8c03\u5927\u578b\u8bed\u8a00\u6a21\u578b\u65f6\uff0c\u76f8\u8f83\u4e8e\u7eaf\u79bb\u7ebf\u65b9\u6cd5\u8868\u73b0\u66f4\u4f73\uff0c\u800c\u591a\u4efb\u52a1\u7ed3\u5408\u53ef\u9a8c\u8bc1\u4e0e\u4e0d\u53ef\u9a8c\u8bc1\u4efb\u52a1\u80fd\u8fdb\u4e00\u6b65\u63d0\u5347\u8868\u73b0\u3002"}}
{"id": "2506.21508", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG", "68T50", "I.2.7"], "pdf": "https://arxiv.org/pdf/2506.21508", "abs": "https://arxiv.org/abs/2506.21508", "authors": ["Marek \u0160uppa", "Andrej Ridzik", "Daniel Hl\u00e1dek", "Tom\u00e1\u0161 Jav\u016frek", "Vikt\u00f3ria Ondrejov\u00e1", "Krist\u00edna S\u00e1sikov\u00e1", "Martin Tamajka", "Mari\u00e1n \u0160imko"], "title": "skLEP: A Slovak General Language Understanding Benchmark", "comment": "ACL 2025 Findings", "summary": "In this work, we introduce skLEP, the first comprehensive benchmark\nspecifically designed for evaluating Slovak natural language understanding\n(NLU) models. We have compiled skLEP to encompass nine diverse tasks that span\ntoken-level, sentence-pair, and document-level challenges, thereby offering a\nthorough assessment of model capabilities. To create this benchmark, we curated\nnew, original datasets tailored for Slovak and meticulously translated\nestablished English NLU resources. Within this paper, we also present the first\nsystematic and extensive evaluation of a wide array of Slovak-specific,\nmultilingual, and English pre-trained language models using the skLEP tasks.\nFinally, we also release the complete benchmark data, an open-source toolkit\nfacilitating both fine-tuning and evaluation of models, and a public\nleaderboard at https://github.com/slovak-nlp/sklep in the hopes of fostering\nreproducibility and drive future research in Slovak NLU.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5e76\u516c\u5f00\u4e86skLEP\u2014\u2014\u9996\u4e2a\u4e13\u4e3a\u65af\u6d1b\u4f10\u514b\u8bed\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u8bbe\u8ba1\u7684\u7efc\u5408\u6027\u8bc4\u6d4b\u57fa\u51c6\uff0c\u5e76\u8bc4\u4f30\u591a\u7c7b\u8bed\u8a00\u6a21\u578b\u8868\u73b0\uff0c\u4e3a\u8be5\u9886\u57df\u540e\u7eed\u7814\u7a76\u5960\u5b9a\u57fa\u7840\u3002", "motivation": "\u76ee\u524d\u65af\u6d1b\u4f10\u514b\u8bed\u81ea\u7136\u8bed\u8a00\u7406\u89e3\uff08NLU\uff09\u9886\u57df\u7f3a\u4e4f\u4e13\u95e8\u7684\u8bc4\u6d4b\u57fa\u51c6\uff0c\u9650\u5236\u4e86\u6a21\u578b\u6027\u80fd\u7684\u5168\u9762\u8bc4\u4f30\u548c\u7814\u7a76\u8fdb\u5c55\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3askLEP\u7684\u5168\u65b0\u8bc4\u6d4b\u57fa\u51c6\uff0c\u878d\u5408\u4e86\u4e5d\u9879\u6db5\u76d6\u4e0d\u540c\u5c42\u6b21\uff08Token\u3001\u53e5\u5bf9\u548c\u6587\u6863\u7ea7\u522b\uff09\u7684\u4efb\u52a1\u3002\u6570\u636e\u96c6\u5305\u62ec\u4e3a\u65af\u6d1b\u4f10\u514b\u8bed\u5b9a\u5236\u7684\u65b0\u6570\u636e\uff0c\u540c\u65f6\u7cbe\u5fc3\u7ffb\u8bd1\u4e86\u5df2\u6709\u7684\u82f1\u6587NLU\u8d44\u6e90\u3002\u4f5c\u8005\u8fd8\u5229\u7528skLEP\u7cfb\u7edf\u6027\u5730\u8bc4\u4f30\u4e86\u591a\u79cd\u65af\u6d1b\u4f10\u514b\u8bed\u3001\u82f1\u6587\u53ca\u591a\u8bed\u8a00\u9884\u8bad\u7ec3\u6a21\u578b\u3002", "result": "skLEP\u57fa\u51c6\u5168\u9762\u8bc4\u4f30\u4e86\u591a\u79cd\u8bed\u8a00\u6a21\u578b\u5728\u65af\u6d1b\u4f10\u514b\u8bedNLU\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u5bf9\u5404\u7c7b\u6a21\u578b\u4f18\u7f3a\u70b9\u8fdb\u884c\u4e86\u7cfb\u7edf\u5bf9\u6bd4\u5206\u6790\u3002\u57fa\u51c6\u6570\u636e\u3001\u5f00\u6e90\u5de5\u5177\u5305\u548c\u6392\u884c\u699c\u5168\u90e8\u516c\u5f00\uff0c\u4fbf\u4e8e\u793e\u533a\u590d\u73b0\u548c\u8fdb\u4e00\u6b65\u7814\u7a76\u3002", "conclusion": "skLEP\u4f5c\u4e3a\u9996\u4e2a\u65af\u6d1b\u4f10\u514b\u8bedNLU\u7efc\u5408\u57fa\u51c6\uff0c\u4e3a\u7814\u7a76\u548c\u63a8\u52a8\u9ad8\u8d28\u91cf\u65af\u6d1b\u4f10\u514b\u8bed\u81ea\u7136\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\u8bbe\u65bd\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u672c\u9886\u57df\u672a\u6765\u53d1\u5c55\u3002"}}
{"id": "2506.21497", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.21497", "abs": "https://arxiv.org/abs/2506.21497", "authors": ["Jiashuo Wang", "Kaitao Song", "Chunpu Xu", "Changhe Song", "Yang Xiao", "Dongsheng Li", "Lili Qiu", "Wenjie Li"], "title": "Enhancing User Engagement in Socially-Driven Dialogue through Interactive LLM Alignments", "comment": null, "summary": "Enhancing user engagement through interactions plays an essential role in\nsocially-driven dialogues. While prior works have optimized models to reason\nover relevant knowledge or plan a dialogue act flow, the relationship between\nuser engagement and knowledge or dialogue acts is subtle and does not guarantee\nuser engagement in socially-driven dialogues. To this end, we enable\ninteractive LLMs to learn user engagement by leveraging signals from the future\ndevelopment of conversations. Specifically, we adopt a more direct and relevant\nindicator of user engagement, i.e., the user's reaction related to dialogue\nintention after the interaction, as a reward to align interactive LLMs. To\nachieve this, we develop a user simulator to interact with target interactive\nLLMs and explore interactions between the user and the interactive LLM system\nvia \\textit{i$\\times$MCTS} (\\textit{M}onte \\textit{C}arlo \\textit{T}ree\n\\textit{S}earch for \\textit{i}nteraction). In this way, we collect a dataset\ncontaining pairs of higher and lower-quality experiences using\n\\textit{i$\\times$MCTS}, and align interactive LLMs for high-level user\nengagement by direct preference optimization (DPO) accordingly. Experiments\nconducted on two socially-driven dialogue scenarios (emotional support\nconversations and persuasion for good) demonstrate that our method effectively\nenhances user engagement in interactive LLMs.", "AI": {"tldr": "\u8be5\u5de5\u4f5c\u63d0\u51fa\u5229\u7528\u7528\u6237\u771f\u5b9e\u540e\u7eed\u53cd\u5e94\u4f5c\u4e3a\u8bad\u7ec3\u4fe1\u53f7\uff0c\u901a\u8fc7\u4ea4\u4e92\u6a21\u62df\u548c\u504f\u597d\u4f18\u5316\u63d0\u5347\u5927\u6a21\u578b\u5728\u793e\u4f1a\u6027\u5bf9\u8bdd\u573a\u666f\u4e2d\u7684\u7528\u6237\u53c2\u4e0e\u5ea6\uff0c\u5e76\u53d6\u5f97\u4e86\u663e\u8457\u6548\u679c\u3002", "motivation": "\u4ee5\u5f80\u5de5\u4f5c\u867d\u7136\u901a\u8fc7\u4f18\u5316\u77e5\u8bc6\u63a8\u7406\u6216\u5bf9\u8bdd\u884c\u4e3a\u4fc3\u8fdb\u6a21\u578b\u6548\u679c\uff0c\u4f46\u5728\u793e\u4f1a\u6027\u5bf9\u8bdd\u4e2d\uff0c\u77e5\u8bc6\u6216\u5bf9\u8bdd\u884c\u4e3a\u4e0e\u7528\u6237\u53c2\u4e0e\u5ea6\u7684\u5173\u7cfb\u5e76\u4e0d\u76f4\u63a5\uff0c\u65e0\u6cd5\u4fdd\u8bc1\u7528\u6237\u6709\u6548\u53c2\u4e0e\u3002\u56e0\u800c\u9700\u8981\u65b0\u7684\u65b9\u5f0f\u4f7f\u4ea4\u4e92\u5f0f\u5927\u6a21\u578b\u80fd\u591f\u4e3b\u52a8\u63d0\u5347\u548c\u5b66\u4e60\u7528\u6237\u53c2\u4e0e\u5ea6\u3002", "method": "\u63d0\u51fa\u7528\u7528\u6237\u5728\u5bf9\u8bdd\u540e\u7684\u771f\u5b9e\u53cd\u5e94\u4f5c\u4e3a\u7528\u6237\u53c2\u4e0e\u5ea6\u7684\u76f4\u63a5\u6307\u6807\uff0c\u901a\u8fc7\u5f00\u53d1\u7528\u6237\u6a21\u62df\u5668\u4e0e\u4ea4\u4e92\u5f0f\u5927\u6a21\u578b\u8fdb\u884c\u4e92\u52a8\uff0c\u5e76\u7ed3\u5408\u4ea4\u4e92\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08i\u00d7MCTS\uff09\u63a2\u7d22\u7528\u6237\u4e0e\u5927\u6a21\u578b\u4e4b\u95f4\u7684\u4e92\u52a8\uff0c\u91c7\u96c6\u9ad8\u3001\u4f4e\u8d28\u91cf\u4e92\u52a8\u4f53\u9a8c\u6570\u636e\u3002\u6700\u540e\uff0c\u91c7\u7528\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u65b9\u6cd5\u6765\u5fae\u8c03\u4ea4\u4e92\u5f0f\u5927\u6a21\u578b\uff0c\u63d0\u5347\u5176\u4fc3\u8fdb\u7528\u6237\u53c2\u4e0e\u5ea6\u7684\u80fd\u529b\u3002", "result": "\u5728\u60c5\u611f\u652f\u6301\u548c\u5584\u610f\u529d\u8bf4\u4e24\u4e2a\u793e\u4f1a\u6027\u5bf9\u8bdd\u573a\u666f\u4e0b\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u4ea4\u4e92\u5f0f\u5927\u6a21\u578b\u7684\u7528\u6237\u53c2\u4e0e\u5ea6\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u672a\u6765\u7528\u6237\u53cd\u5e94\u4f5c\u4e3a\u5956\u52b1\u4fe1\u53f7\uff0c\u7ed3\u5408\u6a21\u62df\u4e0e\u504f\u597d\u4f18\u5316\uff0c\u5728\u793e\u4f1a\u6027\u5bf9\u8bdd\u4e2d\u5927\u5e45\u63d0\u5347\u4e86\u5927\u6a21\u578b\u4fc3\u8fdb\u7528\u6237\u53c2\u4e0e\u5ea6\u7684\u8868\u73b0\u3002"}}
