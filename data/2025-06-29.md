<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 13]
- [cs.CL](#cs.CL) [Total: 39]
- [cs.CE](#cs.CE) [Total: 3]
- [cs.CY](#cs.CY) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [The Singapore Consensus on Global AI Safety Research Priorities](https://arxiv.org/abs/2506.20702)
*Yoshua Bengio,Tegan Maharaj,Luke Ong,Stuart Russell,Dawn Song,Max Tegmark,Lan Xue,Ya-Qin Zhang,Stephen Casper,Wan Sie Lee,Sören Mindermann,Vanessa Wilfred,Vidhisha Balachandran,Fazl Barez,Michael Belinsky,Imane Bello,Malo Bourgon,Mark Brakel,Siméon Campos,Duncan Cass-Beggs,Jiahao Chen,Rumman Chowdhury,Kuan Chua Seah,Jeff Clune,Juntao Dai,Agnes Delaborde,Nouha Dziri,Francisco Eiras,Joshua Engels,Jinyu Fan,Adam Gleave,Noah Goodman,Fynn Heide,Dan Hendrycks,Cyrus Hodes,Bryan Low Kian Hsiang,Minlie Huang,Sami Jawhar,Wang Jingyu,Adam Tauman Kalai,Meindert Kamphuis,Mohan Kankanhalli,Subhash Kantamneni,Mathias Bonde Kirk,Thomas Kwa,Jeffrey Ladish,Kwok-Yan Lam,Wan Lee Sie,Taewhi Lee,Xiaojian Li,Jiajun Liu,Chaochao Lu,Yifan Mai,Richard Mallah,Julian Michael,Nick Moës,Simon Möller,Kihyuk Nam,Kwan Yee Ng,Mark Nitzberg,Besmira Nushi,Seán O hÉigeartaigh,Alejandro Ortega,Pierre Peigné,James Petrie,Benjamin Prud'Homme,Reihaneh Rabbany,Nayat Sanchez-Pi,Sarah Schwettmann,Buck Shlegeris,Saad Siddiqui,Aradhana Sinha,Martín Soto,Cheston Tan,Dong Ting,Robert Trager,Brian Tse,Anthony Tung K. H.,Vanessa Wilfred,John Willes,Denise Wong,Wei Xu,Rongwu Xu,Yi Zeng,HongJiang Zhang,Djordje Žikelić*

Main category: cs.AI

TL;DR: 本报告总结了国际科学家在AI安全领域的研究优先事项和分领域组织思路，强调纵深防御和全流程治理，推动全球建立AI信任生态系统。


<details>
  <summary>Details</summary>
Motivation: 随着AI能力和自主性的快速提升，确保AI的安全性（可信、可靠、可控）成为迫切讨论议题，需要建立信任生态系统以促进AI的发展与创新，避免公众对AI产生抵触。

Method: 通过召开“2025年新加坡人工智能大会：AI安全的国际科学交流”，汇聚全球AI科学家，识别并整合AI安全的研究优先事项。本报告借鉴了由Yoshua Bengio主持的国际AI安全报告，并得到33个政府的支持，采用纵深防御模型将AI安全研究分为三大领域：开发阶段的可信性问题（Development）、风险评估（Assessment）、部署后的监控与干预（Control）。

Result: 本报告清晰梳理了AI系统的安全性挑战，并为未来AI安全研究指明了发展方向和优先领域。提出将AI安全研究体系化，覆盖开发、评估、部署与控制的全过程。

Conclusion: 建立强有力的AI安全生态系统亟需国际合作和系统性的研究框架，为发展可信赖的AI系统与推动可持续创新奠定基础。

Abstract: Rapidly improving AI capabilities and autonomy hold significant promise of
transformation, but are also driving vigorous debate on how to ensure that AI
is safe, i.e., trustworthy, reliable, and secure. Building a trusted ecosystem
is therefore essential -- it helps people embrace AI with confidence and gives
maximal space for innovation while avoiding backlash.
  The "2025 Singapore Conference on AI (SCAI): International Scientific
Exchange on AI Safety" aimed to support research in this space by bringing
together AI scientists across geographies to identify and synthesise research
priorities in AI safety. This resulting report builds on the International AI
Safety Report chaired by Yoshua Bengio and backed by 33 governments. By
adopting a defence-in-depth model, this report organises AI safety research
domains into three types: challenges with creating trustworthy AI systems
(Development), challenges with evaluating their risks (Assessment), and
challenges with monitoring and intervening after deployment (Control).

</details>


### [2] [MAGPIE: A dataset for Multi-AGent contextual PrIvacy Evaluation](https://arxiv.org/abs/2506.20737)
*Gurusha Juneja,Alon Albalak,Wenyue Hua,William Yang Wang*

Main category: cs.AI

TL;DR: 本文提出了真实高风险场景下的隐私保护评测，实验证明主流大模型在多轮对话和协作时无法充分理解和执行隐私保护要求，隐私泄露和任务失败率均较高。


<details>
  <summary>Details</summary>
Motivation: LLM智能体被广泛部署于协作场景（如调度、协商、资源分配），但涉及的专有工具和数据库对隐私有极高要求。目前尚不清楚这些模型在多轮对话任务中的隐私理解能力和保护能力如何，尤其是在非对抗性情境下。

Method: 作者提出MAGPIE基准，包括158个涵盖15个领域的高风险实际场景。这些场景无法简单删除私人数据来完成任务，考查模型对情境隐私的理解及在协作中保护用户隐私的能力，并对主流LLM（如GPT-4o、Claude-2.7-Sonnet）进行实验评估。

Result: 实验发现，即使有明确隐私保护指令，主流LLM在25.2%和43.6%的单轮任务中错误地将私密数据分类为可分享信息；在多轮对话中，隐私信息泄露比率分别达到59.9%和50.5%。多智能体系统在71%的高风险任务场景下无法完成任务。

Conclusion: 当前LLM并未在隐私保护和协作任务解决上实现充分对齐，现有模型对情境隐私的理解和保护能力均存在显著不足。

Abstract: The proliferation of LLM-based agents has led to increasing deployment of
inter-agent collaboration for tasks like scheduling, negotiation, resource
allocation etc. In such systems, privacy is critical, as agents often access
proprietary tools and domain-specific databases requiring strict
confidentiality. This paper examines whether LLM-based agents demonstrate an
understanding of contextual privacy. And, if instructed, do these systems
preserve inference time user privacy in non-adversarial multi-turn
conversation. Existing benchmarks to evaluate contextual privacy in LLM-agents
primarily assess single-turn, low-complexity tasks where private information
can be easily excluded. We first present a benchmark - MAGPIE comprising 158
real-life high-stakes scenarios across 15 domains. These scenarios are designed
such that complete exclusion of private data impedes task completion yet
unrestricted information sharing could lead to substantial losses. We then
evaluate the current state-of-the-art LLMs on (a) their understanding of
contextually private data and (b) their ability to collaborate without
violating user privacy. Empirical experiments demonstrate that current models,
including GPT-4o and Claude-2.7-Sonnet, lack robust understanding of contextual
privacy, misclassifying private data as shareable 25.2\% and 43.6\% of the
time. In multi-turn conversations, these models disclose private information in
59.9\% and 50.5\% of cases even under explicit privacy instructions.
Furthermore, multi-agent systems fail to complete tasks in 71\% of scenarios.
These results underscore that current models are not aligned towards both
contextual privacy preservation and collaborative task-solving.

</details>


### [3] [Dynamic Context-Aware Prompt Recommendation for Domain-Specific AI Applications](https://arxiv.org/abs/2506.20815)
*Xinye Tang,Haijun Zhai,Chaitanya Belwal,Vineeth Thayanithi,Philip Baumann,Yogesh K Roy*

Main category: cs.AI

TL;DR: 该文提出一种面向领域AI应用、动态上下文感知的提示词推荐系统，有效提升提示建议的质量和相关性。


<details>
  <summary>Details</summary>
Motivation: LLM应用高度依赖用户提示词质量，尤其在特定领域中，编写高质量提示词具有挑战性。为降低用户门槛、增强模型表现，亟需自动化提升提示词质量的方法。

Method: 提出了一个动态、上下文感知的领域特定AI应用提示词推荐系统。该系统融合上下文查询分析、检索增强知识接入、分层技能组织及自适应技能排序，并结合行为遥测及二阶段分层推理选取排名相关技能，利用预定义与自适应模板（支持few-shot学习）综合生成高相关性、可操作性的提示建议。

Result: 在真实数据集上的实验证明，该系统在自动化评测和专家评审中均表现出高效用性与相关性。

Conclusion: 所提出的方法能有效提升领域特定LLM应用中的提示词相关性与实用性，为用户带来更优的体验并简化应用开发。

Abstract: LLM-powered applications are highly susceptible to the quality of user
prompts, and crafting high-quality prompts can often be challenging especially
for domain-specific applications. This paper presents a novel dynamic
context-aware prompt recommendation system for domain-specific AI applications.
Our solution combines contextual query analysis, retrieval-augmented knowledge
grounding, hierarchical skill organization, and adaptive skill ranking to
generate relevant and actionable prompt suggestions.
  The system leverages behavioral telemetry and a two-stage hierarchical
reasoning process to dynamically select and rank relevant skills, and
synthesizes prompts using both predefined and adaptive templates enhanced with
few-shot learning. Experiments on real-world datasets demonstrate that our
approach achieves high usefulness and relevance, as validated by both automated
and expert evaluations.

</details>


### [4] [Beyond Reactive Safety: Risk-Aware LLM Alignment via Long-Horizon Simulation](https://arxiv.org/abs/2506.20949)
*Chenkai Sun,Denghui Zhang,ChengXiang Zhai,Heng Ji*

Main category: cs.AI

TL;DR: 提出了一种能预测语言模型建议长期社会影响的框架，并创建了间接伤害场景数据集，实验表明模型安全性显著提升，推动了更安全AI智能体的发展。


<details>
  <summary>Details</summary>
Motivation: 语言模型在公共政策、医疗等高风险社会决策中影响日益加深，在此背景下，需要理解其建议可能带来的长远影响，以确保其正面作用。

Method: 提出了一个概念性框架，用于预测模型建议在社会系统中随时间传播的过程。同时，构建了包含100个间接伤害场景的数据集，以评估语言模型预测不明显负面后果的能力。

Result: 新方法在该数据集上性能提升超过20%，并在现有安全基准测试（AdvBench、SafeRLHF、WildGuardMix）上平均胜率超过70%。

Conclusion: 框架能更有效评估并提升语言模型长期安全意识，为更安全的智能体设计提供了有前景的方向。

Abstract: Given the growing influence of language model-based agents on high-stakes
societal decisions, from public policy to healthcare, ensuring their beneficial
impact requires understanding the far-reaching implications of their
suggestions. We propose a proof-of-concept framework that projects how
model-generated advice could propagate through societal systems on a
macroscopic scale over time, enabling more robust alignment. To assess the
long-term safety awareness of language models, we also introduce a dataset of
100 indirect harm scenarios, testing models' ability to foresee adverse,
non-obvious outcomes from seemingly harmless user prompts. Our approach
achieves not only over 20% improvement on the new dataset but also an average
win rate exceeding 70% against strong baselines on existing safety benchmarks
(AdvBench, SafeRLHF, WildGuardMix), suggesting a promising direction for safer
agents.

</details>


### [5] [Unveiling Causal Reasoning in Large Language Models: Reality or Mirage?](https://arxiv.org/abs/2506.21215)
*Haoang Chi,He Li,Wenjing Yang,Feng Liu,Long Lan,Xiaoguang Ren,Tongliang Liu,Bo Han*

Main category: cs.AI

TL;DR: 本文指出现有LLM只能做浅层因果推理，通过新基准CausalProbe-2024和新方法G^2-Reasoner验证并提升LLM的因果推理能力，朝着类人高阶因果推理迈进一步。


<details>
  <summary>Details</summary>
Motivation: 虽然大型语言模型（LLMs）在理解因果关系和做出遵循因果律的回答方面展示出一定能力，但尚不明确它们是否能够像人类一样进行真正的因果推理。已有证据表明，LLMs更多地依赖于已有知识，可能只停留在浅层（一级）的因果推理。

Method: 作者首先分析了基于Transformer的LLMs自回归机制，发现其本质上并不具备内在的因果性。然后构建了一个新的因果问答基准CausalProbe-2024，数据新颖并对被测LLM基本‘陌生’。实验考查LLM在该基准下的推理表现。另外，提出G^2-Reasoner方法，在推理过程中引入通用知识和目标导向提示，以提升LLM的因果推理能力。

Result: LLMs在CausalProbe-2024这一全新数据集上的表现显著下降，说明它们主要是一级因果推理。采用G^2-Reasoner方法后，LLMs应用于新颖及反事实场景时，推理能力得到显著提升，向二级因果推理能力迈进。

Conclusion: 现有LLM只能进行浅层（一级）因果推理，离真正的人类式（高级）因果推理还有差距。针对该问题，G^2-Reasoner通过引入通用知识与目标导向提示有效提升了LLMs高级因果推理能力，为推动LLMs向真正的因果智能发展提供了一条新途径。

Abstract: Causal reasoning capability is critical in advancing large language models
(LLMs) toward strong artificial intelligence. While versatile LLMs appear to
have demonstrated capabilities in understanding contextual causality and
providing responses that obey the laws of causality, it remains unclear whether
they perform genuine causal reasoning akin to humans. However, current evidence
indicates the contrary. Specifically, LLMs are only capable of performing
shallow (level-1) causal reasoning, primarily attributed to the causal
knowledge embedded in their parameters, but they lack the capacity for genuine
human-like (level-2) causal reasoning. To support this hypothesis,
methodologically, we delve into the autoregression mechanism of
transformer-based LLMs, revealing that it is not inherently causal.
Empirically, we introduce a new causal Q&A benchmark called CausalProbe-2024,
whose corpora are fresh and nearly unseen for the studied LLMs. The LLMs
exhibit a significant performance drop on CausalProbe-2024 compared to earlier
benchmarks, indicating the fact that they primarily engage in level-1 causal
reasoning. To bridge the gap towards level-2 causal reasoning, we draw
inspiration from the fact that human reasoning is usually facilitated by
general knowledge and intended goals. We propose G^2-Reasoner, a method that
incorporates general knowledge and goal-oriented prompts into LLMs' causal
reasoning processes. Experiments demonstrate that G^2-Reasoner significantly
enhances LLMs' causal reasoning capability, particularly in fresh and
counterfactual contexts. This work sheds light on a new path for LLMs to
advance towards genuine causal reasoning, going beyond level-1 and making
strides towards level-2.

</details>


### [6] [World-aware Planning Narratives Enhance Large Vision-Language Model Planner](https://arxiv.org/abs/2506.21230)
*Junhao Shi,Zhaoye Fei,Siyin Wang,Qipeng Guo,Jingjing Gong,Xipeng QIu*

Main category: cs.AI

TL;DR: 提出了WAP框架，通过引入多种认知能力并采用课程学习，极大提升了LVLMs在具身规划中的表现，实现了对现有主流系统的大幅超越。


<details>
  <summary>Details</summary>
Motivation: 大规模视觉语言模型（LVLMs）在具身规划任务中表现出潜力，但在复杂环境和多步目标任务中表现不佳。主要问题在于现有的环境无关的模仿学习方法将指令与环境脱钩，导致模型难以理解与环境相关的指令，对视觉推理能力的依赖性较弱。

Method: 提出了世界感知规划叙事增强（World-Aware Planning Narrative Enhancement，WAP）框架，通过四种认知能力（视觉外观建模、空间推理、功能抽象和句法落地），使用课程学习，仅依赖原始视觉观测，不依赖环境先验或额外线索，系统性提升LVLMs的环境理解能力。

Result: 在EB-ALFRED基准测试中，经过WAP增强的模型（如Qwen2.5-VL）在任务成功率、常识推理和长程规划方面实现了显著提升，对比前后任务成功率绝对提升60.7，常识推理提升60.0，长水平规划提升70.0。增强后的开源模型在多个维度显著超越了闭源系统如GPT-4o和Claude-3.5-Sonnet。

Conclusion: 通过WAP框架，使LVLMs获得更强的环境感知与推理能力，大幅提升了其在具身多步任务中的表现，显示该方法在实现通用具身智能方面具有重要潜力。

Abstract: Large Vision-Language Models (LVLMs) show promise for embodied planning tasks
but struggle with complex scenarios involving unfamiliar environments and
multi-step goals. Current approaches rely on environment-agnostic imitation
learning that disconnects instructions from environmental contexts, causing
models to struggle with context-sensitive instructions and rely on
supplementary cues rather than visual reasoning during long-horizon
interactions. In this work, we propose World-Aware Planning Narrative
Enhancement (WAP), a framework that infuses LVLMs with comprehensive
environmental understanding through four cognitive capabilities (visual
appearance modeling, spatial reasoning, functional abstraction, and syntactic
grounding) while developing and evaluating models using only raw visual
observations through curriculum learning. Evaluations on the EB-ALFRED
benchmark demonstrate substantial improvements, with Qwen2.5-VL achieving a
60.7 absolute improvement in task success rates, particularly in commonsense
reasoning (+60.0) and long-horizon planning (+70.0). Notably, our enhanced
open-source models outperform proprietary systems like GPT-4o and
Claude-3.5-Sonnet by a large margin.

</details>


### [7] [IXAII: An Interactive Explainable Artificial Intelligence Interface for Decision Support Systems](https://arxiv.org/abs/2506.21310)
*Pauline Speckmann,Mario Nadj,Christian Janiesch*

Main category: cs.AI

TL;DR: 本文提出了一个交互式可解释AI系统IXAII，支持多种解释方法与定制视图，并通过专家及普通用户评估证实其在提升AI透明度和人机交互方面的有效性和新颖性。


<details>
  <summary>Details</summary>
Motivation: 目前大多数可解释AI方法为事后解释且多为静态，忽略了用户视角，限制了其对目标用户的有效性。

Method: 开发了交互式可解释智能系统IXAII，整合了LIME、SHAP、Anchors和DiCE四种可解释AI方法，并根据五类用户群体提供定制视图和内容可控的解释格式。通过专家及普通用户访谈对系统进行评估。

Result: 结果表明，IXAII的多种解释和多样化可视化选项被用户认为有助于提升透明度。

Conclusion: IXAII通过弥合可解释AI方法、交互性和实际应用之间的差距，为AI解释实践和人机交互提供了新视角。

Abstract: Although several post-hoc methods for explainable AI have been developed,
most are static and neglect the user perspective, limiting their effectiveness
for the target audience. In response, we developed the interactive explainable
intelligent system called IXAII that offers explanations from four explainable
AI methods: LIME, SHAP, Anchors, and DiCE. Our prototype provides tailored
views for five user groups and gives users agency over the explanations'
content and their format. We evaluated IXAII through interviews with experts
and lay users. Our results indicate that IXAII, which provides different
explanations with multiple visualization options, is perceived as helpful to
increase transparency. By bridging the gaps between explainable AI methods,
interactivity, and practical implementation, we provide a novel perspective on
AI explanation practices and human-AI interaction.

</details>


### [8] [Active Inference AI Systems for Scientific Discovery](https://arxiv.org/abs/2506.21329)
*Karthik Duraisamy*

Main category: cs.AI

TL;DR: 当前AI科学发现受限于内部表征、因果推理与现实联系三大问题。作者提出一个闭环主动推断架构，通过自监督因果模型、符号规划、动态知识图谱和与实验/仿真的互动来克服这些挑战，并强调人类判断的不可替代性。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统在科学发现领域面临抽象、因果推理与现实连接三大根本局限，无法满足科学研究对因果机制、长期知识成长和实验验证的需求。

Method: 基于已有研究，提出了主动推断驱动的AI科学系统架构，具体包括因果自监督模型、符号或神经符号规划器（带贝叶斯保障机制）、知识图谱的动态生长与剪枝，以及与仿真和自动化实验室的闭环交互。通过这种架构，AI可实现心智模拟与现实互动的统一。

Result: 提出主动推断式科学发现AI的新范式与系统设计，并强调人类判断作为核心组成部分的必要性。新架构可促进AI不断校正认知，从而更有效进行科学发现。

Conclusion: 提出了一种AI驱动科学发现的架构，强调仅仅扩大模型规模和算力已无法满足科学发现的需要，需填补抽象、推理、现实三大鸿沟。能实现内外模型互动的推理-验证循环才是科学AI未来方向，同时指出人类判断在其中不可或缺。

Abstract: The rapid evolution of artificial intelligence has led to expectations of
transformative scientific discovery, yet current systems remain fundamentally
limited by their operational architectures, brittle reasoning mechanisms, and
their separation from experimental reality. Building on earlier work, we
contend that progress in AI-driven science now depends on closing three
fundamental gaps -- the abstraction gap, the reasoning gap, and the reality gap
-- rather than on model size/data/test time compute. Scientific reasoning
demands internal representations that support simulation of actions and
response, causal structures that distinguish correlation from mechanism, and
continuous calibration. We define active inference AI systems for scientific
discovery as those that (i) maintain long-lived research memories grounded in
causal self-supervised foundation models, (ii) symbolic or neuro-symbolic
planners equipped with Bayesian guardrails, (iii) grow persistent knowledge
graphs where thinking generates novel conceptual nodes, reasoning establishes
causal edges, and real-world interaction prunes false connections while
strengthening verified pathways, and (iv) refine their internal representations
through closed-loop interaction with both high-fidelity simulators and
automated laboratories - an operational loop where mental simulation guides
action and empirical surprise reshapes understanding. In essence, we outline an
architecture where discovery arises from the interplay between internal models
that enable counterfactual reasoning and external validation that grounds
hypotheses in reality. It is also argued that the inherent ambiguity in
feedback from simulations and experiments, and underlying uncertainties makes
human judgment indispensable, not as a temporary scaffold but as a permanent
architectural component.

</details>


### [9] [TableMoE: Neuro-Symbolic Routing for Structured Expert Reasoning in Multimodal Table Understanding](https://arxiv.org/abs/2506.21393)
*Junwen Zhang,Pu Chen,Yin Zhang*

Main category: cs.AI

TL;DR: 提出专为复杂多模态表格推理而设计的TableMoE（神经-符号混合多专家架构），结合创新路由机制和大规模新数据集，在公开高难评测集上效果远超同类模型，具备高鲁棒性与可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前大规模多模态语言模型（MLLMs）在真实世界复杂表格（结构繁杂、符号密集、视觉退化如模糊、倾斜、水印、不完整结构等）理解任务中表现有限，泛化能力较差。因此，亟需新的方法实现更强鲁棒性与结构化推理能力。

Method: 提出TableMoE结构，这是一种神经-符号混合的多专家连接架构（MoCE），使用神经-符号路由机制预测表格元素的语义角色并据此动态分配给不同的专家模块（如Table-to-HTML、Table-to-JSON、Table-to-Code）。模型采用信心感知门控，结合符号推理图进行结构化决策。为支持对齐式预训练，作者构建了大规模TableMoE-Align数据集（含120万表格-HTML-JSON-代码四元组），涵盖多个真实领域。

Result: 实验结果显示TableMoE在四个表格多模态理解与推理高难基准测试（WMMFinQA、WMMTatQA、WMMTabDialog、WMMFinanceMath）上显著优于现有最先进模型。消融实验证明神经符号路由和结构专家对齐的关键作用，定性分析也展示了TableMoE的可解释性和鲁棒性增强。

Conclusion: TableMoE通过结合神经网络与符号推理，显著提升了模型在复杂、多模态退化表格场景下的结构化推理和理解能力。该架构为下一代多模态表格智能处理奠定了坚实基础。

Abstract: Multimodal understanding of tables in real-world contexts is challenging due
to the complexity of structure, symbolic density, and visual degradation (blur,
skew, watermarking, incomplete structures or fonts, multi-span or
hierarchically nested layouts). Existing multimodal large language models
(MLLMs) struggle with such WildStruct conditions, resulting in limited
performance and poor generalization. To address these challenges, we propose
TableMoE, a neuro-symbolic Mixture-of-Connector-Experts (MoCE) architecture
specifically designed for robust, structured reasoning over multimodal table
data. TableMoE features an innovative Neuro-Symbolic Routing mechanism, which
predicts latent semantic token roles (e.g., header, data cell, axis, formula)
and dynamically routes table elements to specialized experts (Table-to-HTML,
Table-to-JSON, Table-to-Code) using a confidence-aware gating strategy informed
by symbolic reasoning graphs. To facilitate effective alignment-driven
pretraining, we introduce the large-scale TableMoE-Align dataset, consisting of
1.2M table-HTML-JSON-code quadruples across finance, science, biomedicine and
industry, utilized exclusively for model pretraining. For evaluation, we curate
and release four challenging WildStruct benchmarks: WMMFinQA, WMMTatQA,
WMMTabDialog, and WMMFinanceMath, designed specifically to stress-test models
under real-world multimodal degradation and structural complexity. Experimental
results demonstrate that TableMoE significantly surpasses existing
state-of-the-art models. Extensive ablation studies validate each core
component, emphasizing the critical role of Neuro-Symbolic Routing and
structured expert alignment. Through qualitative analyses, we further showcase
TableMoE's interpretability and enhanced robustness, underscoring the
effectiveness of integrating neuro-symbolic reasoning for multimodal table
understanding.

</details>


### [10] [Spatial Mental Modeling from Limited Views](https://arxiv.org/abs/2506.21458)
*Baiqiao Yin,Qineng Wang,Pingyue Zhang,Jianshu Zhang,Kangrui Wang,Zihan Wang,Jieyu Zhang,Keshigeyan Chandrasegaran,Han Liu,Ranjay Krishna,Saining Xie,Manling Li,Jiajun Wu,Li Fei-Fei*

Main category: cs.AI

TL;DR: 本文针对VLM在空间心理建模上的不足，提出了新基准MindCube，并开发了“先建图后推理”方法，结合强化学习后模型准确率提升至70.7%。主动构建空间内部表示是提升模型空间理解力的关键。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型（VLMs）在根据有限视角想象完整场景方面，与人类相比存在显著不足。人类通过空间心理模型，有能力推理不可见空间，这种能力对于布局、视角以及动态推理等任务相当关键。目前缺乏针对模型空间心理建模能力的系统性评测。

Method: 提出了MindCube基准，包含21,154个问题及3,268张图片，从空间位置、朝向和动态等三个维度系统考察VLMs在空间心理建模上的能力。并探索了三种改进VLMs空间建模的方法：引入未见中间视图、自然语言推理链和认知地图。最终提出“先建图后推理”（map-then-reason）方法，即模型先学习生成认知地图，再基于此进行推理，并结合强化学习进一步优化。

Result: 提出的map-then-reason方法显著提升了模型表现，准确率从37.8%提升到60.8%，结合强化学习后进一步提升至70.7%。三种提升方案中，联合空间映射与推理的方式带来最大收益。

Conclusion: 通过推动视觉语言模型主动构建并利用内部空间结构表示，并通过灵活推理过程进行空间推断，可以显著提升VLMs对不可见空间的理解和泛化能力。

Abstract: Can Vision Language Models (VLMs) imagine the full scene from just a few
views, like humans do? Humans form spatial mental models, internal
representations of unseen space, to reason about layout, perspective, and
motion. Our new MindCube benchmark with 21,154 questions across 3,268 images
exposes this critical gap, where existing VLMs exhibit near-random performance.
Using MindCube, we systematically evaluate how well VLMs build robust spatial
mental models through representing positions (cognitive mapping), orientations
(perspective-taking), and dynamics (mental simulation for "what-if" movements).
We then explore three approaches to help VLMs approximate spatial mental
models, including unseen intermediate views, natural language reasoning chains,
and cognitive maps. The significant improvement comes from a synergistic
approach, "map-then-reason", that jointly trains the model to first generate a
cognitive map and then reason upon it. By training models to reason over these
internal maps, we boosted accuracy from 37.8% to 60.8% (+23.0%). Adding
reinforcement learning pushed performance even further to 70.7% (+32.9%). Our
key insight is that such scaffolding of spatial mental models, actively
constructing and utilizing internal structured spatial representations with
flexible reasoning processes, significantly improves understanding of
unobservable space.

</details>


### [11] [Ad-Hoc Human-AI Coordination Challenge](https://arxiv.org/abs/2506.21490)
*Tin Dizdarević,Ravi Hammond,Tobias Gessler,Anisoara Calinescu,Jonathan Cook,Matteo Gallici,Andrei Lupu,Jakob Nicolaus Foerster*

Main category: cs.AI

TL;DR: 本文提出了一种新的比赛环境和工具，为测试AI与人的协作能力提供了低成本、可复现的方法，并发布了基线和数据，有助于推动人机协作研究。


<details>
  <summary>Details</summary>
Motivation: 实现AI代理与人类之间的无缝协作对于现实应用至关重要，但目前在该领域仍面临巨大挑战。Hanabi是一款具备不完全信息、受限交流、推理和协同行动等要素的合作类纸牌游戏，是测试人机协调的理想平台，但由于人类评测的困难和成本高昂，其在该研究方向的应用有限。

Method: 作者提出了“即席人机协调挑战赛（AH2AC2）”，通过开发基于大规模人类数据集的“人类代理代理”作为评测对手，以提供稳健、廉价且可复现的人类行为代理。为鼓励数据高效方法的发展，公开了3079局游戏的数据集，同时刻意限制可用的人类游戏数据量。

Result: 文中给出了二人和三人Hanabi场景下的基线实验结果。代理的评测通过受控系统对外提供，而不公布完整代理，实现了公平的评测机制。

Conclusion: 该工作通过创新方法为Hanabi领域的人机协作研究提供了开放可复现的环境，降低了人类评测的成本和难度，同时提出了可用于公平对比的基准和数据集。

Abstract: Achieving seamless coordination between AI agents and humans is crucial for
real-world applications, yet it remains a significant open challenge. Hanabi is
a cooperative card game featuring imperfect information, constrained
communication, theory of mind requirements, and coordinated action -- making it
an ideal testbed for human-AI coordination. However, its use for human-AI
interaction has been limited by the challenges of human evaluation. In this
work, we introduce the Ad-Hoc Human-AI Coordination Challenge (AH2AC2) to
overcome the constraints of costly and difficult-to-reproduce human
evaluations. We develop \textit{human proxy agents} on a large-scale human
dataset that serve as robust, cheap, and reproducible human-like evaluation
partners in AH2AC2. To encourage the development of data-efficient methods, we
open-source a dataset of 3,079 games, deliberately limiting the amount of
available human gameplay data. We present baseline results for both two- and
three- player Hanabi scenarios. To ensure fair evaluation, we host the proxy
agents through a controlled evaluation system rather than releasing them
publicly. The code is available at
\href{https://github.com/FLAIROx/ah2ac2}{https://github.com/FLAIROx/ah2ac2}.

</details>


### [12] [Mind2Web 2: Evaluating Agentic Search with Agent-as-a-Judge](https://arxiv.org/abs/2506.21506)
*Boyu Gou,Zanming Huang,Yuting Ning,Yu Gu,Michael Lin,Weijian Qi,Andrei Kopanev,Botao Yu,Bernal Jiménez Gutiérrez,Yiheng Shu,Chan Hee Song,Jiaman Wu,Shijie Chen,Hanane Nour Moussa,Tianshu Zhang,Jian Xie,Yifei Li,Tianci Xue,Zeyi Liao,Kai Zhang,Boyuan Zheng,Zhaowei Cai,Viktor Rozgic,Morteza Ziyadi,Huan Sun,Yu Su*

Main category: cs.AI

TL;DR: 本文提出了Mind2Web 2基准与Agent-as-a-Judge评测框架，有效填补了agentic search系统在复杂任务下的评测空白，系统表现接近人类，推动领域发展。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型能够自主搜索和整合网络信息，Agentic search（具备自主性搜索的智能体）类系统如Deep Research不断涌现，极大地改变了用户与海量信息的交互方式。现有的评测方法和基准无法满足这些系统在复杂、开放性任务中的评估需求。

Method: 作者提出了Mind2Web 2评测基准，包含130个真实、高质量、长流程任务，这些任务要求智能体实时浏览网页并综合大量信息。基准由1000多个人工小时构建。同时，作者引入了Agent-as-a-Judge新型评测框架，通过树形评分标准让智能体自动评估答案的正确性及其引用来源。

Result: 综合评测了9个前沿agentic search系统及人类表现，并进行了错误分析。表现最好的Deep Research系统已能在用时减少一半的情况下达到人类50-70%的表现。

Conclusion: Mind2Web 2为agentic search系统的开发和测评提供了严格且创新的基础，有助于推动此类系统的发展。

Abstract: Agentic search such as Deep Research systems, where large language models
autonomously browse the web, synthesize information, and return comprehensive
citation-backed answers, represents a major shift in how users interact with
web-scale information. While promising greater efficiency and cognitive
offloading, the growing complexity and open-endedness of agentic search have
outpaced existing evaluation benchmarks and methodologies, which largely assume
short search horizons and static answers. In this paper, we introduce Mind2Web
2, a benchmark of 130 realistic, high-quality, and long-horizon tasks that
require real-time web browsing and extensive information synthesis, constructed
with over 1,000 hours of human labor. To address the challenge of evaluating
time-varying and complex answers, we propose a novel Agent-as-a-Judge
framework. Our method constructs task-specific judge agents based on a
tree-structured rubric design to automatically assess both answer correctness
and source attribution. We conduct a comprehensive evaluation of nine frontier
agentic search systems and human performance, along with a detailed error
analysis to draw insights for future development. The best-performing system,
OpenAI Deep Research, can already achieve 50-70% of human performance while
spending half the time, showing a great potential. Altogether, Mind2Web 2
provides a rigorous foundation for developing and benchmarking the next
generation of agentic search systems.

</details>


### [13] [PsyLite Technical Report](https://arxiv.org/abs/2506.21536)
*Fangjun Ding,Renyu Zhang,Xinyu Feng,Chengye Xie,Zheng Zhang,Yanting Zhang*

Main category: cs.AI

TL;DR: PsyLite是面向心理咨询的轻量大模型，经过优化提升了推理与安全能力，在多项评测中显著超越现有模型，并具备低资源消耗的优势，适用于实际环境部署。


<details>
  <summary>Details</summary>
Motivation: 随着数字技术的快速发展，AI驱动的心理咨询逐渐成为心理健康领域的重要研究方向。然而，现有模型在对话安全、细致场景处理及轻量化部署方面仍存在不足。

Method: 本研究提出了PsyLite——一个基于InternLM2.5-7B-chat基础模型开发的轻量级心理咨询大语言模型Agent。通过双阶段训练策略（混合蒸馏数据微调和ORPO偏好优化），提升模型的深度推理、心理咨询与安全对话能力。模型部署利用Ollama与Open WebUI，并通过Pipelines创建自定义工作流，还设计了条件RAG以适时引入幽默元素并增强安全性。同时采用量化技术减少硬件资源消耗。

Result: PsyLite在中文通用评测（CEval）、心理咨询专业评测（CPsyCounE）、对话安全评测（SafeDialBench）上均优于基线模型，特别是在心理咨询专业性（CPsyCounE分数提升47.6%）和对话安全性（SafeDialBench分数提升2.4%）上效果显著。此外，模型实现了低硬件资源部署（仅需5GB内存运行）。

Conclusion: PsyLite不仅在心理咨询专业能力和安全性上表现出色，还大大降低了部署门槛，适合资源受限环境中的心理咨询应用。

Abstract: With the rapid development of digital technology, AI-driven psychological
counseling has gradually become an important research direction in the field of
mental health. However, existing models still have deficiencies in dialogue
safety, detailed scenario handling, and lightweight deployment. To address
these issues, this study proposes PsyLite, a lightweight psychological
counseling large language model agent developed based on the base model
InternLM2.5-7B-chat. Through a two-stage training strategy (hybrid distillation
data fine-tuning and ORPO preference optimization), PsyLite enhances the
model's deep-reasoning ability, psychological counseling ability, and safe
dialogue ability. After deployment using Ollama and Open WebUI, a custom
workflow is created with Pipelines. An innovative conditional RAG is designed
to introduce crosstalk humor elements at appropriate times during psychological
counseling to enhance user experience and decline dangerous requests to
strengthen dialogue safety. Evaluations show that PsyLite outperforms the
baseline models in the Chinese general evaluation (CEval), psychological
counseling professional evaluation (CPsyCounE), and dialogue safety evaluation
(SafeDialBench), particularly in psychological counseling professionalism
(CPsyCounE score improvement of 47.6\%) and dialogue safety (\safe{} score
improvement of 2.4\%). Additionally, the model uses quantization technology
(GGUF q4\_k\_m) to achieve low hardware deployment (5GB memory is sufficient
for operation), providing a feasible solution for psychological counseling
applications in resource-constrained environments.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [14] [The Ideation-Execution Gap: Execution Outcomes of LLM-Generated versus Human Research Ideas](https://arxiv.org/abs/2506.20803)
*Chenglei Si,Tatsunori Hashimoto,Diyi Yang*

Main category: cs.CL

TL;DR: LLM生成的研究创意虽新颖，但在实际研究执行中效果逊于人类专家，存在明显创意-执行落差，表明不能只看创意阶段就评判研究想法的价值。


<details>
  <summary>Details</summary>
Motivation: 虽然大型语言模型（LLMs）在提出新颖研究想法方面表现出色，有些情况下甚至被评为比人类更具新意，但真正优秀的想法还应当在实际研究中产生更好效果。本论文旨在检验LLM生成的想法在实际执行后能否带来更优研究成果。

Method: 招募43名专家研究者，随机分配由LLM生成或由人类专家撰写的研究想法，每人实际执行超过100小时并撰写4页短文，总结实验过程和成果。所有项目由NLP领域专家进行盲审。比较同一想法在提出阶段和执行阶段的评审分数。

Result: 在所有评价指标上（新颖性、激动性、有效性及整体评分），LLM生成想法的分数在执行后下降幅度明显大于专家想法，显著收窄了在创意阶段LLM与人类的差距，部分指标甚至出现人类逆转领先。

Conclusion: 当前LLM虽能提出看似新奇的研究想法，但在实际执行后往往效果不及人类专家，表明LLM生成的想法存在“创意-执行落差”，提示仅依赖创意阶段评价难以判断研究想法的实际有效性。

Abstract: Large Language Models (LLMs) have shown promise in accelerating the
scientific research pipeline. A key capability for this process is the ability
to generate novel research ideas, and prior studies have found settings in
which LLM-generated research ideas were judged as more novel than human-expert
ideas. However, a good idea should not simply appear to be novel, it should
also result in better research after being executed. To test whether
AI-generated ideas lead to better research outcomes, we conduct an execution
study by recruiting 43 expert researchers to execute randomly-assigned ideas,
either written by experts or generated by an LLM. Each expert spent over 100
hours implementing the idea and wrote a 4-page short paper to document the
experiments. All the executed projects are then reviewed blindly by expert NLP
researchers. Comparing the review scores of the same ideas before and after
execution, the scores of the LLM-generated ideas decrease significantly more
than expert-written ideas on all evaluation metrics (novelty, excitement,
effectiveness, and overall; p < 0.05), closing the gap between LLM and human
ideas observed at the ideation stage. When comparing the aggregated review
scores from the execution study, we even observe that for many metrics there is
a flip in rankings where human ideas score higher than LLM ideas. This
ideation-execution gap highlights the limitations of current LLMs in generating
truly effective research ideas and the challenge of evaluating research ideas
in the absence of execution outcomes.

</details>


### [15] ["What's Up, Doc?": Analyzing How Users Seek Health Information in Large-Scale Conversational AI Datasets](https://arxiv.org/abs/2506.21532)
*Akshay Paruchuri,Maryam Aziz,Rohit Vartak,Ayman Ali,Best Uchehara,Xin Liu,Ishan Chatterjee,Monica Agrawal*

Main category: cs.CL

TL;DR: 本文创建并分析了HealthChat-11K健康对话数据集，揭示了用户与大语言模型进行健康咨询时的行为特点及潜在风险，强调了现有健康对话AI需在理解用户意图和应对引导性提问等方面进一步提升能力。


<details>
  <summary>Details</summary>
Motivation: 人们越来越习惯通过大语言模型获取医疗健康信息，但关于这些对话的本质及其潜在风险，学界了解甚少，因此亟需系统性研究以指导更安全、有效的AI健康助手开发。

Method: 研究者从大规模对话AI数据集中筛选并整理出11,000个真实健康相关对话，涵盖25,000条用户消息，并结合临床医生主导的用户互动分类法，系统分析21个健康专科领域的用户与LLM互动方式。

Result: 数据集展示了用户常见的互动模式，包括信息不完全、情感表达以及可能诱发LLM迎合用户错误认知的提问方式，为后续改进AI健康支持系统提供了数据基础和实证参考。

Conclusion: 通过对真实健康相关对话的系统分析，发现用户在与大语言模型互动时存在信息不完整、情绪化表达和引导性提问等风险，这凸显了现有对话式AI在健康支持能力上有改进空间。

Abstract: People are increasingly seeking healthcare information from large language
models (LLMs) via interactive chatbots, yet the nature and inherent risks of
these conversations remain largely unexplored. In this paper, we filter
large-scale conversational AI datasets to achieve HealthChat-11K, a curated
dataset of 11K real-world conversations composed of 25K user messages. We use
HealthChat-11K and a clinician-driven taxonomy for how users interact with LLMs
when seeking healthcare information in order to systematically study user
interactions across 21 distinct health specialties. Our analysis reveals
insights into the nature of how and why users seek health information, such as
common interactions, instances of incomplete context, affective behaviors, and
interactions (e.g., leading questions) that can induce sycophancy, underscoring
the need for improvements in the healthcare support capabilities of LLMs
deployed as conversational AI. Code and artifacts to retrieve our analyses and
combine them into a curated dataset can be found here:
https://github.com/yahskapar/HealthChat

</details>


### [16] [Towards Probabilistic Question Answering Over Tabular Data](https://arxiv.org/abs/2506.20747)
*Chen Shen,Sajjadur Rahman,Estevam Hruschka*

Main category: cs.CL

TL;DR: 本文提出了LUCARIO基准和概率QA新框架，通过贝叶斯网络与LLMs结合，解决了概率性表格问答难题，在实验中优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 目前基于表格数据的问答方法主要适用于可以从表格中直接查找答案的事实性问题，但在处理需要在不确定性下推理的概率性问题时表现不佳。为解决这一不足，作者提出了新的任务和方法。

Method: 提出了LUCARIO基准和一个新的概率问答框架：首先从表格中诱导贝叶斯网络，再将自然语言查询转换为概率查询，最后利用大型语言模型（LLMs）生成最终答案。方法结合了符号推理与神经网络。

Result: 实验证明该方法相比以往的基线方法有显著性能提升，验证了混合符号-神经推理方法的有效性。

Conclusion: 混合符号推理与大型语言模型，可以有效提升基于表格数据的概率性问答能力，适用于大规模表格数据的推理任务。

Abstract: Current approaches for question answering (QA) over tabular data, such as
NL2SQL systems, perform well for factual questions where answers are directly
retrieved from tables. However, they fall short on probabilistic questions
requiring reasoning under uncertainty. In this paper, we introduce a new
benchmark LUCARIO and a framework for probabilistic QA over large tabular data.
Our method induces Bayesian Networks from tables, translates natural language
queries into probabilistic queries, and uses large language models (LLMs) to
generate final answers. Empirical results demonstrate significant improvements
over baselines, highlighting the benefits of hybrid symbolic-neural reasoning.

</details>


### [17] [Multi-lingual Functional Evaluation for Large Language Models](https://arxiv.org/abs/2506.20793)
*Victor Ojewale,Inioluwa Deborah Raji,Suresh Venkatasubramanian*

Main category: cs.CL

TL;DR: 作者提出了两个新的多语言功能型基准，通过多语言实测发现，现有静态评测不能充分反映模型实际表现，不同基准间存在显著性能差异，阿拉伯语和英语表现更稳定。


<details>
  <summary>Details</summary>
Motivation: 目前主流多语言大模型（LLM）的能力主要通过静态数据集（如Belebele、M-MMLU和M-GSM）评测，但这些方法难以全面反映模型在实际多语言功能场景下的表现与鲁棒性。为此，作者提出更贴合实际应用的多语言功能型基准评测方法。

Method: 作者将现有的功能性英文基准（如数学和指令跟随任务）模板翻译成法语、西班牙语、印地语、阿拉伯语和约鲁巴语，并构建了CL-GSM Symbolic与CL-IFEval两个新的多语言基准。通过这两个基准，系统性评估主流大模型在多语言环境下的功能表现和鲁棒性。

Result: 实验证明不同静态多语言基准反映模型实际功能表现的程度有较大不同。例如，主流模型在M-GSM与CL-GSM Symbolic间在英语、法语、西班牙语上性能分别下降24%、17%、18%；Belebele与CL-IFEval间在多语言下性能下降15%-24%；M-MMLU与CL-IFEval的性能下降仅为0.5%-3%。另外，不同语言下模型表现鲁棒性差异显著，其中阿拉伯语和英语最为稳定、表现最好。

Conclusion: 单纯依赖静态多语言评测基准可能无法充分刻画模型实际能力，新提出的多语言功能性评测为评估LLM真实表现和鲁棒性提供了更有效的手段。评测过程中，不同基准的相关性以及语言间的鲁棒性需被重点关注。

Abstract: Multi-lingual competence in large language models is often evaluated via
static data benchmarks such as Belebele, M-MMLU and M-GSM. However, these
evaluations often fail to provide an adequate understanding of the practical
performance and robustness of models across multi-lingual settings. In
response, we create multi-lingual functional benchmarks -- Cross-Lingual Grade
School Math Symbolic (CL-GSM Symbolic) and Cross-Lingual Instruction-Following
Eval (CL-IFEval)-- by translating existing functional benchmark templates from
English to five additional languages that span the range of resources available
for NLP: French, Spanish, Hindi, Arabic and Yoruba. Our results reveal that
some static multi-lingual benchmarks capture functional performance much more
closely than others (i.e. across models, there is a 24%, 17% and 18% decrease
in performance between M-GSM and CL-GSM Symbolic in English, French and Spanish
respectively; similarly there's a 15 - 24% performance drop across languages
between Belebele and CL-IFEval, and only a 0.5% to 3% performance drop between
M-MMLU and CL-IFEval). Similarly, we find that model robustness across
languages varies significantly, with certain languages (eg. Arabic, English)
being the most consistently well performing across evaluation iterations.

</details>


### [18] [MultiFinRAG: An Optimized Multimodal Retrieval-Augmented Generation (RAG) Framework for Financial Question Answering](https://arxiv.org/abs/2506.20821)
*Chinmay Gondhalekar,Urjitkumar Patel,Fang-Chun Yeh*

Main category: cs.CL

TL;DR: 现有LLM和RAG方法难以处理长篇多模态金融文档。MultiFinRAG框架通过多模态批量处理、精确检索与分层策略，在普通硬件上将金融问答准确率提升了19个百分点，优于ChatGPT-4o免费版。


<details>
  <summary>Details</summary>
Motivation: 传统的大语言模型（LLM）和检索增强生成（RAG）方法在处理包含叙述文本、表格和复杂图形等多模态内容的金融文档时，常受到token限制、布局丢失以及跨模态上下文碎片化等问题的困扰。金融领域的问答任务需要跨越多模态内容进行联合推理，现有方法无法高效应对。

Method: 提出了MultiFinRAG框架。首先对表格和图像进行批量分组处理，使用轻量化、量化的开源多模态LLM来生成结构化JSON结果和摘要文本。然后，将这些输出与叙述性文本一同进行嵌入，并通过模态感知的相似度阈值进行精确检索。最终通过分层回退策略，动态从仅文本到文本+表格+图片扩展，实现有效的跨模态推理并减少无关噪声。

Result: MultiFinRAG在运行于普通硬件上时，针对包含文本、表格、图片和组合多模态推理的复杂金融问答任务，比ChatGPT-4o（免费版）高出19个百分点的准确率。

Conclusion: MultiFinRAG是专为金融领域复杂跨模态问答构建的高效RAG框架，实现了在实际硬件环境下远超现有主流模型的表现。

Abstract: Financial documents--such as 10-Ks, 10-Qs, and investor presentations--span
hundreds of pages and combine diverse modalities, including dense narrative
text, structured tables, and complex figures. Answering questions over such
content often requires joint reasoning across modalities, which strains
traditional large language models (LLMs) and retrieval-augmented generation
(RAG) pipelines due to token limitations, layout loss, and fragmented
cross-modal context. We introduce MultiFinRAG, a retrieval-augmented generation
framework purpose-built for financial QA. MultiFinRAG first performs multimodal
extraction by grouping table and figure images into batches and sending them to
a lightweight, quantized open-source multimodal LLM, which produces both
structured JSON outputs and concise textual summaries. These outputs, along
with narrative text, are embedded and indexed with modality-aware similarity
thresholds for precise retrieval. A tiered fallback strategy then dynamically
escalates from text-only to text+table+image contexts when necessary, enabling
cross-modal reasoning while reducing irrelevant context. Despite running on
commodity hardware, MultiFinRAG achieves 19 percentage points higher accuracy
than ChatGPT-4o (free-tier) on complex financial QA tasks involving text,
tables, images, and combined multimodal reasoning.

</details>


### [19] [Uncovering Hidden Violent Tendencies in LLMs: A Demographic Analysis via Behavioral Vignettes](https://arxiv.org/abs/2506.20822)
*Quintin Myers,Yanjun Gao*

Main category: cs.CL

TL;DR: 本文系统评估了6种主流大语言模型在应对包含人口变量的美国现实冲突情景下的表现，发现它们生成的文本与实际偏好存在分歧，且对不同群体展现出与常识相悖的暴力倾向，说明当前LLM在道德推理和无偏性方面存在明显不足。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）被广泛用于检测和应对网络暴力内容，其在处理现实中存在道德模糊性的情景下的推理能力尚未被充分检验。因此本研究旨在系统评估LLM在面对真实世界冲突情景时的道德与行为反应能力，以及其潜在的偏见。

Method: 本研究首次采用社会科学领域用于衡量人类对日常冲突反应的有效工具——暴力行为情景问卷（VBVQ）对LLM进行评估。为检测偏见，研究引入了基于角色的提示（persona-based prompting），对美国不同种族、年龄和地域特征进行变量控制。在统一的零样本（zero-shot）环境下，对6个不同组织及地缘政治背景下开发的LLM进行了系统测试。

Result: 1）LLM在表层文本生成时的反应与其内部对暴力反应的倾向存在分歧；2）LLM表现出的暴力倾向在不同人口统计特征下存在差异，且这些结果常常与犯罪学、社会科学和心理学中的既有结论相矛盾。

Conclusion: 目前的LLM在处理社会现实复杂的冲突和道德情景时，既存在潜在偏见，也显示出其价值观与内部决策机制的不一致性，这限制了其在实际暴力内容甄别与响应场景中的可信度和适用性。

Abstract: Large language models (LLMs) are increasingly proposed for detecting and
responding to violent content online, yet their ability to reason about morally
ambiguous, real-world scenarios remains underexamined. We present the first
study to evaluate LLMs using a validated social science instrument designed to
measure human response to everyday conflict, namely the Violent Behavior
Vignette Questionnaire (VBVQ). To assess potential bias, we introduce
persona-based prompting that varies race, age, and geographic identity within
the United States. Six LLMs developed across different geopolitical and
organizational contexts are evaluated under a unified zero-shot setting. Our
study reveals two key findings: (1) LLMs surface-level text generation often
diverges from their internal preference for violent responses; (2) their
violent tendencies vary across demographics, frequently contradicting
established findings in criminology, social science, and psychology.

</details>


### [20] [Decide less, communicate more: On the construct validity of end-to-end fact-checking in medicine](https://arxiv.org/abs/2506.20876)
*Sebastian Joseph,Lily Chen,Barry Wei,Michael Mackert,Iain J. Marshall,Paul Pu Liang,Ramez Kouzy,Byron C. Wallace,Junyi Jessy Li*

Main category: cs.CL

TL;DR: 医学领域自动事实核查系统应用有限，作者通过分析临床专家验证社交媒体声明过程，揭示了标准端到端事实核查在医学领域面临的根本性挑战，建议以互动式沟通视角推进医疗事实核查。


<details>
  <summary>Details</summary>
Motivation: 自动事实核查技术取得进展，且在公共健康和医疗领域需求迫切，但由于医学决策风险高、文献庞杂且专业，绝大多数用户医疗素养有限，这导致医学事实核查系统无法广泛应用。论文旨在探究背后原因并分析应用挑战。

Method: 论文采取实证方法，首次研究了临床专家如何通过合成医学证据来验证社交媒体上的真实医学相关声明，并从中分析和归纳了医疗事实核查系统面临的核心问题。

Result: 研究发现医学事实核查难点主要为：一是将现实世界中的声明与科学证据（如临床试验）相匹配存在困难；二是社交媒体声明经常表述不明确或意图模糊；三是真伪标签带有主观性。这些因素共同制约了医疗事实核查系统的实际应用。

Conclusion: 作者认为医学事实核查系统面临许多根本性挑战，如难以把现实世界的陈述和科学证据（如临床实验）有效关联、不明确或意图不明的表述、以及主观性较强的真伪判断。最终，作者建议事实核查应被视为互动式沟通问题进行研究和评估，而不是追求纯粹端到端方案。

Abstract: Technological progress has led to concrete advancements in tasks that were
regarded as challenging, such as automatic fact-checking. Interest in adopting
these systems for public health and medicine has grown due to the high-stakes
nature of medical decisions and challenges in critically appraising a vast and
diverse medical literature. Evidence-based medicine connects to every
individual, and yet the nature of it is highly technical, rendering the medical
literacy of majority users inadequate to sufficiently navigate the domain. Such
problems with medical communication ripens the ground for end-to-end
fact-checking agents: check a claim against current medical literature and
return with an evidence-backed verdict. And yet, such systems remain largely
unused. To understand this, we present the first study examining how clinical
experts verify real claims from social media by synthesizing medical evidence.
In searching for this upper-bound, we reveal fundamental challenges in
end-to-end fact-checking when applied to medicine: Difficulties connecting
claims in the wild to scientific evidence in the form of clinical trials;
ambiguities in underspecified claims mixed with mismatched intentions; and
inherently subjective veracity labels. We argue that fact-checking should be
approached and evaluated as an interactive communication problem, rather than
an end-to-end process.

</details>


### [21] [Optimising Language Models for Downstream Tasks: A Post-Training Perspective](https://arxiv.org/abs/2506.20917)
*Zhengyan Shi*

Main category: cs.CL

TL;DR: 本论文提出多种创新方法以提升大规模语言模型在实际任务中的适应能力，包括更好利用无标注数据、高效微调及新型评测体系，显著增强了模型的效率、鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型（LMs）规模和复杂度的提升，如何高效且稳健地适应具体任务成为一大挑战。传统方法在小规模任务数据集上容易过拟合，难以利用充足的无标注数据，并伴随高昂算力开销，这些问题限制了大规模语言模型在真实应用中的广泛使用。

Method: 论文提出了一系列改进LMs下游适应性的算法：1）通过提出创新型继续预训练方法，从无标注数据中提取任务相关知识，并优于当前最优的半监督方法；2）介绍了参数高效的微调技术，在降低计算和内存开销的同时保持良好性能；3）改进有监督微调方法，增强模型指令遵循能力，尤其是在标注数据稀缺时，为开放式生成等多个NLP任务提升性能；4）提出了新的评测方法和基准（如多跳空间推理任务），以全面评估语言模型的适应性和能力。

Result: 通过在多种NLP任务上的大量实证研究，结果显示这些方法显著提升了语言模型的稳健性、效率和泛化能力，使其更好地适应广泛实际应用场景。

Conclusion: 本论文为提升大规模语言模型的下游适应性、模型鲁棒性和计算效率提供了系统性方法，为实现更通用、更高效的人工智能迈出了重要一步。

Abstract: Language models (LMs) have demonstrated remarkable capabilities in NLP, yet
adapting them efficiently and robustly to specific tasks remains challenging.
As their scale and complexity grow, fine-tuning LMs on labelled data often
underutilizes available unlabelled data, leads to overfitting on small
task-specific sets, and imposes significant computational costs. These
limitations hamper their application to the open-ended landscape of real-world
language tasks.
  This thesis proposes a series of methods to better adapt LMs to downstream
applications. First, we explore strategies for extracting task-relevant
knowledge from unlabelled data, introducing a novel continued pre-training
technique that outperforms state-of-the-art semi-supervised approaches. Next,
we present a parameter-efficient fine-tuning method that substantially reduces
memory and compute costs while maintaining competitive performance. We also
introduce improved supervised fine-tuning methods that enable LMs to better
follow instructions, especially when labelled data is scarce, enhancing their
performance across a range of NLP tasks, including open-ended generation.
Finally, we develop new evaluation methods and benchmarks, such as multi-hop
spatial reasoning tasks, to assess LM capabilities and adaptation more
comprehensively.
  Through extensive empirical studies across diverse NLP tasks, our results
demonstrate that these approaches substantially improve LM robustness,
efficiency, and generalization, making them more adaptable to a broad range of
applications. These advances mark a significant step towards more robust and
efficient LMs, bringing us closer to the goal of artificial general
intelligence.

</details>


### [22] [FineWeb2: One Pipeline to Scale Them All -- Adapting Pre-Training Data Processing to Every Language](https://arxiv.org/abs/2506.20920)
*Guilherme Penedo,Hynek Kydlíček,Vinko Sabolčec,Bettina Messmer,Negar Foroutan,Amir Hossein Kargaran,Colin Raffel,Martin Jaggi,Leandro Von Werra,Thomas Wolf*

Main category: cs.CL

TL;DR: 论文提出了一套支持任意语言的大规模文本数据集筛选流程，显著提升了多语种LLM训练数据的质量，可扩展性强，并发布了新的超大多语种数据集FineWeb2。


<details>
  <summary>Details</summary>
Motivation: 目前多语言大型语言模型（LLMs）训练受限于高质量多语种语料库的缺乏，尤其是数据过滤和去重难以适应上百种语言。本文旨在解决多语种训练数据集筛选流程难以拓展的问题。

Method: 作者提出了一套基于FineWeb的数据集筛选流程，该流程可自动适配任意语言。设计过程在9种语言上进行了消融实验，并通过基于可测评标准选择的评测任务进行分析。此外，提出了一种简便的再平衡方法，兼顾数据重复率和质量。

Result: 该流程可生成优于现有数据集的非英语语料库，使训练出的模型性能更佳。再平衡方法进一步提升了模型表现。

Conclusion: 作者构建了FineWeb2：一个覆盖1000余种语言、包含20TB（50亿文档）的多语种语料库，并开源了流程、训练和评测代码。

Abstract: Pre-training state-of-the-art large language models (LLMs) requires vast
amounts of clean and diverse text data. While the open development of large
high-quality English pre-training datasets has seen substantial recent
progress, training performant multilingual LLMs remains a challenge, in large
part due to the inherent difficulty of tailoring filtering and deduplication
pipelines to a large number of languages. In this work, we introduce a new
pre-training dataset curation pipeline based on FineWeb that can be
automatically adapted to support any language. We extensively ablate our
pipeline design choices on a set of nine diverse languages, guided by a set of
meaningful and informative evaluation tasks that were chosen through a novel
selection process based on measurable criteria. Ultimately, we show that our
pipeline can be used to create non-English corpora that produce more performant
models than prior datasets. We additionally introduce a straightforward and
principled approach to rebalance datasets that takes into consideration both
duplication count and quality, providing an additional performance uplift.
Finally, we scale our pipeline to over 1000 languages using almost 100 Common
Crawl snapshots to produce FineWeb2, a new 20 terabyte (5 billion document)
multilingual dataset which we release along with our pipeline, training, and
evaluation codebases.

</details>


### [23] [KaLM-Embedding-V2: Superior Training Techniques and Data Inspire A Versatile Embedding Model](https://arxiv.org/abs/2506.20923)
*Xinping Zhao,Xinshuo Hu,Zifei Shan,Shouzheng Huang,Yao Zhou,Zetian Sun,Zhenyu Liu,Dongfang Li,Xinyuan Wei,Qian Chen,Youcheng Pan,Yang Xiang,Meishan Zhang,Haofen Wang,Jun Yu,Baotian Hu,Min Zhang*

Main category: cs.CL

TL;DR: 作者提出了KaLM-Embedding-V2，这是一种高效且紧凑的文本嵌入模型，采用全新架构设计、分阶段训练以及多样化数据，使其以不到1B参数显著超越同类，同时媲美更大模型，刷新了通用嵌入模型的性能标杆。


<details>
  <summary>Details</summary>
Motivation: 当前通用嵌入模型在保持紧凑体积和优良性能间存在权衡，本文旨在用全新技术突破小模型性能瓶颈，提升嵌入模型效能并促进泛化能力。

Method: 1）移除因果注意力掩码，采用完全双向Transformer架构，通过均值池化生成定长嵌入；2）多阶段训练流程：大规模弱监督语料预训练、高质量检索及非检索数据微调、模型“汤”参数平均以增强泛化；3）引入focal-style重新加权机制和在线难负例混合策略提升难样本学习与负例多样性；4）丰富的数据覆盖预训练和微调，共采集20余类预训练数据和100类微调数据。

Result: KaLM-Embedding-V2在MTEB中英双语任务上取得最佳或极具竞争力的表现，大幅度领先同参数量模型，在少于1B参数体量下可与远大于自身的模型实力媲美。

Conclusion: KaLM-Embedding-V2显著超越了同等规模的其它嵌入模型，在中英文任务上与更大规模（3倍、14倍、18倍和26倍参数量）模型媲美，树立了紧凑型通用嵌入模型的新标杆。

Abstract: In this paper, we propose KaLM-Embedding-V2, a versatile and compact
embedding model, which achieves impressive performance in general-purpose text
embedding tasks by leveraging superior training techniques and data. Our key
innovations include: (1) To better align the architecture with representation
learning, we remove the causal attention mask and adopt a fully bidirectional
transformer with simple yet effective mean-pooling to produce fixed-length
embeddings; (2) We employ a multi-stage training pipeline: (i) pre-training on
large-scale weakly supervised open-source corpora; (ii) fine-tuning on
high-quality retrieval and non-retrieval datasets; and (iii) model-soup
parameter averaging for robust generalization. Besides, we introduce a
focal-style reweighting mechanism that concentrates learning on difficult
samples and an online hard-negative mixing strategy to continuously enrich hard
negatives without expensive offline mining; (3) We collect over 20 categories
of data for pre-training and 100 categories of data for fine-tuning, to boost
both the performance and generalization of the embedding model. Extensive
evaluations on the Massive Text Embedding Benchmark (MTEB) Chinese and English
show that our model significantly outperforms others of comparable size, and
competes with 3x, 14x, 18x, and 26x larger embedding models, setting a new
standard for a versatile and compact embedding model with less than 1B
parameters.

</details>


### [24] [Can Gradient Descent Simulate Prompting?](https://arxiv.org/abs/2506.20989)
*Eric Zhang,Leshem Choshen,Jacob Andreas*

Main category: cs.CL

TL;DR: 本文提出将元训练与梯度更新相结合，使微调后语言模型能模仿prompting的泛化能力，无需真实标签即可学习新知识，实验显示该方法在相关任务上效果显著。


<details>
  <summary>Details</summary>
Motivation: 目前将新信息融入语言模型主要有两种方式：通过改变提示（prompting）或参数（如微调）。虽然参数更新无需长期存储，但在许多情况下，prompting效果更佳，模型能从单例进行更好推广和推理。因此，能否让微调效果媲美prompting成为一个问题。

Method: 本文提出了一种meta-training方法，通过梯度型元学习，训练语言模型使其参数更新能模拟prompt带来的效果。该方法用模型自身prompt预测作为训练目标，无需真实标签。

Result: 通过实验，梯度下降训练能够恢复部分甚至全部prompt模型的性能，尤其在‘逆转诅咒’任务和阅读理解类问题中，单步梯度更新后有明显提升。说明只需合适初始化，梯度下降具有很强表达性。

Conclusion: 本研究证明，通过适当的元训练，模型参数更新可以模拟prompt方式对新信息的利用，扩展了长上下文建模能力，也有助于理解梯度型学习的一般化能力。

Abstract: There are two primary ways of incorporating new information into a language
model (LM): changing its prompt or changing its parameters, e.g. via
fine-tuning. Parameter updates incur no long-term storage cost for model
changes. However, for many model updates, prompting is significantly more
effective: prompted models can generalize robustly from single examples and
draw logical inferences that do not occur under standard fine-tuning. Can
models be modified so that fine-tuning does emulate prompting? This paper
describes a method for meta-training LMs such that gradient updates emulate the
effects of conditioning on new information. Our approach uses tools from
gradient-based meta-learning but uses an LM's own prompted predictions as
targets, eliminating the need for ground-truth labels. Subsequent gradient
descent training recovers some (and occasionally all) of prompted model
performance -- showing improvement on the ``reversal curse'' tasks, and
answering questions about text passages after a single gradient update. These
results suggest that, with appropriate initialization, gradient descent can be
surprisingly expressive. Our results suggest new avenues for long-context
modeling and offer insight into the generalization capabilities of
gradient-based learning.

</details>


### [25] [SAC: A Framework for Measuring and Inducing Personality Traits in LLMs with Dynamic Intensity Control](https://arxiv.org/abs/2506.20993)
*Adithya Chittem,Aishna Shrivastava,Sai Tarun Pendela,Jagat Sesh Challa,Dhruv Kumar*

Main category: cs.CL

TL;DR: 本研究突破了大语言模型个性建模的粗粒度与不可控问题，提出能控制16种人格特质量及其强度的新方法，实现了更真实、细致、可控的人格模拟，为提升人机交互质量提供了新思路。


<details>
  <summary>Details</summary>
Motivation: 当前大多数针对大语言模型（LLM）个性建模的方法存在局限：一是普遍依赖于粗粒度的Big Five（OCEAN）人格框架，无法细致地描述多维人格特质；二是缺乏对特质强度的可控机制。为实现更细粒和可控的人格模拟，需扩展现有模型。

Method: 论文将Machine Personality Inventory（MPI）从仅使用Big Five模型拓展到包含16 Personality Factor（16PF）模型，实现对16种人格特质量的表现控制。同时引入了结构化的Specific Attribute Control（SAC）框架以评估与动态调控人格特质强度。具体方法包括基于形容词的语义锚定引导，并通过‘频率、深度、阈值、努力、意愿’五个强度因子设计行为问题，来引导与评估特质强度表达。

Result: 实验表明，将强度作为连续光谱建模比简单的二元开关更能实现一致且可控的人格表达。并且，目标特质强度的变化会系统性地影响心理学上相关的其它特质，显示LLM能内化多维人格结构，而非孤立处理各特质。

Conclusion: 本文首次实现了细粒度、可调控的大语言模型多维人格表达，并提出能引导与动态控制特质强度的结构化方法，为医疗、教育、面试等场景中的人机交互带来了更真实、细腻的个性体验。

Abstract: Large language models (LLMs) have gained significant traction across a wide
range of fields in recent years. There is also a growing expectation for them
to display human-like personalities during interactions. To meet this
expectation, numerous studies have proposed methods for modelling LLM
personalities through psychometric evaluations. However, most existing models
face two major limitations: they rely on the Big Five (OCEAN) framework, which
only provides coarse personality dimensions, and they lack mechanisms for
controlling trait intensity. In this paper, we address this gap by extending
the Machine Personality Inventory (MPI), which originally used the Big Five
model, to incorporate the 16 Personality Factor (16PF) model, allowing
expressive control over sixteen distinct traits. We also developed a structured
framework known as Specific Attribute Control (SAC) for evaluating and
dynamically inducing trait intensity in LLMs. Our method introduces
adjective-based semantic anchoring to guide trait intensity expression and
leverages behavioural questions across five intensity factors:
\textit{Frequency}, \textit{Depth}, \textit{Threshold}, \textit{Effort}, and
\textit{Willingness}. Through experimentation, we find that modelling intensity
as a continuous spectrum yields substantially more consistent and controllable
personality expression compared to binary trait toggling. Moreover, we observe
that changes in target trait intensity systematically influence closely related
traits in psychologically coherent directions, suggesting that LLMs internalize
multi-dimensional personality structures rather than treating traits in
isolation. Our work opens new pathways for controlled and nuanced human-machine
interactions in domains such as healthcare, education, and interviewing
processes, bringing us one step closer to truly human-like social machines.

</details>


### [26] [Large Language Models Acing Chartered Accountancy](https://arxiv.org/abs/2506.21031)
*Jatin Gupta,Akhil Sharma,Saransh Singhania,Mohammad Adnan,Sakshi Deo,Ali Imam Abidi,Keshav Gupta*

Main category: cs.CL

TL;DR: 本文构建了基于印度会计考试的CA-Ben评测体系，对六种LLM在金融、法律、数据推理上的表现进行了系统评测，发现其概念推理占优但数值与法律分析存在短板，建议结合混合推理和外部检索技术优化模型。


<details>
  <summary>Details</summary>
Motivation: 虽然NLP和LLM在金融领域应用广泛，但对其是否真正掌握和应用特定金融领域知识仍存疑，尤其在印度规模庞大的金融环境下缺乏相关基准和客观评估。

Method: 设计并提出了CA-Ben基准，基于印度特许会计师（CA）考试不同阶段的真实数据集，对六种主流LLM在金融、法律和定量推理能力上进行标准化评测和对比。

Result: 评测显示大型模型在不同维度表现差异明显，Claude 3.5 Sonnet和GPT-4o在概念和法律推理上最佳，但所有模型在数值与法律解释任务上均遇到挑战。结果揭示了目前模型的优劣势和未来改进方向。

Conclusion: 现有的大型语言模型（LLMs）在金融领域有突出表现，尤其在概念和法律推理方面如Claude 3.5 Sonnet和GPT-4o表现优越，但在数值计算和法律解释准确性上仍有不足。未来需通过混合推理和检索增强的方法优化其在定量分析和法律解释方面的能力。

Abstract: Advanced intelligent systems, particularly Large Language Models (LLMs), are
significantly reshaping financial practices through advancements in Natural
Language Processing (NLP). However, the extent to which these models
effectively capture and apply domain-specific financial knowledge remains
uncertain. Addressing a critical gap in the expansive Indian financial context,
this paper introduces CA-Ben, a Chartered Accountancy benchmark specifically
designed to evaluate the financial, legal, and quantitative reasoning
capabilities of LLMs. CA-Ben comprises structured question-answer datasets
derived from the rigorous examinations conducted by the Institute of Chartered
Accountants of India (ICAI), spanning foundational, intermediate, and advanced
CA curriculum stages. Six prominent LLMs i.e. GPT 4o, LLAMA 3.3 70B, LLAMA 3.1
405B, MISTRAL Large, Claude 3.5 Sonnet, and Microsoft Phi 4 were evaluated
using standardized protocols. Results indicate variations in performance, with
Claude 3.5 Sonnet and GPT-4o outperforming others, especially in conceptual and
legal reasoning. Notable challenges emerged in numerical computations and legal
interpretations. The findings emphasize the strengths and limitations of
current LLMs, suggesting future improvements through hybrid reasoning and
retrieval-augmented generation methods, particularly for quantitative analysis
and accurate legal interpretation.

</details>


### [27] [A Semi-supervised Scalable Unified Framework for E-commerce Query Classification](https://arxiv.org/abs/2506.21049)
*Chunyuan Yuan,Chong Zhang,Zheng Fang,Ming Pang,Xue Jiang,Changping Peng,Zhangang Lin,Ching Law*

Main category: cs.CL

TL;DR: 针对电商短查询、信息不足及现有查询分类方法局限，本文提出了半监督可扩展统一框架（SSUF），整合知识增强、标签增强和结构增强模块。该框架通过多模块合作提升查询分类性能，且广泛实验表明在准确性和效率上均显著优于主流方法。


<details>
  <summary>Details</summary>
Motivation: 电商中的查询分类任务受限于查询短小且信息不足，导致模型先验信息有限。同时，现有方法依赖用户后验点击行为生成训练样本，形成马太效应恶性循环，并且各子任务缺乏统一框架，算法优化效率低。

Method: 提出了一个新颖的半监督可扩展统一框架（SSUF），包含知识增强模块（利用世界知识丰富查询表示）、标签增强模块（结合标签语义与半监督信号，降低对后验标签依赖）、结构增强模块（基于复杂标签关系加强表示）。每个模块均为高度可插拔，输入特征可根据子任务增减。

Result: 通过大量离线及在线A/B实验，SSUF显著优于现有最优模型。

Conclusion: SSUF系统性地解决了查询信息贫乏、过度依赖后验样本和子任务割裂的问题，提升了查询分类效果和算法优化效率。

Abstract: Query classification, including multiple subtasks such as intent and category
prediction, is vital to e-commerce applications. E-commerce queries are usually
short and lack context, and the information between labels cannot be used,
resulting in insufficient prior information for modeling. Most existing
industrial query classification methods rely on users' posterior click behavior
to construct training samples, resulting in a Matthew vicious cycle.
Furthermore, the subtasks of query classification lack a unified framework,
leading to low efficiency for algorithm optimization.
  In this paper, we propose a novel Semi-supervised Scalable Unified Framework
(SSUF), containing multiple enhanced modules to unify the query classification
tasks. The knowledge-enhanced module uses world knowledge to enhance query
representations and solve the problem of insufficient query information. The
label-enhanced module uses label semantics and semi-supervised signals to
reduce the dependence on posterior labels. The structure-enhanced module
enhances the label representation based on the complex label relations. Each
module is highly pluggable, and input features can be added or removed as
needed according to each subtask. We conduct extensive offline and online A/B
experiments, and the results show that SSUF significantly outperforms the
state-of-the-art models.

</details>


### [28] [MT2-CSD: A New Dataset and Multi-Semantic Knowledge Fusion Method for Conversational Stance Detection](https://arxiv.org/abs/2506.21053)
*Fuqiang Niu,Genan Dai,Yisha Lu,Jiayu Liao,Xiang Li,Hu Huang,Bowen Zhang*

Main category: cs.CL

TL;DR: 文章构建了目前最大、最具对话深度的多目标多轮对话立场检测数据集MT2-CSD，并提出融合大模型推理优势的LLM-CRAN模型，在数据集上显著优于现有主流方法。


<details>
  <summary>Details</summary>
Motivation: 现有的立场检测研究大多针对单一实例，不适应社交媒体常见的多方、多轮对话场景，且缺乏能真实反映社交互动动态的数据集，阻碍了对话立场检测的进展。

Method: 提出了MT2-CSD 一个多目标、多轮对话立场检测的大型数据集，并提出融合大模型推理能力的LLM-CRAN模型来提升对话立场检测能力。

Result: LLM-CRAN模型在MT2-CSD数据集上进行了广泛实验，实验结果表明LLM-CRAN显著优于强基线模型。

Conclusion: 本文丰富了对话立场检测研究，对实际社交媒体多方对话场景建模提供了新数据集和有效模型，推动了领域的技术进展。

Abstract: In the realm of contemporary social media, automatic stance detection is
pivotal for opinion mining, as it synthesizes and examines user perspectives on
contentious topics to uncover prevailing trends and sentiments. Traditional
stance detection research often targets individual instances, thereby limiting
its capacity to model multi-party discussions typical in real social media
scenarios. This shortcoming largely stems from the scarcity of datasets that
authentically capture the dynamics of social media interactions, hindering
advancements in conversational stance detection. In this paper, we introduce
MT2-CSD, a comprehensive dataset for multi-target, multi-turn conversational
stance detection. To the best of our knowledge, MT2-CSD is the largest dataset
available for this purpose, comprising 24,457 annotated instances and
exhibiting the greatest conversational depth, thereby presenting new challenges
for stance detection. To address these challenges, we propose the Large
Language model enhanced Conversational Relational Attention Network (LLM-CRAN),
which exploits the reasoning capabilities of LLMs to improve conversational
understanding. We conduct extensive experiments to evaluate the efficacy of
LLM-CRAN on the MT2-CSD dataset. The experimental results indicate that
LLM-CRAN significantly outperforms strong baseline models in the task of
conversational stance detection.

</details>


### [29] [CBF-AFA: Chunk-Based Multi-SSL Fusion for Automatic Fluency Assessment](https://arxiv.org/abs/2506.20243)
*Papa Séga Wade,Mihai Andries,Ioannis Kanellos,Thierry Moudenc*

Main category: cs.CL

TL;DR: 该文提出一种基于chunk的多SSL特征融合框架，通过精细语音分割、特征加权及深度网络，提升流利度评估表现，并在公开数据集上超越现有主流方法。


<details>
  <summary>Details</summary>
Motivation: 自动流利度评估（AFA）难以准确捕捉非母语者的语音节奏、停顿与不流利现象，需提出更细粒度且能整合多模态信息的新方法。

Method: 采用Silero-VAD对语音分割为breath-group chunk，并结合多种自监督学习（SSL）模型（Wav2Vec2、HuBERT、WavLM）提取嵌入，通过可学习加权融合机制和chunk级流利度标记（如语速、停顿、n-gram重复）特征，最后用CNN-BiLSTM对chunk序列建模。

Result: 在Speechocean762上F1提升2.8分、Pearson相关提升6.2分；在Avalinguo上F1提升4.2分、Pearson提升4分，均优于Pyannote.audio分割及单SSL基线。

Conclusion: 基于chunk的多SSL融合方法能够实现更为鲁棒和细致的口语流利度评估，相较于传统SSL方法和现有分割基线有显著性能提升。

Abstract: Automatic fluency assessment (AFA) remains challenging, particularly in
capturing speech rhythm, pauses, and disfluencies in non-native speakers. We
introduce a chunk-based approach integrating self-supervised learning (SSL)
models (Wav2Vec2, HuBERT, and WavLM) selected for their complementary strengths
in phonetic, prosodic, and noisy speech modeling, with a hierarchical
CNN-BiLSTM framework. Speech is segmented into breath-group chunks using Silero
voice activity detection (Silero-VAD), enabling fine-grained temporal analysis
while mitigating over-segmentation artifacts. SSL embeddings are fused via a
learnable weighted mechanism, balancing acoustic and linguistic features, and
enriched with chunk-level fluency markers (e.g., speech rate, pause durations,
n-gram repetitions). The CNN-BiLSTM captures local and long-term dependencies
across chunks. Evaluated on Avalinguo and Speechocean762, our approach improves
F1-score by 2.8 and Pearson correlation by 6.2 points over single SSL baselines
on Speechocean762, with gains of 4.2 F1-score and 4.0 Pearson points on
Avalinguo, surpassing Pyannote.audio-based segmentation baselines. These
findings highlight chunk-based multi-SSL fusion for robust fluency evaluation,
though future work should explore generalization to dialects with irregular
prosody.

</details>


### [30] [DALR: Dual-level Alignment Learning for Multimodal Sentence Representation Learning](https://arxiv.org/abs/2506.21096)
*Kang He,Yuzhe Ding. Haining Wang,Fei Li,Chong Teng,Donghong Ji*

Main category: cs.CL

TL;DR: 本文提出DALR方法，通过细粒度对齐和排序蒸馏，缓解跨模态和模态内部偏差，在文本相似度和迁移任务上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前多模态句子表示学习虽然取得了较高性能，但现有方法往往只在粗粒度上对齐图片与文本，容易出现跨模态对齐偏差和模态内部的语义分歧问题，进而降低句子表示质量。

Method: 提出DALR方法，包含两大创新：（1）通过一致性学习模块，对负样本柔化并借助辅助任务的语义相似性，实现细粒度的跨模态对齐；（2）提出句子关系不仅限于二元正负标签，而是更复杂的排序结构，因而将排序蒸馏和全局模态内对齐学习结合，提升表达能力。

Result: 在语义文本相似度（STS）和迁移任务（TR）上的实验，DALR方法性能优于现有主流方法，证明了其有效性和优越性。

Conclusion: DALR在解决多模态句子表达中的跨模态对齐与模态内部语义分歧方面取得显著进展，能够显著提升句子表征效果，优于SOTA方法。

Abstract: Previous multimodal sentence representation learning methods have achieved
impressive performance. However, most approaches focus on aligning images and
text at a coarse level, facing two critical challenges:cross-modal misalignment
bias and intra-modal semantic divergence, which significantly degrade sentence
representation quality. To address these challenges, we propose DALR
(Dual-level Alignment Learning for Multimodal Sentence Representation). For
cross-modal alignment, we propose a consistency learning module that softens
negative samples and utilizes semantic similarity from an auxiliary task to
achieve fine-grained cross-modal alignment. Additionally, we contend that
sentence relationships go beyond binary positive-negative labels, exhibiting a
more intricate ranking structure. To better capture these relationships and
enhance representation quality, we integrate ranking distillation with global
intra-modal alignment learning. Comprehensive experiments on semantic textual
similarity (STS) and transfer (TR) tasks validate the effectiveness of our
approach, consistently demonstrating its superiority over state-of-the-art
baselines.

</details>


### [31] [ComRAG: Retrieval-Augmented Generation with Dynamic Vector Stores for Real-time Community Question Answering in Industry](https://arxiv.org/abs/2506.21098)
*Qinwen Chen,Wenbiao Tao,Zhiwei Zhu,Mingfan Xi,Liangzhong Guo,Yuan Wang,Wei Wang,Yunshi Lan*

Main category: cs.CL

TL;DR: 本文提出了ComRAG，一种基于记忆机制的检索增强生成框架，可整合历史问答与领域知识，在多个工业数据集上取得了显著性能和效率提升。


<details>
  <summary>Details</summary>
Motivation: 社区问答（CQA）平台在社区知识库中起到重要作用，但如何在实际应用中有效利用历史交互和外部领域知识仍是一大挑战。现有方法对外部知识利用不足、未能整合动态历史问答上下文或缺乏适用于工业部署的记忆机制。

Method: 提出了ComRAG框架，通过一种基于中心体的记忆机制，将静态知识与动态历史问答对结合，支持检索、生成和高效存储，实现了检索增强生成（RAG）方法。

Result: 在三个工业CQA数据集上，ComRAG在所有基线算法上均获得显著优势：向量相似度提升高达25.9%、延迟降低8.7%至23.3%、并有效控制每轮数据增长率，从20.23%降至2.06%。

Conclusion: ComRAG能够更有效地整合静态和动态知识，提升CQA系统的性能和效率，具有工业应用价值。

Abstract: Community Question Answering (CQA) platforms can be deemed as important
knowledge bases in community, but effectively leveraging historical
interactions and domain knowledge in real-time remains a challenge. Existing
methods often underutilize external knowledge, fail to incorporate dynamic
historical QA context, or lack memory mechanisms suited for industrial
deployment. We propose ComRAG, a retrieval-augmented generation framework for
real-time industrial CQA that integrates static knowledge with dynamic
historical QA pairs via a centroid-based memory mechanism designed for
retrieval, generation, and efficient storage. Evaluated on three industrial CQA
datasets, ComRAG consistently outperforms all baselines--achieving up to 25.9%
improvement in vector similarity, reducing latency by 8.7% to 23.3%, and
lowering chunk growth from 20.23% to 2.06% over iterations.

</details>


### [32] [Progtuning: Progressive Fine-tuning Framework for Transformer-based Language Models](https://arxiv.org/abs/2506.21119)
*Xiaoshuang Ji,Zhendong Zhao,Xiaojun Chen,Xin Zhao,Zeyao Liu*

Main category: cs.CL

TL;DR: 本文提出Progtuning，一种对Transformer高贡献块进行逐步微调的新方法，减少约25%的参数更新量同时性能不降，资源分配更高效，适配广泛。


<details>
  <summary>Details</summary>
Motivation: 随着Transformer模型规模的增大，全部参数的微调成本昂贵，并且现有高效微调方法未考虑各块贡献度差异，导致资源分配低效。该研究旨在提升微调中的参数和计算资源利用效率。

Method: 提出了Progtuning框架，结合渐进式学习方式，只对部分高贡献度Transformer块进行更新。具体过程为：根据不同块对任务的贡献逐步减少需要细调的块数，从而降低更新参数总量。

Result: Progtuning方法减少了约25%的参数更新量(对比原始方法)，性能依然具有竞争力，并能很好地适配其他参数高效微调方法，在多种场景下展现优异表现。

Conclusion: Progtuning通过对贡献度高的Transformer块逐步精细调整，有效减少参数更新量，可优化计算资源、提升参数效率，同时保持良好的下游任务表现。

Abstract: Fine-tuning is a promising technique for leveraging Transformer-based
language models in downstream tasks. As model sizes continue to grow, updating
all model parameters becomes increasingly costly. Parameter-efficient
fine-tuning methods effectively address this issue by selectively updating a
small subset of parameters. However, fine-tuning and most existing
parameter-efficient fine-tuning methods require updating the same number of
parameters as the initial size, ignoring the unequal contribution across
Transformer blocks and leading to extremely inefficient allocation of computing
resources. In this paper, we propose Progtuning, the novel fine-tuning
framework combined with progressive learning for Transformer-based language
models. Specifically, Progtuning progressively reduces the number of updated
transformer blocks based on the contribution. Remarkably, Progtuning optimizes
resource allocation and reduces the number of updated parameters by
approximately 25\%, while still maintaining competitive performance. And it
also exhibits high adaptability with parameter-efficient fine-tuning methods,
demonstrating excellent performance across various adaptation scenarios.

</details>


### [33] [Compressed and Smooth Latent Space for Text Diffusion Modeling](https://arxiv.org/abs/2506.21170)
*Viacheslav Meshchaninov,Egor Chimbulatov,Alexander Shabalin,Aleksandr Abramov,Dmitry Vetrov*

Main category: cs.CL

TL;DR: Cosmos 利用自动编码器学得的潜在空间，实现了并行高效的文本扩散生成，质量和速度均优于传统自回归和扩散模型，为文本生成提供了新的强有力的技术路线。


<details>
  <summary>Details</summary>
Motivation: 自回归语言模型虽然在现代文本生成中占主导地位，但由于其顺序生成的特性，导致解码速度慢且难以保证全局连贯性。扩散模型能够实现并行生成和灵活控制，但高维的 token 级表示又成为实际应用的障碍。

Method: 提出了一种新的文本生成方法 Cosmos。该方法利用自动编码器，将文本表征压缩到为扩散过程专门设计的平滑潜在空间中。该自动编码器在训练时同时考虑了 token 级重构和与预训练语言编码器激活对齐，从而实现语义上的稳健支撑和高效的数据增强。

Result: 实验证明：Cosmos 能将文本表征压缩 8 倍的同时，生成质量与 token 级扩散模型相当。进一步增加潜在序列长度后，Cosmos 甚至超过了扩散模型和自回归方法的基线结果。Cosmos 在故事生成、问答生成、摘要和去毒化四个多样化任务上评测，生成质量在多种生成范式下可比肩甚至超越现有方法，且推理速度提升超过 2 倍。

Conclusion: Cosmos 实现了高效、潜在空间驱动的文本扩散生成，在保持或提升文本质量的同时显著加速推理，展示了在文本生成领域替代自回归模型的潜力。

Abstract: Autoregressive language models dominate modern text generation, yet their
sequential nature introduces fundamental limitations: decoding is slow, and
maintaining global coherence remains challenging. Diffusion models offer a
promising alternative by enabling parallel generation and flexible control;
however, their application to text generation is hindered by the high
dimensionality of token-level representations. We introduce Cosmos, a novel
approach to text generation that operates entirely in a compressed, smooth
latent space tailored specifically for diffusion. This space is learned using
an autoencoder trained simultaneously for token-level reconstruction and
alignment with frozen activations from a pretrained language encoder, providing
robust semantic grounding and enabling effective perturbation-based
augmentations. Empirically, we demonstrate that text representations can be
compressed by $8\times$ while maintaining generation quality comparable to
token-level diffusion models. Furthermore, increasing the latent sequence
length allows Cosmos to surpass both diffusion-based and autoregressive
baselines. We evaluate Cosmos on four diverse generative tasks including story
generation, question generation, summarization, and detoxification and compare
it with various generative paradigms. Cosmos achieves comparable or superior
generation quality while offering more than $2\times$ faster inference.

</details>


### [34] [Maintaining MTEB: Towards Long Term Usability and Reproducibility of Embedding Benchmarks](https://arxiv.org/abs/2506.21182)
*Isaac Chung,Imene Kerboua,Marton Kardos,Roman Solomatin,Kenneth Enevoldsen*

Main category: cs.CL

TL;DR: 本文总结了MTEB基准在工程实践方面的关键举措，通过健全的持续集成、自动化和社区贡献管理，保障了平台的可复现性和扩展性，为同类平台维护者提供了宝贵经验。


<details>
  <summary>Details</summary>
Motivation: MTEB（Massive Text Embedding Benchmark）已成为文本嵌入模型评价的标准平台。作者关注于保证MTEB可持续复现性和可扩展性的工程实现方面。

Method: 维护健壮的持续集成（CI）流程，验证数据集完整性，自动化测试执行，评估基准结果的泛化能力。设计增强可复现性和可用性的方案，并探讨社区贡献和任务扩展策略。

Result: 这些工程实践提升了MTEB的全面性和质量，同时保持其在领域内的相关性。作者积累了对相似基准框架维护者有价值的经验。

Conclusion: 良好的工程实践对于保障基准平台的可复现性、可用性和可扩展性至关重要。作者的方法为维护机器学习评价框架提供了重要指导。

Abstract: The Massive Text Embedding Benchmark (MTEB) has become a standard evaluation
platform for text embedding models. While previous work has established the
core benchmark methodology, this paper focuses on the engineering aspects that
ensure MTEB's continued reproducibility and extensibility. We present our
approach to maintaining robust continuous integration pipelines that validate
dataset integrity, automate test execution, and assess benchmark results'
generalizability. We detail the design choices that collectively enhance
reproducibility and usability. Furthermore, we discuss our strategies for
handling community contributions and extending the benchmark with new tasks and
datasets. These engineering practices have been instrumental in scaling MTEB to
become more comprehensive while maintaining quality and, ultimately, relevance
to the field. Our experiences offer valuable insights for benchmark maintainers
facing similar challenges in ensuring reproducibility and usability in machine
learning evaluation frameworks. The MTEB repository is available at:
https://github.com/embeddings-benchmark/mteb

</details>


### [35] [Prompt-Guided Turn-Taking Prediction](https://arxiv.org/abs/2506.21191)
*Koji Inoue,Mikey Elmers,Yahui Fu,Zi Haur Pang,Divesh Lala,Keiko Ochi,Tatsuya Kawahara*

Main category: cs.CL

TL;DR: 该论文提出了一种能通过文本提示灵活控制轮流交互模型的创新方法，在大规模数据集评测中取得了更高精度和自适应表现，有效解决了现有模型缺乏灵活控制的问题。


<details>
  <summary>Details</summary>
Motivation: 在对话系统和对话机器人中，实现合适的轮流交互（turn-taking）对于实现自然的交流至关重要，现有方法虽然使用了transformer架构进行实时预测，但缺乏灵活和直观的控制。

Method: 提出了一种基于transformer的模型，将文本提示（如“更快”“更冷静”）的嵌入引入声活动投射模型（VAP），使用户能够动态、显式地通过文本指令控制轮流交互预测。对于无现成提示语的数据，利用大语言模型（LLM）生成合成提示，实现在模型训练中的补充。

Result: 模型在950小时的人类对话数据上进行评估，结果显示，加入文本提示的模型不仅提升了预测准确率，还能根据不同文本指令灵活调整轮流交互的行为。

Conclusion: 将文本指令集成到turn-taking预测模型中，可以显著提高轮流交互的自然性和灵活性，便于适应多样化的对话场景。

Abstract: Turn-taking prediction models are essential components in spoken dialogue
systems and conversational robots. Recent approaches leverage transformer-based
architectures to predict speech activity continuously and in real-time. In this
study, we propose a novel model that enables turn-taking prediction to be
dynamically controlled via textual prompts. This approach allows intuitive and
explicit control through instructions such as "faster" or "calmer" adapting
dynamically to conversational partners and contexts. The proposed model builds
upon a transformer-based voice activity projection (VAP) model, incorporating
textual prompt embeddings into both channel-wise transformers and a
cross-channel transformer. We evaluated the feasibility of our approach using
over 950 hours of human-human spoken dialogue data. Since textual prompt data
for the proposed approach was not available in existing datasets, we utilized a
large language model (LLM) to generate synthetic prompt sentences. Experimental
results demonstrated that the proposed model improved prediction accuracy and
effectively varied turn-taking timing behaviors according to the textual
prompts.

</details>


### [36] [Enhancing Automatic Term Extraction with Large Language Models via Syntactic Retrieval](https://arxiv.org/abs/2506.21222)
*Yongchan Chun,Minhyuk Kim,Dongjun Kim,Chanjun Park,Heuiseok Lim*

Main category: cs.CL

TL;DR: 本文提出了一种基于句法检索的演示选择方法，用于提升LLM在自动术语提取任务中的表现。通过在不同领域和基准上的实验，验证了该方法对F1分数的提升及句法提示对LLM术语提取能力的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有的自动术语提取（ATE）任务尚未充分探索大型语言模型（LLM）的潜力。常用的展示样本选择方法多依赖语义相似性，可能不适用于ATE对术语边界的精确定位。

Method: 提出了一种基于检索的prompting方法，在小样本环境下依据句法相似性（而非语义相似性）选择演示样例，这种方法具有领域无关性，更能准确指导模型识别术语边界。

Result: 在三个专业ATE基准上的实验表明，句法相似性检索能够提升F1分数。进一步分析表明，查询句与检索样例间的词汇重叠对表现有影响。

Conclusion: 句法线索对于将LLM适配于术语提取任务具有重要意义，句法检索方法能有效提升术语抽取的效果。

Abstract: Automatic Term Extraction (ATE) identifies domain-specific expressions that
are crucial for downstream tasks such as machine translation and information
retrieval. Although large language models (LLMs) have significantly advanced
various NLP tasks, their potential for ATE has scarcely been examined. We
propose a retrieval-based prompting strategy that, in the few-shot setting,
selects demonstrations according to \emph{syntactic} rather than semantic
similarity. This syntactic retrieval method is domain-agnostic and provides
more reliable guidance for capturing term boundaries. We evaluate the approach
in both in-domain and cross-domain settings, analyzing how lexical overlap
between the query sentence and its retrieved examples affects performance.
Experiments on three specialized ATE benchmarks show that syntactic retrieval
improves F1-score. These findings highlight the importance of syntactic cues
when adapting LLMs to terminology-extraction tasks.

</details>


### [37] [Agent-RewardBench: Towards a Unified Benchmark for Reward Modeling across Perception, Planning, and Safety in Real-World Multimodal Agents](https://arxiv.org/abs/2506.21252)
*Tianyi Men,Zhuoran Jin,Pengfei Cao,Yubo Chen,Kang Liu,Jun Zhao*

Main category: cs.CL

TL;DR: 作者针对当前多模态大模型智能体缺乏有效奖励模型评估，提出了Agent-RewardBench多维度高质量基准，能细粒度评价奖励建模能力。实验发现SOTA模型表现有限，凸显该方向的重要性。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大模型（MLLMs）快速发展，相关多模态智能体在实际任务（如网页导航、具身智能等）展现出应用前景。但受限于缺乏外部反馈，这些智能体的自我纠错与泛化能力较差。利用奖励模型作为外部反馈已被认为是有前景的方法，但如何为智能体选择合适的奖励模型目前尚无定论。因此，急需专门面向智能体的奖励基准。

Method: 提出Agent-RewardBench基准，用于评估多模态大模型在奖励建模（reward modeling）上的能力。该基准具备三个关键特点：（1）多维度、涵盖真实场景，涉及感知、规划与安全等7个子场景；（2）支持步骤级奖励评估，可细粒度分析智能体在每一步任务中的表现；（3）控制任务难度且人工校验数据，保证挑战性及数据质量。基准样本来自10种多样化模型，经难度控制与人工验证。

Result: 实验表明，即使是当前最先进的多模态模型在奖励建模能力上表现仍有限，显示出该方向仍需专门训练。

Conclusion: Agent-RewardBench是首个专门针对多模态智能体奖励模型能力的基准，有助于推动该领域研究，提升智能体表现。

Abstract: As Multimodal Large Language Models (MLLMs) advance, multimodal agents show
promise in real-world tasks like web navigation and embodied intelligence.
However, due to limitations in a lack of external feedback, these agents
struggle with self-correction and generalization. A promising approach is to
use reward models as external feedback, but there is no clear on how to select
reward models for agents. Thus, there is an urgent need to build a reward bench
targeted at agents. To address these challenges, we propose Agent-RewardBench,
a benchmark designed to evaluate reward modeling ability in MLLMs. The
benchmark is characterized by three key features: (1) Multiple dimensions and
real-world agent scenarios evaluation. It covers perception, planning, and
safety with 7 scenarios; (2) Step-level reward evaluation. It allows for the
assessment of agent capabilities at the individual steps of a task, providing a
more granular view of performance during the planning process; and (3)
Appropriately difficulty and high-quality. We carefully sample from 10 diverse
models, difficulty control to maintain task challenges, and manual verification
to ensure the integrity of the data. Experiments demonstrate that even
state-of-the-art multimodal models show limited performance, highlighting the
need for specialized training in agent reward modeling. Code is available at
github.

</details>


### [38] [Cat and Mouse -- Can Fake Text Generation Outpace Detector Systems?](https://arxiv.org/abs/2506.21274)
*Andrea McGlinchey,Peter J Barclay*

Main category: cs.CL

TL;DR: 虽然大模型伪造文本能力提升，但简单检测方法依然能有效识别；但架构创新如Gemini需警惕其增强的欺骗性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）能够生成以学术、评论、新闻等为代表的高仿“假文本”，对自动生成文本的检测成为热点问题。尽管似乎不断升级的人机对抗不可避免，但检测方法相对简单，且资源消耗低，因此研究其极限十分重要。

Method: 本文利用统计分类器，对生成古典侦探小说风格的文本进行真假检测。通过对比不同语言模型及其版本（如Gemini、GPT），评估检测工具的性能和模型生成“骗真”文本的能力。

Result: 实验表明，Gemini模型版本提升后，生成欺骗性文本的能力增强，但GPT的表现没有提升。即使面对更大参数和数据训练的新LLM，检测伪文本仍然可行。然而，不同模型结构的创新（如Gemini）可能会增强文本的欺骗性。

Conclusion: 即便大型语言模型不断扩展参数、数据和能耗，现有简单的统计分类检测仍具备有效性。但新型模型架构在伪文本欺骗性方面的进步值得关注。

Abstract: Large language models can produce convincing "fake text" in domains such as
academic writing, product reviews, and political news. Many approaches have
been investigated for the detection of artificially generated text. While this
may seem to presage an endless "arms race", we note that newer LLMs use ever
more parameters, training data, and energy, while relatively simple classifiers
demonstrate a good level of detection accuracy with modest resources. To
approach the question of whether the models' ability to beat the detectors may
therefore reach a plateau, we examine the ability of statistical classifiers to
identify "fake text" in the style of classical detective fiction. Over a 0.5
version increase, we found that Gemini showed an increased ability to generate
deceptive text, while GPT did not. This suggests that reliable detection of
fake text may remain feasible even for ever-larger models, though new model
architectures may improve their deceptiveness

</details>


### [39] [Double-Checker: Enhancing Reasoning of Slow-Thinking LLMs via Self-Critical Fine-Tuning](https://arxiv.org/abs/2506.21285)
*Xin Xu,Tianhao Chen,Fan Zhang,Wanlong Liu,Pengxiang Li,Ajay Kumar Jaiswal,Yuchen Yan,Jishan Hu,Yang Wang,Hao Chen,Shiwei Liu,Shizhe Diao,Can Yang,Lu Yin*

Main category: cs.CL

TL;DR: 本文提出Double-Checker，通过微调和自我批判迭代，提升LLM推理及自我完善能力，在AIME等基准上显著优于原模型。


<details>
  <summary>Details</summary>
Motivation: 当前慢思考的大型语言模型虽然能够进行类似反思的推理（即“顿悟时刻”），但在生成有信息量的自我批判和改进前一方案方面存在局限。动机在于提升LLM在推理和自我纠错迭代方面的能力。

Method: 本文提出了Double-Checker框架。该框架通过微调包含1,730个人工自我批判实例的数据集，使LLM在推理输出过程中能够不断自我批判和迭代完善自己的答案，直至通过自我生成的批判标准认可自己的解决方案为止。

Result: Double-Checker在多个推理基准测试中进行了验证。结果表明，经过迭代自我批判后，LLM在推理能力上有明显提升。其中在具有挑战性的AIME基准测试中，pass@1的表现从原始long-CoT LLM的4.4%提升到了18.2%。

Conclusion: Double-Checker框架通过显式自我批判与迭代自我改进显著增强了long-CoT LLM的推理表现，表明引入结构化自我批判是提升LLM可信度和有效性的有前景方向。

Abstract: While slow-thinking large language models (LLMs) exhibit reflection-like
reasoning, commonly referred to as the "aha moment:, their ability to generate
informative critiques and refine prior solutions remains limited. In this
paper, we introduce Double-Checker, a principled framework designed to enhance
the reasoning capabilities of slow-thinking LLMs by fostering explicit
self-critique and iterative refinement of their previous solutions. By
fine-tuning on our curated 1,730 self-critical instances, Double-Checker
empowers long-CoT LLMs to iteratively critique and refine their outputs during
inference until they evaluate their solutions as correct under self-generated
critiques. We validate the efficacy of Double-Checker across a comprehensive
suite of reasoning benchmarks, demonstrating that iterative self-critique
significantly enhances the reasoning capabilities of long-CoT LLMs. Notably,
our Double-Checker increases the pass@1 performance on challenging AIME
benchmarks from 4.4% to 18.2% compared to the original long-CoT LLMs. These
results highlight a promising direction for developing more trustworthy and
effective LLMs capable of structured self-critique.

</details>


### [40] [Small Encoders Can Rival Large Decoders in Detecting Groundedness](https://arxiv.org/abs/2506.21288)
*Istabrak Abbes,Gabriele Prato,Quentin Fournier,Fernando Rodriguez,Alaa Boukhary,Adam Elwood,Sarath Chandar*

Main category: cs.CL

TL;DR: 本文提出用轻量级编码器模型（如RoBERTa、NomicBERT）替代LLM完成查询“依据性”检测，取得与高性能LLM类似的准确率，并显著降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型增强了外部上下文后任务表现提升，但如果所给上下文信息不足，仍容易产生不受支持的推理甚至胡编乱造。为保障事实性与可信度，需在生成回复前，快速判断查询是否被上下文支持。

Method: 将RoBERTa和NomicBERT等编码器模型在特定数据集上微调，并与大型语言模型在“基于上下文的查询检测”任务上进行对比评测。

Result: 微调的轻量级编码器不仅检测“是否以文献为基础”这一任务上达SOTA LLM准确率，同时大幅减少推理所需时间和资源消耗。

Conclusion: 经过微调的轻量级编码器模型（如RoBERTa和NomicBERT）在查询是否依赖上下文的检测任务中，能够实现与先进LLM（如Llama3 8B和GPT4o）相当的准确率，但推理延迟大幅降低。

Abstract: Augmenting large language models (LLMs) with external context significantly
improves their performance in natural language processing (NLP) tasks. However,
LLMs struggle to answer queries reliably when the provided context lacks
information, often resorting to ungrounded speculation or internal knowledge.
Groundedness - generating responses strictly supported by the context - is
essential for ensuring factual consistency and trustworthiness. This study
focuses on detecting whether a given query is grounded in a document provided
in context before the costly answer generation by LLMs. Such a detection
mechanism can significantly reduce both inference time and resource
consumption. We show that lightweight, task specific encoder models such as
RoBERTa and NomicBERT, fine-tuned on curated datasets, can achieve accuracy
comparable to state-of-the-art LLMs, such as Llama3 8B and GPT4o, in
groundedness detection while reducing inference latency by orders of magnitude.
The code is available at : https://github.com/chandarlab/Hallucinate-less

</details>


### [41] [Detecting Referring Expressions in Visually Grounded Dialogue with Autoregressive Language Models](https://arxiv.org/abs/2506.21294)
*Bram Willemsen,Gabriel Skantze*

Main category: cs.CL

TL;DR: 本研究表明，只用文本和中等规模的LLM就能较好提取视觉对话中的指代表达，但这种单模态方法仍有局限，任务本质上需要多模态信息。


<details>
  <summary>Details</summary>
Motivation: 探究仅依赖语言上下文在视觉对话中提取具有视觉指代的表达能力有多大，即在没有视觉信息的情况下，语言模型能否有效识别这些指代表达。

Method: 对经过预训练的大型语言模型（LLM）进行适应性微调，仅输入文本，通过预测下一个token来标记会话文本中的指代表达边界，验证语言上下文在提取视觉指代表达中的能力。

Result: 实验表明，在只用文本、使用适中规模的预训练LLM和小数据集并进行高效微调下，仍可有效提取会话中的指代表达，说明语言上下文的重要性。

Conclusion: 作者发现仅利用文本、适中规模的LLM和小规模数据集进行参数高效微调，依然能有效提取视觉对话中的指代表达，但同时认为该任务天然是多模态问题，单一模态方法存在一定局限性。

Abstract: In this paper, we explore the use of a text-only, autoregressive language
modeling approach for the extraction of referring expressions from visually
grounded dialogue. More specifically, the aim is to investigate the extent to
which the linguistic context alone can inform the detection of mentions that
have a (visually perceivable) referent in the visual context of the
conversation. To this end, we adapt a pretrained large language model (LLM) to
perform a relatively course-grained annotation of mention spans in unfolding
conversations by demarcating mention span boundaries in text via next-token
prediction. Our findings indicate that even when using a moderately sized LLM,
relatively small datasets, and parameter-efficient fine-tuning, a text-only
approach can be effective, highlighting the relative importance of the
linguistic context for this task. Nevertheless, we argue that the task
represents an inherently multimodal problem and discuss limitations fundamental
to unimodal approaches.

</details>


### [42] [Structuralist Approach to AI Literary Criticism: Leveraging Greimas Semiotic Square for Large Language Models](https://arxiv.org/abs/2506.21360)
*Fangzhou Dong,Yifan Zeng,Yingpeng Sang,Hong Shen*

Main category: cs.CL

TL;DR: 本文提出了GLASS框架，结合结构化的符号学分析方法与大语言模型，显著提升了LLM在深入、专业的文学评论领域的能力，并构建了相关数据集与评价标准，取得了优异成果。


<details>
  <summary>Details</summary>
Motivation: 当前的大语言模型（LLMs）虽然在文本理解和生成方面表现优异，但在为具有深刻思想和复杂叙事的文学作品提供专业批评时存在不足。为解决这一痛点，作者希望提升LLM在深入文学评论方面的能力。

Method: 本文提出了GLASS（基于Greimas符号学方格的文学分析框架），利用结构化分析方法（Greimas Semiotic Square，GSS）引导LLM进行系统的文学分析。作者构建了首个基于GSS的数据集（包含48部作品的详细分析），并提出了基于LLM评价的量化指标（LLM-as-a-judge paradigm）来评估分析质量。之后将GLASS应用于39部经典作品。

Result: GLASS框架的分析结果在与专家评论和多种LLM对比中表现优异，能够产出原创且高质量的文学分析。该方法弥补了现有研究的空白。

Conclusion: GLASS为文学研究和教育提供了一种有效的AI工具，能够帮助深入理解和分析文学叙事结构及其深层含义，同时为认知文学机制的研究提供新见解。

Abstract: Large Language Models (LLMs) excel in understanding and generating text but
struggle with providing professional literary criticism for works with profound
thoughts and complex narratives. This paper proposes GLASS (Greimas Literary
Analysis via Semiotic Square), a structured analytical framework based on
Greimas Semiotic Square (GSS), to enhance LLMs' ability to conduct in-depth
literary analysis. GLASS facilitates the rapid dissection of narrative
structures and deep meanings in narrative works. We propose the first dataset
for GSS-based literary criticism, featuring detailed analyses of 48 works. Then
we propose quantitative metrics for GSS-based literary criticism using the
LLM-as-a-judge paradigm. Our framework's results, compared with expert
criticism across multiple works and LLMs, show high performance. Finally, we
applied GLASS to 39 classic works, producing original and high-quality analyses
that address existing research gaps. This research provides an AI-based tool
for literary research and education, offering insights into the cognitive
mechanisms underlying literary engagement.

</details>


### [43] [Leveraging LLM-Assisted Query Understanding for Live Retrieval-Augmented Generation](https://arxiv.org/abs/2506.21384)
*Guanting Dong,Xiaoxi Li,Yuyao Zhang,Mengjie Deng*

Main category: cs.CL

TL;DR: Omni-RAG通过三阶段流程（净化/分解查询、意图感知检索、优化生成）提升RAG系统在真实环境下对复杂、多意图、噪声查询的适应力和性能。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统主要在干净数据上训练与评估，不善处理真实场景下查询中的噪声、歧义和多意图。需帮助RAG系统更适应实际需求。

Method: 设计了Omni-RAG框架，包括三大模块：1) 利用LLM进行查询理解与分解，净化输入并细化多意图查询；2) 针对每个子意图检索相关知识并聚合；3) 使用reranker优化检索结果，由LLM链式推理生成最终答案。采用开源FineWeb/OpenSearch和Falcon-10B等组件。

Result: Omni-RAG能更高效、准确地处理真实、复杂的用户查询，其能力适用于SIGIR 2025 LiveRAG等实际挑战。

Conclusion: Omni-RAG显著提升了RAG系统在真实世界中的健壮性和有效性，能更好地应对噪声、高度复杂和多意图查询。

Abstract: Real-world live retrieval-augmented generation (RAG) systems face significant
challenges when processing user queries that are often noisy, ambiguous, and
contain multiple intents. While RAG enhances large language models (LLMs) with
external knowledge, current systems typically struggle with such complex
inputs, as they are often trained or evaluated on cleaner data. This paper
introduces Omni-RAG, a novel framework designed to improve the robustness and
effectiveness of RAG systems in live, open-domain settings. Omni-RAG employs
LLM-assisted query understanding to preprocess user inputs through three key
modules: (1) Deep Query Understanding and Decomposition, which utilizes LLMs
with tailored prompts to denoise queries (e.g., correcting spelling errors) and
decompose multi-intent queries into structured sub-queries; (2) Intent-Aware
Knowledge Retrieval, which performs retrieval for each sub-query from a corpus
(i.e., FineWeb using OpenSearch) and aggregates the results; and (3) Reranking
and Generation, where a reranker (i.e., BGE) refines document selection before
a final response is generated by an LLM (i.e., Falcon-10B) using a
chain-of-thought prompt. Omni-RAG aims to bridge the gap between current RAG
capabilities and the demands of real-world applications, such as those
highlighted by the SIGIR 2025 LiveRAG Challenge, by robustly handling complex
and noisy queries.

</details>


### [44] [Domain Knowledge-Enhanced LLMs for Fraud and Concept Drift Detection](https://arxiv.org/abs/2506.21443)
*Ali Şenol,Garima Agrawal,Huan Liu*

Main category: cs.CL

TL;DR: 该文提出一种结合领域知识的大模型框架，用于应对动态平台中的欺诈与语义漂移检测问题。通过结构化提示及多模块协作，系统在多个对话欺诈任务上取得高准确率，显著优于单纯的大模型基线。


<details>
  <summary>Details</summary>
Motivation: 在动态平台上检测欺诈性对话变得越来越困难，主要因为语言模式不断演变以及概念漂移（CD），即语义或主题的变化会改变交互的上下文或意图。这些变化会掩盖恶意意图或模仿正常对话，从而使准确分类极具挑战性。

Method: 提出了一种结合领域知识（DK）增强大语言模型（LLM）的框架，包含三个模块：1）检测伪造/欺骗性对话的DK-LLM模块；2）用于检测语义漂移的漂移检测单元（OCDD）；3）用于将漂移分类为良性或欺诈性的第二个DK-LLM模块。该方法首先在虚假评论数据集上验证领域知识的价值，然后应用于包含多种欺诈和垃圾攻击场景的SEConvo对话集。

Result: 系统能高准确率检测伪造对话，并有效分类漂移类型。基于LLaMA的实现，在结构化提示引导下达到98%的分类准确率。

Conclusion: 结合领域知识和概念漂移感知，能够极大提升在高风险自然语言处理任务中的性能、可解释性和鲁棒性。实验表明相较于零样本基准，DK-LLM框架表现更优。

Abstract: Detecting deceptive conversations on dynamic platforms is increasingly
difficult due to evolving language patterns and Concept Drift (CD)-i.e.,
semantic or topical shifts that alter the context or intent of interactions
over time. These shifts can obscure malicious intent or mimic normal dialogue,
making accurate classification challenging. While Large Language Models (LLMs)
show strong performance in natural language tasks, they often struggle with
contextual ambiguity and hallucinations in risk-sensitive scenarios. To address
these challenges, we present a Domain Knowledge (DK)-Enhanced LLM framework
that integrates pretrained LLMs with structured, task-specific insights to
perform fraud and concept drift detection. The proposed architecture consists
of three main components: (1) a DK-LLM module to detect fake or deceptive
conversations; (2) a drift detection unit (OCDD) to determine whether a
semantic shift has occurred; and (3) a second DK-LLM module to classify the
drift as either benign or fraudulent. We first validate the value of domain
knowledge using a fake review dataset and then apply our full framework to
SEConvo, a multiturn dialogue dataset that includes various types of fraud and
spam attacks. Results show that our system detects fake conversations with high
accuracy and effectively classifies the nature of drift. Guided by structured
prompts, the LLaMA-based implementation achieves 98% classification accuracy.
Comparative studies against zero-shot baselines demonstrate that incorporating
domain knowledge and drift awareness significantly improves performance,
interpretability, and robustness in high-stakes NLP applications.

</details>


### [45] [Text2Cypher Across Languages: Evaluating Foundational Models Beyond English](https://arxiv.org/abs/2506.21445)
*Makbule Gulcin Ozsoy,William Tai*

Main category: cs.CL

TL;DR: 研究发现，大语言模型在非英语Text2Cypher任务效果明显下降，尤其是低资源语言，简单的prompt翻译难以弥补差距，未来需更加重视多语言场景的模型开发和测试。


<details>
  <summary>Details</summary>
Motivation: 当前大多数基于大语言模型的自然语言数据库接口（如Text2SQL、Text2SPARQL、Text2Cypher）主要聚焦于英语，其他语言的评测和适配非常有限。

Method: 作者构建了一个多语言测试集，将英语问题翻译为西班牙语和土耳其语，同时保持原有的Cypher查询不变，用于多语言间的公平比较；并用标准化的prompt和评测指标对多种基础大模型进行评估。

Result: 模型在英语上的表现最佳，其次是西班牙语，土耳其语则最差。原因归结为训练数据的可用性及语言特性差异。将任务prompt翻译为西班牙语和土耳其语对结果影响很小。

Conclusion: 现有大语言模型在多语言Text2Cypher任务中存在显著性能差异，尤其对低资源语言不理想。仅靠prompt翻译并不能显著提升表现。未来需加强对多语言、尤其是低资源语言的数据库查询生成的评估和改进，并探索模式本地化和多语言微调。

Abstract: Recent advances in large language models have enabled natural language
interfaces that translate user questions into database queries, such as
Text2SQL, Text2SPARQL, and Text2Cypher. While these interfaces enhance database
accessibility, most research today focuses solely on English, with limited
evaluation in other languages. This paper investigates the performance of
foundational LLMs on the Text2Cypher task across multiple languages. We create
and release a multilingual test set by translating English questions into
Spanish and Turkish while preserving the original Cypher queries, enabling fair
cross-lingual comparison. We evaluate multiple foundational models using
standardized prompts and metrics. Our results show a consistent performance
pattern: highest on English, then Spanish, and lowest on Turkish. We attribute
this to differences in training data availability and linguistic
characteristics. Additionally, we explore the impact of translating task
prompts into Spanish and Turkish. Results show little to no change in
evaluation metrics, suggesting prompt translation has minor impact. Our
findings highlight the need for more inclusive evaluation and development in
multilingual query generation. Future work includes schema localization and
fine-tuning across diverse languages.

</details>


### [46] [skLEP: A Slovak General Language Understanding Benchmark](https://arxiv.org/abs/2506.21508)
*Marek Šuppa,Andrej Ridzik,Daniel Hládek,Tomáš Javůrek,Viktória Ondrejová,Kristína Sásiková,Martin Tamajka,Marián Šimko*

Main category: cs.CL

TL;DR: 本文提出了首个用于斯洛伐克语自然语言理解的综合性基准skLEP，包含九项多粒度任务，并系统评测了多类现有模型。作者公开了数据、工具包和排行榜，致力于加速该语言的NLP研究。


<details>
  <summary>Details</summary>
Motivation: 斯洛伐克语自然语言理解（NLU）领域缺乏系统且全面的基准，限制了模型评估和发展。本研究希望通过建立适用于斯洛伐克语的NLP基准，推动该领域的发展。

Method: 提出skLEP基准，涵盖九项不同的任务，包括token级、句对级和文档级。方法包括独立构建针对斯洛伐克语的新数据集，以及精准翻译现有英语NLU资源，并对各种斯洛伐克语、多语种和英语预训练语言模型在skLEP任务上的表现进行系统评测。

Result: 对多种模型在skLEP基准下进行了全面评测，并公开发布了数据集、工具包和公共排行榜，以支持模型微调、评测及后续研究。

Conclusion: skLEP是首个专为评估斯洛伐克语NLU设计的基准，涵盖多类型任务和全面评测，显著推动了斯洛伐克语NLP研究的可复现性和发展。

Abstract: In this work, we introduce skLEP, the first comprehensive benchmark
specifically designed for evaluating Slovak natural language understanding
(NLU) models. We have compiled skLEP to encompass nine diverse tasks that span
token-level, sentence-pair, and document-level challenges, thereby offering a
thorough assessment of model capabilities. To create this benchmark, we curated
new, original datasets tailored for Slovak and meticulously translated
established English NLU resources. Within this paper, we also present the first
systematic and extensive evaluation of a wide array of Slovak-specific,
multilingual, and English pre-trained language models using the skLEP tasks.
Finally, we also release the complete benchmark data, an open-source toolkit
facilitating both fine-tuning and evaluation of models, and a public
leaderboard at https://github.com/slovak-nlp/sklep in the hopes of fostering
reproducibility and drive future research in Slovak NLU.

</details>


### [47] [Aligning Spoken Dialogue Models from User Interactions](https://arxiv.org/abs/2506.21463)
*Anne Wu,Laurent Mazaré,Neil Zeghidour,Alexandre Défossez*

Main category: cs.CL

TL;DR: 提出新颖的偏好对齐框架，通过大规模数据集和AI反馈训练改进语音对话模型，有效提升了其实时交互表现和用户体验。


<details>
  <summary>Details</summary>
Motivation: 当前对话偏好学习方法多集中在基于文本的语言模型，难以适应实时语音交互中的复杂动态，如打断、插话，以及说话人轮次的非显式划分。本文尝试解决语音对话系统在实际应用中的对齐与偏好提升难题。

Method: 构建了一个包含超过15万语音对话偏好对的大规模数据集，通过AI反馈对多轮原始语音对话在语言内容和时间上下文偏好方面进行标注，采用离线对齐方法微调全双工自回归语音对语音模型。

Result: 实验证明，即便是基于通用对话的反馈，也能持续有效地提升语音对话模型在事实准确性、安全性和上下文对齐性上的表现。并通过大规模人类评估，证实了模型改进在多轮、实时语音交互中的正面影响。

Conclusion: 高质量的多维动态偏好对齐对于自然、实时语音对话系统至关重要。所提出的方法和数据集有效提升了语音模型的对话质量，对自然语音AI发展有重要指导价值。

Abstract: We propose a novel preference alignment framework for improving spoken
dialogue models on real-time conversations from user interactions. Current
preference learning methods primarily focus on text-based language models, and
are not directly suited to the complexities of real-time speech interactions,
with richer dynamics (e.g. interruption, interjection) and no explicit
segmentation between speaker turns.We create a large-scale dataset of more than
150,000 preference pairs from raw multi-turn speech conversations, annotated
with AI feedback, to cover preferences over both linguistic content and
temporal context variations. We leverage offline alignment methods to finetune
a full-duplex autoregressive speech-to-speech model. Extensive experiments
demonstrate that feedback on generic conversations can be consistently
effective in improving spoken dialogue models to produce more factual, safer
and more contextually aligned interactions. We deploy the finetuned model and
conduct holistic human evaluations to assess the impact beyond single-turn
conversations. Our findings shed light on the importance of a well-calibrated
balance among various dynamics, crucial for natural real-time speech dialogue
systems.

</details>


### [48] [Potemkin Understanding in Large Language Models](https://arxiv.org/abs/2506.21521)
*Marina Mancoridis,Bec Weeks,Keyon Vafa,Sendhil Mullainathan*

Main category: cs.CL

TL;DR: 该文提出衡量语言模型‘表象理解’的正式框架和定量方法，发现模型基准测试成绩背后常隐藏着对概念的浅层甚至错乱理解，模型内部表征与人类理解严重不一致。


<details>
  <summary>Details</summary>
Motivation: 虽然大型语言模型经常通过标准基准测试来衡量能力，但现有评测隐含假设模型对概念的误解与人类的误解方式相似。若非如此，模型在基准测试上的成功仅是表象理解，而非真正具备人类水平的能力。迫切需要检验这种表象理解的普遍性及其底层机制。

Method: 提出两种量化表象理解的方法：一种是设计特殊基准测试用于三个不同领域，另一种是一个通用的程序，用于估算其普遍存在的下界。通过对模型在不同任务和领域的测试，量化了表象理解的现象。

Result: 发现这类表象理解在各种模型、任务和领域中普遍存在，且模型不仅是错误理解概念，背后还存在更深层次的概念表示内在混乱问题。

Conclusion: 模型在基准测试中表现出的“理解”在很多情况下只是表象，这种表象理解（potemkin understanding）广泛存在，说明大型语言模型的内部概念表示可能存在深层次的不一致性。

Abstract: Large language models (LLMs) are regularly evaluated using benchmark
datasets. But what justifies making inferences about an LLM's capabilities
based on its answers to a curated set of questions? This paper first introduces
a formal framework to address this question. The key is to note that the
benchmarks used to test LLMs -- such as AP exams -- are also those used to test
people. However, this raises an implication: these benchmarks are only valid
tests if LLMs misunderstand concepts in ways that mirror human
misunderstandings. Otherwise, success on benchmarks only demonstrates potemkin
understanding: the illusion of understanding driven by answers irreconcilable
with how any human would interpret a concept. We present two procedures for
quantifying the existence of potemkins: one using a specially designed
benchmark in three domains, the other using a general procedure that provides a
lower-bound on their prevalence. We find that potemkins are ubiquitous across
models, tasks, and domains. We also find that these failures reflect not just
incorrect understanding, but deeper internal incoherence in concept
representations.

</details>


### [49] [TopK Language Models](https://arxiv.org/abs/2506.21468)
*Ryosuke Takahashi,Tatsuro Inaba,Kentaro Inui,Benjamin Heinzerling*

Main category: cs.CL

TL;DR: 本文提出在transformer模型中集成TopK激活函数，从源头保障稀疏、可解释的神经元激活，绕开SAE需后训练和不稳定等问题，实现更稳定、有效的语言模型功能解释和干预。


<details>
  <summary>Details</summary>
Motivation: 稀疏自编码器（SAEs）被广泛用于分析和解释基于transformer的语言模型激活空间，但其后训练和特征稳定性存在问题，导致可解释性与内部有效性受到限制。

Method: 提出在transformer架构中引入TopK激活函数，将隐藏状态直接变为TopK SAE的潜在特征，无需后训练，自然具备可解释性。

Result: TopK语言模型在保留原始能力的同时，提升了模型的可解释性与特征稳定性，实现了参数量、计算效率和可解释性的良好平衡。实验证明，该模型的稀疏表示支持针对神经元的目标干预，并便于跨训练检查点追踪神经元形成过程。

Conclusion: TopK LMs通过结构上的简单调整实现了稳健、直观的特征表达，是理解与控制语言模型概念学习过程的有力工具，将助力后续模型可解释性和可控性研究。

Abstract: Sparse autoencoders (SAEs) have become an important tool for analyzing and
interpreting the activation space of transformer-based language models (LMs).
However, SAEs suffer several shortcomings that diminish their utility and
internal validity. Since SAEs are trained post-hoc, it is unclear if the
failure to discover a particular concept is a failure on the SAE's side or due
to the underlying LM not representing this concept. This problem is exacerbated
by training conditions and architecture choices affecting which features an SAE
learns. When tracing how LMs learn concepts during training, the lack of
feature stability also makes it difficult to compare SAEs features across
different checkpoints. To address these limitations, we introduce a
modification to the transformer architecture that incorporates a TopK
activation function at chosen layers, making the model's hidden states
equivalent to the latent features of a TopK SAE. This approach eliminates the
need for post-hoc training while providing interpretability comparable to SAEs.
The resulting TopK LMs offer a favorable trade-off between model size,
computational efficiency, and interpretability. Despite this simple
architectural change, TopK LMs maintain their original capabilities while
providing robust interpretability benefits. Our experiments demonstrate that
the sparse representations learned by TopK LMs enable successful steering
through targeted neuron interventions and facilitate detailed analysis of
neuron formation processes across checkpoints and layers. These features make
TopK LMs stable and reliable tools for understanding how language models learn
and represent concepts, which we believe will significantly advance future
research on model interpretability and controllability.

</details>


### [50] [Bridging Offline and Online Reinforcement Learning for LLMs](https://arxiv.org/abs/2506.21495)
*Jack Lanchantin,Angelica Chen,Janice Lan,Xian Li,Swarnadeep Saha,Tianlu Wang,Jing Xu,Ping Yu,Weizhe Yuan,Jason E Weston,Sainbayar Sukhbaatar,Ilia Kulikov*

Main category: cs.CL

TL;DR: 强化学习微调方法在大语言模型中，从离线向半在线及在线过渡时，在线（或半在线）方法均显著优于离线方案，不同变种之间表现接近；多任务奖励促进了多类任务的性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前在语言模型微调中，强化学习方法主要在离线环境下应用效果较好，但缺乏系统性研究其在半在线及完全在线场景下的表现，尤其是在可验证与不可验证任务中的效果对比。因此，作者着手探索强化学习微调方法跨越不同数据获取和反馈实时性场景的效果差异。

Method: 作者在可验证的数学任务和不可验证的指令跟随任务上，对多种微调范式（在线、半在线）下的Direct Preference Optimization（DPO）和Group Reward Policy Optimization（GRPO）目标函数进行了系统实验，并设计了一套基准评测方法。作者还详细分析了训练过程中的动态和超参数选择对结果的影响。

Result: 实验结果显示，在线与半在线DPO和GRPO在性能和收敛速度上表现相似，且均远优于传统离线方法。此外，通过联合多任务训练（融合可验证与不可验证奖励信号），能够进一步提升两类任务的整体性能。

Conclusion: （1）在线和半在线强化学习微调范式间表现接近，推荐应用于实际任务；（2）多任务联合奖励可兼顾可验证与不可验证任务，有效提升模型的泛化能力。

Abstract: We investigate the effectiveness of reinforcement learning methods for
finetuning large language models when transitioning from offline to semi-online
to fully online regimes for both verifiable and non-verifiable tasks. Our
experiments cover training on verifiable math as well as non-verifiable
instruction following with a set of benchmark evaluations for both. Across
these settings, we extensively compare online and semi-online Direct Preference
Optimization and Group Reward Policy Optimization objectives, and surprisingly
find similar performance and convergence between these variants, which all
strongly outperform offline methods. We provide a detailed analysis of the
training dynamics and hyperparameter selection strategies to achieve optimal
results. Finally, we show that multi-tasking with verifiable and non-verifiable
rewards jointly yields improved performance across both task types.

</details>


### [51] [Enhancing User Engagement in Socially-Driven Dialogue through Interactive LLM Alignments](https://arxiv.org/abs/2506.21497)
*Jiashuo Wang,Kaitao Song,Chunpu Xu,Changhe Song,Yang Xiao,Dongsheng Li,Lili Qiu,Wenjie Li*

Main category: cs.CL

TL;DR: 本文提出用用户模拟器及树搜索自动收集互动体验，以用户反馈为直接优化信号，使大模型产生更高参与度的社交对话，经验证方法有效。


<details>
  <summary>Details</summary>
Motivation: 以往的对话系统优化主要关注知识推理或对话计划，但这些方式未必能够保证提升用户参与度。尤其是在社交驱动的对话场景中，用户参与度与知识/对话行为的关系较为复杂，因此有必要通过新的方式直接提升用户参与度。

Method: 采用用户未来对话发展的反应作为衡量参与度的直接指标，把用户的对话意图相关反馈作为奖励信号，通过用户模拟器与目标大模型交互，利用i×MCTS（交互蒙特卡洛树搜索）探索互动过程，收集高低质量体验数据。随后，利用直接偏好优化（DPO）来对模型进行优化，目标是对齐高用户参与度的对话。

Result: 在两个社交型对话场景（情感支持与善意劝说）上进行实验，结果显示该方法能有效提升大模型的用户参与度。

Conclusion: 通过利用用户模拟器和i×MCTS自动收集体验数据，并以用户反馈为奖励优化LLM，可显著提升社交对话中的用户参与度。

Abstract: Enhancing user engagement through interactions plays an essential role in
socially-driven dialogues. While prior works have optimized models to reason
over relevant knowledge or plan a dialogue act flow, the relationship between
user engagement and knowledge or dialogue acts is subtle and does not guarantee
user engagement in socially-driven dialogues. To this end, we enable
interactive LLMs to learn user engagement by leveraging signals from the future
development of conversations. Specifically, we adopt a more direct and relevant
indicator of user engagement, i.e., the user's reaction related to dialogue
intention after the interaction, as a reward to align interactive LLMs. To
achieve this, we develop a user simulator to interact with target interactive
LLMs and explore interactions between the user and the interactive LLM system
via \textit{i$\times$MCTS} (\textit{M}onte \textit{C}arlo \textit{T}ree
\textit{S}earch for \textit{i}nteraction). In this way, we collect a dataset
containing pairs of higher and lower-quality experiences using
\textit{i$\times$MCTS}, and align interactive LLMs for high-level user
engagement by direct preference optimization (DPO) accordingly. Experiments
conducted on two socially-driven dialogue scenarios (emotional support
conversations and persuasion for good) demonstrate that our method effectively
enhances user engagement in interactive LLMs.

</details>


### [52] [Data Efficacy for Language Model Training](https://arxiv.org/abs/2506.21545)
*Yalun Dai,Yangyu Huang,Xin Zhang,Wenshan Wu,Chong Li,Wenhui Lu,Shijie Cao,Li Dong,Scarlett Li*

Main category: cs.CL

TL;DR: 本文首次提出'数据效能'的概念，通过创新的数据组织范式DELT显著提升语言模型训练效果，且与传统的数据筛选方法互补。


<details>
  <summary>Details</summary>
Motivation: 现有关于数据效率（data efficiency）的工作主要集中于数据筛选和子集选择，忽视了通过优化数据组织提升表现。本工作希望填补这一空白，提出通过数据组织最大化语言模型训练性能。

Method: 提出了DELT范式，包括数据评分、数据选择和数据排序三个组成部分。实现了Learnability-Quality Scoring (LQS)用于数据评分，以及Folding Ordering (FO)用于数据排序。通过梯度一致性评价数据可学性和质量，通过分层排序解决遗忘和分布偏移问题。

Result: 通过多组实验证明，不增加数据量和模型规模的情况下，DELT各实例均提升了性能，尤其LQS与Folding的组合效果最佳。同时数据效能可与数据效率兼得。

Conclusion: 数据效能（Data Efficacy）是提升语言模型训练性能的重要新方向，合理组织数据通过DELT范式显著提升模型效果。

Abstract: Data is fundamental to the training of language models (LM). Recent research
has been dedicated to data efficiency, which aims to maximize performance by
selecting a minimal or optimal subset of training data. Techniques such as data
filtering, sampling, and selection play a crucial role in this area. To
complement it, we define Data Efficacy, which focuses on maximizing performance
by optimizing the organization of training data and remains relatively
underexplored. This work introduces a general paradigm, DELT, for considering
data efficacy in LM training, which highlights the significance of training
data organization. DELT comprises three components: Data Scoring, Data
Selection, and Data Ordering. Among these components, we design
Learnability-Quality Scoring (LQS), as a new instance of Data Scoring, which
considers both the learnability and quality of each data sample from the
gradient consistency perspective. We also devise Folding Ordering (FO), as a
novel instance of Data Ordering, which addresses issues such as model
forgetting and data distribution bias. Comprehensive experiments validate the
data efficacy in LM training, which demonstrates the following: Firstly,
various instances of the proposed DELT enhance LM performance to varying
degrees without increasing the data scale and model size. Secondly, among these
instances, the combination of our proposed LQS for data scoring and Folding for
data ordering achieves the most significant improvement. Lastly, data efficacy
can be achieved together with data efficiency by applying data selection.
Therefore, we believe that data efficacy is a promising foundational area in LM
training.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [53] [A generalised framework for phase field-based modelling of coupled problems: application to thermo-mechanical fracture, hydraulic fracture, hydrogen embrittlement and corrosion](https://arxiv.org/abs/2506.20763)
*Y. Navidtehrani,C. Betegón,E. Martínez-Pañeda*

Main category: cs.CE

TL;DR: 文章提出一种结合相场法与多物理场建模的新框架，可方便地在Abaqus中处理结构完整性耦合问题，并取得与实验及现有解高度一致的结果，相关子程序已开放下载。


<details>
  <summary>Details</summary>
Motivation: 现有的结构完整性耦合问题通常处理复杂，难以在商业有限元软件中直接实现。为简化处理流程，提高工程适应性，提出新的通用方法。

Method: 结合相场法与多物理场建模，利用传热方程的通用性，只需在有限元软件中实现积分点级别的用户子程序（UMAT, UMATHT），即可实现多变量耦合现象建模。基于Abaqus软件实现该方法。

Result: 所提出的理论与计算框架已经针对热-力破裂、水力破裂、氢致开裂和金属腐蚀等四类工程与科学相关问题进行了验证，涵盖二维和三维情形。计算结果与实验数据、已有数值及解析解高度吻合。相关子程序已公开。

Conclusion: 新方法为多物理场耦合的结构完整性问题提供了通用、便捷的建模与分析框架，并验证了其实用性与准确性。相关实现便于在主流有限元软件中推广应用。

Abstract: We present a novel, generalised formulation to treat coupled structural
integrity problems by combining phase field and multi-physics modelling. The
approach exploits the versatility of the heat transfer equation and is
therefore well suited to be adopted in commercial finite element packages,
requiring only integration point-level implementation. This aspect is
demonstrated here by implementing coupled, multi-variable phenomena through
simple \texttt{UMAT} and \texttt{UMATHT} subroutines in the finite element
package \texttt{Abaqus}. The generalised theoretical and computational
framework presented is particularised to four problems of engineering and
scientific relevance: thermo-mechanical fracture, hydraulic fracture,
hydrogen-assisted cracking and metallic corrosion. 2D and 3D problems are
considered. The results reveal a very good agreement with experimental data,
and existing numerical and analytical solutions.The user subroutines developed
are made freely available at https://mechmat.web.ox.ac.uk/codes.

</details>


### [54] [A Hereditary Integral, Transient Network Approach to Modeling Permanent Set and Viscoelastic Response in Polymers](https://arxiv.org/abs/2506.20773)
*Stephen T. Castonguay,Joshua B. Fernandes,Michael A. Puso,Sylvie Aubry*

Main category: cs.CE

TL;DR: 本文提出一种高效的粘弹性和永久定型聚合物数值建模方法，核心在于用递推代替全历史积分，适配多种材料模型，既能处理复杂加载又能捕捉残余应变，提升了模拟效率和适用性。


<details>
  <summary>Details</summary>
Motivation: 为了解决聚合物粘弹性和永久定型（残余应变）的高效数值模拟问题，克服传统方法需对全部时间历史积分、计算量大等缺点。

Method: 基于瞬态网络理论的遗传积分形式，将聚合物链分配到具有不同自然平衡态的独立网络中。链不断从过去网络分离并以零应力状态再结合到新网络。通过对不同自由能核的分解，建立递推关系，无需对所有时间历史进行积分。适用于高度可压缩和近不可压缩材料，涵盖neo-Hookean、Blatz-Ko、Yeoh和Ogden-Hill等模型。

Result: 提出的数值框架能有效处理复杂加载条件下的速率相关响应和残余应变，验证其适用性强并能模拟聚合物的实际行为。

Conclusion: 该工作开发了一种高效且通用的数值方法，可以准确高效地模拟聚合物的粘弹性和永久定型，显著优化了传统时间积分方式，适合多种典型材料模型，已通过多个典型工况的数值实例验证。

Abstract: An efficient numerical framework is presented for modeling viscoelasticity
and permanent set of polymers. It is based on the hereditary integral form of
transient network theory, in which polymer chains belong to distinct networks
each with different natural equilibrium states. Chains continually detach from
previously formed networks and reattach to new networks in a state of zero
stress. The free energy of these networks is given in terms of the deformation
gradient relative to the configuration at which the network was born. A
decomposition of the kernel for various free energies allows for a recurrence
relationship to be established, bypassing the need to integrate over all time
history. The technique is established for both highly compressible and nearly
incompressible materials through the use of neo-Hookean, Blatz-Ko, Yeoh, and
Ogden-Hill material models. Multiple examples are presented showing the ability
to handle rate-dependent response and residual strains under complex loading
histories.

</details>


### [55] [Counterfactual Voting Adjustment for Quality Assessment and Fairer Voting in Online Platforms with Helpfulness Evaluation](https://arxiv.org/abs/2506.21362)
*Chang Liu,Yixin Wang,Moontae Lee*

Main category: cs.CE

TL;DR: 该论文针对网络平台中投票偏见导致的内容质量评估不公问题，提出了CVA反事实因果方法。实验显示，CVA不仅有效消除了位置和从众偏见，还能更好地恢复真实内容质量，并提升排序与用户和模型的评价一致性。


<details>
  <summary>Details</summary>
Motivation: 网络平台需要有效地获取高质量信息，而用户投票用于评估内容帮助平台排序，但这些投票常因内容展示位置和先前投票的影响而存在偏见，导致信息质量评估不公。

Method: 提出了一种反事实投票调整（CVA）因果框架，根据用户投票所处的具体上下文，纠正位置偏见和从众效应。通过预实验、半合成实验以及真实数据实验，比较了CVA与传统方法在内容质量恢复上的表现。

Result: CVA能够有效建模位置和从众偏见，并准确恢复内容的真实质量。在真实实验中，依据CVA学习到的质量对内容重新排序，更加符合用户情感和GPT-4o的质量评分，优于传统的总投票数排序和无因果推断的模型重排。此外，CVA嵌入还能用于分析不同StackExchange社区内专业用户群体的行为动态。

Conclusion: CVA因果框架能更公平、准确地衡量和排序内容质量，在实际平台应用中优于传统的投票聚合或非因果模型，为理解和提升社区内容评估机制提供了新工具。

Abstract: Efficient access to high-quality information is vital for online platforms.
To promote more useful information, users not only create new content but also
evaluate existing content, often through helpfulness voting. Although
aggregated votes help service providers rank their user content, these votes
are often biased by disparate accessibility per position and the cascaded
influence of prior votes. For a fairer assessment of information quality, we
propose the Counterfactual Voting Adjustment (CVA), a causal framework that
accounts for the context in which individual votes are cast. Through
preliminary and semi-synthetic experiments, we show that CVA effectively models
the position and herding biases, accurately recovering the predefined content
quality. In a real experiment, we demonstrate that reranking content based on
the learned quality by CVA exhibits stronger alignment with both user sentiment
and quality evaluation assessed by GPT-4o, outperforming system rankings based
on aggregated votes and model-based rerankings without causal inference. Beyond
the individual quality inference, our embeddings offer comparative insights
into the behavioral dynamics of expert user groups across 120 major
StackExchange communities.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [56] [Our Coding Adventure: Using LLMs to Personalise the Narrative of a Tangible Programming Robot for Preschoolers](https://arxiv.org/abs/2506.20982)
*Martin Ruskov*

Main category: cs.CY

TL;DR: 本文提出通过LLM辅助教师为学前儿童个性化设计编程机器人Cubetto的故事教学流程，验证了方法的可行性和模型无关性，不直接让儿童接触LLM，为学前教育提供新路径，同时注意到一致性等挑战并积极探索改进。


<details>
  <summary>Details</summary>
Motivation: 在教育领域使用大型语言模型（LLM）存在挑战，尤其是面对年幼儿童时，如何安全、有效地发挥LLM的优势成为关键。儿童对屏幕时间和科技理解有限，需要间接方式利用LLM辅助学习。

Method: 作者通过与可编程机器人Cubetto结合，利用LLM为学前儿童的机器人指令操作准备个性化故事叙述。他们进行行动研究，开发了一个正式流程的早期版本，能快速为Cubetto原型化游戏故事，并用5种不同LLM测试流程的模型无关性，明确流程、材料和提示词。

Result: 该方法可重现，模型无关，成功实现了教师辅助所需的故事生成，在4种不同任务场景中测试发现了一致性与幻觉问题，并记录了解决过程。

Conclusion: 这一方法未让儿童直接接触LLM，而是辅助教师为儿童生成个性化故事，适合学前班使用，并计划在现实教育环境中进一步试验。

Abstract: Finding balanced ways to employ Large Language Models (LLMs) in education is
a challenge due to inherent risks of poor understanding of the technology and
of a susceptible audience. This is particularly so with younger children, who
are known to have difficulties with pervasive screen time. Working with a
tangible programming robot called Cubetto, we propose an approach to benefit
from the capabilities of LLMs by employing such models in the preparation of
personalised storytelling, necessary for preschool children to get accustomed
to the practice of commanding the robot. We engage in action research to
develop an early version of a formalised process to rapidly prototype game
stories for Cubetto. Our approach has both reproducible results, because it
employs open weight models, and is model-agnostic, because we test it with 5
different LLMs. We document on one hand the process, the used materials and
prompts, and on the other the learning experience and outcomes. We deem the
generation successful for the intended purposes of using the results as a
teacher aid. Testing the models on 4 different task scenarios, we encounter
issues of consistency and hallucinations and document the corresponding
evaluation process and attempts (some successful and some not) to overcome
these issues. Importantly, the process does not expose children to LLMs
directly. Rather, the technology is used to help teachers easily develop
personalised narratives on children's preferred topics. We believe our method
is adequate for preschool classes and we are planning to further experiment in
real-world educational settings.

</details>
