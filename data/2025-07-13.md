<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 11]
- [cs.CL](#cs.CL) [Total: 8]
- [cs.CE](#cs.CE) [Total: 2]
- [cs.CY](#cs.CY) [Total: 7]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Autonomous Control Leveraging LLMs: An Agentic Framework for Next-Generation Industrial Automation](https://arxiv.org/abs/2507.07115)
*Javal Vyas,Mehmet Mercangoz*

Main category: cs.AI

TL;DR: 该文提出用大语言模型与有限状态机结合，实现化学过程的智能化、统一自动化控制，实验结果表明方法在故障恢复和过程控制上均具高效和鲁棒性，优于现有传统与开源方案。


<details>
  <summary>Details</summary>
Motivation: 现代化学过程越来越复杂，人工短缺和复杂故障也更普遍，亟需融合符号推理与自适应控制的新型自动化方法。

Method: 提出了一个统一的基于大型语言模型（LLMs）的智能体框架，既能实现离散故障恢复规划，又可进行连续过程控制。方法采用有限状态机（FSM）做为可解释的操作边界，由LLM驱动的规划代理、仿真代理和验证-重提示循环协同工作，迭代优化控制与故障恢复方案。

Result: 案例1表明，在180个不同规模的FSM中，GPT-4o与GPT-4o-mini均能在五次内100%找到有效路径，优于开源LLM。案例2中，该框架能控制实验双加热器平台在干扰下维持目标温度，性能与经典PID控制相当，且验证了提示循环在非线性动态响应中的关键作用。还分析了如指令跟随失误、微分方程近似等关键失效模式。

Conclusion: 采用结构化反馈和模块化智能体的LLM框架可将高层符号规划与底层连续控制统一，推动化学工程中更具韧性、以语言为驱动的自动化发展。

Abstract: The increasing complexity of modern chemical processes, coupled with
workforce shortages and intricate fault scenarios, demands novel automation
paradigms that blend symbolic reasoning with adaptive control. In this work, we
introduce a unified agentic framework that leverages large language models
(LLMs) for both discrete fault-recovery planning and continuous process control
within a single architecture. We adopt Finite State Machines (FSMs) as
interpretable operating envelopes: an LLM-driven planning agent proposes
recovery sequences through the FSM, a Simulation Agent executes and checks each
transition, and a Validator-Reprompting loop iteratively refines invalid plans.
In Case Study 1, across 180 randomly generated FSMs of varying sizes (4-25
states, 4-300 transitions), GPT-4o and GPT-4o-mini achieve 100% valid-path
success within five reprompts-outperforming open-source LLMs in both accuracy
and latency. In Case Study 2, the same framework modulates dual-heater inputs
on a laboratory TCLab platform (and its digital twin) to maintain a target
average temperature under persistent asymmetric disturbances. Compared to
classical PID control, our LLM-based controller attains similar performance,
while ablation of the prompting loop reveals its critical role in handling
nonlinear dynamics. We analyze key failure modes-such as instruction following
lapses and coarse ODE approximations. Our results demonstrate that, with
structured feedback and modular agents, LLMs can unify high-level symbolic
planningand low-level continuous control, paving the way towards resilient,
language-driven automation in chemical engineering.

</details>


### [2] [BOOST: Out-of-Distribution-Informed Adaptive Sampling for Bias Mitigation in Stylistic Convolutional Neural Networks](https://arxiv.org/abs/2507.07134)
*Mridula Vijendran,Shuang Chen,Jingjing Deng,Hubert P. H. Shum*

Main category: cs.AI

TL;DR: 本文针对AI绘画分类中的偏见问题，提出了分布外信息驱动的BOOST采样方法，并设计了新评估指标SODC。实验证明该方法能在提升分类性能的同时，显著减少类别间的偏见，实现更公平的模型输出。


<details>
  <summary>Details</summary>
Motivation: AI在绘画分类中面临显著偏见问题，尤其随着AI系统应用于艺术策展与修复等领域，该偏见愈发严重。偏见常源于训练数据集中某些艺术风格占主导，造成模型在少见风格上表现不佳。现有研究多注重提升分类性能，缺乏针对模型基础性偏见，特别是分布外（OOD）数据的处理。

Method: 提出了一种新颖的分布外信息驱动的模型偏见自适应采样方法BOOST（Bias-Oriented OOD Sampling and Tuning），通过动态调整温度缩放和采样概率，实现各类别更均衡的表示。并引入了新的评估指标SODC（Same-Dataset OOD Detection Score），专门评估类别分离和每类偏见的减少。

Result: BOOST方法在KaoKore和PACS数据集上验证，其不仅有效降低了类别偏见，还能保持高分类性能。新引入的SODC指标显示出模型在平衡各类别表现上的改善。

Conclusion: BOOST是一种能够同时提升模型表现与公平性的鲁棒方法，为艺术领域AI模型消除偏见提供了有效方案。

Abstract: The pervasive issue of bias in AI presents a significant challenge to
painting classification, and is getting more serious as these systems become
increasingly integrated into tasks like art curation and restoration. Biases,
often arising from imbalanced datasets where certain artistic styles dominate,
compromise the fairness and accuracy of model predictions, i.e., classifiers
are less accurate on rarely seen paintings. While prior research has made
strides in improving classification performance, it has largely overlooked the
critical need to address these underlying biases, that is, when dealing with
out-of-distribution (OOD) data. Our insight highlights the necessity of a more
robust approach to bias mitigation in AI models for art classification on
biased training data. We propose a novel OOD-informed model bias adaptive
sampling method called BOOST (Bias-Oriented OOD Sampling and Tuning). It
addresses these challenges by dynamically adjusting temperature scaling and
sampling probabilities, thereby promoting a more equitable representation of
all classes. We evaluate our proposed approach to the KaoKore and PACS
datasets, focusing on the model's ability to reduce class-wise bias. We further
propose a new metric, Same-Dataset OOD Detection Score (SODC), designed to
assess class-wise separation and per-class bias reduction. Our method
demonstrates the ability to balance high performance with fairness, making it a
robust solution for unbiasing AI models in the art domain.

</details>


### [3] [State-Inference-Based Prompting for Natural Language Trading with Game NPCs](https://arxiv.org/abs/2507.07203)
*Minkyung Kim,Junsik Kim,Hwidong Bae,Woongcheol Yang,Sangdon Park,Sohee Bae*

Main category: cs.AI

TL;DR: 本文提出SIBP方法，有效提升大模型在游戏交易中的规约遵循性和准确性，为可信的NPC交易交互提供了高效、可落地的技术方案。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽能增强动态游戏互动，但在规则驱动的交易体系中易出现违规则、物品“幻觉”、计算出错等问题，导致玩家对模型失去信任，因此需要提升大模型在交易场景的可控性和准确性。

Method: 提出State-Inference-Based Prompting（SIBP）方法，将交易过程分为六个状态，并在统一的Prompt框架下进行对话状态推断、上下文规则遵循、物品引用和基于占位符的价格计算。

Result: 在100个交易对话测试中，SIBP实现了97%以上的状态合规度、95%以上的物品引用准确率和99.7%的价格计算精度，并且计算效率高于基线方法。

Conclusion: SIBP方法使大模型在游戏内实现高效、可靠的规则型交易，显著提升了交易准确性和计算精度，能够应用于商业化游戏中，实现可信的NPC交易互动。

Abstract: Large Language Models enable dynamic game interactions but struggle with
rule-governed trading systems. Current implementations suffer from rule
violations, such as item hallucinations and calculation errors, that erode
player trust. Here, State-Inference-Based Prompting (SIBP) enables reliable
trading through autonomous dialogue state inference and context-specific rule
adherence. The approach decomposes trading into six states within a unified
prompt framework, implementing context-aware item referencing and
placeholder-based price calculations. Evaluation across 100 trading dialogues
demonstrates >97% state compliance, >95% referencing accuracy, and 99.7%
calculation precision. SIBP maintains computational efficiency while
outperforming baseline approaches, establishing a practical foundation for
trustworthy NPC interactions in commercial games.

</details>


### [4] [Neurosymbolic Feature Extraction for Identifying Forced Labor in Supply Chains](https://arxiv.org/abs/2507.07217)
*Zili Wang,Frank Montabon,Kristin Yvonne Rozier*

Main category: cs.AI

TL;DR: 本文提出结合神经符号方法和大型语言模型，利用新闻资料，在数据稀少和质量受限的供应链环境下，有效检测非法活动，并系统比较人工与自动化方法的表现。


<details>
  <summary>Details</summary>
Motivation: 供应链网络本身极其复杂，分析难度大，当其涉及如假冒零件、强迫劳动或贩卖人口等非法活动时，这一问题更加严峻。传统机器学习需大量训练数据，但非法供应链的数据非常稀缺且数据质量堪忧，因被刻意篡改以掩盖犯罪。因此，亟需一种无需大型数据集，也能检测复杂数据中特殊模式的自动方法。

Method: 探讨了神经符号方法识别供应链中非法活动的可行性，并比较了基于新闻报道的手动与自动特征提取效果。提出了一种利用问题树方式，驱动大型语言模型（LLM）来分析和量化相关新闻报道的相关性。

Result: 通过该方法，能够系统性评估由人工与机器对供应链中强迫劳动相关新闻的分类差异，为应用于数据稀缺背景下的非法活动检测提供新途径。

Conclusion: 神经符号方法结合LLM可为数据稀疏且数据质量受限时，有效从复杂供应链数据中识别非法活动，且比单纯人工或传统ML方法更具系统性和自动化优势。

Abstract: Supply chain networks are complex systems that are challenging to analyze;
this problem is exacerbated when there are illicit activities involved in the
supply chain, such as counterfeit parts, forced labor, or human trafficking.
While machine learning (ML) can find patterns in complex systems like supply
chains, traditional ML techniques require large training data sets. However,
illicit supply chains are characterized by very sparse data, and the data that
is available is often (purposely) corrupted or unreliable in order to hide the
nature of the activities. We need to be able to automatically detect new
patterns that correlate with such illegal activity over complex, even temporal
data, without requiring large training data sets. We explore neurosymbolic
methods for identifying instances of illicit activity in supply chains and
compare the effectiveness of manual and automated feature extraction from news
articles accurately describing illicit activities uncovered by authorities. We
propose a question tree approach for querying a large language model (LLM) to
identify and quantify the relevance of articles. This enables a systematic
evaluation of the differences between human and machine classification of news
articles related to forced labor in supply chains.

</details>


### [5] [Open Source Planning & Control System with Language Agents for Autonomous Scientific Discovery](https://arxiv.org/abs/2507.07257)
*Licong Xu,Milind Sarkar,Anto I. Lonappan,Íñigo Zubeldia,Pablo Villanueva-Domingo,Santiago Casas,Christian Fidler,Chetana Amancharla,Ujjwal Tiwari,Adrian Bayer,Chadi Ait Ekiou,Miles Cranmer,Adrian Dimitrov,James Fergusson,Kahaan Gandhi,Sven Krippendorf,Andrew Laverick,Julien Lesgourgues,Antony Lewis,Thomas Meier,Blake Sherwin,Kristen Surrao,Francisco Villaescusa-Navarro,Chi Wang,Xueqing Xu,Boris Bolliet*

Main category: cs.AI

TL;DR: 作者提出了一个基于约30个LLM智能体的cmbagent系统，实现了科研任务全自动化，并在宇宙学等任务上效果优于现有方法，系统已上线并开源。


<details>
  <summary>Details</summary>
Motivation: 科学研究任务往往需要较高的自动化和智能化手段，现有大语言模型在单一任务上表现良好，但难以胜任复杂的多任务科研流程。因此，作者希望开发能够协同完成不同科研步骤的多智能体系统，实现从检索、编程、分析到点评的全流程自动化。

Method: 提出了cmbagent多智能体系统，包含约30个专用的大语言模型智能体。系统采用计划与控制策略（Planning & Control）进行流程编排，无需人工介入。每个智能体专注于特定任务，如检索文献、代码编写、结果解释和互相批判输出，并具备本地代码执行能力。

Result: 系统成功应用于宇宙学参数测量的博士级科研任务，并在两套基准测试中，cmbagent的表现优于当前最先进的大语言模型。

Conclusion: cmbagent在无需人工干预的前提下，实现了科学研究任务的全流程自动化，在复杂科研任务处理上展现出优越性能。系统已开源并提供了在线展示和云端部署。

Abstract: We present a multi-agent system for automation of scientific research tasks,
cmbagent. The system is formed by about 30 Large Language Model (LLM) agents
and implements a Planning & Control strategy to orchestrate the agentic
workflow, with no human-in-the-loop at any point. Each agent specializes in a
different task (performing retrieval on scientific papers and codebases,
writing code, interpreting results, critiquing the output of other agents) and
the system is able to execute code locally. We successfully apply cmbagent to
carry out a PhD level cosmology task (the measurement of cosmological
parameters using supernova data) and evaluate its performance on two benchmark
sets, finding superior performance over state-of-the-art LLMs. The source code
is available on GitHub, demonstration videos are also available, and the system
is deployed on HuggingFace and will be available on the cloud.

</details>


### [6] [Application of LLMs to Multi-Robot Path Planning and Task Allocation](https://arxiv.org/abs/2507.07302)
*Ashish Kumar*

Main category: cs.AI

TL;DR: 多智能体强化学习探索困难，本文尝试使用大型语言模型作为专家引导，提高多智能体在规划任务中的探索效率。


<details>
  <summary>Details</summary>
Motivation: 在多智能体强化学习中，如何高效探索环境一直是一个难题。由于多智能体之间的复杂交互，这一问题比单一智能体更为严峻。

Method: 研究利用大型语言模型（LLMs）作为专家规划器，帮助多智能体在规划任务中更高效地探索环境。

Result: 通过引入大型语言模型，能够提升多智能体在环境中的探索效率，尤其是在依赖规划的任务场景下。

Conclusion: 大型语言模型可以作为专家，辅助多智能体强化学习实现更高效的探索，有望提升解决特定任务的能力。

Abstract: Efficient exploration is a well known problem in deep reinforcement learning
and this problem is exacerbated in multi-agent reinforcement learning due the
intrinsic complexities of such algorithms. There are several approaches to
efficiently explore an environment to learn to solve tasks by multi-agent
operating in that environment, of which, the idea of expert exploration is
investigated in this work. More specifically, this work investigates the
application of large-language models as expert planners for efficient
exploration in planning based tasks for multiple agents.

</details>


### [7] [ViDove: A Translation Agent System with Multimodal Context and Memory-Augmented Reasoning](https://arxiv.org/abs/2507.07306)
*Yichen Lu,Wei Dai,Jiaen Liu,Ching Wing Kwok,Zongheng Wu,Xudong Xiao,Ao Sun,Sheng Fu,Jianyuan Zhan,Yian Wang,Takatomo Saito,Sicheng Lai*

Main category: cs.AI

TL;DR: ViDove是一个面向多模态输入的翻译系统，能够利用图像和上下文信息提升翻译质量，在多个任务和指标上显著优于现有方法，并推出了新的视频字幕和翻译基准数据集DoveBench。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型（LLM）的翻译系统虽然在人类级别的文本翻译上表现优异，还能处理较长和复杂的文本，但主要局限于纯文本输入，无法充分利用图像和上下文等多模态信息。

Method: 提出ViDove系统，可处理多模态输入（包括视觉与上下文背景信息）。系统融合了多模态记忆系统和具备领域知识的长短期记忆模块，从而模拟人类翻译工作流程，提升翻译准确性和适应性。

Result: ViDove在字幕生成和常规翻译任务上，BLEU分数提升28%，SubER提升15%，超过现有最先进的基线系统。

Conclusion: ViDove能有效利用视觉和领域上下文背景，解决多模态翻译难题，显著提升翻译质量。

Abstract: LLM-based translation agents have achieved highly human-like translation
results and are capable of handling longer and more complex contexts with
greater efficiency. However, they are typically limited to text-only inputs. In
this paper, we introduce ViDove, a translation agent system designed for
multimodal input. Inspired by the workflow of human translators, ViDove
leverages visual and contextual background information to enhance the
translation process. Additionally, we integrate a multimodal memory system and
long-short term memory modules enriched with domain-specific knowledge,
enabling the agent to perform more accurately and adaptively in real-world
scenarios. As a result, ViDove achieves significantly higher translation
quality in both subtitle generation and general translation tasks, with a 28%
improvement in BLEU scores and a 15% improvement in SubER compared to previous
state-of-the-art baselines. Moreover, we introduce DoveBench, a new benchmark
for long-form automatic video subtitling and translation, featuring 17 hours of
high-quality, human-annotated data. Our code is available here:
https://github.com/pigeonai-org/ViDove

</details>


### [8] [Meek Models Shall Inherit the Earth](https://arxiv.org/abs/2507.07931)
*Hans Gundlach,Jayson Lynch,Neil Thompson*

Main category: cs.AI

TL;DR: 随着算力扩展的边际收益递减，AI模型能力在不同主体间差距将缩小，AI领域或将从极度集中的格局向更多主体共享能力演变，政策和战略应据此调整。


<details>
  <summary>Details</summary>
Motivation: 过去十年里只有少数几家公司通过大规模扩展算力推动AI系统发展，这导致了AI模型性能不平等。本文关注算力扩展收益递减这一现象，并探讨其对AI能力和行业格局的影响。

Method: 作者提出了一个理论模型，分析在固定分布的下一步预测任务下，AI模型对算力的边际收益如何快速递减。同时，结合训练损失等代理指标和理论、实证数据，对AI模型能力差异随时间的变化进行了实证分析。

Result: 本文发现，伴随当前的扩展逻辑，即便少数公司能更快扩展计算资源，其获得的模型能力优势将越来越小；“弱小”的低算力模型最终将接近领先模型的表现。

Conclusion: AI能力不会持续集中于极少数算力强大的公司，而是将趋于收敛。弱算力模型的能力提升将对AI战略和政策产生重要影响，现有政策需重新评估和调整。

Abstract: The past decade has seen incredible scaling of AI systems by a few companies,
leading to inequality in AI model performance. This paper argues that, contrary
to prevailing intuition, the diminishing returns to compute scaling will lead
to a convergence of AI model capabilities. In other words, meek models (those
with limited computation budget) shall inherit the earth, approaching the
performance level of the best models overall. We develop a model illustrating
that under a fixed-distribution next-token objective, the marginal capability
returns to raw compute shrink substantially. Given current scaling practices,
we argue that these diminishing returns are strong enough that even companies
that can scale their models exponentially faster than other organizations will
eventually have little advantage in capabilities. As part of our argument, we
give several reasons that proxies like training loss differences capture
important capability measures using evidence from benchmark data and
theoretical performance models. In addition, we analyze empirical data on the
capability difference of AI models over time. Finally, in light of the
increasing ability of meek models, we argue that AI strategy and policy require
reexamination, and we outline the areas this shift will affect.

</details>


### [9] [On the Impossibility of Separating Intelligence from Judgment: The Computational Intractability of Filtering for AI Alignment](https://arxiv.org/abs/2507.07341)
*Sarah Ball,Greg Gluch,Shafi Goldwasser,Frauke Kreuter,Omer Reingold,Guy N. Rothblum*

Main category: cs.AI

TL;DR: 外部过滤器（无论对输入还是输出）无法有效防止大语言模型生成有害内容，只有深入模型内部实现对齐才能保障安全。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）部署的增多，人们担忧其可能被滥用来生成有害内容。为了防止这种情况，需要研究模型对齐性，尤其关注通过过滤器防止不安全信息生成的方式。

Method: 本文分析了两种过滤策略：在输入提示到达模型前进行过滤，以及在模型生成内容后再对输出进行过滤。通过在密码学难度假设下建立理论分析和证明，评估这两种过滤策略的计算可行性。

Result: 研究证明：对于某些LLM，无法设计高效的输入过滤器，攻击者可以轻松构造与正常输入计算上无法区分的有害输入。此外，文章还发现，在自然设定下，输出过滤同样在计算上难以实现。这些发现都证实外部过滤难以保障安全。

Conclusion: 结论是，只依赖于模型外部的过滤器（无论输入或输出）都无法实现真正的安全防护，仅靠黑盒访问无法让LLM对齐，因此AI系统的智能与其判断力不可分离。

Abstract: With the increased deployment of large language models (LLMs), one concern is
their potential misuse for generating harmful content. Our work studies the
alignment challenge, with a focus on filters to prevent the generation of
unsafe information. Two natural points of intervention are the filtering of the
input prompt before it reaches the model, and filtering the output after
generation. Our main results demonstrate computational challenges in filtering
both prompts and outputs. First, we show that there exist LLMs for which there
are no efficient prompt filters: adversarial prompts that elicit harmful
behavior can be easily constructed, which are computationally indistinguishable
from benign prompts for any efficient filter. Our second main result identifies
a natural setting in which output filtering is computationally intractable. All
of our separation results are under cryptographic hardness assumptions. In
addition to these core findings, we also formalize and study relaxed mitigation
approaches, demonstrating further computational barriers. We conclude that
safety cannot be achieved by designing filters external to the LLM internals
(architecture and weights); in particular, black-box access to the LLM will not
suffice. Based on our technical results, we argue that an aligned AI system's
intelligence cannot be separated from its judgment.

</details>


### [10] [Working with AI: Measuring the Occupational Implications of Generative AI](https://arxiv.org/abs/2507.07935)
*Kiran Tomlinson,Sonia Jaffe,Will Wang,Scott Counts,Siddharth Suri*

Main category: cs.AI

TL;DR: 作者基于大规模Bing Copilot对话数据，分析了AI如何介入各类职业活动，发现AI更善于辅助信息型和知识密集型工作，对相关职业群体有显著影响，同时揭示了工资、教育等对AI适用性的影响。


<details>
  <summary>Details</summary>
Motivation: 生成式人工智能（AI）被迅速采用，可能影响广泛的职业任务，社会亟需了解AI对经济的影响。本文旨在分析人们与AI协作的工作活动、成功率及其适用职业，推动相关认知。

Method: 作者分析了20万条来自微软Bing Copilot用户的匿名化对话数据，统计用户寻求AI协助的主要工作活动以及AI实际执行的活动，并将活动类别与职业任务数据结合，计算每个职业的AI适用分数。

Result: 最常被请求AI协助的活动包括信息收集与写作，而AI最常执行的包括信息提供、写作、教学和建议。知识型职业（如计算机、数学、行政支持）、以及涉及信息传递的职业（如销售）在AI适用性上得分最高。论文还分析了与工资、教育水平的关联及实际应用与职业AI影响预测之间的差异。

Conclusion: AI尤其适用于知识工作与信息处理领域，一些职业受到的影响较大。结合活动成功率、影响范围和现实应用，本文为未来经济结构变化和职业规划提供了数据支持。

Abstract: Given the rapid adoption of generative AI and its potential to impact a wide
range of tasks, understanding the effects of AI on the economy is one of
society's most important questions. In this work, we take a step toward that
goal by analyzing the work activities people do with AI, how successfully and
broadly those activities are done, and combine that with data on what
occupations do those activities. We analyze a dataset of 200k anonymized and
privacy-scrubbed conversations between users and Microsoft Bing Copilot, a
publicly available generative AI system. We find the most common work
activities people seek AI assistance for involve gathering information and
writing, while the most common activities that AI itself is performing are
providing information and assistance, writing, teaching, and advising.
Combining these activity classifications with measurements of task success and
scope of impact, we compute an AI applicability score for each occupation. We
find the highest AI applicability scores for knowledge work occupation groups
such as computer and mathematical, and office and administrative support, as
well as occupations such as sales whose work activities involve providing and
communicating information. Additionally, we characterize the types of work
activities performed most successfully, how wage and education correlate with
AI applicability, and how real-world usage compares to predictions of
occupational AI impact.

</details>


### [11] [Supply Chain Optimization via Generative Simulation and Iterative Decision Policies](https://arxiv.org/abs/2507.07355)
*Haoyue Bai,Haoyu Wang,Nanxu Gong,Xinyuan Wang,Wangyang Ying,Haifeng Chen,Yanjie Fu*

Main category: cs.AI

TL;DR: 文章提出了一种端到端集成仿真与决策的新型框架Sim-to-Dec，结合生成式自回归模型与历史-未来双感知决策机制，在真实数据集上显著提升了供应链运输的及时交付率和利润表现。


<details>
  <summary>Details</summary>
Motivation: 供应链运输需要在高响应性和经济效率之间取得平衡，而运输模式的战略决策对这两个目标都有重要影响。由于实际环境存在不确定性和风险，需借助低风险、可观测的环境来辅助运输策略的设计。

Method: 提出了Sim-to-Dec框架，包括：（1）自回归建模的生成式仿真模块，实现对运输过程连续状态变化的模拟，减少对手工规则的依赖，提高对数据波动的鲁棒性；（2）历史-未来双感知决策模型，通过与仿真器的交互进行端到端优化，实现策略不断精炼。

Result: 在三个真实数据集上进行大量实验，Sim-to-Dec相比于传统方法在及时交付率和利润上均有显著提升。

Conclusion: Sim-to-Dec框架能够有效提高供应链运输决策的响应性和经济效率，为运输策略的仿真与优化提供了新范式。

Abstract: High responsiveness and economic efficiency are critical objectives in supply
chain transportation, both of which are influenced by strategic decisions on
shipping mode. An integrated framework combining an efficient simulator with an
intelligent decision-making algorithm can provide an observable, low-risk
environment for transportation strategy design. An ideal simulation-decision
framework must (1) generalize effectively across various settings, (2) reflect
fine-grained transportation dynamics, (3) integrate historical experience with
predictive insights, and (4) maintain tight integration between simulation
feedback and policy refinement. We propose Sim-to-Dec framework to satisfy
these requirements. Specifically, Sim-to-Dec consists of a generative
simulation module, which leverages autoregressive modeling to simulate
continuous state changes, reducing dependence on handcrafted domain-specific
rules and enhancing robustness against data fluctuations; and a history-future
dual-aware decision model, refined iteratively through end-to-end optimization
with simulator interactions. Extensive experiments conducted on three
real-world datasets demonstrate that Sim-to-Dec significantly improves timely
delivery rates and profit.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [12] [Planted in Pretraining, Swayed by Finetuning: A Case Study on the Origins of Cognitive Biases in LLMs](https://arxiv.org/abs/2507.07186)
*Itay Itzhak,Yonatan Belinkov,Gabriel Stanovsky*

Main category: cs.CL

TL;DR: 论文提出新的实验方法区分预训练、微调和随机性对大语言模型认知偏差的影响，发现偏差主要源于预训练阶段，为后续偏差评估和缓解策略提供了理论支持。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）会展现出类似人类的不理性决策的认知偏差。已有研究发现，不同模型之间的偏差有差异，并可能因指令微调被放大，但偏差的来源尚不明确，是来自预训练、微调，还是训练过程中的随机性噪声？

Method: 提出了两步因果实验方法：第一步，用不同随机种子多次微调同一模型，考察训练随机性对30多种认知偏差的影响；第二步，设计cross-tuning（交叉微调），即交换不同模型的指令数据集，检验偏差是否依赖于数据集本身。

Result: 实验发现，训练的随机性会带来一定的偏差变化，但偏差主要由预训练决定：拥有相同预训练主干的模型，其偏差模式更为相似，而共享微调数据的模型相似性较低。

Conclusion: 研究说明，理解微调后模型的偏差时，必须关注其预训练阶段的来源，不能仅归因于微调影响。为未来评估和消解LLMs偏差提供了新的思路。

Abstract: Large language models (LLMs) exhibit cognitive biases -- systematic
tendencies of irrational decision-making, similar to those seen in humans.
Prior work has found that these biases vary across models and can be amplified
by instruction tuning. However, it remains unclear if these differences in
biases stem from pretraining, finetuning, or even random noise due to training
stochasticity. We propose a two-step causal experimental approach to
disentangle these factors. First, we finetune models multiple times using
different random seeds to study how training randomness affects over $30$
cognitive biases. Second, we introduce \emph{cross-tuning} -- swapping
instruction datasets between models to isolate bias sources. This swap uses
datasets that led to different bias patterns, directly testing whether biases
are dataset-dependent. Our findings reveal that while training randomness
introduces some variability, biases are mainly shaped by pretraining: models
with the same pretrained backbone exhibit more similar bias patterns than those
sharing only finetuning data. These insights suggest that understanding biases
in finetuned models requires considering their pretraining origins beyond
finetuning effects. This perspective can guide future efforts to develop
principled strategies for evaluating and mitigating bias in LLMs.

</details>


### [13] [Prompt Perturbations Reveal Human-Like Biases in LLM Survey Responses](https://arxiv.org/abs/2507.07188)
*Jens Rupprecht,Georg Ahnert,Markus Strohmaier*

Main category: cs.CL

TL;DR: 大语言模型在调查问卷中存在‘新近性偏差’，模型对问题和选项微调敏感，大模型更稳健但仍存脆弱性，提示社科研究需严谨测试与优化模型使用流程。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在社会科学调研中作为人类代理被广泛使用，对其问卷反应的可靠性和是否会引入人类已知偏差的理解尚不充分，因此研究其在标准化问卷语境下的稳健性和偏差问题。

Method: 选择九种不同的大语言模型，使用世界价值观调查（World Values Survey, WVS）中的问题，设计了11种针对问题措辞及选项结构的扰动，总共进行了超过167,000次模拟问卷实验，系统性地评估了各种扰动对模型问卷回复的影响。

Result: 所有受测LLM都对选项顺序敏感，明显倾向于选择最后一个选项，即存在人类常见的“新近性偏差”。大模型表现抗扰能力更强，但针对语义扰动（如同义改写）和复合扰动依然脆弱。提示人们在使用LLM生成问卷数据时，需重视prompt设计和健壮性测试。

Conclusion: 所有测试的LLMs在应对语义扰动时都表现出不同程度的脆弱性，并且普遍存在偏好最后一个选项的“新近性偏差”。大模型相较更稳健，但对同义改写等语义变化和多重扰动依然敏感。LLMs的问卷反应偏差与人类受试者存在一定一致性。

Abstract: Large Language Models (LLMs) are increasingly used as proxies for human
subjects in social science surveys, but their reliability and susceptibility to
known response biases are poorly understood. This paper investigates the
response robustness of LLMs in normative survey contexts -- we test nine
diverse LLMs on questions from the World Values Survey (WVS), applying a
comprehensive set of 11 perturbations to both question phrasing and answer
option structure, resulting in over 167,000 simulated interviews. In doing so,
we not only reveal LLMs' vulnerabilities to perturbations but also reveal that
all tested models exhibit a consistent \textit{recency bias} varying in
intensity, disproportionately favoring the last-presented answer option. While
larger models are generally more robust, all models remain sensitive to
semantic variations like paraphrasing and to combined perturbations. By
applying a set of perturbations, we reveal that LLMs partially align with
survey response biases identified in humans. This underscores the critical
importance of prompt design and robustness testing when using LLMs to generate
synthetic survey data.

</details>


### [14] [SynthTextEval: Synthetic Text Data Generation and Evaluation for High-Stakes Domains](https://arxiv.org/abs/2507.07229)
*Krithika Ramesh,Daniel Smolyak,Zihao Zhao,Nupoor Gandhi,Ritu Agarwal,Margrét Bjarnadóttir,Anjalie Field*

Main category: cs.CL

TL;DR: 提出合成文本多维度评估工具SynthTextEval，覆盖效用、公平、隐私、安全和专家反馈，推动其在高风险领域的隐私保护应用。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型合成文本越来越流畅，对隐私保护和实际应用的需求增加，但缺乏系统化、多维度的合成文本评估方法。

Method: 提出名为SynthTextEval的工具包，允许用户对合成文本在多个维度（如下游系统效用、公平性、隐私泄露风险、分布差异、专家定性反馈等）进行系统化和标准化评估。工具包既支持上传自有合成数据，也可用其生成模块生成数据，在包括医疗和法律在内的高风险领域数据集上展示了其功能。

Result: SynthTextEval能够便捷地进行包括效用、公平、隐私、分布和专家反馈在内的多维度合成文本评估，提高了隐私保护型AI开发中合成文本的实用性和可行性。

Conclusion: 通过统一和标准化评估流程，提升了合成文本在AI开发中用于隐私保护场景的可用性和信心，特别是在医疗和法律等高风险领域。

Abstract: We present SynthTextEval, a toolkit for conducting comprehensive evaluations
of synthetic text. The fluency of large language model (LLM) outputs has made
synthetic text potentially viable for numerous applications, such as reducing
the risks of privacy violations in the development and deployment of AI systems
in high-stakes domains. Realizing this potential, however, requires principled
consistent evaluations of synthetic data across multiple dimensions: its
utility in downstream systems, the fairness of these systems, the risk of
privacy leakage, general distributional differences from the source text, and
qualitative feedback from domain experts. SynthTextEval allows users to conduct
evaluations along all of these dimensions over synthetic data that they upload
or generate using the toolkit's generation module. While our toolkit can be run
over any data, we highlight its functionality and effectiveness over datasets
from two high-stakes domains: healthcare and law. By consolidating and
standardizing evaluation metrics, we aim to improve the viability of synthetic
text, and in-turn, privacy-preservation in AI development.

</details>


### [15] [Medical Red Teaming Protocol of Language Models: On the Importance of User Perspectives in Healthcare Settings](https://arxiv.org/abs/2507.07248)
*Minseon Kim,Jean-Philippe Corbeil,Alessandro Sordoni,Francois Beaulieu,Paul Vozila*

Main category: cs.CL

TL;DR: 针对医疗领域大模型安全评估盲点，提出多视角安全协议和基准集，有力支撑医疗LLM的安全应用。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）在医学等多个领域应用的扩展，其在医疗领域的应用带来了重要的安全隐患，尤其是由于使用者包括患者和临床医生，模型输出可能直接影响人类健康。而此前的安全评估主要集中在通用安全基准，缺乏针对医疗场景和不同用户视角的分析。

Method: 作者提出了一套专为医疗领域量身定制的安全评估协议，分别从患者和临床医生的视角进行分析，并辅以通用安全标准的评估。同时，作者构建了PatientSafetyBench基准集，涵盖5个关键安全类别共466个样本，并应用red-teaming协议对MediPhi模型集进行了案例研究和实证评估。

Result: 建立了首个面向医疗LLM的多视角安全评价标准，填补了患者、临床医生和一般用户三重视角下的安全评估空白，通过定量实验论证其有效性，为医疗类LLM模型的更安全落地应用提供了基础支撑。

Conclusion: 本工作提出了创新性医疗LLM安全评估协议和基准集，从三种用户视角促进了领域内安全标准体系的建立，有助于医疗AI模型更负责任、规范和安全地推广应用。

Abstract: As the performance of large language models (LLMs) continues to advance,
their adoption is expanding across a wide range of domains, including the
medical field. The integration of LLMs into medical applications raises
critical safety concerns, particularly due to their use by users with diverse
roles, e.g. patients and clinicians, and the potential for model's outputs to
directly affect human health. Despite the domain-specific capabilities of
medical LLMs, prior safety evaluations have largely focused only on general
safety benchmarks. In this paper, we introduce a safety evaluation protocol
tailored to the medical domain in both patient user and clinician user
perspectives, alongside general safety assessments and quantitatively analyze
the safety of medical LLMs. We bridge a gap in the literature by building the
PatientSafetyBench containing 466 samples over 5 critical categories to measure
safety from the perspective of the patient. We apply our red-teaming protocols
on the MediPhi model collection as a case study. To our knowledge, this is the
first work to define safety evaluation criteria for medical LLMs through
targeted red-teaming taking three different points of view - patient,
clinician, and general user - establishing a foundation for safer deployment in
medical domains.

</details>


### [16] [The Impact of Background Speech on Interruption Detection in Collaborative Groups](https://arxiv.org/abs/2507.07280)
*Mariah Bradford,Nikhil Krishnaswamy,Nathaniel Blanchard*

Main category: cs.CL

TL;DR: 本研究提出了适用于多组协作学习场景的鲁棒打断检测方法，有效应对了教室中普遍存在的重叠语音，揭示了打断的语言和韵律特征，为未来群体对话研究提供了新的技术基础。


<details>
  <summary>Details</summary>
Motivation: 在协作学习环境中，打断行为对于塑造小组互动和知识构建有重要影响。AI辅助教师监测这些互动有很大潜力，但目前大多数打断检测和解释的相关研究仅限于单一对话且音频环境较干净，无法应对教室多小组、多并发对话的复杂情境。

Method: 分析并比较单一对话和多组对话环境中的打断检测问题，提出了一种对重叠语音具有鲁棒性的先进打断识别方法，适用于教室场景。此外，对打断行为的语言和韵律特征进行了深入分析。

Result: 提出的方法可以在存在多重叠语音的复杂场景下准确识别打断行为，验证了其在教室多组协作学习中的应用潜力，并揭示了打断在群体互动中的语言和韵律表现特点。

Conclusion: 研究推动了多组对话场景下的打断自动检测技术发展，为以后跟踪群体对话时更好地考虑多组重叠语音的影响奠定了基础。

Abstract: Interruption plays a crucial role in collaborative learning, shaping group
interactions and influencing knowledge construction. AI-driven support can
assist teachers in monitoring these interactions. However, most previous work
on interruption detection and interpretation has been conducted in
single-conversation environments with relatively clean audio. AI agents
deployed in classrooms for collaborative learning within small groups will need
to contend with multiple concurrent conversations -- in this context,
overlapping speech will be ubiquitous, and interruptions will need to be
identified in other ways. In this work, we analyze interruption detection in
single-conversation and multi-group dialogue settings. We then create a
state-of-the-art method for interruption identification that is robust to
overlapping speech, and thus could be deployed in classrooms. Further, our work
highlights meaningful linguistic and prosodic information about how
interruptions manifest in collaborative group interactions. Our investigation
also paves the way for future works to account for the influence of overlapping
speech from multiple groups when tracking group dialog.

</details>


### [17] [Multi-Agent Retrieval-Augmented Framework for Evidence-Based Counterspeech Against Health Misinformation](https://arxiv.org/abs/2507.07307)
*Anirban Saha Anik,Xiaoying Song,Elliott Wang,Bryan Wang,Bengisu Yarimbas,Lingzi Hong*

Main category: cs.CL

TL;DR: 本文提出多智能体RAG框架提升反健康错误信息生成效果，在相关性、信息性等方面超越基线，并通过实验和人工评测证实方法有效。


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法在生成反驳健康错误信息时，证据有限且对输出结果控制力不足。希望通过优化检索与生成机制，提升生成内容的质量和相关性。

Method: 采用多智能体检索增强生成框架，整合多个大型语言模型，分别负责知识检索、证据增强和响应优化。融合静态与动态证据信息，并通过消融实验和人工评测对框架有效性进行验证。

Result: 提出的框架生成的反驳言论在多个维度上优于以往方法，且通过消融研究和人工评测证实了各环节和整体方法的有效性。

Conclusion: 实验结果表明，所提出的多智能体RAG框架在礼貌性、相关性、信息性和事实准确性方面均优于基线方法。同时，细化流程显著提升了反错误信息言论的质量，并获得了人类偏好。各组件的消融实验验证了其必要性。

Abstract: Large language models (LLMs) incorporated with Retrieval-Augmented Generation
(RAG) have demonstrated powerful capabilities in generating counterspeech
against misinformation. However, current studies rely on limited evidence and
offer less control over final outputs. To address these challenges, we propose
a Multi-agent Retrieval-Augmented Framework to generate counterspeech against
health misinformation, incorporating multiple LLMs to optimize knowledge
retrieval, evidence enhancement, and response refinement. Our approach
integrates both static and dynamic evidence, ensuring that the generated
counterspeech is relevant, well-grounded, and up-to-date. Our method
outperforms baseline approaches in politeness, relevance, informativeness, and
factual accuracy, demonstrating its effectiveness in generating high-quality
counterspeech. To further validate our approach, we conduct ablation studies to
verify the necessity of each component in our framework. Furthermore, human
evaluations reveal that refinement significantly enhances counterspeech quality
and obtains human preference.

</details>


### [18] [GNN-CNN: An Efficient Hybrid Model of Convolutional and Graph Neural Networks for Text Representation](https://arxiv.org/abs/2507.07414)
*Fardin Rastakhiz*

Main category: cs.CL

TL;DR: 本文提出结合GNN与CNN并融合LLM信息的新模型，能够高效处理长文本，在多项文本分类任务中表现与主流模型相当或更优。


<details>
  <summary>Details</summary>
Motivation: 当前主流的Transformer模型在处理长文本时计算复杂度呈平方增长，导致时间、成本和能效低下。因此亟需一种更高效且能够处理长文本信息的深度学习架构。

Method: 提出了一种结合图神经网络（GNN）和卷积神经网络（CNN）的新型模型架构，并集成了实时端到端图生成机制。该模型以字符为单位输入，避免填充和截断；同时通过高效字典查找引入大语言模型（LLM）的词元嵌入与情感极性信息，利用CNN捕捉局部上下文，再通过格子状图结构和小世界图汇聚全局信息。

Result: 生成的图结构具有良好的语义组织性（聚类系数约0.45，平均最短路径4~5），在情感分析、新闻分类等多个文本分类任务上，与当前先进模型相比表现高效且具有竞争力。

Conclusion: 该模型在长文本处理上实现了效率与性能的平衡，实验验证了其实时处理能力以及在多个任务中的优良表现。

Abstract: Time, cost, and energy efficiency are critical considerations in
Deep-Learning (DL), particularly when processing long texts. Transformers,
which represent the current state of the art, exhibit quadratic computational
complexity relative to input length, making them inefficient for extended
documents. This study introduces a novel model architecture that combines Graph
Neural Networks (GNNs) and Convolutional Neural Networks (CNNs), integrated
with a real-time, end-to-end graph generation mechanism. The model processes
compact batches of character-level inputs without requiring padding or
truncation. To enhance performance while maintaining high speed and efficiency,
the model incorporates information from Large Language Models (LLMs), such as
token embeddings and sentiment polarities, through efficient dictionary
lookups. It captures local contextual patterns using CNNs, expands local
receptive fields via lattice-based graph structures, and employs small-world
graphs to aggregate document-level information. The generated graphs exhibit
structural properties indicative of meaningful semantic organization, with an
average clustering coefficient of approximately 0.45 and an average shortest
path length ranging between 4 and 5. The model is evaluated across multiple
text classification tasks, including sentiment analysis and
news-categorization, and is compared against state-of-the-art models.
Experimental results confirm the proposed model's efficiency and competitive
performance.

</details>


### [19] [MedReadCtrl: Personalizing medical text generation with readability-controlled instruction learning](https://arxiv.org/abs/2507.07419)
*Hieu Tran,Zonghai Yao,Won Seok Jang,Sharmin Sultana,Allen Chang,Yuan Zhang,Hong Yu*

Main category: cs.CL

TL;DR: 提出了一种能控制输出可读性的医疗大模型MedReadCtrl，在多个任务上优于GPT-4，显著提升医疗文本对低健康素养人群的友好度，为患者教育和AI医疗普及提供了有效解决方案。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在医疗领域应用潜力巨大，但实际部署时面临人机沟通的挑战，特别是如何生成既个性化又易于理解的内容。医疗专业内容复杂，普通患者往往难以理解，因此需要一种能调整生成内容可读性的AI框架。

Method: 提出了MedReadCtrl，这是一种可控可读性指令微调框架，通过对大语言模型进行训练，使其能够在不损失语义的情况下，根据需求调整输出文本的复杂度。研究设计了多数据集、多任务的实验，用以评估该框架在医学和通用领域的表现。

Result: 在九个数据集和三项任务上，MedReadCtrl在跟随可读性指令方面错误率显著低于GPT-4（如ReadMe上1.39 vs. 1.59，p<0.001），且在新颖的临床任务上表现有明显提升（如MTSamples上ROUGE-L提升14.7，SARI提升6.18）。专家评审时，MedReadCtrl的输出偏好率大幅领先于对照组（71.7% vs 23.3%），尤其是在低健康素养水平下表现突出。

Conclusion: MedReadCtrl能够根据需求调整医学内容的可读性，有效兼顾医学意图与文本易懂性，能大规模提升患者教育质量，推动AI医疗服务公平普及。

Abstract: Generative AI has demonstrated strong potential in healthcare, from clinical
decision support to patient-facing chatbots that improve outcomes. A critical
challenge for deployment is effective human-AI communication, where content
must be both personalized and understandable. We introduce MedReadCtrl, a
readability-controlled instruction tuning framework that enables LLMs to adjust
output complexity without compromising meaning. Evaluations of nine datasets
and three tasks across medical and general domains show that MedReadCtrl
achieves significantly lower readability instruction-following errors than
GPT-4 (e.g., 1.39 vs. 1.59 on ReadMe, p<0.001) and delivers substantial gains
on unseen clinical tasks (e.g., +14.7 ROUGE-L, +6.18 SARI on MTSamples).
Experts consistently preferred MedReadCtrl (71.7% vs. 23.3%), especially at low
literacy levels. These gains reflect MedReadCtrl's ability to restructure
clinical content into accessible, readability-aligned language while preserving
medical intent, offering a scalable solution to support patient education and
expand equitable access to AI-enabled care.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [20] [The Pandora's Box Problem with Sequential Inspections](https://arxiv.org/abs/2507.07508)
*Ali Aouad,Jingwei Ji,Yaron Shaposhnik*

Main category: cs.CE

TL;DR: 本文扩展了经典潘多拉盒子问题，引入部分开启获得部分信息的设定，通过结构分析、松弛解法与数值实验，系统刻画了该环境下的信息-成本权衡及最优（近似）策略，并发现阈值策略依然具有高效性。


<details>
  <summary>Details</summary>
Motivation: 潘多拉盒子问题是经济理论的重要模型，传统问题假设代理只能以固定成本打开盒子获取全部信息；本文关注更现实的一般化场景，即允许代理选择以较低成本部分开启盒子获取部分信息，从而引入信息获取深度与成本效率间的新权衡。

Method: 本文利用随机优化方法，首先揭示最优策略的结构性质，给出决策原理的洞见；其次推导问题的松弛解并提出可证明近似最优的算法；再次在特殊但非平凡情形下对最优策略做精确刻画；最后通过大量数值实验比较不同策略效果。

Result: 分析显示，基于阈值扩展的直觉策略（可视为潘多拉盒子最优解的推广）在该问题中能够有效指导决策。作者还证明了该模型具有一定求解难度，并在具体场景下给出了最优或近似最优的策略结构。数值结果进一步验证了所提出策略的优越性与洞见。

Conclusion: 通过对扩展版潘多拉盒子问题的系统分析，本文极大丰富了关于成本与信息获取权衡下最优决策的理论理解，为经济学与智能搜索提供了结构化决策的有力工具。

Abstract: The Pandora's box problem (Weitzman 1979) is a core model in economic theory
that captures an agent's (Pandora's) search for the best alternative (box). We
study an important generalization of the problem where the agent can either
fully open boxes for a certain fee to reveal their exact values or partially
open them at a reduced cost. This introduces a new tradeoff between information
acquisition and cost efficiency. We establish a hardness result and employ an
array of techniques in stochastic optimization to provide a comprehensive
analysis of this model. This includes (1) the identification of structural
properties of the optimal policy that provide insights about optimal decisions;
(2) the derivation of problem relaxations and provably near-optimal solutions;
(3) the characterization of the optimal policy in special yet non-trivial
cases; and (4) an extensive numerical study that compares the performance of
various policies, and which provides additional insights about the optimal
policy. Throughout, we show that intuitive threshold-based policies that extend
the Pandora's box optimal solution can effectively guide search decisions.

</details>


### [21] [Meshless projection model-order reduction via reference spaces for smoothed-particle hydrodynamics](https://arxiv.org/abs/2507.07830)
*Steven N. Rodriguez,Steven L. Brunton,Liam K. Magargal,Parisa Khodabakshi,Justin W. Jaworski,Nicoleta A. Apetre,John C. Steuben,John G. Michopoulos,Athanasios Iliopoulos*

Main category: cs.CE

TL;DR: 本文提出了面向SPH方法的无网格模型降阶新框架，能在保持无网格性的同时用低维子空间准确预测流场，大幅提升计算效率，为SPH类问题快速计算奠定基础。


<details>
  <summary>Details</summary>
Motivation: 在使用无网格弱可压缩光滑粒子流体动力学（SPH）方法的数值仿真中，由于其结构无关和动态混合的数值拓扑特性，难以寻找到低维的模拟子空间，从而导致高昂的计算成本。

Method: 引入模态参考空间，将SPH快照数据投影到参考空间，通过传统的模态分解方法（如POD）发现低维表示，并在预测阶段通过散点数据插值将模态量映射回SPH空间。构建了无网格Galerkin POD（GPOD）和对偶Petrov--Galerkin（APG）投影模型降阶（PMOR）框架。

Result: 在泰勒-格林涡流、盖板驱动腔流和开放腔流三个数值实验上测试，结果显示重构和预测速度场与高维原模拟较为一致，能够有效地在低维子空间推演SPH方程；但压力场对投影误差敏感，不过可以通过如APG等非线性近似方法改善。

Conclusion: 提出的无网格模型降阶框架可以大幅降低SPH模拟的计算成本，并为处理结构无关、动态混合的数值模拟提供一种有效途径。

Abstract: This work proposes a model-order reduction framework for the meshless weakly
compressible smoothed particle hydrodynamics (SPH) method. The proposed
framework introduces the concept of modal reference spaces to overcome the
challenges of discovering low-dimensional subspaces from unstructured, dynamic,
and mixing numerical topology that is often seen in SPH simulations. The
proposed modal reference spaces enable a low-dimensional representation of the
SPH field equations while maintaining their inherent meshless qualities. Modal
reference spaces are constructed by projecting SPH snapshot data onto a
reference space where low-dimensionality of field quantities can be discovered
via traditional modal decomposition techniques (e.g., the proper orthogonal
decomposition (POD)). Modal quantities are mapped back to the meshless SPH
space via scattered data interpolation during the online predictive stage. The
proposed model-order reduction framework is cast into the \emph{meshless}
Galerkin POD (GPOD) and the Adjoint Petrov--Galerkin (APG) projection
model-order reduction (PMOR) formulation. The PMORs are tested on three
numerical experiments: 1) the Taylor--Green vortex; 2) lid-driven cavity; and
3) flow past an open cavity. Results show good agreement in reconstructed and
predictive velocity fields, which showcase the ability of the proposed
framework to evolve the unstructured, dynamic, and mixing SPH field equations
in a low-dimensional subspace. Results also show that the pressure field is
sensitive to the projection error due to the stiff weakly-compressible
assumption made in the current SPH framework, but can be alleviated through
nonlinear approximations, such as the APG approach. Ultimately, the presented
meshless model-order reduction framework marks a step toward enabling drastic
cost savings of SPH simulations.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [22] [Short-Term Gains, Long-Term Gaps: The Impact of GenAI and Search Technologies on Retention](https://arxiv.org/abs/2507.07357)
*Mahir Akgun,Sacip Toker*

Main category: cs.CY

TL;DR: AI工具能提升低阶任务的即时学习表现，但对长期记忆和高阶任务帮助有限。教育中需将AI辅助与有效的教学策略结合，才能实现深度和持久的学习效果。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI（如ChatGPT）的兴起，学生获取和处理信息的方式发生转变，本研究旨在探讨这些新兴工具对学生学习效果和知识保留的影响。

Method: 以123名学生为样本，分组使用ChatGPT、Google和电子教材，基于Bloom认知分类法，测试其在不同认知复杂度任务中的表现，并对比其即时评估和后续记忆测试成绩。

Result: 在低阶认知任务中，ChatGPT和Google组即时表现优于对照组，但在后续记忆测试中的优势消失，高阶任务无组间显著差异，反而对照组记忆表现较好。指出AI工具有助短期完成任务，但不保证长期掌握。

Conclusion: AI工具（如ChatGPT）和搜索引擎（如Google）能够提升学生在低阶认知任务中的即时表现，但对长期记忆的促进作用有限，且高阶认知任务的效果并不优于传统工具。AI工具应与结构化学习策略结合，才能更好促进知识保留与深度学习。

Abstract: The rise of Generative AI (GenAI) tools, such as ChatGPT, has transformed how
students access and engage with information, raising questions about their
impact on learning outcomes and retention. This study investigates how GenAI
(ChatGPT), search engines (Google), and e-textbooks influence student
performance across tasks of varying cognitive complexity, based on Bloom's
Taxonomy. Using a sample of 123 students, we examined performance in three
tasks: [1] knowing and understanding, [2] applying, and [3] synthesizing,
evaluating, and creating. Results indicate that ChatGPT and Google groups
outperformed the control group in immediate assessments for lower-order
cognitive tasks, benefiting from quick access to structured information.
However, their advantage diminished over time, with retention test scores
aligning with those of the e-textbook group. For higher-order cognitive tasks,
no significant differences were observed among groups, with the control group
demonstrating the highest retention. These findings suggest that while
AI-driven tools facilitate immediate performance, they do not inherently
reinforce long-term retention unless supported by structured learning
strategies. The study highlights the need for balanced technology integration
in education, ensuring that AI tools are paired with pedagogical approaches
that promote deep cognitive engagement and knowledge retention.

</details>


### [23] [The Evolution of Scientific Credit: When Authorship Norms Impede Collaboration](https://arxiv.org/abs/2507.07364)
*Toby Handfield,Kevin Zollman*

Main category: cs.CY

TL;DR: 本文利用演化博弈论模型分析了科学署名规范的演化机制及其对合作行为的影响，指出贡献敏感的署名规范优于贡献不敏感的规范，有助于提升科学合作和生产效率，而字母序或资深末位等规范则可能妨碍合作并降低学科生产力。


<details>
  <summary>Details</summary>
Motivation: 作者关注于不同学科中作者署名规则的极大差异，以及这些规则对科学合作行为的影响。其核心动机在于探究为何会出现“按贡献排序”和“按惯例排序（如按字母顺序或资深作者落款）”两种截然不同的署名规范，并进一步分析这些署名规范的行为后果。

Method: 本文采用演化博弈论建模，提出并分析了两个模型。第一个模型用于解释不同作者署名规范是如何演化出来的；第二个模型用于探究这些规范对研究者合作意愿与合作成功率的影响。

Result: 第一模型显示，当资深研究者牺牲署名位置的优势面临最大的适应压力时，会演化出与贡献无关的署名规范，这解释了为何有些领域资深作者反而在末位。第二模型发现，贡献敏感的署名规范更能促进合作，贡献不敏感的规范则更易产生主要贡献者和次贡献者的不满，导致合作协调失败。

Conclusion: 按贡献排序的署名规范有利于科学合作和生产力的提升，而按字母顺序或资深作者末位等规范会成为学科间合作与生产效率的阻碍。这些惯例可能是有害的体制性摩擦，而非中立的组织惯例。

Abstract: Scientific authorship norms vary dramatically across disciplines, from
contribution-sensitive systems where first author is the greatest contributor
and subsequent author order reflects relative input, to
contribution-insensitive conventions like alphabetical ordering or
senior-author-last. We develop evolutionary game-theoretic models to examine
both how these divergent norms emerge and their subsequent effects on
collaborative behavior. Our first model reveals that contribution-insensitive
norms evolve when researchers who sacrifice positional advantage face the
strongest adaptive pressure -- for example senior authors managing larger
collaboration portfolios or bearing heavier reputational stakes. This "Red
King" dynamic potentially explains why fields in which senior researchers
command large labs, major grants, and extensive collaboration portfolios may
paradoxically evolve conventions that favour junior-author positioning. Our
second model demonstrates that established norms influence researchers'
willingness to collaborate, with contribution-sensitive norms consistently
outperforming insensitive alternatives in fostering successful partnerships.
Contribution-insensitive norms create systematic coordination failures through
two mechanisms: "main contributor resentment" when exceptional work goes
unrecognized, and "second contributor resentment" when comparable efforts
receive unequal credit. These findings suggest that widely adopted practices
like senior-last positioning and alphabetical ordering may function as
institutional frictions that impede valuable scientific collaborations rather
than neutral organizational conventions, potentially reducing overall
scientific productivity across affected disciplines.

</details>


### [24] [Vaccine Hesitancy on YouTube: a Competition between Health and Politics](https://arxiv.org/abs/2507.07517)
*Yelena Mejova,Michele Tizzani*

Main category: cs.CY

TL;DR: 本研究系统收集并分析了三个月内YouTube上所有提及疫苗接种的视频，发现反疫苗内容主要来自社会政治评论员，并且此类信息的下架力度非常有限。这揭示了平台在疫苗相关舆论监管上的不足，对公共健康传播策略具有借鉴意义。


<details>
  <summary>Details</summary>
Motivation: 随着YouTube成为主流信息平台，健康信息质量直接影响公众安全，尤其是疫苗接种等关键公共卫生措施，有必要了解并评估该平台相关内容的现状与影响。

Method: 采用持续三个月、系统性、每日收集提及疫苗接种的YouTube视频的方法，分析不同创作者和视频内容立场（支持/反对）及其影响因素。

Result: 社会和政治评论员贡献了最多反对疫苗的视频，这类视频常涉及政治人物和媒体。支持疫苗的视频则更多提及疾病本身或健康话题。20.8%视频表达疫苗犹豫，但被删除比例仅2.7%。

Conclusion: 通过三个月系统性收集YouTube上与疫苗接种相关的视频，研究发现反对疫苗的视频主要是由社会和政治评论员发布，且该类视频更倾向提及政治人物及新闻媒体。尽管20.8%视频持有疫苗犹豫立场，但仅2.7%被下架，表明对犹豫内容的审核不足。

Abstract: YouTube has rapidly emerged as a predominant platform for content
consumption, effectively displacing conventional media such as television and
news outlets. A part of the enormous video stream uploaded to this platform
includes health-related content, both from official public health
organizations, and from any individual or group that can make an account. The
quality of information available on YouTube is a critical point of public
health safety, especially when concerning major interventions, such as
vaccination. This study differentiates itself from previous efforts of auditing
YouTube videos on this topic by conducting a systematic daily collection of
posted videos mentioning vaccination for the duration of 3 months. We show that
the competition for the public's attention is between public health messaging
by institutions and individual educators on one side, and commentators on
society and politics on the other, the latest contributing the most to the
videos expressing stances against vaccination. Videos opposing vaccination are
more likely to mention politicians and publication media such as podcasts,
reports, and news analysis, on the other hand, videos in favor are more likely
to mention specific diseases or health-related topics. Finally, we find that,
at the time of analysis, only 2.7% of the videos have been taken down (by the
platform or the channel), despite 20.8% of the collected videos having a
vaccination hesitant stance, pointing to a lack of moderation activity for
hesitant content. The availability of high-quality information is essential to
improve awareness and compliance with public health interventions. Our findings
help characterize the public discourse around vaccination on one of the largest
media platforms, disentangling the role of the different creators and their
stances, and as such, they provide important insights for public health
communication policy.

</details>


### [25] [AI Human Impact: Toward a Model for Ethical Investing in AI-Intensive Companies](https://arxiv.org/abs/2507.07703)
*James Brusseau*

Main category: cs.CY

TL;DR: 提出针对AI密集型公司的人本伦理评价体系，弥补ESG框架不足，为投资者提供兼顾价值观与客观数据的投资决策工具。


<details>
  <summary>Details</summary>
Motivation: 随着AI技术的深入应用，AI密集型公司的伦理问题日益突出，投资者关心企业在技术发展与以人为本之间如何平衡。现有ESG框架无法充分评估这类公司，亟需专门的评价体系。

Method: 构建了基于九项绩效指标的评价体系，用于衡量AI技术的人本导向。指标来源于AI伦理原则，并针对AI密集型公司定制化，形成可操作的评价及投资决策工具。

Result: 开发出一套面向AI密集型公司的伦理评价指标，能够为投资者提供客观的投资指导，并使其能依据个人价值观做出决策。该体系对于分析师、投资组合经理等具有实用性和可信度。

Conclusion: 传统ESG框架对AI公司评价不足，需基于AI伦理原则制定专门评价指标，以推动以人为本的投资方法，赋能投资者理性参与AI领域。

Abstract: Does AI conform to humans, or will we conform to AI? An ethical evaluation of
AI-intensive companies will allow investors to knowledgeably participate in the
decision. The evaluation is built from nine performance indicators that can be
analyzed and scored to reflect a technology's human-centering. The result is
objective investment guidance, as well as investors empowered to act in
accordance with their own values. Incorporating ethics into financial decisions
is a strategy that will be recognized by participants in environmental, social,
and governance investing, however, this paper argues that conventional ESG
frameworks are inadequate to companies that function with AI at their core.
Fully accounting for contemporary big data, predictive analytics, and machine
learning requires specialized metrics customized from established AI ethics
principles. With these metrics established, the larger goal is a model for
humanist investing in AI-intensive companies that is intellectually robust,
manageable for analysts, useful for portfolio managers, and credible for
investors.

</details>


### [26] [Distributed and Decentralised Training: Technical Governance Challenges in a Shifting AI Landscape](https://arxiv.org/abs/2507.07765)
*Jakub Kryś,Yashvardhan Sharma,Janet Egan*

Main category: cs.CY

TL;DR: 本文梳理并区分了分布式与去中心化训练，分析其对AI治理的新挑战与机遇，建议制定更有针对性的政策应对能力扩散和去中心化发展。


<details>
  <summary>Details</summary>
Motivation: 目前分布式和去中心化训练常被政策话语混淆，理解不足，亟需澄清以便更有效的AI治理。

Method: 通过区分和分析分布式与去中心化AI训练，并探讨其对技术治理与政策制定的影响，提出洞见和建议。

Result: 发现这两类训练模式可能导致算力架构化、能力扩散、检测及关停难度提升等风险，同时带来数据隐私保护和分散权力等潜在利好。

Conclusion: 分布式与去中心化训练模式正在改变AI算力治理的关键假设，未来可能带来对传统计算治理框架的挑战，但政策工具如出口管制依然具有作用。

Abstract: Advances in low-communication training algorithms are enabling a shift from
centralised model training to compute setups that are either distributed across
multiple clusters or decentralised via community-driven contributions. This
paper distinguishes these two scenarios - distributed and decentralised
training - which are little understood and often conflated in policy discourse.
We discuss how they could impact technical AI governance through an increased
risk of compute structuring, capability proliferation, and the erosion of
detectability and shutdownability. While these trends foreshadow a possible new
paradigm that could challenge key assumptions of compute governance, we
emphasise that certain policy levers, like export controls, remain relevant. We
also acknowledge potential benefits of decentralised AI, including
privacy-preserving training runs that could unlock access to more data, and
mitigating harmful power concentration. Our goal is to support more precise
policymaking around compute, capability proliferation, and decentralised AI
development.

</details>


### [27] [Structured Prompts, Better Outcomes? Exploring the Effects of a Structured Interface with ChatGPT in a Graduate Robotics Course](https://arxiv.org/abs/2507.07767)
*Jerome Brender,Laila El-Hamamsy,Kim Uittenhove,Francesco Mondada,Engin Bumbacher*

Main category: cs.CY

TL;DR: 结构化GPT平台短期可改善提问，但难以持续影响行为或提升成绩。多数学生不认可该工具，仅靠界面调整引导学习成效有限，应更多关注学生动机与交互模式价值。


<details>
  <summary>Details</summary>
Motivation: 现有研究显示学生与大模型互动方式会影响学习效果，希望探索结构化平台能否引导学生形成促进学习的良好提问、提升学习成绩。

Method: 将研究生分成干预组（使用结构化GPT平台）和对照组（自由使用ChatGPT），经过两次实验室任务后，在第三次任务中所有学生自由使用ChatGPT。通过日志、测试成绩、问卷等分析其学习和提问行为。

Result: 干预组激发了更好的提问行为，也与更多学习收获相关，但这些行为无法持续迁移。学习成绩和表现无显著提升，多数学生不认可结构化平台的价值。

Conclusion: 结构化GPT平台能短期促进良好的提问行为（比如更清晰、聚焦理解代码的提问），但这些习惯在平台约束解除后难以持续，且学生大多对平台持保留或负面态度。单靠下推的交互界面修改对学习成效影响有限。

Abstract: Prior research shows that how students engage with Large Language Models
(LLMs) influences their problem-solving and understanding, reinforcing the need
to support productive LLM-uses that promote learning. This study evaluates the
impact of a structured GPT platform designed to promote 'good' prompting
behavior with data from 58 students in a graduate-level robotics course. The
students were assigned to either an intervention group using the structured
platform or a control group using ChatGPT freely for two practice lab sessions,
before a third session where all students could freely use ChatGPT. We analyzed
student perception (pre-post surveys), prompting behavior (logs), performance
(task scores), and learning (pre-post tests). Although we found no differences
in performance or learning between groups, we identified prompting behaviors -
such as having clear prompts focused on understanding code - that were linked
with higher learning gains and were more prominent when students used the
structured platform. However, such behaviors did not transfer once students
were no longer constrained to use the structured platform. Qualitative survey
data showed mixed perceptions: some students perceived the value of the
structured platform, but most did not perceive its relevance and resisted
changing their habits. These findings contribute to ongoing efforts to identify
effective strategies for integrating LLMs into learning and question the
effectiveness of bottom-up approaches that temporarily alter user interfaces to
influence students' interaction. Future research could instead explore top-down
strategies that address students' motivations and explicitly demonstrate how
certain interaction patterns support learning.

</details>


### [28] [Opting Out of Generative AI: a Behavioral Experiment on the Role of Education in Perplexity AI Avoidance](https://arxiv.org/abs/2507.07881)
*Roberto Ulloa,Juhi Kulshrestha,Celina Kacperski*

Main category: cs.CY

TL;DR: 大规模在线实验发现，低学历人群更易回避使用对话式AI，数字不平等问题有加剧风险，需强化技术包容性设计。


<details>
  <summary>Details</summary>
Motivation: 对话式AI兴起后，人们获取信息方式正在改变，但可能加剧现有数字不平等。本研究探索正式教育差异是否与对CAI的回避行为相关。

Method: 在线实验（N=1,636），将参与者随机分为传统搜索、CAI工具和对照组，并利用结构方程模型和LASSO回归分析教育水平与AI回避之间的关联。

Result: 在CAI组中任务回避率（51%）显著高于传统搜索（30.9%）和对照组（16.8%），低教育组CAI回避率高达74.4%；教育水平对AI回避有显著预测作用。

Conclusion: 教育水平与对对话式AI的回避行为密切相关，教育程度较低者更倾向于避免使用CAI工具。设计新技术时应重视包容性，以避免加剧数字鸿沟。

Abstract: The rise of conversational AI (CAI), powered by large language models, is
transforming how individuals access and interact with digital information.
However, these tools may inadvertently amplify existing digital inequalities.
This study investigates whether differences in formal education are associated
with CAI avoidance, leveraging behavioral data from an online experiment (N =
1,636). Participants were randomly assigned to a control or an
information-seeking task, either a traditional online search or a CAI
(Perplexity AI). Task avoidance (operationalized as survey abandonment or
providing unrelated responses during task assignment) was significantly higher
in the CAI group (51%) compared to the search (30.9%) and control (16.8%)
groups, with the highest CAI avoidance among participants with lower education
levels (~74.4%). Structural equation modeling based on the theoretical
framework UTAUT2 and LASSO regressions reveal that education is strongly
associated with CAI avoidance, even after accounting for various cognitive and
affective predictors of technology adoption. These findings underscore
education's central role in shaping AI adoption and the role of self-selection
biases in AI-related research, stressing the need for inclusive design to
ensure equitable access to emerging technologies.

</details>
