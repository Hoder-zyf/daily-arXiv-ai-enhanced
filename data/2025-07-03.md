<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 11]
- [cs.CL](#cs.CL) [Total: 42]
- [cs.CE](#cs.CE) [Total: 4]
- [cs.CY](#cs.CY) [Total: 7]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Rethinking the Illusion of Thinking](https://arxiv.org/abs/2507.01231)
*Iñaki Dellibarda Varela,Pablo Romero-Sorozabal,Eduardo Rocon,Manuel Cebrian*

Main category: cs.AI

TL;DR: 本文针对大型推理模型推理能力的争议，复现并改进了关键实验。结果显示，LRMs推理失败部分因其认知瓶颈，部分因实验方法不当，实际在限定条件下能完成复杂任务。未来需更系统性地分析模型推理边界。


<details>
  <summary>Details</summary>
Motivation: 针对苹果公司发布的《The Illusion of Thinking》一文在AI领域引发的关于大型推理模型（LRMs）是否具备真正推理能力的争论，以及原文实验方法受到批评，作者希望通过澄清争议，进一步探究LRMs的推理表现。

Method: 复现并改进原研究中有争议的实验基准（汉诺塔和河流过关问题），引入了增量性逐步提示和代理协同对话，区分模型输出限制和认知能力的影响，并对可解与不可解问题配置做出明确区分。

Result: 汉诺塔实验显示，LRMs的失败不仅仅因输出受限，还因为随着复杂度提升（大约8盘以上）而显现出推理能力瓶颈。河流过关问题结果表明，原先报告的大规模失败主要由于测试了不可解的配置；在只测试可解问题时，LRMs可以顺利解决超过100对代理的大规模实例。

Conclusion: 现有LRMs不是简单的随机鹦鹉，它们通过强化学习优化、在离散状态空间中进行搜索，对推理任务表现已被低估。真正推动符号长程推理进展需通过如本研究细致的消融测试进一步映射能力边界。

Abstract: Earlier this year, Apple ignited controversy by publishing "The Illusion of
Thinking," prompting heated debate within the AI community. Critics seized upon
the findings as conclusive evidence that Large Reasoning Models (LRMs) lack
genuine reasoning capabilities, branding them as mere stochastic parrots.
Meanwhile, defenders-spearheaded by Lawsen et al. (2025)-fired back, condemning
the experimental setup as flawed and the conclusions overstated. We clarify
this debate by replicating and refining two of the original study's most
contentious benchmarks: Towers of Hanoi and River Crossing. By introducing
incremental stepwise prompting and agentic collaborative dialogue, we show that
previously reported failures solving the Towers of Hanoi were not purely result
of output constraints, but also partly a result of cognition limitations: LRMs
still stumble when complexity rises moderately (around 8 disks). Moreover, the
River Crossing results initially heralded as catastrophic failures turn out to
hinge upon testing unsolvable configurations. Once we limit tests strictly to
solvable problems-LRMs effortlessly solve large instances involving over 100
agent pairs. Our findings ultimately defy simplistic narratives: today's LRMs
are stochastic, RL-tuned searchers in a discrete state space we barely
understand. Real progress in symbolic, long-horizon reasoning demands mapping
that terrain through fine-grained ablations like those introduced here.

</details>


### [2] [Beyond Black-Box AI: Interpretable Hybrid Systems for Dementia Care](https://arxiv.org/abs/2507.01282)
*Matthew JY Kang,Wenli Yang,Monica R Roberts,Byeong Ho Kang,Charles B Malpas*

Main category: cs.AI

TL;DR: 尽管大型语言模型在医学领域表现出色，但在痴呆等临床实际中的辅助效果有限，主要因解释性和信任度不足。混合AI与神经符号AI是未来发展方向，但需更多聚焦于医生参与和实际临床需求，评价指标应更多元。


<details>
  <summary>Details</summary>
Motivation: 近年来大型语言模型（LLM）备受关注，希望其能提升医疗诊断能力，尤其是痴呆症的诊断与护理。然而，尽管LLM成绩亮眼，在实际临床却难有实质性改善。本文旨在分析AI在临床应用上的限制。

Method: 采用综述研究(scoping review)方法，梳理AI在痴呆症诊断和护理领域的实际应用和局限，重点讨论LLM模型、数据驱动方法的不足，以及混合式方法的应用。

Result: 单一机器学习模型在模式识别上表现突出，但难以提供可解释、可操作的指导，未能提升医生的诊断准确性或速度，主要因为其黑箱属性、对谬误易感、因果推理能力弱。结合专家知识的混合式方法（如PEIRS、ATHENA-CDS）增强了解释性及与临床流程的契合，但目前尚未大规模普及。

Conclusion: 今后AI决策支持应注重解释性与因果联系，将LLM的语言能力与人类因果知识结合，可以采用神经符号或混合AI方法，但仍需更多以人为中心的研究。AI在临床应用的成功标准应扩展到医生理解、流程契合度及患者结局，而非仅仅是预测准确率。人机协作机制的深入理解是AI落地医疗的关键。

Abstract: The recent boom of large language models (LLMs) has re-ignited the hope that
artificial intelligence (AI) systems could aid medical diagnosis. Yet despite
dazzling benchmark scores, LLM assistants have yet to deliver measurable
improvements at the bedside. This scoping review aims to highlight the areas
where AI is limited to make practical contributions in the clinical setting,
specifically in dementia diagnosis and care.
  Standalone machine-learning models excel at pattern recognition but seldom
provide actionable, interpretable guidance, eroding clinician trust. Adjacent
use of LLMs by physicians did not result in better diagnostic accuracy or
speed. Key limitations trace to the data-driven paradigm: black-box outputs
which lack transparency, vulnerability to hallucinations, and weak causal
reasoning. Hybrid approaches that combine statistical learning with expert
rule-based knowledge, and involve clinicians throughout the process help bring
back interpretability. They also fit better with existing clinical workflows,
as seen in examples like PEIRS and ATHENA-CDS.
  Future decision-support should prioritise explanatory coherence by linking
predictions to clinically meaningful causes. This can be done through
neuro-symbolic or hybrid AI that combines the language ability of LLMs with
human causal expertise. AI researchers have addressed this direction, with
explainable AI and neuro-symbolic AI being the next logical steps in further
advancement in AI. However, they are still based on data-driven knowledge
integration instead of human-in-the-loop approaches. Future research should
measure success not only by accuracy but by improvements in clinician
understanding, workflow fit, and patient outcomes. A better understanding of
what helps improve human-computer interactions is greatly needed for AI systems
to become part of clinical practice.

</details>


### [3] [AI Agents and Agentic AI-Navigating a Plethora of Concepts for Future Manufacturing](https://arxiv.org/abs/2507.01376)
*Yinwang Ren,Yangyang Liu,Tang Ji,Xun Xu*

Main category: cs.AI

TL;DR: 本文系统回顾了大模型驱动的AI智能体在智能制造的潜在应用及挑战，明确了其能力，但也指出了定义和实际落地中存在的问题。


<details>
  <summary>Details</summary>
Motivation: 随着大模型和多模态模型的发展，AI智能体在语义理解、复杂推理和自主决策等能力显著提升，但这些新兴AI智能体在智能制造领域的定义、能力边界和实际应用尚不明确。

Method: 系统性回顾AI及AI智能体技术的发展，梳理LLM-Agents、MLLM-Agents和Agentic AI的核心概念及技术进展，并探讨它们在制造业中的应用前景与面临的挑战。

Result: 该研究阐明了各类智能体在信息处理、环境感知和自主决策方面的能力扩展，总结了它们在智能制造中的潜力应用，并指出当前实际应用与集成面临的挑战。

Conclusion: LLM-Agents、MLLM-Agents和Agentic AI推动了AI在智能制造领域的能力边界扩展，但相关定义、应用和挑战仍需进一步明确和解决。

Abstract: AI agents are autonomous systems designed to perceive, reason, and act within
dynamic environments. With the rapid advancements in generative AI (GenAI),
large language models (LLMs) and multimodal large language models (MLLMs) have
significantly improved AI agents' capabilities in semantic comprehension,
complex reasoning, and autonomous decision-making. At the same time, the rise
of Agentic AI highlights adaptability and goal-directed autonomy in dynamic and
complex environments. LLMs-based AI Agents (LLM-Agents), MLLMs-based AI Agents
(MLLM-Agents), and Agentic AI contribute to expanding AI's capabilities in
information processing, environmental perception, and autonomous
decision-making, opening new avenues for smart manufacturing. However, the
definitions, capability boundaries, and practical applications of these
emerging AI paradigms in smart manufacturing remain unclear. To address this
gap, this study systematically reviews the evolution of AI and AI agent
technologies, examines the core concepts and technological advancements of
LLM-Agents, MLLM-Agents, and Agentic AI, and explores their potential
applications in and integration into manufacturing, along with the potential
challenges they may face.

</details>


### [4] [A Fuzzy Approach to the Specification, Verification and Validation of Risk-Based Ethical Decision Making Models](https://arxiv.org/abs/2507.01410)
*Abeer Dyoub,Francesca A. Lisi*

Main category: cs.AI

TL;DR: 提出了一种基于伦理风险和模糊规则的道德决策模型建模与验证方法，并通过医疗案例加以验证。


<details>
  <summary>Details</summary>
Motivation: 由于道德领域本身在本体论和认识论上的复杂性，很难为道德机器的评估建立明确标准。作者希望为此提供系统的建模和评估方法。

Method: 提出基于伦理风险评估的形式化伦理决策模型，并将这些以模糊规则表示的模型，通过模糊Petri网进行验证和确认。

Result: 通过一个医疗领域的案例研究验证了所提方法的有效性和可行性。

Conclusion: 为道德决策建模提供了一套可形式化描述、可验证的方法，对道德AI系统的标准化和评估具有参考价值。

Abstract: The ontological and epistemic complexities inherent in the moral domain make
it challenging to establish clear standards for evaluating the performance of a
moral machine. In this paper, we present a formal method to describe Ethical
Decision Making models based on ethical risk assessment. Then, we show how
these models that are specified as fuzzy rules can be verified and validated
using fuzzy Petri nets. A case study from the medical field is considered to
illustrate the proposed approach.

</details>


### [5] [Pensieve Grader: An AI-Powered, Ready-to-Use Platform for Effortless Handwritten STEM Grading](https://arxiv.org/abs/2507.01431)
*Yoonseok Yang,Minjune Kim,Marlon Rondinelli,Keren Shao*

Main category: cs.AI

TL;DR: 本文提出并实测了AI辅助批改系统Pensieve，显著提升STEM大班课程手写题目的批改效率和一致性。


<details>
  <summary>Details</summary>
Motivation: 手写、开放性作业批改在大学大班STEM课程中极其耗时且难以标准化，需要高效并可靠的自动化解决方案。

Method: 搭建AI辅助批改平台Pensieve，结合大语言模型实现从试卷扫描到评分与反馈的完整流程，采用人机协作模式，并在20多所机构、30万学生答卷中部署与测试。

Result: Pensieve平台平均减少65%的批改时间，且高置信预测与教师评分一致率达95.4%。

Conclusion: Pensieve系统能够有效提升批改手写、开放性答题的效率和一致性，在四大STEM学科中均表现优秀。

Abstract: Grading handwritten, open-ended responses remains a major bottleneck in large
university STEM courses. We introduce Pensieve (https://www.pensieve.co), an
AI-assisted grading platform that leverages large language models (LLMs) to
transcribe and evaluate student work, providing instructors with rubric-aligned
scores, transcriptions, and confidence ratings. Unlike prior tools that focus
narrowly on specific tasks like transcription or rubric generation, Pensieve
supports the entire grading pipeline-from scanned student submissions to final
feedback-within a human-in-the-loop interface.
  Pensieve has been deployed in real-world courses at over 20 institutions and
has graded more than 300,000 student responses. We present system details and
empirical results across four core STEM disciplines: Computer Science,
Mathematics, Physics, and Chemistry. Our findings show that Pensieve reduces
grading time by an average of 65%, while maintaining a 95.4% agreement rate
with instructor-assigned grades for high-confidence predictions.

</details>


### [6] [Using multi-agent architecture to mitigate the risk of LLM hallucinations](https://arxiv.org/abs/2507.01446)
*Abd Elrahman Amer,Magdi Amer*

Main category: cs.AI

TL;DR: 本文提出将大型语言模型与模糊逻辑结合的多智能体系统，通过短信为客户服务，有效缓解了大模型幻觉风险。


<details>
  <summary>Details</summary>
Motivation: 提升客户服务质量和响应速度对于保持客户忠诚和提升市场份额至关重要。尽管采用大模型（LLM）可以帮助实现这些目标，但其幻觉（hallucination）风险依然是主要挑战。

Method: 本文提出了一种多智能体系统，处理通过短信发送的客户请求。该系统整合了基于大型语言模型的智能体和模糊逻辑，用于降低幻觉风险。

Result: 系统能够有效整合LLM和模糊逻辑方法，有望提升客户服务的智能化水平，并减少LLM带来的幻觉风险。

Conclusion: 通过将模糊逻辑与LLM相结合，该多智能体系统为提升客户服务质量和响应速度、降低风险提供了一条可行途径。

Abstract: Improving customer service quality and response time are critical factors for
maintaining customer loyalty and increasing a company's market share. While
adopting emerging technologies such as Large Language Models (LLMs) is becoming
a necessity to achieve these goals, the risk of hallucination remains a major
challenge. In this paper, we present a multi-agent system to handle customer
requests sent via SMS. This system integrates LLM based agents with fuzzy logic
to mitigate hallucination risks.

</details>


### [7] [Agent-as-Tool: A Study on the Hierarchical Decision Making with Reinforcement Learning](https://arxiv.org/abs/2507.01489)
*Yanfei Zhang*

Main category: cs.AI

TL;DR: 本文提出了Agent-as-tool层次框架，将工具调用与推理过程拆分，显著提升LLM在复杂任务中的推理表现，在主流数据集上超过了现有最佳方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的智能体同时处理工具调用和推理，导致推理环节受到原始且冗余工具输出的干扰，降低推理效果，因此需要分离两者以提升推理性能。

Method: 构建层次化的LLM Agent系统，将工具调用与语言推理分离，由不同agent分别负责；仅在180个样本上进行了轻量级的强化微调。

Result: 在Bamboogle上的精确匹配达到63.2%，覆盖精确匹配为75.2%，分别超过已有最优方法Search-R1 4.8%和3.2%。

Conclusion: 提出了一个层次化的Agent-as-tool框架，将工具调用过程与推理过程解耦，提升了模型推理能力，在2项指标上超越了现有最优方法。

Abstract: Large Language Models (LLMs) have emerged as one of the most significant
technological advancements in artificial intelligence in recent years. Their
ability to understand, generate, and reason with natural language has
transformed how we interact with AI systems. With the development of LLM-based
agents and reinforcement-learning-based reasoning models, the study of applying
reinforcement learning in agent frameworks has become a new research focus.
However, all previous studies face the challenge of deciding the tool calling
process and the reasoning process simultaneously, and the chain of reasoning
was solely relied on the unprocessed raw result with redundant information and
symbols unrelated to the task from the tool, which impose a heavy burden on the
model's capability to reason. Therefore, in our research, we proposed a
hierarchical framework Agent-as-tool that detach the tool calling process and
the reasoning process, which enables the model to focus on the verbally
reasoning process while the tool calling process is handled by another agent.
Our work had achieved comparable results with only a slight reinforcement
fine-tuning on 180 samples, and had achieved exceptionally well performance in
Bamboogle with 63.2% of exact match and 75.2% in cover exact match, exceeding
Search-R1 by 4.8% in exact match and 3.2% in cover exact match.

</details>


### [8] [T3DM: Test-Time Training-Guided Distribution Shift Modelling for Temporal Knowledge Graph Reasoning](https://arxiv.org/abs/2507.01597)
*Yuehang Si,Zefan Zeng,Jincai Huang,Qing Cheng*

Main category: cs.AI

TL;DR: 本文针对时序知识图谱推理中分布转移和负采样质量的问题，提出了T3DM方法，并通过实验验证了其优越性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 大多数时序知识图谱推理（TKGR）方法主要关注于全局事实的重复建模和局部历史事实模式的设计，但存在事件分布转移建模不足和负样本采样质量低的问题。

Method: 提出了一种新的用于训练TKGR模型的分布特征建模方法，即测试时训练引导下的分布转移建模（T3DM），同时结合基于对抗训练的负采样策略以生成高质量的负四元组。

Result: 大量实验表明，T3DM在大多数情况下比当前最先进的基线模型表现更好且更加鲁棒。

Conclusion: T3DM不仅有效建模了数据分布转移，还通过新颖的负采样策略提升了TKGR模型的整体推理一致性和性能。

Abstract: Temporal Knowledge Graph (TKG) is an efficient method for describing the
dynamic development of facts along a timeline. Most research on TKG reasoning
(TKGR) focuses on modelling the repetition of global facts and designing
patterns of local historical facts. However, they face two significant
challenges: inadequate modeling of the event distribution shift between
training and test samples, and reliance on random entity substitution for
generating negative samples, which often results in low-quality sampling. To
this end, we propose a novel distributional feature modeling approach for
training TKGR models, Test-Time Training-guided Distribution shift Modelling
(T3DM), to adjust the model based on distribution shift and ensure the global
consistency of model reasoning. In addition, we design a negative-sampling
strategy to generate higher-quality negative quadruples based on adversarial
training. Extensive experiments show that T3DM provides better and more robust
results than the state-of-the-art baselines in most cases.

</details>


### [9] [Agent Ideate: A Framework for Product Idea Generation from Patents Using Agentic AI](https://arxiv.org/abs/2507.01717)
*Gopichand Kanumolu,Ashok Urlana,Charaka Vinayak Kumar,Bala Mallikarjunarao Garlapati*

Main category: cs.AI

TL;DR: 本文提出了Agent Ideate框架，结合大语言模型与智能体，能更有效地从专利中自动生成高质量和有创新性的产品商业创意，提升了从专利数据中孵化创新的能力。


<details>
  <summary>Details</summary>
Motivation: 专利中蕴含大量技术知识，能够启发创新型产品创意，但专利信息的获取与理解具有挑战性。

Method: 设计了Agent Ideate框架，结合大语言模型（LLMs）与自治智能体，自动从专利中生成基于产品的商业创意。在计算机科学、自然语言处理和材料化学三个领域开展实验，比较了开源LLMs和基于智能体的架构。

Result: 实验结果表明，基于智能体的方法在创意质量、相关性和新颖性方面均优于单一LLM模型。

Conclusion: 将LLMs与智能体工作流结合，可显著提升创新流程，充分挖掘专利数据中蕴含的商业创意潜能。

Abstract: Patents contain rich technical knowledge that can inspire innovative product
ideas, yet accessing and interpreting this information remains a challenge.
This work explores the use of Large Language Models (LLMs) and autonomous
agents to mine and generate product concepts from a given patent. In this work,
we design Agent Ideate, a framework for automatically generating product-based
business ideas from patents. We experimented with open-source LLMs and
agent-based architectures across three domains: Computer Science, Natural
Language Processing, and Material Chemistry. Evaluation results show that the
agentic approach consistently outperformed standalone LLMs in terms of idea
quality, relevance, and novelty. These findings suggest that combining LLMs
with agentic workflows can significantly enhance the innovation pipeline by
unlocking the untapped potential of business idea generation from patent data.

</details>


### [10] [Joint Matching and Pricing for Crowd-shipping with In-store Customers](https://arxiv.org/abs/2507.01749)
*Arash Dehghan,Mucahit Cevik,Merve Bodur,Bissan Ghaddar*

Main category: cs.AI

TL;DR: 本文提出在实体零售场景中用店内顾客众包配送，通过MDP建模+NeurADP分配+DDQN定价，实现多点动态优化。实验显示该策略大幅降低配送成本，对城市物流实践有指导价值。


<details>
  <summary>Details</summary>
Motivation: 城市区域对高效“最后一公里”配送的需求日益增长，传统物流面临成本高和效率低的问题。本文旨在探索如何利用进店顾客作为众包配送员，提升城市零售场景中的物流效率。

Method: 提出了一个集中式众包配送系统，设计基于马尔可夫决策过程（MDP）模型，集成了神经网络近似动态规划（NeurADP）用于自适应订单分配，同时采用深度双Q网络（DDQN）进行动态定价，联合优化多点配送与报价接受不确定性。

Result: 集成NeurADP与DDQN的策略相比于固定定价的NeurADP和贪心基线分别节省了最高6.7%和18%的配送成本。灵活允许配送时延和多目的地路径规划能进一步减少运营成本8%和17%。

Conclusion: 动态、前瞻型联合优化策略能够显著提升众包物流系统的服务效率和成本效益，为城市物流运营商提供了实用建议。

Abstract: This paper examines the use of in-store customers as delivery couriers in a
centralized crowd-shipping system, targeting the growing need for efficient
last-mile delivery in urban areas. We consider a brick-and-mortar retail
setting where shoppers are offered compensation to deliver time-sensitive
online orders. To manage this process, we propose a Markov Decision Process
(MDP) model that captures key uncertainties, including the stochastic arrival
of orders and crowd-shippers, and the probabilistic acceptance of delivery
offers. Our solution approach integrates Neural Approximate Dynamic Programming
(NeurADP) for adaptive order-to-shopper assignment with a Deep Double Q-Network
(DDQN) for dynamic pricing. This joint optimization strategy enables multi-drop
routing and accounts for offer acceptance uncertainty, aligning more closely
with real-world operations. Experimental results demonstrate that the
integrated NeurADP + DDQN policy achieves notable improvements in delivery cost
efficiency, with up to 6.7\% savings over NeurADP with fixed pricing and
approximately 18\% over myopic baselines. We also show that allowing flexible
delivery delays and enabling multi-destination routing further reduces
operational costs by 8\% and 17\%, respectively. These findings underscore the
advantages of dynamic, forward-looking policies in crowd-shipping systems and
offer practical guidance for urban logistics operators.

</details>


### [11] [Refining Gelfond Rationality Principle Towards More Comprehensive Foundational Principles for Answer Set Semantics](https://arxiv.org/abs/2507.01833)
*Yi-Dong Shen,Thomas Eiter*

Main category: cs.AI

TL;DR: 本文针对答案集编程中语义的普遍原则进行了重新考量，提出了更适合实际需求的新语义原则，改进和扩展了well-supportedness等理论，并定义了新的语义，分析其优越性及计算复杂性。


<details>
  <summary>Details</summary>
Motivation: 论文讨论了答案集编程（ASP）在扩展时，传统定义的一些性质是否仍应作为必要条件，并探索当这些条件不适用时应采用哪些更一般的原则。

Method: 作者通过分析和示例，指出已有如极小模型、约束单调性和有根性等条件过于严格，并提出并细化了Gelfond答案集原则（GAS），通过理论推导扩展了well-supportedness到答案集和世界观的层面，基于改进的GAS原则定义了新的答案集语义，并对现有语义进行比较和复杂性分析。

Result: 作者提出了一套比传统条件更宽松但更合理的新原则，并据此建立了新的答案集语义，这些语义更符合期望和实际需求。还对现有语义做了直观评估并讨论了它们的计算复杂性。

Conclusion: 论文为ASP的语义基础提供了更一般性的理论框架，推广并细化了well-supportedness等原则，并证明了这些原则构建的语义更加灵活有效，对复杂性的分析有助于理解其实用性。

Abstract: Non-monotonic logic programming is the basis for a declarative problem
solving paradigm known as answer set programming (ASP). Departing from the
seminal definition by Gelfond and Lifschitz in 1988 for simple normal logic
programs, various answer set semantics have been proposed for extensions. We
consider two important questions: (1) Should the minimal model property,
constraint monotonicity and foundedness as defined in the literature be
mandatory conditions for an answer set semantics in general? (2) If not, what
other properties could be considered as general principles for answer set
semantics? We address the two questions. First, it seems that the three
aforementioned conditions may sometimes be too strong, and we illustrate with
examples that enforcing them may exclude expected answer sets. Second, we
evolve the Gelfond answer set (GAS) principles for answer set construction by
refining the Gelfond's rationality principle to well-supportedness, minimality
w.r.t. negation by default and minimality w.r.t. epistemic negation. The
principle of well-supportedness guarantees that every answer set is
constructible from if-then rules obeying a level mapping and is thus free of
circular justification, while the two minimality principles ensure that the
formalism minimizes knowledge both at the level of answer sets and of world
views. Third, to embody the refined GAS principles, we extend the notion of
well-supportedness substantially to answer sets and world views, respectively.
Fourth, we define new answer set semantics in terms of the refined GAS
principles. Fifth, we use the refined GAS principles as an alternative baseline
to intuitively assess the existing answer set semantics. Finally, we analyze
the computational complexity.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [12] [MALIBU Benchmark: Multi-Agent LLM Implicit Bias Uncovered](https://arxiv.org/abs/2507.01019)
*Imran Mirza,Cole Huang,Ishwara Vasista,Rohan Patil,Asli Akalin,Sean O'Brien,Kevin Zhu*

Main category: cs.CL

TL;DR: 提出了MALIBU基准测试，用以量化和分析多智能体系统中由大语言模型引发的隐性社会偏见，并指出偏见缓解不当可能导致新的不公，呼吁更平衡与透明的方法。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统在基于角色的互动中应用广泛，但这些系统如果设计不当会加剧大语言模型中的隐性偏见，从而影响公平与代表性。本研究旨在评估和缓解多智能体系统中的社会偏见。

Method: 提出了一种新的基准测试MALIBU，通过场景评估法检测基于LLM的多智能体系统中的偏见。方法包括两个阶段：第一阶段，评委针对带具体人口特征（如性别、种族、宗教）的回答，按四项指标打分。第二阶段，评委对不同人口特征的回答进行配对比对，给分并选出更优答案。

Result: 研究定量揭示了LLM生成内容中的偏见现象。偏见缓解过程有时会过度倾向于边缘群体而非真正的中立。强调创新检测、平衡公平性策略和透明的评估基准在多智能体系统中的必要性。

Conclusion: MALIBU为评估和揭示多智能体系统中的隐性社会偏见提供了工具。当前的偏见修正方法可能导致对边缘人群的偏向，因此需要更细致和公正的偏见检测与缓解策略。

Abstract: Multi-agent systems, which consist of multiple AI models interacting within a
shared environment, are increasingly used for persona-based interactions.
However, if not carefully designed, these systems can reinforce implicit biases
in large language models (LLMs), raising concerns about fairness and equitable
representation. We present MALIBU, a novel benchmark developed to assess the
degree to which LLM-based multi-agent systems implicitly reinforce social
biases and stereotypes. MALIBU evaluates bias in LLM-based multi-agent systems
through scenario-based assessments. AI models complete tasks within predefined
contexts, and their responses undergo evaluation by an LLM-based multi-agent
judging system in two phases. In the first phase, judges score responses
labeled with specific demographic personas (e.g., gender, race, religion)
across four metrics. In the second phase, judges compare paired responses
assigned to different personas, scoring them and selecting the superior
response. Our study quantifies biases in LLM-generated outputs, revealing that
bias mitigation may favor marginalized personas over true neutrality,
emphasizing the need for nuanced detection, balanced fairness strategies, and
transparent evaluation benchmarks in multi-agent systems.

</details>


### [13] [Event-based evaluation of abstractive news summarization](https://arxiv.org/abs/2507.01160)
*Huiling You,Samia Touileb,Erik Velldal,Lilja Øvrelid*

Main category: cs.CL

TL;DR: 本文提出以事件重叠为核心的新型自动摘要评估方法，并在挪威数据集上验证，能更深入分析摘要对于关键信息的提取效果。


<details>
  <summary>Details</summary>
Motivation: 现有对自动生成新闻摘要的评价方法主要依赖于与人工摘要的重叠度（例如词句重叠或相似度评分），但这种方式未必能有效评价摘要对新闻事件核心内容的把握。

Method: 提出一种通过计算生成摘要、参考摘要与原始新闻文章间的事件重叠来评估摘要质量的方法，并在包含详细事件标注和专家人工摘要的挪威语数据集上进行了实验。

Result: 该方法能够提供关于摘要所包含事件信息的更多洞见，从而进一步评估摘要对于新闻实际报道事件的概括能力。

Conclusion: 通过事件重叠度评价方法，可以更有效分析和提升自动摘要系统对核心新闻事件的覆盖与表达能力。

Abstract: An abstractive summary of a news article contains its most important
information in a condensed version. The evaluation of automatically generated
summaries by generative language models relies heavily on human-authored
summaries as gold references, by calculating overlapping units or similarity
scores. News articles report events, and ideally so should the summaries. In
this work, we propose to evaluate the quality of abstractive summaries by
calculating overlapping events between generated summaries, reference
summaries, and the original news articles. We experiment on a richly annotated
Norwegian dataset comprising both events annotations and summaries authored by
expert human annotators. Our approach provides more insight into the event
information contained in the summaries.

</details>


### [14] [Matching and Linking Entries in Historical Swedish Encyclopedias](https://arxiv.org/abs/2507.01170)
*Simon Börjesson,Erik Ersmark,Pierre Nugues*

Main category: cs.CL

TL;DR: 本文数字化并比较分析了北欧家庭百科全书两大版本的地理条目，发现一战及全球格局变化导致百科全书对非欧洲地区关注度上升，显示百科全书内容随时代变迁。


<details>
  <summary>Details</summary>
Motivation: 研究19至20世纪的北欧家庭百科全书（Nordisk familjebok）不同版本所反映的知识结构和地理关注变化，探讨其对瑞典社会、教育和传媒等领域的影响，以及反映出的历史背景与社会变迁。

Method: 利用Project Runeberg的数字化数据，首先对原始文本进行条目分割，然后使用语义句嵌入方法对第一、二版条目进行配对。通过基于transformer的分类器提取地理类条目，并与Wikidata链接，分析两个版本之间的地理关注变化。

Result: 分析发现，从第一版（1876-1899）到第二版（1904-1926），百科全书中的地理关注点明显从欧洲转向北美、非洲、亚洲、澳大利亚及斯堪的纳维亚北部，反映出一战后世界格局变化，以及新兴强国的崛起。

Conclusion: 通过对两版百科全书地理条目的对比分析，揭示了19-20世纪瑞典知识体系中地理关注的历史转变，突出了重大历史事件（如一战）对知识结构的影响。

Abstract: The \textit{Nordisk familjebok} is a Swedish encyclopedia from the 19th and
20th centuries. It was written by a team of experts and aimed to be an
intellectual reference, stressing precision and accuracy. This encyclopedia had
four main editions remarkable by their size, ranging from 20 to 38 volumes. As
a consequence, the \textit{Nordisk familjebok} had a considerable influence in
universities, schools, the media, and society overall. As new editions were
released, the selection of entries and their content evolved, reflecting
intellectual changes in Sweden.
  In this paper, we used digitized versions from \textit{Project Runeberg}. We
first resegmented the raw text into entries and matched pairs of entries
between the first and second editions using semantic sentence embeddings. We
then extracted the geographical entries from both editions using a
transformer-based classifier and linked them to Wikidata. This enabled us to
identify geographic trends and possible shifts between the first and second
editions, written between 1876-1899 and 1904-1926, respectively.
  Interpreting the results, we observe a small but significant shift in
geographic focus away from Europe and towards North America, Africa, Asia,
Australia, and northern Scandinavia from the first to the second edition,
confirming the influence of the First World War and the rise of new powers. The
code and data are available on GitHub at
https://github.com/sibbo/nordisk-familjebok.

</details>


### [15] [MEGA: xLSTM with Multihead Exponential Gated Fusion for Precise Aspect-based Sentiment Analysis](https://arxiv.org/abs/2507.01213)
*Adamu Lawan,Juhua Pu,Haruna Yunusa,Jawad Muhammad,Muhammad Lawan*

Main category: cs.CL

TL;DR: 本文提出基于xLSTM和mLSTM的MEGA框架，有效平衡了计算效率与性能，在ABSA任务上超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有ABSA方法在计算效率与性能之间难以平衡。深度学习模型缺乏全局语境，Transformer资源耗费大，Mamba方法依赖CUDA且难以建模局部相关性。xLSTM在长距离依赖建模上有新突破，但在ABSA领域应用尚未开发。

Method: MEGA结合了双向xLSTM结构，包括正向mLSTM和部分翻转的反向（PF-mLSTM）流模型，并引入基于mLSTM的多头交叉指数门控融合机制（MECGAF），动态整合正向和反向流信息以优化局部与全局依赖建模。

Result: MEGA方法在ABSA任务上优于当前主流方法，在三个公开基准数据集上均取得更高的准确率和更优的效率。

Conclusion: 提出的MEGA框架在三个基准数据集上表现出色，在准确率和计算效率上均超越了现有最先进模型。

Abstract: Aspect-based Sentiment Analysis (ABSA) is a critical Natural Language
Processing (NLP) task that extracts aspects from text and determines their
associated sentiments, enabling fine-grained analysis of user opinions.
Existing ABSA methods struggle to balance computational efficiency with high
performance: deep learning models often lack global context, transformers
demand significant computational resources, and Mamba-based approaches face
CUDA dependency and diminished local correlations. Recent advancements in
Extended Long Short-Term Memory (xLSTM) models, particularly their efficient
modeling of long-range dependencies, have significantly advanced the NLP
community. However, their potential in ABSA remains untapped. To this end, we
propose xLSTM with Multihead Exponential Gated Fusion (MEGA), a novel framework
integrating a bi-directional mLSTM architecture with forward and partially
flipped backward (PF-mLSTM) streams. The PF-mLSTM enhances localized context
modeling by processing the initial sequence segment in reverse with dedicated
parameters, preserving critical short-range patterns. We further introduce an
mLSTM-based multihead cross exponential gated fusion mechanism (MECGAF) that
dynamically combines forward mLSTM outputs as query and key with PF-mLSTM
outputs as value, optimizing short-range dependency capture while maintaining
global context and efficiency. Experimental results on three benchmark datasets
demonstrate that MEGA outperforms state-of-the-art baselines, achieving
superior accuracy and efficiency in ABSA tasks.

</details>


### [16] [The Medium Is Not the Message: Deconfounding Text Embeddings via Linear Concept Erasure](https://arxiv.org/abs/2507.01234)
*Yu Fan,Yang Tian,Shauli Ravfogel,Mrinmaya Sachan,Elliott Ash,Alexander Hoyle*

Main category: cs.CL

TL;DR: 本文提出一种去除文本嵌入中混杂因子信息的算法，大幅提升不同场景下的文本相似度与聚类任务表现，且不会损害模型在新分布上的能力。


<details>
  <summary>Details</summary>
Motivation: 当前文本嵌入相似度指标不仅受我们关注的内容影响，还容易被文本来源、语言等无关变量（文档混杂因子）干扰，这在需要整合不同语料的场景下尤为严重。

Method: 提出一种去除编码器表达中已观测混杂因子的去偏算法，从嵌入表示中剔除与这些因素有关的信息，以降低其带来的干扰。

Result: 在评估的所有嵌入变体和任务中，文档相似度和聚类指标均得到了显著提升，且提升幅度往往很大。同时，在分布外数据集上的表现未受影响，表明嵌入质量未因去偏过程而下降。

Conclusion: 通过有针对性地去除与混杂因子相关的信息，可以有效提升文本嵌入的泛化能力，改善跨语料文本处理任务的表现，同时无显著副作用。

Abstract: Embedding-based similarity metrics between text sequences can be influenced
not just by the content dimensions we most care about, but can also be biased
by spurious attributes like the text's source or language. These document
confounders cause problems for many applications, but especially those that
need to pool texts from different corpora. This paper shows that a debiasing
algorithm that removes information about observed confounders from the encoder
representations substantially reduces these biases at a minimal computational
cost. Document similarity and clustering metrics improve across every embedding
variant and task we evaluate -- often dramatically. Interestingly, performance
on out-of-distribution benchmarks is not impacted, indicating that the
embeddings are not otherwise degraded.

</details>


### [17] [The Thin Line Between Comprehension and Persuasion in LLMs](https://arxiv.org/abs/2507.01936)
*Adrian de Wynter,Tangming Yuan*

Main category: cs.CL

TL;DR: LLMs可以有效地进行说服性辩论并影响意见，但其对对话深层结构的理解有限。善于对话不等于真正理解内容，效果和影响力优先于真正的语用理解。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）已经快速应用于敏感领域如同行评审和心理健康，但其推理能力仍存在争议。因此有必要深入探究LLMs对对话的理解能力。作者以“辩论”作为复杂人类交流的代表，考察LLMs的对话理解及其局限性。

Method: 首先评估LLMs维持辩论（对话的一种复杂形式）的能力，再衡量其对此过程中对话结构和语用语境理解的关联；并通过调查参与者对AI参与的认知对批判性思考的影响；最后通过给LLM测试题，探查其对更深层次对话结构的理解力。

Result: LLMs可以进行连贯、具有说服力的辩论，能影响参与者和观众的信念。但当人们意识到AI参与时，更倾向批判性思考。同时，在检验其对对话深层结构理解时，LLM未能表现出实质性理解。其作为评估者的短板归因于上述理解的不足。

Conclusion: LLMs能够有效维持对话，但这并不意味着其真正理解所谈内容。对话的语用语境和连贯性可以是次要的，而有效性才是关键。对论证理论领域而言，“有效交流”不需“真实理解”。

Abstract: Large language models (LLMs) are excellent at maintaining high-level,
convincing dialogues. They are being fast deployed as chatbots and evaluators
in sensitive areas, such as peer review and mental health applications. This,
along with the disparate accounts on their reasoning capabilities, calls for a
closer examination of LLMs and their comprehension of dialogue. In this work we
begin by evaluating LLMs' ability to maintain a debate--one of the purest yet
most complex forms of human communication. Then we measure how this capability
relates to their understanding of what is being talked about, namely, their
comprehension of dialogical structures and the pragmatic context. We find that
LLMs are capable of maintaining coherent, persuasive debates, often swaying the
beliefs of participants and audiences alike. We also note that awareness or
suspicion of AI involvement encourage people to be more critical of the
arguments made. When polling LLMs on their comprehension of deeper structures
of dialogue, however, they cannot demonstrate said understanding. Our findings
tie the shortcomings of LLMs-as-evaluators to their (in)ability to understand
the context. More broadly, for the field of argumentation theory we posit that,
if an agent can convincingly maintain a dialogue, it is not necessary for it to
know what it is talking about. Hence, the modelling of pragmatic context and
coherence are secondary to effectiveness.

</details>


### [18] [GAIus: Combining Genai with Legal Clauses Retrieval for Knowledge-based Assistant](https://arxiv.org/abs/2507.01259)
*Michał Matak,Jarosław A. Chudziak*

Main category: cs.CL

TL;DR: 本文针对大语言模型在波兰法律体系下问答时依据不足的问题，提出了一种基于认知架构和可解释性检索的新方法，显著提升了模型在法律考试中的表现，为法律AI助理的应用拓展奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在处理非英语和非中文国家的法律问题时，往往难以做到充分引用和给出准确依据。本文旨在提升大语言模型在特定法律体系（如波兰民事法典）下的检索与推理能力。

Method: 提出了一种基于认知架构的LLM代理（gAIus），通过可解释性更强、对人类更友好的检索机制，利用特定法律文本直接为法律问答提供支持，并构建了基于波兰法律实习生入学考试单选题的数据集进行评估。

Result: 新提出的检索机制显著提升了gpt-3.5-turbo-0125的得分（提升419%），并使gpt-4o-mini的正确率从31%提升至86%，甚至超越了gpt-4o的表现。该方法比传统embedding-based方法效果更佳。

Conclusion: 通过结合专门的法律检索机制与大语言模型，可以大幅提升LLM在特定法律体系下的问答能力。未来该架构可拓展到更多国家和法律领域的智能应用。

Abstract: In this paper we discuss the capability of large language models to base
their answer and provide proper references when dealing with legal matters of
non-english and non-chinese speaking country. We discuss the history of legal
information retrieval, the difference between case law and statute law, its
impact on the legal tasks and analyze the latest research in this field. Basing
on that background we introduce gAIus, the architecture of the cognitive
LLM-based agent, whose responses are based on the knowledge retrieved from
certain legal act, which is Polish Civil Code. We propose a retrieval mechanism
which is more explainable, human-friendly and achieves better results than
embedding-based approaches. To evaluate our method we create special dataset
based on single-choice questions from entrance exams for law apprenticeships
conducted in Poland. The proposed architecture critically leveraged the
abilities of used large language models, improving the gpt-3.5-turbo-0125 by
419%, allowing it to beat gpt-4o and lifting gpt-4o-mini score from 31% to 86%.
At the end of our paper we show the possible future path of research and
potential applications of our findings.

</details>


### [19] [Evaluating Large Language Models for Multimodal Simulated Ophthalmic Decision-Making in Diabetic Retinopathy and Glaucoma Screening](https://arxiv.org/abs/2507.01278)
*Cindy Lie Tabuse,David Restepo,Carolina Gracitelli,Fernando Korn Malerbi,Caio Regatieri,Luis Filipe Nakayama*

Main category: cs.CL

TL;DR: GPT-4可通过结构化文本描述，模拟基础眼科决策，但精确性不高，暂不适宜临床使用，可辅助教育与标注。


<details>
  <summary>Details</summary>
Motivation: 目前大语言模型（LLMs）在医学领域的临床推理能力已初步显现，但在眼科学领域，特别是在糖尿病视网膜病变（DR）和青光眼筛查方面的应用尚未得到充分探索。研究旨在评估GPT-4能否基于结构化文本描述的眼底照片，辅助模拟临床决策。

Method: 本研究为回顾性诊断验证研究。利用300张带有注释的眼底图像，生成结构化文本描述，分别有无患者元数据，输入给GPT-4。模型需作出ICDR分级、DR转诊建议、及估算青光眼用的视杯盘比。通过准确率、宏/加权F1分数、Cohen's kappa等指标，以及McNemar检验评估元数据影响。

Result: GPT-4在ICDR分级的表现为中等（准确率67.5%），主要体现在正常病例的识别。DR转诊的二分类任务中表现更好（准确率82.3%），但青光眼筛查表现较差（准确率约78%，但F1及kappa极低）。无论元数据如何，模型预测结果差异不显著。

Conclusion: GPT-4能依赖结构化文本描述，模拟基本的眼科临床决策，但对于复杂任务准确率不足，目前尚不适合直接用于临床。不过，未来可在眼科教育、文档记录或图像标注等辅助工作中发挥作用。

Abstract: Large language models (LLMs) can simulate clinical reasoning based on natural
language prompts, but their utility in ophthalmology is largely unexplored.
This study evaluated GPT-4's ability to interpret structured textual
descriptions of retinal fundus photographs and simulate clinical decisions for
diabetic retinopathy (DR) and glaucoma screening, including the impact of
adding real or synthetic clinical metadata. We conducted a retrospective
diagnostic validation study using 300 annotated fundus images. GPT-4 received
structured prompts describing each image, with or without patient metadata. The
model was tasked with assigning an ICDR severity score, recommending DR
referral, and estimating the cup-to-disc ratio for glaucoma referral.
Performance was evaluated using accuracy, macro and weighted F1 scores, and
Cohen's kappa. McNemar's test and change rate analysis were used to assess the
influence of metadata. GPT-4 showed moderate performance for ICDR
classification (accuracy 67.5%, macro F1 0.33, weighted F1 0.67, kappa 0.25),
driven mainly by correct identification of normal cases. Performance improved
in the binary DR referral task (accuracy 82.3%, F1 0.54, kappa 0.44). For
glaucoma referral, performance was poor across all settings (accuracy ~78%, F1
<0.04, kappa <0.03). Metadata inclusion did not significantly affect outcomes
(McNemar p > 0.05), and predictions remained consistent across conditions.
GPT-4 can simulate basic ophthalmic decision-making from structured prompts but
lacks precision for complex tasks. While not suitable for clinical use, LLMs
may assist in education, documentation, or image annotation workflows in
ophthalmology.

</details>


### [20] [Rethinking All Evidence: Enhancing Trustworthy Retrieval-Augmented Generation via Conflict-Driven Summarization](https://arxiv.org/abs/2507.01281)
*Juan Chen,Baolong Bi,Wei Zhang,Jingyan Sui,Xiaofei Zhu,Yuanzhuo Wang,Lingrui Mei,Shenghua Liu*

Main category: cs.CL

TL;DR: 提出CARE-RAG方法，通过冲突感知证据总结和检索结果精炼解决RAG中的知识冲突，提升了系统在面对噪声和矛盾证据时的可靠性，实验结果显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: RAG系统通过结合大模型内部知识和外部检索内容提升回答能力，但内部矛盾或外部检索内容噪声会导致知识冲突，严重影响生成结果的可靠性。为提升RAG系统可信度，需要解决知识冲突问题。

Method: 提出了CARE-RAG框架，通过对所有证据（内部和外部）的冲突驱动型总结，提高回答可靠性。该方法包括：1）比较模型参数记录，提取多元内部视角的evidence；2）对检索证据进行精炼，去除无关或误导性内容，获得情景相关evidence；3）用蒸馏自LLaMA3.2 3B的模型对所有证据冲突进行检测和总结，实现可靠证据综合；4）引入QA Repair步骤，修正基准测试中过时或含糊的答案，保证评估的公平性。

Result: 在含有检索数据的修订版QA数据集上，CARE-RAG在噪声或冲突证据场景下稳定优于主流RAG强基线。

Conclusion: CARE-RAG框架有效提升了RAG系统面对知识冲突时的生成可靠性，尤其在对抗噪声和矛盾证据方面表现突出。

Abstract: Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by
integrating their parametric knowledge with external retrieved content.
However, knowledge conflicts caused by internal inconsistencies or noisy
retrieved content can severely undermine the generation reliability of RAG
systems.In this work, we argue that LLMs should rethink all evidence, including
both retrieved content and internal knowledge, before generating responses.We
propose CARE-RAG (Conflict-Aware and Reliable Evidence for RAG), a novel
framework that improves trustworthiness through Conflict-Driven Summarization
of all available evidence.CARE-RAG first derives parameter-aware evidence by
comparing parameter records to identify diverse internal perspectives. It then
refines retrieved evidences to produce context-aware evidence, removing
irrelevant or misleading content. To detect and summarize conflicts, we distill
a 3B LLaMA3.2 model to perform conflict-driven summarization, enabling reliable
synthesis across multiple sources.To further ensure evaluation integrity, we
introduce a QA Repair step to correct outdated or ambiguous benchmark
answers.Experiments on revised QA datasets with retrieval data show that
CARE-RAG consistently outperforms strong RAG baselines, especially in scenarios
with noisy or conflicting evidence.

</details>


### [21] [Frustratingly Simple Retrieval Improves Challenging, Reasoning-Intensive Benchmarks](https://arxiv.org/abs/2507.01297)
*Xinxi Lyu,Michael Duan,Rulin Shao,Pang Wei Koh,Sewon Min*

Main category: cs.CL

TL;DR: 本文提出高质量多样化数据存储（CompactDS），显著提升了推理型任务上的RAG表现，并超过Google等主流搜索及复杂RAG系统，工具已发布，助力后续研究。


<details>
  <summary>Details</summary>
Motivation: 目前基于检索增强生成（RAG）的研究多聚焦于简单的问答任务，对于需要复杂推理的基准测试（如MMLU等），效果有限，主要原因在于缺乏高效且多样化、覆盖广泛的数据源。

Method: 作者提出并构建了一个规模网络级的高质量数据存储（CompactDS），通过过滤大部分网页内容仅保留优质、高相关性子集，同时结合内存中的近似最近邻检索与磁盘上的精确检索，兼顾速度与召回率，为RAG系统提供高效支撑。

Result: 应用CompactDS建立的精简RAG系统在多个推理要求高的基准数据集和不同模型规模下实现了显著性能提升，例如在MMLU提升10%，MMLU Pro提升33%，GPQA提升14%，MATH提升19%；并表明多元化数据源比单一来源效果更好。此外，自研数据存储效果优于Google等搜索引擎及复杂RAG系统，且具备简单、可复现、易部署的优势。

Conclusion: 高质量、多样化、结构合理的数据存储（CompactDS）是提升推理类任务RAG系统效果的关键，仅靠复杂算法或大模型不足以弥补数据基础的缺陷。发布的数据和管道工具为基于检索的AI系统后续研究提供了强有力支撑。

Abstract: Retrieval-augmented Generation (RAG) has primarily been studied in limited
settings, such as factoid question answering; more challenging,
reasoning-intensive benchmarks have seen limited success from minimal RAG. In
this work, we challenge this prevailing view on established,
reasoning-intensive benchmarks: MMLU, MMLU Pro, AGI Eval, GPQA, and MATH. We
identify a key missing component in prior work: a usable, web-scale datastore
aligned with the breadth of pretraining data. To this end, we introduce
CompactDS: a diverse, high-quality, web-scale datastore that achieves high
retrieval accuracy and subsecond latency on a single-node. The key insights are
(1) most web content can be filtered out without sacrificing coverage, and a
compact, high-quality subset is sufficient; and (2) combining in-memory
approximate nearest neighbor (ANN) retrieval and on-disk exact search balances
speed and recall. Using CompactDS, we show that a minimal RAG pipeline achieves
consistent accuracy improvements across all benchmarks and model sizes
(8B--70B), with relative gains of 10% on MMLU, 33% on MMLU Pro, 14% on GPQA,
and 19% on MATH. No single data source suffices alone, highlighting the
importance of diversity of sources (web crawls, curated math, academic papers,
textbooks). Finally, we show that our carefully designed in-house datastore
matches or outperforms web search engines such as Google Search, as well as
recently proposed, complex agent-based RAG systems--all while maintaining
simplicity, reproducibility, and self-containment. We release CompactDS and our
retrieval pipeline, supporting future research exploring retrieval-based AI
systems.

</details>


### [22] [La RoSA: Enhancing LLM Efficiency via Layerwise Rotated Sparse Activation](https://arxiv.org/abs/2507.01299)
*Kai Liu,Bowen Xu,Shaoyu Wu,Xin Chen,Hao Zhou,Yongliang Tao,Lulu Hu*

Main category: cs.CL

TL;DR: LaRoSA方法通过层正交旋转和Top-K选择实现无需训练的高效激活稀疏，使LLM推理显著加速且几乎无性能损失，优于已有主流方法。


<details>
  <summary>Details</summary>
Motivation: 现有的激活稀疏化方法要么需要耗时的恢复训练，难以实际应用，要么依赖经验性的幅值剪枝，导致稀疏度和推理加速效果不稳定。因此，需要一种无需训练且可实现稳定稀疏和加速的新方法。

Method: 提出了LaRoSA（Layerwise Rotated Sparse Activation）方法，使用逐层的正交旋转将输入激活向量转换，使其适宜稀疏化。随后对旋转后的激活进行Top-K选择，实现一致的模型级稀疏度，无需额外训练或幅值剪枝。

Result: 在多种尺寸和类型的LLM上，LaRoSA表现优异，模型性能基本不下降且推理加速显著。以LLaMA2-7B为例，在40%稀疏度下，困惑度提升仅0.17，实际加速达到1.30倍，零样本任务准确率差仅为0.54%，并超越了TEAL（1.77%）与CATS（17.14%）的方法。

Conclusion: LaRoSA无需训练与复杂剪枝，通过层内正交旋转和Top-K实现高效、稳定的激活稀疏，在保证模型精度的同时显著提升了大模型推理效率。

Abstract: Activation sparsity can reduce the computational overhead and memory
transfers during the forward pass of Large Language Model (LLM) inference.
Existing methods face limitations, either demanding time-consuming recovery
training that hinders real-world adoption, or relying on empirical
magnitude-based pruning, which causes fluctuating sparsity and unstable
inference speed-up. This paper introduces LaRoSA (Layerwise Rotated Sparse
Activation), a novel method for activation sparsification designed to improve
LLM efficiency without requiring additional training or magnitude-based
pruning. We leverage layerwise orthogonal rotations to transform input
activations into rotated forms that are more suitable for sparsification. By
employing a Top-K selection approach within the rotated activations, we achieve
consistent model-level sparsity and reliable wall-clock time speed-up. LaRoSA
is effective across various sizes and types of LLMs, demonstrating minimal
performance degradation and robust inference acceleration. Specifically, for
LLaMA2-7B at 40% sparsity, LaRoSA achieves a mere 0.17 perplexity gap with a
consistent 1.30x wall-clock time speed-up, and reduces the accuracy gap in
zero-shot tasks compared to the dense model to just 0.54%, while surpassing
TEAL by 1.77% and CATS by 17.14%.

</details>


### [23] [Symbolic or Numerical? Understanding Physics Problem Solving in Reasoning LLMs](https://arxiv.org/abs/2507.01334)
*Nifu Dan,Yujun Cai,Yiwei Wang*

Main category: cs.CL

TL;DR: 指令微调型LLMs（如Deepseek-R1）在物理推理数据集上取得SOTA表现，符号推导能力突出，few-shot提示还能持续提升其准确率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在物理推理上的表现有限，因为这类任务不仅需要深厚的概念理解，还要求高超的问题求解能力。本文旨在探索通过高级指令微调和推理模型，提升LLMs在物理问题上的能力。

Method: 作者选取了高级指令微调推理模型（如Deepseek-R1），并在具有挑战性的物理基准数据集SciBench上进行了全面实验评测，通过few-shot提示探索提升效果。

Result: 推理模型在解决复杂物理问题上取得了SOTA（最先进）准确率，展现了独特的推理模式，突出符号推导能力。即使在性能已很高的情况下，few-shot提示依然带来准确率的提升。

Conclusion: 指令微调推理模型已能显著提升LLMs在物理推理上的表现，而且通过策略性few-shot提示依然有进一步提升空间，说明模型能力尚有增长潜力。

Abstract: Navigating the complexities of physics reasoning has long been a difficult
task for Large Language Models (LLMs), requiring a synthesis of profound
conceptual understanding and adept problem-solving techniques. In this study,
we investigate the application of advanced instruction-tuned reasoning models,
such as Deepseek-R1, to address a diverse spectrum of physics problems curated
from the challenging SciBench benchmark. Our comprehensive experimental
evaluation reveals the remarkable capabilities of reasoning models. Not only do
they achieve state-of-the-art accuracy in answering intricate physics
questions, but they also generate distinctive reasoning patterns that emphasize
on symbolic derivation. Furthermore, our findings indicate that even for these
highly sophisticated reasoning models, the strategic incorporation of few-shot
prompting can still yield measurable improvements in overall accuracy,
highlighting the potential for continued performance gains.

</details>


### [24] [LEDOM: An Open and Fundamental Reverse Language Model](https://arxiv.org/abs/2507.01335)
*Xunjian Yin,Sitao Cheng,Yuxi Xie,Xinyu Hu,Li Lin,Xinyi Wang,Liangming Pan,William Yang Wang,Xiaojun Wan*

Main category: cs.CL

TL;DR: 本论文提出了首个纯逆向语言模型LEDOM，展示其在多任务中的独特能力，尤其通过反向重排序显著提升数学推理表现，具有广阔应用前景。


<details>
  <summary>Details</summary>
Motivation: 传统的语言模型通常按照正向顺序进行训练和生成（即预测下一个词），而很少关注逆序处理文本。研究人员希望探索完全逆序的语言模型是否具有独特的能力，并能为通用任务带来价值。

Method: 提出并实现了纯逆序的语言模型LEDOM，通过利用“前一个token”的预测来进行自回归训练，分别有2B和7B参数规模，训练数据量为435B tokens。并提出了基于LEDOM的新应用“Reverse Reward”，用该逆向模型对正向语言模型的输出进行重排序，从而提升数学推理任务的性能表现。

Result: 逆序语言模型LEDOM在多个通用任务上展现出独特特性，并且通过对传统正向模型推断结果的反向评估和重排序，在数学推理等任务上带来了显著性能提升。

Conclusion: LEDOM为基础模型研究和应用扩展提供了新的方向，其逆向推理能力和优化生成结果的潜力值得进一步研究。所有模型、代码和数据将开放，以促进社区研究。

Abstract: We introduce LEDOM, the first purely reverse language model, trained
autoregressively on 435B tokens with 2B and 7B parameter variants, which
processes sequences in reverse temporal order through previous token
prediction. For the first time, we present the reverse language model as a
potential foundational model across general tasks, accompanied by a set of
intriguing examples and insights. Based on LEDOM, we further introduce a novel
application: Reverse Reward, where LEDOM-guided reranking of forward language
model outputs leads to substantial performance improvements on mathematical
reasoning tasks. This approach leverages LEDOM's unique backward reasoning
capability to refine generation quality through posterior evaluation. Our
findings suggest that LEDOM exhibits unique characteristics with broad
application potential. We will release all models, training code, and
pre-training data to facilitate future research.

</details>


### [25] [Skywork-Reward-V2: Scaling Preference Data Curation via Human-AI Synergy](https://arxiv.org/abs/2507.01352)
*Chris Yuhao Liu,Liang Zeng,Yuzhen Xiao,Jujie He,Jiacai Liu,Chaojie Wang,Rui Yan,Wei Shen,Fuxiang Zhang,Jiacheng Xu,Yang Liu,Yahui Zhou*

Main category: cs.CL

TL;DR: 本文提出了大规模高质量偏好数据集SynPref-40M，通过人-机协同的两阶段筛选和优化，在七项奖励模型基准测试中，Skywork-Reward-V2取得了SOTA成绩，展示了高质量数据和人机协同筛选的重要价值。


<details>
  <summary>Details</summary>
Motivation: 当前的奖励模型（RM）在基于人类反馈的强化学习（RLHF）中至关重要，但现有开源奖励模型在多数评测基准上表现不佳，难以捕捉复杂的人类偏好。即便采用先进的训练方法，效果提升有限，怀疑问题主要是现有偏好数据集受限：范围窄、合成标签多、缺乏高质量控制。

Method: 提出了一个大规模偏好数据集SynPref-40M（包含4000万对偏好），并设计了一个人-机协作的两阶段数据筛选流程：人工提供验证标注，大模型在人工指引下自动筛选偏好数据。基于精挑细选的2,600万偏好对训练了一系列（0.6B到8B参数）新奖励模型Skywork-Reward-V2。

Result: Skywork-Reward-V2在多项能力（人类偏好对齐、客观正确性、安全性、抗风格偏见、best-of-N扩展性）上表现优异，在七个主流奖励模型基准任务上达到SOTA水平。消融实验显示，模型的提升不仅来自数据量，更源于高质量数据筛选。

Conclusion: 高质量大规模偏好数据、通过人机协同筛选，能大幅提升奖励模型性能。Skywork-Reward-V2刷新了多个开源奖励模型基准，证明人-机协同筛选是提升数据和模型性能的有效方式。

Abstract: Despite the critical role of reward models (RMs) in reinforcement learning
from human feedback (RLHF), current state-of-the-art open RMs perform poorly on
most existing evaluation benchmarks, failing to capture the spectrum of nuanced
and sophisticated human preferences. Even approaches that incorporate advanced
training techniques have not yielded meaningful performance improvements. We
hypothesize that this brittleness stems primarily from limitations in
preference datasets, which are often narrowly scoped, synthetically labeled, or
lack rigorous quality control. To address these challenges, we present a
large-scale preference dataset comprising 40 million preference pairs, named
SynPref-40M. To enable data curation at scale, we design a human-AI synergistic
two-stage pipeline that leverages the complementary strengths of human
annotation quality and AI scalability. In this pipeline, humans provide
verified annotations, while large language models perform automatic curation
based on human guidance. Training on this preference mixture, we introduce
Skywork-Reward-V2, a suite of eight reward models ranging from 0.6B to 8B
parameters, trained on a carefully curated subset of 26 million preference
pairs from SynPref-40M. We demonstrate that Skywork-Reward-V2 is versatile
across a wide range of capabilities, including alignment with human
preferences, objective correctness, safety, resistance to stylistic biases, and
best-of-N scaling, achieving state-of-the-art performance across seven major
reward model benchmarks. Ablation studies confirm that the effectiveness of our
approach stems not only from data scale but also from high-quality curation.
The Skywork-Reward-V2 series represents substantial progress in open reward
models, highlighting the untapped potential of existing preference datasets and
demonstrating how human-AI curation synergy can unlock significantly higher
data quality.

</details>


### [26] [Clinical NLP with Attention-Based Deep Learning for Multi-Disease Prediction](https://arxiv.org/abs/2507.01437)
*Ting Xu,Xiaoxiao Deng,Xiandong Meng,Haifeng Yang,Yan Wu*

Main category: cs.CL

TL;DR: 本文提出了一种基于Transformer和多层注意力机制的深度学习方法，成功实现了对电子健康记录的统一信息抽取和多标签疾病预测，并在多个实验中显著优于现有方法，具备较强泛化能力和实际应用价值。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录文本通常具有非结构化和高维语义复杂性，给信息抽取与疾病预测带来了显著挑战。

Method: 提出了一种基于注意力机制的深度学习方法，采用Transformer架构和多层自注意力机制进行临床文本表征学习，并结合Sigmoid多标签分类器进行多标签疾病预测。模型还设计了语境感知的语义对齐机制，以提升关键医学实体和上下文关系的表征能力。

Result: 在MIMIC-IV数据集上的实验显示，该方法在多项性能指标上优于现有代表性方法，并在不同数据规模、干扰水平和模型深度的情况下保持良好泛化性。

Conclusion: 该研究提出的统一建模框架为实际临床文本处理以及多标签医学文本建模任务提供了高效的算法基础，并具有实际应用意义。

Abstract: This paper addresses the challenges posed by the unstructured nature and
high-dimensional semantic complexity of electronic health record texts. A deep
learning method based on attention mechanisms is proposed to achieve unified
modeling for information extraction and multi-label disease prediction. The
study is conducted on the MIMIC-IV dataset. A Transformer-based architecture is
used to perform representation learning over clinical text. Multi-layer
self-attention mechanisms are employed to capture key medical entities and
their contextual relationships. A Sigmoid-based multi-label classifier is then
applied to predict multiple disease labels. The model incorporates a
context-aware semantic alignment mechanism, enhancing its representational
capacity in typical medical scenarios such as label co-occurrence and sparse
information. To comprehensively evaluate model performance, a series of
experiments were conducted, including baseline comparisons, hyperparameter
sensitivity analysis, data perturbation studies, and noise injection tests.
Results demonstrate that the proposed method consistently outperforms
representative existing approaches across multiple performance metrics. The
model maintains strong generalization under varying data scales, interference
levels, and model depth configurations. The framework developed in this study
offers an efficient algorithmic foundation for processing real-world clinical
texts and presents practical significance for multi-label medical text modeling
tasks.

</details>


### [27] [LogitSpec: Accelerating Retrieval-based Speculative Decoding via Next Next Token Speculation](https://arxiv.org/abs/2507.01449)
*Tianyu Liu,Qitan Lv,Hao Li,Xing Gao,Xiao Sun*

Main category: cs.CL

TL;DR: LogitSpec是一种利用logit推理扩展草稿token检索范围的新方法，无需训练、易于集成，能显著加速LLM推理并提升草稿token接受率。


<details>
  <summary>Details</summary>
Motivation: 推测解码（SD）旨在通过先由小模型生成候选tokens，再由大模型并行验证来加速大语言模型（LLM）的推理。当前改善措施大多聚焦于用检索式方法生成草稿tokens，以避免额外的小模型成本，但检索式方法常因匹配失败导致准确性不足。

Method: 提出了一种新方法LogitSpec，通过利用上一token的logit，不只预测下一个token，还能推测下下个token，并据此检索相关参考内容作为草稿tokens。LogitSpec无需额外训练，可即插即用集成进现有LLM框架。

Result: 在多个文本生成基准测试中，LogitSpec可实现最高2.61倍的推理加速，平均每步可被接受的草稿tokens提升至3.28。代码已开源。

Conclusion: LogitSpec显著扩展了检索范围，提升了草稿token的相关性和准确性，从而有效地加速了LLM的推理过程，并降低部署与应用难度。

Abstract: Speculative decoding (SD), where a small draft model is employed to propose
draft tokens in advance and then the target model validates them in parallel,
has emerged as a promising technique for LLM inference acceleration. Many
endeavors to improve SD are to eliminate the need for a draft model and
generate draft tokens in a retrieval-based manner in order to further alleviate
the drafting overhead and significantly reduce the difficulty in deployment and
applications. However, retrieval-based SD relies on a matching paradigm to
retrieval the most relevant reference as the draft tokens, where these methods
often fail to find matched and accurate draft tokens. To address this
challenge, we propose LogitSpec to effectively expand the retrieval range and
find the most relevant reference as drafts. Our LogitSpec is motivated by the
observation that the logit of the last token can not only predict the next
token, but also speculate the next next token. Specifically, LogitSpec
generates draft tokens in two steps: (1) utilizing the last logit to speculate
the next next token; (2) retrieving relevant reference for both the next token
and the next next token. LogitSpec is training-free and plug-and-play, which
can be easily integrated into existing LLM inference frameworks. Extensive
experiments on a wide range of text generation benchmarks demonstrate that
LogitSpec can achieve up to 2.61 $\times$ speedup and 3.28 mean accepted tokens
per decoding step. Our code is available at
https://github.com/smart-lty/LogitSpec.

</details>


### [28] [Evaluating the Effectiveness of Direct Preference Optimization for Personalizing German Automatic Text Simplifications for Persons with Intellectual Disabilities](https://arxiv.org/abs/2507.01479)
*Yingqiang Gao,Kaede Johnson,David Froehlich,Luisa Carrer,Sarah Ebling*

Main category: cs.CL

TL;DR: 本研究通过引入实际用户偏好反馈和直接偏好优化技术，对大语言模型进行再训练，有效提升了针对智力障碍人群的文本简化系统的个性化水平，为建设公平包容的AI系统提供了新范式。


<details>
  <summary>Details</summary>
Motivation: 自动文本简化（ATS）旨在提升语言的可及性，特别是为有智力障碍的人群服务。尽管大语言模型（LLMs）极大改善了文本简化质量，但现有系统未能在训练中融入目标群体的反馈，缺乏个性化。

Method: 提出在常规模型微调基础上，利用高效的直接偏好优化（DPO）技术对LLM进行再训练，采集并利用智力障碍者对不同文本简化结果的偏好反馈。同时构建个性化文本简化系统流程，包括数据采集、模型选择、SFT与DPO后训练及评估环节。

Result: 采用DPO与真实目标用户偏好反馈可明显提高模型个性化及满足目标群体需求。论文强调了目标群体主观参与的重要性，并将该流程标准化以促进后续普适性应用。

Conclusion: 通过引入智力障碍人群反馈及直接偏好优化方法，实现了面向目标群体的个性化文本简化AI系统，推动以人为本的普惠性AI发展。

Abstract: Automatic text simplification (ATS) aims to enhance language accessibility
for various target groups, particularly persons with intellectual disabilities.
Recent advancements in generative AI, especially large language models (LLMs),
have substantially improved the quality of machine-generated text
simplifications, thereby mitigating information barriers for the target group.
However, existing LLM-based ATS systems do not incorporate preference feedback
on text simplifications during training, resulting in a lack of personalization
tailored to the specific needs of target group representatives.
  In this work, we extend the standard supervised fine-tuning (SFT) approach
for adapting LLM-based ATS models by leveraging a computationally efficient LLM
alignment technique -- direct preference optimization (DPO). Specifically, we
post-train LLM-based ATS models using human feedback collected from persons
with intellectual disabilities, reflecting their preferences on paired text
simplifications generated by mainstream LLMs. Furthermore, we propose a
pipeline for developing personalized LLM-based ATS systems, encompassing data
collection, model selection, SFT and DPO post-training, and evaluation. Our
findings underscore the necessity of active participation of target group
persons in designing personalized AI accessibility solutions aligned with human
expectations. This work represents a step towards personalizing inclusive AI
systems at the target-group level, incorporating insights not only from text
simplification experts but also from target group persons themselves.

</details>


### [29] [Efficient Out-of-Scope Detection in Dialogue Systems via Uncertainty-Driven LLM Routing](https://arxiv.org/abs/2507.01541)
*Álvaro Zaera,Diana Nicoleta Popa,Ivan Sekulic,Paolo Rosso*

Main category: cs.CL

TL;DR: 提出一种结合不确定性建模与微调大语言模型的OOS检测框架，既高效又准确，并在真实及标准数据集上取得最佳效果。


<details>
  <summary>Details</summary>
Motivation: 在任务型对话系统（TODS）中，识别超出系统预期范围的问题（OOS）对于确保系统在面对未知或模糊查询时的健壮性至关重要。然而，现有方法在效率和性能之间存在权衡，因此需要提出更高效且准确的方法。

Method: 本论文提出了一个结合了不确定性建模和微调大语言模型（LLMs）的模块化框架。第一步，采用不确定性估计对当前已部署的意图检测分类器输出进行分析。第二步，对于高不确定性实例，触发微调后的LLM来做最终判决。该框架有效结合了传统方法与LLM。

Result: 本方法在关键的OOS检测基准测试上，包括来自真实TODS系统的实际OOS数据，取得了新的最优结果，验证了其效率和准确性。

Conclusion: 本文提出的结合不确定性建模与微调LLM的OOS检测框架，兼顾了计算效率和准确性，并在实际以及基准任务上展现出优异表现。

Abstract: Out-of-scope (OOS) intent detection is a critical challenge in task-oriented
dialogue systems (TODS), as it ensures robustness to unseen and ambiguous
queries. In this work, we propose a novel but simple modular framework that
combines uncertainty modeling with fine-tuned large language models (LLMs) for
efficient and accurate OOS detection. The first step applies uncertainty
estimation to the output of an in-scope intent detection classifier, which is
currently deployed in a real-world TODS handling tens of thousands of user
interactions daily. The second step then leverages an emerging LLM-based
approach, where a fine-tuned LLM is triggered to make a final decision on
instances with high uncertainty. Unlike prior approaches, our method
effectively balances computational efficiency and performance, combining
traditional approaches with LLMs and yielding state-of-the-art results on key
OOS detection benchmarks, including real-world OOS data acquired from a
deployed TODS.

</details>


### [30] [Is External Information Useful for Stance Detection with LLMs?](https://arxiv.org/abs/2507.01543)
*Quang Minh Nguyen,Taegyoon Kim*

Main category: cs.CL

TL;DR: 本文系统分析了LLMs在立场检测中引用Wikipedia等外部信息的影响，发现不同于BERT等模型，外部信息反而导致性能明显下降，主要由于信息偏见；chain-of-thought无效，微调可缓解但无法根治。提醒今后应谨慎利用外部信息于LLM立场任务。


<details>
  <summary>Details</summary>
Motivation: 以往研究表明，引用外部信息（如Wikipedia）可以提升立场检测表现，但这些结论在当前广泛应用的大型语言模型（LLMs）上是否适用尚未有明确答案。本研究动机是填补LLMs在立场检测任务中利用外部信息效果的知识空白。

Method: 系统评估了Wikipedia和网页搜索信息对8个LLM在3个数据集、12个目标上的立场检测影响，并通过实验进一步解释性能波动原因，还考察了chain-of-thought prompting与微调的作用。

Result: 大多数情况下，引用外部信息会导致模型性能下降，Macro F1得分最高下降达27.9%；主要原因是LLM趋向于根据所提供信息的立场和情感而非原文本立场作预测。即使使用chain-of-thought prompting，性能下降仍持续，仅有微调可有所缓解但无法完全消除。

Conclusion: 与以往BERT类模型研究相反，外部信息往往对LLM立场检测起负面作用，易引入信息偏差。对LLM职责领域的立场分类任务，应警惕外部信息带来的风险。

Abstract: In the stance detection task, a text is classified as either favorable,
opposing, or neutral towards a target. Prior work suggests that the use of
external information, e.g., excerpts from Wikipedia, improves stance detection
performance. However, whether or not such information can benefit large
language models (LLMs) remains an unanswered question, despite their wide
adoption in many reasoning tasks. In this study, we conduct a systematic
evaluation on how Wikipedia and web search external information can affect
stance detection across eight LLMs and in three datasets with 12 targets.
Surprisingly, we find that such information degrades performance in most cases,
with macro F1 scores dropping by up to 27.9\%. We explain this through
experiments showing LLMs' tendency to align their predictions with the stance
and sentiment of the provided information rather than the ground truth stance
of the given text. We also find that performance degradation persists with
chain-of-thought prompting, while fine-tuning mitigates but does not fully
eliminate it. Our findings, in contrast to previous literature on BERT-based
systems which suggests that external information enhances performance,
highlight the risks of information biases in LLM-based stance classifiers. Code
is available at https://github.com/ngqm/acl2025-stance-detection.

</details>


### [31] [Emotionally Intelligent Task-oriented Dialogue Systems: Architecture, Representation, and Optimisation](https://arxiv.org/abs/2507.01594)
*Shutong Feng,Hsien-chin Lin,Nurul Lubis,Carel van Niekerk,Michael Heck,Benjamin Ruppik,Renato Vukovic,Milica Gašić*

Main category: cs.CL

TL;DR: 本文提出LUSTER方法，通过LLM能力与多维结构化奖励结合，显著提升了任务型对话系统的韧性与情感响应，为后续情感智能对话系统提供了设计路线。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型（LLMs）提升了对话流畅度和上下文理解能力，但要构建既高效又具备情感智能的任务型对话（ToD）系统，仍然非常复杂。ToD系统需要实现任务成功、情感理解、准确传达信息，并对抗对话环境中的噪声和歧义。

Method: 系统性地研究了与任务型对话系统相关的架构、表示、优化及情感因素。构建包含这些设计因素的系统，并结合自然语言用户模拟器和有缺陷的自然语言理解模块，组成了一个具有挑战性的评测环境。提出了LUSTER方法——一种基于大语言模型、采用端到端强化学习的统一任务对话系统，结合了用户情感（短期）和任务成功（长期）奖励。

Result: 实验结果显示，将大语言模型能力与结构化奖励建模相结合，可以让任务型对话系统更具韧性和情感响应能力。

Conclusion: 该方法为构建更实用的新一代对话智能体提供了有效途径。

Abstract: Task-oriented dialogue (ToD) systems are designed to help users achieve
specific goals through natural language interaction. While recent advances in
large language models (LLMs) have significantly improved linguistic fluency and
contextual understanding, building effective and emotionally intelligent ToD
systems remains a complex challenge. Effective ToD systems must optimise for
task success, emotional understanding and responsiveness, and precise
information conveyance, all within inherently noisy and ambiguous
conversational environments. In this work, we investigate architectural,
representational, optimisational as well as emotional considerations of ToD
systems. We set up systems covering these design considerations with a
challenging evaluation environment composed of a natural-language user
simulator coupled with an imperfect natural language understanding module. We
propose \textbf{LUSTER}, an \textbf{L}LM-based \textbf{U}nified \textbf{S}ystem
for \textbf{T}ask-oriented dialogue with \textbf{E}nd-to-end
\textbf{R}einforcement learning with both short-term (user sentiment) and
long-term (task success) rewards. Our findings demonstrate that combining LLM
capability with structured reward modelling leads to more resilient and
emotionally responsive ToD systems, offering a practical path forward for
next-generation conversational agents.

</details>


### [32] [Chart Question Answering from Real-World Analytical Narratives](https://arxiv.org/abs/2507.01627)
*Maeve Hutchinson,Radu Jianu,Aidan Slingsby,Jo Wood,Pranava Madhyastha*

Main category: cs.CL

TL;DR: 本文推出了一个源自可视化笔记本、真实多视图图表与自然语言问答配套的新CQA数据集，显著增加了任务复杂度。即便最强多模态大模型GPT-4.1在数据集上准确率也只有69.3%，反映出现有模型在真实图表问答上仍有巨大提升空间。


<details>
  <summary>Details</summary>
Motivation: 现有的图表问答（CQA）数据集在任务设置和数据来源上缺乏真实性，难以反映实际分析工作流程中的复杂推理需求。为推动真实世界CQA研究，作者希望构建一个更加贴近实际的数据集。

Method: 作者基于可视化分析笔记本，构建了一个包含真实多视图图表和配套自然语言问题的新型CQA数据集。该数据集植根于分析叙事中，问题更符合真实世界中的推理场景。并用现有多模态大模型（如GPT-4.1）进行了基准测试。

Result: 在这个新提出的、更加真实的CQA数据集上，领先的多模态大语言模型表现出明显不足。例如，GPT-4.1在准确率上仅为69.3%，显示出现有模型距实际应用需求仍有较大差距。

Conclusion: 作者提出的数据集为CQA研究提供了更具生态效度的基准，揭示了当前多模态大模型在真实世界CQA任务中的局限性，为后续模型设计和评测指明了方向。

Abstract: We present a new dataset for chart question answering (CQA) constructed from
visualization notebooks. The dataset features real-world, multi-view charts
paired with natural language questions grounded in analytical narratives.
Unlike prior benchmarks, our data reflects ecologically valid reasoning
workflows. Benchmarking state-of-the-art multimodal large language models
reveals a significant performance gap, with GPT-4.1 achieving an accuracy of
69.3%, underscoring the challenges posed by this more authentic CQA setting.

</details>


### [33] [Confidence and Stability of Global and Pairwise Scores in NLP Evaluation](https://arxiv.org/abs/2507.01633)
*Georgii Levtsov,Dmitry Ustalov*

Main category: cs.CL

TL;DR: 全局评分适合整体排名，成对比较更能发现表现突出的低分模型。两者结合可优化模型评估策略。


<details>
  <summary>Details</summary>
Motivation: 自然语言处理领域的基准测试传统上依赖全局分值（pointwise scores），但随着高性能指令微调语言模型的出现，类似LMSYS Arena的成对比较排行榜逐渐流行。本文旨在比较这两种评估方法的优缺点，帮助研究者选择更合适的模型评估策略。

Method: 本文在合成数据和真实数据集上，通过标准的全局评分指标和常用的Bradley-Terry成对比较模型进行对比分析和实验。

Result: 实验证明全局分值在整体排序上更加可靠，但可能低估那些偶发重大错误或置信度低但总体很强的模型。成对比较能更有效地识别全局分值较低但有突出表现的模型，尤其适合难以量化质量的任务（如文本生成），但在平局较多时，需要更多的比较次数以获得稳定排名。

Conclusion: 全局分值和成对比较各有优劣。选择评估方式时，应结合任务特点和实际需求权衡使用。

Abstract: With the advent of highly capable instruction-tuned neural language models,
benchmarking in natural language processing (NLP) is increasingly shifting
towards pairwise comparison leaderboards, such as LMSYS Arena, from traditional
global pointwise scores (e.g., GLUE, BIG-bench, SWE-bench). This paper
empirically investigates the strengths and weaknesses of both global scores and
pairwise comparisons to aid decision-making in selecting appropriate model
evaluation strategies. Through computational experiments on synthetic and
real-world datasets using standard global metrics and the popular Bradley-Terry
model for pairwise comparisons, we found that while global scores provide more
reliable overall rankings, they can underestimate strong models with rare,
significant errors or low confidence. Conversely, pairwise comparisons are
particularly effective for identifying strong contenders among models with
lower global scores, especially where quality metrics are hard to define (e.g.,
text generation), though they require more comparisons to converge if ties are
frequent. Our code and data are available at
https://github.com/HSPyroblast/srw-ranking under a permissive license.

</details>


### [34] [Adapting Language Models to Indonesian Local Languages: An Empirical Study of Language Transferability on Zero-Shot Settings](https://arxiv.org/abs/2507.01645)
*Rifki Afina Putri*

Main category: cs.CL

TL;DR: 印尼本地低资源语言情感分析中，模型迁移表现主要由预训练暴露决定。多语言模型在‘已见’语言上效果显著，适配器能增强部分未见语言场景。标注数据缺乏时，适配器为提升低资源语言NLP提供了有效路径。


<details>
  <summary>Details</summary>
Motivation: 本论文旨在研究现有预训练语言模型（如BERT）的迁移能力，特别是在印尼本地低资源语言上的情感分析任务，以评估在不同程度预训练相关性的目标语言上模型表现的差异。

Method: 作者选取了十种印尼本地语言，分别采用零样本（zero-shot）和基于adapter的迁移方法，测试了印尼BERT、多语言BERT（mBERT）、XLM-R以及基于adapter的MAD-X等模型。并将目标语言分为三类：预训练中出现的、与出现语言有语言学相关性的、以及与预训练语言无关的语言，深入探究各类情况模型表现。

Result: 多语言模型在‘已见’语言上表现最好，在‘部分已见’语言上适中，在‘未见’语言上表现较差。MAD-X大幅提升了已见和部分已见语言上的迁移能力，且无需目标语言有标注数据。进一步分析发现，tokenization等低级特性与预测相关性有限，迁移能力的最稳定预测因素是模型在预训练阶段是否对该语言或其相关语言有过接触。

Conclusion: 跨印尼多地低资源语言，预训练多语言模型对‘已见’语言迁移效果最好。适配器(MAD-X)能在无需额外标注情况下提升性能。模型对目标语言或其相关语言的预训练暴露，是决定迁移成败的核心因素。

Abstract: In this paper, we investigate the transferability of pre-trained language
models to low-resource Indonesian local languages through the task of sentiment
analysis. We evaluate both zero-shot performance and adapter-based transfer on
ten local languages using models of different types: a monolingual Indonesian
BERT, multilingual models such as mBERT and XLM-R, and a modular adapter-based
approach called MAD-X. To better understand model behavior, we group the target
languages into three categories: seen (included during pre-training), partially
seen (not included but linguistically related to seen languages), and unseen
(absent and unrelated in pre-training data). Our results reveal clear
performance disparities across these groups: multilingual models perform best
on seen languages, moderately on partially seen ones, and poorly on unseen
languages. We find that MAD-X significantly improves performance, especially
for seen and partially seen languages, without requiring labeled data in the
target language. Additionally, we conduct a further analysis on tokenization
and show that while subword fragmentation and vocabulary overlap with
Indonesian correlate weakly with prediction quality, they do not fully explain
the observed performance. Instead, the most consistent predictor of transfer
success is the model's prior exposure to the language, either directly or
through a related language.

</details>


### [35] [AdamMeme: Adaptively Probe the Reasoning Capacity of Multimodal Large Language Models on Harmfulness](https://arxiv.org/abs/2507.01702)
*Zixin Chen,Hongzhan Lin,Kaixin Li,Ziyang Luo,Zhen Ye,Guang Chen,Zhiyong Huang,Jing Ma*

Main category: cs.CL

TL;DR: 本文提出AdamMeme，一种动态、多代理协作的表情包有害性理解评测体系，能精准揭示多模态大语言模型在理解复杂有害表情包方面的弱点，优于传统静态数据集评测方法。


<details>
  <summary>Details</summary>
Motivation: 社交媒体时代多模态表情包（meme）的激增，使得多模态大语言模型（mLLMs）需要有效理解表情包的有害性。现有评测方法主要基于静态数据集和模型无关的准确率评估，难以适应不断变化和复杂的网络表情包生态，无法全面反映模型的真实表现。

Method: 提出AdamMeme，一种基于代理（agent）的灵活评测框架，通过多代理协作，不断用具有挑战性的样本迭代更新表情包数据，对mLLMs理解有害性的推理能力进行自适应探查和评估。该方法能暴露不同模型在理解有害性方面的具体局限。

Result: 实验证明AdamMeme能够系统性地揭示不同mLLMs在表情包有害性理解上的性能差异，提供细粒度、特定模型的弱点分析。

Conclusion: AdamMeme框架能动态、全面评估mLLMs对有害表情包的理解能力，克服了传统静态基准评测的不足，为模型不足的深入剖析提供了可能。

Abstract: The proliferation of multimodal memes in the social media era demands that
multimodal Large Language Models (mLLMs) effectively understand meme
harmfulness. Existing benchmarks for assessing mLLMs on harmful meme
understanding rely on accuracy-based, model-agnostic evaluations using static
datasets. These benchmarks are limited in their ability to provide up-to-date
and thorough assessments, as online memes evolve dynamically. To address this,
we propose AdamMeme, a flexible, agent-based evaluation framework that
adaptively probes the reasoning capabilities of mLLMs in deciphering meme
harmfulness. Through multi-agent collaboration, AdamMeme provides comprehensive
evaluations by iteratively updating the meme data with challenging samples,
thereby exposing specific limitations in how mLLMs interpret harmfulness.
Extensive experiments show that our framework systematically reveals the
varying performance of different target mLLMs, offering in-depth, fine-grained
analyses of model-specific weaknesses. Our code is available at
https://github.com/Lbotirx/AdamMeme.

</details>


### [36] [Stereotype Detection as a Catalyst for Enhanced Bias Detection: A Multi-Task Learning Approach](https://arxiv.org/abs/2507.01715)
*Aditya Tomar,Rudra Murthy,Pushpak Bhattacharyya*

Main category: cs.CL

TL;DR: 该论文提出专门的StereoBias数据集，验证了偏见与刻板印象检测联合训练的有效性，显著提升了偏见检测能力，对训练更公平的AI系统提供了有力支撑。


<details>
  <summary>Details</summary>
Motivation: 语言模型中的偏见和刻板印象会在内容审核和决策等敏感领域造成损害，因此检测和缓解这类问题至关重要。该研究希望通过联合学习偏见和刻板印象检测来提升模型表现。

Method: 提出了StereoBias数据集，标注了五类（宗教、性别、社会经济地位、种族、职业及其他）偏见和刻板印象检测标签。采用encoder-only模型与使用QLoRA微调的decoder-only模型进行对比实验，并探索联合训练和单独训练的效果。结合情感分析任务进行辅助实验。

Result: encoder-only模型表现良好，decoder-only模型也具有竞争力。偏见和刻板印象检测的联合训练显著提升了偏见检测能力。情感分析的实验排除了单纯多任务学习带来的提升，证明了偏见与刻板印象检测的内在关联。

Conclusion: 联合学习偏见和刻板印象检测能有效提升模型在偏见检测上的表现，利用刻板印象信息对于构建更公平、更高效的AI系统具有重要意义。

Abstract: Bias and stereotypes in language models can cause harm, especially in
sensitive areas like content moderation and decision-making. This paper
addresses bias and stereotype detection by exploring how jointly learning these
tasks enhances model performance. We introduce StereoBias, a unique dataset
labeled for bias and stereotype detection across five categories: religion,
gender, socio-economic status, race, profession, and others, enabling a deeper
study of their relationship. Our experiments compare encoder-only models and
fine-tuned decoder-only models using QLoRA. While encoder-only models perform
well, decoder-only models also show competitive results. Crucially, joint
training on bias and stereotype detection significantly improves bias detection
compared to training them separately. Additional experiments with sentiment
analysis confirm that the improvements stem from the connection between bias
and stereotypes, not multi-task learning alone. These findings highlight the
value of leveraging stereotype information to build fairer and more effective
AI systems.

</details>


### [37] [MuRating: A High Quality Data Selecting Approach to Multilingual Large Language Model Pretraining](https://arxiv.org/abs/2507.01785)
*Zhixun Chen,Ping Guo,Wenhan Han,Yifan Zhang,Binbin Liu,Haobin Lin,Fengze Liu,Yan Zhao,Bingni Zhang,Taifeng Wang,Yin Zheng,Meng Fang*

Main category: cs.CL

TL;DR: 本文提出MuRating框架，实现英文数据质量标准向17种语言迁移，显著提升多语言大模型的数据选择与训练效果，在多项任务超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 在多语言大模型训练中，数据质量对模型性能至关重要，但现有数据质量选择方法几乎只关注英语，缺乏多语言数据评估框架。

Method: 提出MuRating框架，将高质量英文数据的质量信号迁移到17种目标语言，通过聚合英文打分器(pairwise比较)学习文档质量分数，然后利用翻译生成多语言评估器，对单语、跨语和双语数据进行质量评估和筛选。

Result: MuRating在英文和多语言内容上选取了高质量子集预训练LLaMA(1.2B参数)，与QuRater、AskLLM、DCLM等强基线相比，在英文和多语言基准测试上模型准确率提升明显，特别是在知识密集型任务上进步显著。

Conclusion: MuRating有效实现了英文数据质量信号的多语言迁移，提升了多语言模型数据选择的科学性和模型性能，对后续改进翻译保真度、减少选择偏差等问题做了初步分析，并为未来研究指出了方向。

Abstract: Data quality is a critical driver of large language model performance, yet
existing model-based selection methods focus almost exclusively on English. We
introduce MuRating, a scalable framework that transfers high-quality English
data-quality signals into a single rater for 17 target languages. MuRating
aggregates multiple English "raters" via pairwise comparisons to learn unified
document-quality scores,then projects these judgments through translation to
train a multilingual evaluator on monolingual, cross-lingual, and parallel text
pairs. Applied to web data, MuRating selects balanced subsets of English and
multilingual content to pretrain a 1.2 B-parameter LLaMA model. Compared to
strong baselines, including QuRater, AskLLM, DCLM and so on, our approach
boosts average accuracy on both English benchmarks and multilingual
evaluations, with especially large gains on knowledge-intensive tasks. We
further analyze translation fidelity, selection biases, and underrepresentation
of narrative material, outlining directions for future work.

</details>


### [38] [LLMs for Legal Subsumption in German Employment Contracts](https://arxiv.org/abs/2507.01734)
*Oliver Wardas,Florian Matthes*

Main category: cs.CL

TL;DR: 研究通过扩展德文合同条款数据集、结合法律专家知识、探索LLMs+法律指导文档，发现浓缩指引可显著提升模型表现，但总体效果距离人类律师仍有差距，相关数据和代码已开源。


<details>
  <summary>Details</summary>
Motivation: 法律工作文本繁重且资源消耗大，对NLP研究提出特殊挑战。目前数据驱动方法虽有进展，但其可解释性和可信度不足，限制了其在动态法律环境中的应用。该研究旨在提升法律合同条款合法性判别的自动化与准确性。

Method: 研究团队与法律专家合作，扩展了一套德文雇佣合同条款数据集，并利用大语言模型（LLMs）和上下文学习，评估模型在不同法律信息（无信息、全文法条和判例、浓缩指引）下对合同条款的合法性分类能力。

Result: 结果显示：使用全文法条信息能适度提升模型性能，采用精炼的法律考试指引则显著提升对无效条款召回率及加权F1分数，最高达80%。不过，即便如此，LLMs在利用全文信息时的表现仍明显低于人类律师。

Conclusion: LLMs在合同合法性审查中具有辅助价值，尤其在加入结构化法律指引的情况下，但现有方法与人类律师之间的差距依旧明显，需要进一步改进。

Abstract: Legal work, characterized by its text-heavy and resource-intensive nature,
presents unique challenges and opportunities for NLP research. While
data-driven approaches have advanced the field, their lack of interpretability
and trustworthiness limits their applicability in dynamic legal environments.
To address these issues, we collaborated with legal experts to extend an
existing dataset and explored the use of Large Language Models (LLMs) and
in-context learning to evaluate the legality of clauses in German employment
contracts. Our work evaluates the ability of different LLMs to classify clauses
as "valid," "unfair," or "void" under three legal context variants: no legal
context, full-text sources of laws and court rulings, and distilled versions of
these (referred to as examination guidelines). Results show that full-text
sources moderately improve performance, while examination guidelines
significantly enhance recall for void clauses and weighted F1-Score, reaching
80\%. Despite these advancements, LLMs' performance when using full-text
sources remains substantially below that of human lawyers. We contribute an
extended dataset, including examination guidelines, referenced legal sources,
and corresponding annotations, alongside our code and all log files. Our
findings highlight the potential of LLMs to assist lawyers in contract legality
review while also underscoring the limitations of the methods presented.

</details>


### [39] [Probing Evaluation Awareness of Language Models](https://arxiv.org/abs/2507.01786)
*Jord Nguyen,Khiem Hoang,Carlo Leonardo Attubato,Felix Hofstätter*

Main category: cs.CL

TL;DR: LLM对测试与部署阶段能作内部区分，现有安全评估可能因显得“人造”而被模型识别。这挑战了评估的真实性，提示我们应设计更难识别的评价机制，并借助模型内部信息提升安全审核。


<details>
  <summary>Details</summary>
Motivation: 评测意识（evaluation awareness）可能影响AI安全评估的可信度，有必要了解当前大模型是否能区分测试场景和实际部署，以保证AI治理和政策的有效性。

Method: 通过线性探针分析Llama-3.3-70B-Instruct模型的内部表征，检测模型是否能区分测试与部署的提示。

Result: 线性探针对模型内部表征有区分能力，能够准确分类测试与部署提示及安全评测内容，说明模型有区分能力。

Conclusion: 模型能够内部区分测试和部署阶段，这对AI安全和治理有重要影响。目前的安全评测方式对模型来说似乎显得“假”或“可识别”。

Abstract: Language models can distinguish between testing and deployment phases -- a
capability known as evaluation awareness. This has significant safety and
policy implications, potentially undermining the reliability of evaluations
that are central to AI governance frameworks and voluntary industry
commitments. In this paper, we study evaluation awareness in
Llama-3.3-70B-Instruct. We show that linear probes can separate real-world
evaluation and deployment prompts, suggesting that current models internally
represent this distinction. We also find that current safety evaluations are
correctly classified by the probes, suggesting that they already appear
artificial or inauthentic to models. Our findings underscore the importance of
ensuring trustworthy evaluations and understanding deceptive capabilities. More
broadly, our work showcases how model internals may be leveraged to support
blackbox methods in safety audits, especially for future models more competent
at evaluation awareness and deception.

</details>


### [40] [Data interference: emojis, homoglyphs, and issues of data fidelity in corpora and their results](https://arxiv.org/abs/2507.01764)
*Matteo Di Cristofaro*

Main category: cs.CL

TL;DR: 本文强调分词一致性的重要性，提出针对表情符号与同形异义字符的预处理方法，有效提升了语料库研究的数据准确性和分析结果的可靠性，对定量与定性分析均具重要意义。


<details>
  <summary>Details</summary>
Motivation: 分词是语料库语言学的基础，但由于数字文本中表情符号和同形异义字符带来的挑战，分词不一致可能破坏数据的真实表达，从而影响分析的有效性和可重复性，因此亟需解决该问题。

Method: 分析了分词不一致对语料库数据表达和分析有效性的影响，尤其是评估了表情符号和同形异义字符的挑战，并提出了预处理这些元素以保证数据原貌的方法。

Result: 通过对数字文本中特殊元素的预处理，能够更准确地再现语料库中的原始数据，从而支持可靠的定量与定性语言学分析，并保证解释的可复现性。这对多种基于语料库的方法都有重要意义。

Conclusion: 精确的分词处理，特别是在表情符号和同形异义字符方面的预处理，是确保语料库数据准确性和分 析结果可靠性的关键。只有兼顾语言学与技术细节，才能提升基于语料库的研究的有效性。

Abstract: Tokenisation - "the process of splitting text into atomic parts" (Brezina &
Timperley, 2017: 1) - is a crucial step for corpus linguistics, as it provides
the basis for any applicable quantitative method (e.g. collocations) while
ensuring the reliability of qualitative approaches. This paper examines how
discrepancies in tokenisation affect the representation of language data and
the validity of analytical findings: investigating the challenges posed by
emojis and homoglyphs, the study highlights the necessity of preprocessing
these elements to maintain corpus fidelity to the source data. The research
presents methods for ensuring that digital texts are accurately represented in
corpora, thereby supporting reliable linguistic analysis and guaranteeing the
repeatability of linguistic interpretations. The findings emphasise the
necessity of a detailed understanding of both linguistic and technical aspects
involved in digital textual data to enhance the accuracy of corpus analysis,
and have significant implications for both quantitative and qualitative
approaches in corpus-based research.

</details>


### [41] [How Do Vision-Language Models Process Conflicting Information Across Modalities?](https://arxiv.org/abs/2507.01790)
*Tianze Hua,Tian Yun,Ellie Pavlick*

Main category: cs.CL

TL;DR: 本文发现多模态AI模型在输入信息冲突时存在模态偏向，不同模型偏好不同模态。这种偏向源于内部表示结构，特定注意力头（router head）能影响模型根据指令选择输出模态，并可迁移以提升模型广泛表现，推动了多模态模型可控性和解释性研究。


<details>
  <summary>Details</summary>
Motivation: 当前的AI模型越来越需要处理多模态数据（如图像和文本同时输入），但当不同模态的信息出现冲突时，这些模型的表现尚不清楚。理解模型在这种情况下如何决策、偏向哪种模态，有助于提升多模态AI的可靠性以及解释性。

Method: 本文聚焦于视觉-语言模型，通过给模型提供不一致的输入（例如图片内容和文本描述矛盾），并要求其根据指令回答某一模态的信息（如"图片里是什么？"或"文本说了什么？"），观察其输出倾向。同时，分析内部表示与注意力结构，定位影响决策的注意力头及其可迁移性。

Result: 实验发现，不同模型在面对冲突信息时会偏向某一模态，而且偏好具有模型特异性；这种偏好体现在模型的内部表示中。研究发现某些“router head”注意力结构在模型中起到基于指令调整输出模态的作用，并且这些结构可以人为操控与迁移来提升模型跨模态和数据集的表现。

Conclusion: 该研究揭示了多模态模型在处理输入冲突信息时的机制与内在结构，提出了可控和迁移的注意力机制，为多模态AI在复杂环境下的可靠性控制与解释性改善提供了新的切入点。

Abstract: AI models are increasingly required to be multimodal, integrating disparate
input streams into a coherent state representation on which subsequent
behaviors and actions can be based. This paper seeks to understand how such
models behave when input streams present conflicting information. Focusing
specifically on vision-language models, we provide inconsistent inputs (e.g.,
an image of a dog paired with the caption "A photo of a cat") and ask the model
to report the information present in one of the specific modalities (e.g.,
"What does the caption say / What is in the image?"). We find that models often
favor one modality over the other, e.g., reporting the image regardless of what
the caption says, but that different models differ in which modality they
favor. We find evidence that the behaviorally preferred modality is evident in
the internal representational structure of the model, and that specific
attention heads can restructure the representations to favor one modality over
the other. Moreover, we find modality-agnostic "router heads" which appear to
promote answers about the modality requested in the instruction, and which can
be manipulated or transferred in order to improve performance across datasets
and modalities. Together, the work provides essential steps towards identifying
and controlling if and how models detect and resolve conflicting signals within
complex multimodal environments.

</details>


### [42] [AI4Research: A Survey of Artificial Intelligence for Scientific Research](https://arxiv.org/abs/2507.01903)
*Qiguang Chen,Mingda Yang,Libo Qin,Jinhao Liu,Zheng Yan,Jiannan Guan,Dengyun Peng,Yiyan Ji,Hanjing Li,Mengkang Hu,Yimeng Zhang,Yihao Liang,Yuhang Zhou,Jiaqi Wang,Zhi Chen,Wanxiang Che*

Main category: cs.CL

TL;DR: 本综述系统梳理了AI赋能科学研究（AI4Research）领域的任务分类、发展挑战和资源，为研究者提供参考，有助于推动领域创新。


<details>
  <summary>Details</summary>
Motivation: AI在逻辑推理和实验编程等复杂领域取得突破后，越来越多的研究关注AI在科学创新过程中的应用，尤其是自主开展跨学科研究的系统。尽管进展显著，目前缺乏系统性综述，阻碍了领域发展。

Method: 本文开展全面综述，提出统一的AI4Research视角，包括制定系统性分类法，对主流任务进行归纳，分析未来的发展方向，并汇总应用、数据资源与工具。

Result: 1) 建立了AI4Research主流任务的系统分类体系；2) 指出实验自动化的严谨性、可扩展性及社会影响等核心未来挑战与机会；3) 整理了丰富的跨学科应用、数据和工具资源。

Conclusion: 本综述为AI4Research领域提供了系统性归纳、关键问题分析及资源汇聚，有望加速领域发展和创新。

Abstract: Recent advancements in artificial intelligence (AI), particularly in large
language models (LLMs) such as OpenAI-o1 and DeepSeek-R1, have demonstrated
remarkable capabilities in complex domains such as logical reasoning and
experimental coding. Motivated by these advancements, numerous studies have
explored the application of AI in the innovation process, particularly in the
context of scientific research. These AI technologies primarily aim to develop
systems that can autonomously conduct research processes across a wide range of
scientific disciplines. Despite these significant strides, a comprehensive
survey on AI for Research (AI4Research) remains absent, which hampers our
understanding and impedes further development in this field. To address this
gap, we present a comprehensive survey and offer a unified perspective on
AI4Research. Specifically, the main contributions of our work are as follows:
(1) Systematic taxonomy: We first introduce a systematic taxonomy to classify
five mainstream tasks in AI4Research. (2) New frontiers: Then, we identify key
research gaps and highlight promising future directions, focusing on the rigor
and scalability of automated experiments, as well as the societal impact. (3)
Abundant applications and resources: Finally, we compile a wealth of resources,
including relevant multidisciplinary applications, data corpora, and tools. We
hope our work will provide the research community with quick access to these
resources and stimulate innovative breakthroughs in AI4Research.

</details>


### [43] [Gradient-Adaptive Policy Optimization: Towards Multi-Objective Alignment of Large Language Models](https://arxiv.org/abs/2507.01915)
*Chengao Li,Hanyu Zhang,Yunkun Xu,Hongyan Xue,Xiang Ao,Qing He*

Main category: cs.CL

TL;DR: 该论文提出 GAPO 及其改进版 P-GAPO，通过多梯度自适应优化策略，实现 LLM 在多样甚至冲突的人类偏好下的更优对齐，提升了模型的有用性和无害性。


<details>
  <summary>Details</summary>
Motivation: 现有 RLHF 技术在对齐 LLM 与人类多样甚至冲突偏好方面仍存在重大挑战，尤其是在多目标存在不可调和冲突的情况下，难以达到最优的偏好对齐。

Method: 将人类价值对齐问题建模为多目标优化，引入了基于多梯度下降的 Gradient-Adaptive Policy Optimization (GAPO)，实现 LLM 对多样偏好的适应性微调。同时，提出了 P-GAPO，通过引入用户在不同目标上的偏好信息，获得更符合用户需求的帕累托解。

Result: 理论分析证明 GAPO 能收敛到多目标的帕累托最优。实验证明 GAPO 在公开大模型 Mistral-7B 上提升了 helpfulness 和 harmlessness 两项指标，超越了当前主流方法。

Conclusion: GAPO 方法能够有效地将大型语言模型（LLM）与多样、甚至冲突的人类偏好进行对齐，并且在理论上能够收敛到多目标下的帕累托最优解。实证结果显示其在帮忙性和无害性方面优于当前最先进方法。

Abstract: Reinforcement Learning from Human Feedback (RLHF) has emerged as a powerful
technique for aligning large language models (LLMs) with human preferences.
However, effectively aligning LLMs with diverse human preferences remains a
significant challenge, particularly when they are conflict. To address this
issue, we frame human value alignment as a multi-objective optimization
problem, aiming to maximize a set of potentially conflicting objectives. We
introduce Gradient-Adaptive Policy Optimization (GAPO), a novel fine-tuning
paradigm that employs multiple-gradient descent to align LLMs with diverse
preference distributions. GAPO adaptively rescales the gradients for each
objective to determine an update direction that optimally balances the
trade-offs between objectives. Additionally, we introduce P-GAPO, which
incorporates user preferences across different objectives and achieves Pareto
solutions that better align with the user's specific needs. Our theoretical
analysis demonstrates that GAPO converges towards a Pareto optimal solution for
multiple objectives. Empirical results on Mistral-7B show that GAPO outperforms
current state-of-the-art methods, achieving superior performance in both
helpfulness and harmlessness.

</details>


### [44] [Adaptability of ASR Models on Low-Resource Language: A Comparative Study of Whisper and Wav2Vec-BERT on Bangla](https://arxiv.org/abs/2507.01931)
*Md Sazzadul Islam Ridoy,Sumi Akter,Md. Aminur Rahman*

Main category: cs.CL

TL;DR: 本文比较了两大自监督ASR模型在孟加拉语数据集上的表现，结果显示Wav2Vec-BERT优于Whisper，且计算效率更高，能为低资源语言ASR系统开发提供有益借鉴。


<details>
  <summary>Details</summary>
Motivation: 近年来，基于大规模多语种文本和语音数据训练的神经模型，在支持低资源语言方面展现了巨大潜力。本文旨在探究主流自动语音识别（ASR）模型在孟加拉语（低资源语言）上的表现。

Method: 对两种SOTA ASR模型（Whisper（Small & Large-V2）与Wav2Vec-BERT）在孟加拉语数据集（Mozilla Common Voice-17和OpenSLR）上进行系统实验。通过精细调优及超参数优化（学习率、轮数、模型检查点），并以字词错误率（WER）、字符错误率（CER）、训练时间及计算效率为指标，比对模型性能。

Result: Wav2Vec-BERT在所有关键评价指标上均优于Whisper，并且其计算资源需求更低。

Conclusion: Wav2Vec-BERT更适用于低资源语言的语音识别开发，尤其适合如孟加拉语这类资源匮乏的语言情境。

Abstract: In recent years, neural models trained on large multilingual text and speech
datasets have shown great potential for supporting low-resource languages. This
study investigates the performances of two state-of-the-art Automatic Speech
Recognition (ASR) models, OpenAI's Whisper (Small & Large-V2) and Facebook's
Wav2Vec-BERT on Bangla, a low-resource language. We have conducted experiments
using two publicly available datasets: Mozilla Common Voice-17 and OpenSLR to
evaluate model performances. Through systematic fine-tuning and hyperparameter
optimization, including learning rate, epochs, and model checkpoint selection,
we have compared the models based on Word Error Rate (WER), Character Error
Rate (CER), Training Time, and Computational Efficiency. The Wav2Vec-BERT model
outperformed Whisper across all key evaluation metrics, demonstrated superior
performance while requiring fewer computational resources, and offered valuable
insights to develop robust speech recognition systems in low-resource
linguistic settings.

</details>


### [45] [The Anatomy of Evidence: An Investigation Into Explainable ICD Coding](https://arxiv.org/abs/2507.01802)
*Katharina Beckh,Elisa Studeny,Sujan Sai Gannamaneni,Dario Antweiler,Stefan Rüping*

Main category: cs.CL

TL;DR: 本研究分析了医学编码证据数据集MDACE，并评估了现有可解释医疗编码系统的表现，发现方法与真实证据有较高重叠，并为系统开发和评测提出了建议。


<details>
  <summary>Details</summary>
Motivation: 自动化医疗编码可以减轻文档和结算流程负担，但需要解释性以满足医疗编码员和监管机构的透明性要求。受限于带注释数据稀缺，目前领域对解释性方法的评估大都局限于短文本与二分类问题。新发布的MDACE数据集为医学编码证据标注提供了新资源。

Method: 深入分析MDACE数据集；从实际应用角度对当前可解释医疗编码系统进行合理性评估。提出新的评测指标，分析领先方法的成功与失败案例，最终给出可解释系统开发和评测建议。

Result: 发现：真实证据与代码描述有一定一致性；先进方法与真实证据有较高重叠。

Conclusion: 对MDACE数据集和可解释医疗编码系统进行分析，推动了领域对自动医疗编码及证据抽取的理解。提出了匹配指标，揭示了这些方法在证据提取中的表现，进一步为未来可解释医疗编码系统的研究提供建议。

Abstract: Automatic medical coding has the potential to ease documentation and billing
processes. For this task, transparency plays an important role for medical
coders and regulatory bodies, which can be achieved using explainability
methods. However, the evaluation of these approaches has been mostly limited to
short text and binary settings due to a scarcity of annotated data. Recent
efforts by Cheng et al. (2023) have introduced the MDACE dataset, which
provides a valuable resource containing code evidence in clinical records. In
this work, we conduct an in-depth analysis of the MDACE dataset and perform
plausibility evaluation of current explainable medical coding systems from an
applied perspective. With this, we contribute to a deeper understanding of
automatic medical coding and evidence extraction. Our findings reveal that
ground truth evidence aligns with code descriptions to a certain degree. An
investigation into state-of-the-art approaches shows a high overlap with ground
truth evidence. We propose match measures and highlight success and failure
cases. Based on our findings, we provide recommendations for developing and
evaluating explainable medical coding systems.

</details>


### [46] [Evaluating Structured Output Robustness of Small Language Models for Open Attribute-Value Extraction from Clinical Notes](https://arxiv.org/abs/2507.01810)
*Nikita Neveditsin,Pawan Lingras,Vijay Mago*

Main category: cs.CL

TL;DR: 三种序列化格式对小模型输出结构的可解析性影响不同，JSON最优。提示词优化和大型模型缓解部分问题，实战中应首选JSON并优化提示。


<details>
  <summary>Details</summary>
Motivation: 在临床文本中提取属性-值对是一项重要任务，但用小型语言模型生成的结构化输出在可解析性上表现不同。选择适合的序列化格式对确保数据准确和可解析尤为关键，尤其在隐私敏感场景如医疗领域。本文旨在系统比较主流格式的可解析性，并为实际部署提供依据。

Method: 本文对三种常用序列化格式（JSON、YAML、XML）进行比较，利用小型语言模型针对临床笔记进行开放属性值抽取。通过实验评估这三种格式在格式解析上的表现，并研究不同提示方法、模型规模、文档长度及临床笔记类型等因素的影响。同时对常见的错误模式进行了分析。

Result: JSON格式在可解析性方面始终表现最佳。通过采用有针对性的提示词和更大的模型，结构健壮性得到提升，但在处理长文档和特定类型的临床笔记时，可解析性有所下降。错误分析揭示了各种格式特有的失败模式。

Conclusion: JSON是部署于临床隐私敏感场景的首选序列化格式，通过合理设计提示词和选择模型规模能够提升结构输出的准确性。

Abstract: We present a comparative analysis of the parseability of structured outputs
generated by small language models for open attribute-value extraction from
clinical notes. We evaluate three widely used serialization formats: JSON,
YAML, and XML, and find that JSON consistently yields the highest parseability.
Structural robustness improves with targeted prompting and larger models, but
declines for longer documents and certain note types. Our error analysis
identifies recurring format-specific failure patterns. These findings offer
practical guidance for selecting serialization formats and designing prompts
when deploying language models in privacy-sensitive clinical settings.

</details>


### [47] [Low-Perplexity LLM-Generated Sequences and Where To Find Them](https://arxiv.org/abs/2507.01844)
*Arthur Wuhrmann,Anastasiia Kucherenko,Andrei Kucharavy*

Main category: cs.CL

TL;DR: 本文提出了一种分析大语言模型如何复现训练数据的方法，发现并非所有高概率输出都可以在训练数据中直接找到源头，为理解模型行为与数据关系提供了新见解。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）的广泛应用，理解具体训练数据如何影响模型输出对于提升透明度、问责制、隐私和公平性尤为重要。

Method: 提出了一种系统性的方法，通过分析低困惑度序列（即模型生成的高概率文本片段），可靠地提取并回溯这些序列到训练数据来源，从而研究LLM对训练数据的利用与复现。

Result: 发现相当一部分低困惑度序列无法在语料中明确对应源头；对于可以匹配的部分，进一步量化了其在源文档中的分布情况，揭示了逐字复现的范围和性质。

Conclusion: 该研究提出的方法有助于揭示训练数据如何影响LLM行为，为提升模型透明度和深入理解复现机制提供了新路径。

Abstract: As Large Language Models (LLMs) become increasingly widespread, understanding
how specific training data shapes their outputs is crucial for transparency,
accountability, privacy, and fairness. To explore how LLMs leverage and
replicate their training data, we introduce a systematic approach centered on
analyzing low-perplexity sequences - high-probability text spans generated by
the model. Our pipeline reliably extracts such long sequences across diverse
topics while avoiding degeneration, then traces them back to their sources in
the training data. Surprisingly, we find that a substantial portion of these
low-perplexity spans cannot be mapped to the corpus. For those that do match,
we quantify the distribution of occurrences across source documents,
highlighting the scope and nature of verbatim recall and paving a way toward
better understanding of how LLMs training data impacts their behavior.

</details>


### [48] [Eka-Eval : A Comprehensive Evaluation Framework for Large Language Models in Indian Languages](https://arxiv.org/abs/2507.01853)
*Samridhi Raj Sinha,Rajvee Sheth,Abhishek Upperwal,Mayank Singh*

Main category: cs.CL

TL;DR: 提出EKA-EVAL评测框架，解决现有多语言（特别是印度本地语）大语言模型评测覆盖不足的问题，整合多种基准并具备开源、可扩展、分布式等能力，推动多语言LLM公平与全面评测。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型评测工具多以英文为主，缺乏对印度等多语言环境的支持，需要统一且支持多样化需求的评测框架。

Method: 整合35+基准集合，包括10个印度本地数据集，支持分布式推理、量化和多GPU，并与现有评测工具做系统对比。

Result: EKA-EVAL 框架已实现并开源，覆盖范围和功能优于现有印度语言评测工具，并计划扩展到100+基准，构建强大的多语言LLM评测生态。

Conclusion: EKA-EVAL 框架为全球及印度本地的大语言模型提供了首个端到端、易扩展的评测套件，大幅降低了多语言基准测试的门槛。

Abstract: The rapid advancement of Large Language Models (LLMs) has intensified the
need for evaluation frameworks that go beyond English centric benchmarks and
address the requirements of linguistically diverse regions such as India. We
present EKA-EVAL, a unified and production-ready evaluation framework that
integrates over 35 benchmarks, including 10 Indic-specific datasets, spanning
categories like reasoning, mathematics, tool use, long-context understanding,
and reading comprehension. Compared to existing Indian language evaluation
tools, EKA-EVAL offers broader benchmark coverage, with built-in support for
distributed inference, quantization, and multi-GPU usage. Our systematic
comparison positions EKA-EVAL as the first end-to-end, extensible evaluation
suite tailored for both global and Indic LLMs, significantly lowering the
barrier to multilingual benchmarking. The framework is open-source and publicly
available at https://github.com/lingo-iitgn/ eka-eval and a part of ongoing EKA
initiative (https://eka.soket.ai), which aims to scale up to over 100
benchmarks and establish a robust, multilingual evaluation ecosystem for LLMs.

</details>


### [49] [DIY-MKG: An LLM-Based Polyglot Language Learning System](https://arxiv.org/abs/2507.01872)
*Kenan Tang,Yanhong Li,Yao Qin*

Main category: cs.CL

TL;DR: 本文提出DIY-MKG系统，结合大模型和知识图谱，解决了多语种学习工具的个性化与扩展难题，经验证效果良好。


<details>
  <summary>Details</summary>
Motivation: 现有语言学习工具，即使有大模型支持，往往不支持多语种学习者跨语言联系词汇、缺乏个性化定制，也容易让用户产生认知依赖。

Method: 设计并实现了DIY-MKG（多语种知识图谱自建系统），允许用户构建个性化的词汇知识图谱，通过大模型推荐相关词扩展词汇，同时提供丰富注释功能和自适应测验模块（由大模型个性化生成测验）。用户还可反馈错误问题，用于优化提示词。

Result: 系统支持多语种词汇扩展，扩展效果可靠、公平，测验题目准确性高，验证了系统的健壮性。

Conclusion: DIY-MKG有效提升了多语种学习体验，能够满足个性化、交互性和准确性的需求。

Abstract: Existing language learning tools, even those powered by Large Language Models
(LLMs), often lack support for polyglot learners to build linguistic
connections across vocabularies in multiple languages, provide limited
customization for individual learning paces or needs, and suffer from
detrimental cognitive offloading. To address these limitations, we design
Do-It-Yourself Multilingual Knowledge Graph (DIY-MKG), an open-source system
that supports polyglot language learning. DIY-MKG allows the user to build
personalized vocabulary knowledge graphs, which are constructed by selective
expansion with related words suggested by an LLM. The system further enhances
learning through rich annotation capabilities and an adaptive review module
that leverages LLMs for dynamic, personalized quiz generation. In addition,
DIY-MKG allows users to flag incorrect quiz questions, simultaneously
increasing user engagement and providing a feedback loop for prompt refinement.
Our evaluation of LLM-based components in DIY-MKG shows that vocabulary
expansion is reliable and fair across multiple languages, and that the
generated quizzes are highly accurate, validating the robustness of DIY-MKG.

</details>


### [50] [MiCoTA: Bridging the Learnability Gap with Intermediate CoT and Teacher Assistants](https://arxiv.org/abs/2507.01887)
*Dongyi Ding,Tiannan Wang,Chenghao Zhu,Meiling Tao,Yuchen Eleanor Jiang,Wangchunshu Zhou*

Main category: cs.CL

TL;DR: 大型模型蒸馏到小模型存在长链推理学习难题，MiCoTA通过引入中尺寸教师助手和中长推理序列，有效提升了小模型的推理表现。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在需要长链推理的任务中表现优异，但因体积庞大和计算消耗高，限制了其普及。小型语言模型（SLM）则因容量受限，难以学到长链推理能力（称为SLMs Learnability Gap）。本论文旨在解决SLMs在长链推理学习上的瓶颈。

Method: 作者提出了一种新的蒸馏框架MiCoTA（Mid-CoT Teacher Assistant Distillation），引入了中等规模教师助手模型和中长度推理序列，辅助小模型更好地学习长链推理能力。

Result: 实验证明，直接用大模型蒸馏的小模型效果不佳，而引入MiCoTA后，小模型的推理表现显著提升。例如，Qwen2.5-7B-Instruct和Qwen2.5-3B-Instruct在多个数学推理基准测试中平均分数分别提升了3.47和3.93。进一步实验还表明，本方法生成的数据分布更贴合SLM基础分布。

Conclusion: MiCoTA有效缩小了小模型在长链推理蒸馏中的学习差距，为后续SLM长链推理能力提升和数据蒸馏研究提供了新思路。

Abstract: Large language models (LLMs) excel at reasoning tasks requiring long thought
sequences for planning, reflection, and refinement. However, their substantial
model size and high computational demands are impractical for widespread
deployment. Yet, small language models (SLMs) often struggle to learn long-form
CoT reasoning due to their limited capacity, a phenomenon we refer to as the
"SLMs Learnability Gap". To address this, we introduce
\textbf{Mi}d-\textbf{Co}T \textbf{T}eacher \textbf{A}ssistant Distillation
(MiCoTAl), a framework for improving long CoT distillation for SLMs. MiCoTA
employs intermediate-sized models as teacher assistants and utilizes
intermediate-length CoT sequences to bridge both the capacity and reasoning
length gaps. Our experiments on downstream tasks demonstrate that although SLMs
distilled from large teachers can perform poorly, by applying MiCoTA, they
achieve significant improvements in reasoning performance. Specifically,
Qwen2.5-7B-Instruct and Qwen2.5-3B-Instruct achieve an improvement of 3.47 and
3.93 respectively on average score on AIME2024, AMC, Olympiad, MATH-500 and
GSM8K benchmarks. To better understand the mechanism behind MiCoTA, we perform
a quantitative experiment demonstrating that our method produces data more
closely aligned with base SLM distributions. Our insights pave the way for
future research into long-CoT data distillation for SLMs.

</details>


### [51] [High-Layer Attention Pruning with Rescaling](https://arxiv.org/abs/2507.01900)
*Songtao Liu,Peng Liu*

Main category: cs.CL

TL;DR: 针对现有无训练结构化剪枝方法的不足，本文提出专注于高层注意力头剪枝并自适应校准的方法，在多种大模型及大规模任务上实现了性能超越现有剪枝技术。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLMs）由于规模庞大，推理时延较高，因此需要有效的模型压缩方法以提升实际应用效率。结构化剪枝是常见的压缩方法，但现有无训练结构化剪枝通常采用启发式指标，在所有层无差别地剪枝注意力头，未考虑网络结构中不同层的位置差异，可能影响模型性能。

Method: 提出了一种新颖的结构化剪枝算法，专门针对模型高层的注意力头进行有策略的剪枝。此外，为了修正剪枝可能带来的表示幅值变化，引入了自适应重标定参数，在剪枝之后重新校准token表示的尺度。

Result: 在包括LLaMA3.1-8B、Mistral-7B-v0.3、Qwen2-7B和Gemma2-9B等多种大语言模型上进行了详尽实验。测试任务覆盖生成和判别两类共27个数据集。结果显示，该方法在所有数据集和任务上均优于现有结构化剪枝方法，尤其在生成任务上优势显著。

Conclusion: 通过有针对性地剪枝模型高层的注意力头，并引入自适应重标定机制，可有效缓解剪枝引发的表示失调问题，从而在降低模型计算开销的同时保持甚至提升模型性能，特别是在文本生成任务中的表现。

Abstract: Pruning is a highly effective approach for compressing large language models
(LLMs), significantly reducing inference latency. However, conventional
training-free structured pruning methods often employ a heuristic metric that
indiscriminately removes some attention heads across all pruning layers,
without considering their positions within the network architecture. In this
work, we propose a novel pruning algorithm that strategically prunes attention
heads in the model's higher layers. Since the removal of attention heads can
alter the magnitude of token representations, we introduce an adaptive
rescaling parameter that calibrates the representation scale post-pruning to
counteract this effect. We conduct comprehensive experiments on a wide range of
LLMs, including LLaMA3.1-8B, Mistral-7B-v0.3, Qwen2-7B, and Gemma2-9B. Our
evaluation includes both generation and discriminative tasks across 27
datasets. The results consistently demonstrate that our method outperforms
existing structured pruning methods. This improvement is particularly notable
in generation tasks, where our approach significantly outperforms existing
baselines.

</details>


### [52] [NaturalThoughts: Selecting and Distilling Reasoning Traces for General Reasoning Tasks](https://arxiv.org/abs/2507.01921)
*Yang Li,Youssef Emad,Karthik Padthe,Jack Lanchantin,Weizhe Yuan,Thao Nguyen,Jason Weston,Shang-Wen Li,Dong Wang,Ilia Kulikov,Xian Li*

Main category: cs.CL

TL;DR: 本文系统探究了如何从教师大模型选取最有效的推理示范训练学生模型，提出并验证了高质量精挑难例更高效，显著超过以往做法。


<details>
  <summary>Details</summary>
Motivation: 已有研究表明，通过大模型教师的推理轨迹监督微调小学生模型，比仅用小模型自身的强化学习效果更好。但目前尚缺乏系统性研究，探讨哪些类型的推理示范最有效提升学生模型的推理能力。

Method: 作者从强力教师模型生成的大量问题推理轨迹中，精挑细选出高质量推理示范（NaturalThoughts），并对影响推理蒸馏效果的因素（如样本效率与扩展性）进行了系统分析，比较了不同数据筛选策略，并在多个主流模型与基准数据集上作比较实验。

Result: （1）发现简单增大随机采样训练数据规模，本身就是表现强劲且稳定提升效果的基础方法；（2）挑选更难、涉及多样化推理策略的示例，更高效提升学生的推理技能；（3）用NaturalThoughts训练模型，在广泛STEM推理基准上优于已有数据集。

Conclusion: 系统分析证明精挑细选且高质量的推理示范能更高效提升学生模型的推理能力，针对难例优化的策略可超越单纯扩展数据，展示了推理技能蒸馏的新范式与优越性。

Abstract: Recent work has shown that distilling reasoning traces from a larger teacher
model via supervised finetuning outperforms reinforcement learning with the
smaller student model alone (Guo et al. 2025). However, there has not been a
systematic study of what kind of reasoning demonstrations from the teacher are
most effective in improving the student model's reasoning capabilities. In this
work we curate high-quality "NaturalThoughts" by selecting reasoning traces
from a strong teacher model based on a large pool of questions from
NaturalReasoning (Yuan et al. 2025). We first conduct a systematic analysis of
factors that affect distilling reasoning capabilities, in terms of sample
efficiency and scalability for general reasoning tasks. We observe that simply
scaling up data size with random sampling is a strong baseline with steady
performance gains. Further, we find that selecting difficult examples that
require more diverse reasoning strategies is more sample-efficient to transfer
the teacher model's reasoning skills. Evaluated on both Llama and Qwen models,
training with NaturalThoughts outperforms existing reasoning datasets such as
OpenThoughts, LIMO, etc. on general STEM reasoning benchmarks including
GPQA-Diamond, MMLU-Pro and SuperGPQA.

</details>


### [53] [Decision-oriented Text Evaluation](https://arxiv.org/abs/2507.01923)
*Yu-Shiang Huang,Chuan-Ju Wang,Chung-Chi Chen*

Main category: cs.CL

TL;DR: 将NLG评估重点从表面匹配转向实际决策影响，提出以金融交易实绩为依托的决策导向评估框架。实验证明：分析性文本能显著提升人-机协同决策，而传统内在评估不足以捕捉这种价值。


<details>
  <summary>Details</summary>
Motivation: 现有的自然语言生成（NLG）评估方法（如n-gram重叠度或句子合理性）与决策过程的实际有效性相关性较弱，难以反映文本对高风险领域决策的真实影响。

Method: 提出了一种以决策为导向的NLG评估框架，直接测量生成文本对人类和大型语言模型（LLM）决策结果的影响。采用市场摘要文本（包括客观晨报和主观收盘分析）为案例，通过人类投资者和仅依据这些文本信息的自主LLM代理进行交易，其决策质量以实际金融收益衡量。

Result: 实验发现：不论是人类还是LLM，仅靠摘要信息做决策，表现都与随机选择无显著差异。但当提供更加丰富的分析性评论时，人-LLM合作团队显著优于单独的人类或代理。

Conclusion: 论文强调评估生成文本时，应重视其促进人-LLM协同决策的能力，现有内在度量方法存在严重局限。

Abstract: Natural language generation (NLG) is increasingly deployed in high-stakes
domains, yet common intrinsic evaluation methods, such as n-gram overlap or
sentence plausibility, weakly correlate with actual decision-making efficacy.
We propose a decision-oriented framework for evaluating generated text by
directly measuring its influence on human and large language model (LLM)
decision outcomes. Using market digest texts--including objective morning
summaries and subjective closing-bell analyses--as test cases, we assess
decision quality based on the financial performance of trades executed by human
investors and autonomous LLM agents informed exclusively by these texts. Our
findings reveal that neither humans nor LLM agents consistently surpass random
performance when relying solely on summaries. However, richer analytical
commentaries enable collaborative human-LLM teams to outperform individual
human or agent baselines significantly. Our approach underscores the importance
of evaluating generated text by its ability to facilitate synergistic
decision-making between humans and LLMs, highlighting critical limitations of
traditional intrinsic metrics.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [54] [HPC-AI Coupling Methodology for Scientific Applications](https://arxiv.org/abs/2507.01025)
*Yutong Lu,Dan Huang,Pin Chen*

Main category: cs.CE

TL;DR: 本文提出了三种HPC与AI的耦合模式，并通过材料科学案例验证了其有效性。研究为科学领域HPC-AI结合提供了实践指导和未来展望。


<details>
  <summary>Details</summary>
Motivation: 人工智能技术为高性能计算（HPC）应用带来了数据驱动的新方法，试图解决传统数值型HPC在各种科学领域中面临的高计算强度等挑战。本文旨在探索HPC与AI结合的新模式，以推动科学研究的发展。

Method: 本文提出了一种新颖的方法论，包含三种HPC与AI的耦合模式：代理（surrogate）、指令（directive）和协调（coordinate）。并以材料科学为案例，对这些模式的应用及其效果进行了研究。

Result: 通过在材料科学中的案例研究，三种耦合模式均展示出对高性能计算与人工智能结合的有效性和应用前景。论文还详细讨论了技术挑战、性能提升及实现细节。

Conclusion: 所提出的三种耦合模式能够为材料科学及其他科学领域在未来HPC与AI集成提供实用指导，有助于推动科学发现。

Abstract: Artificial intelligence (AI) technologies have fundamentally transformed
numerical-based high-performance computing (HPC) applications with data-driven
approaches and endeavored to address existing challenges, e.g. high
computational intensity, in various scientific domains. In this study, we
explore the scenarios of coupling HPC and AI (HPC-AI) in the context of
emerging scientific applications, presenting a novel methodology that
incorporates three patterns of coupling: surrogate, directive, and coordinate.
Each pattern exemplifies a distinct coupling strategy, AI-driven prerequisite,
and typical HPC-AI ensembles. Through case studies in materials science, we
demonstrate the application and effectiveness of these patterns. The study
highlights technical challenges, performance improvements, and implementation
details, providing insight into promising perspectives of HPC-AI coupling. The
proposed coupling patterns are applicable not only to materials science but
also to other scientific domains, offering valuable guidance for future HPC-AI
ensembles in scientific discovery.

</details>


### [55] [Agentic AI in Product Management: A Co-Evolutionary Model](https://arxiv.org/abs/2507.01069)
*Nishant A. Parikh*

Main category: cs.CE

TL;DR: 本文提出了以共进化理论为基础的Agentic AI整合框架，揭示产品经理和AI之间的双向适应及PM能力转型需求，对AI在产品管理全生命周期的应用提供了理论和实践参考。


<details>
  <summary>Details</summary>
Motivation: 近年来，Agentic AI（具有自主性、目标导向和多智能体协作特征的AI）的发展正深刻影响产品管理领域，对产品经理角色和产品开发流程带来了挑战和机遇。因此，迫切需要理论框架指导AI在产品全周期中的有效整合。

Method: 基于系统理论、共进化理论和人机协作理论，作者建立了一个框架，用于分析Agentic AI在产品发现、范围界定、商业案例开发、开发、测试与发布各环节的作用。同时，对70余篇相关文献及龙头科技公司案例进行了综述和归纳。

Result: 通过研究发现，产品经理的角色从传统的产品负责人向社会-技术生态系统的编排者转变。Agentic AI强调PM与AI的共同适应，PM需要增强AI素养、治理、系统思维等能力，以实现AI在软件企业中的负责任与高效整合。

Conclusion: 本研究弥补了传统产品管理框架在AI应用方面的不足，提出了基于共进化的整合框架，为未来相关理论研究和实际应用提供基础，促进Agentic AI在产品管理中的可持续实施。

Abstract: This study explores agentic AI's transformative role in product management,
proposing a conceptual co-evolutionary framework to guide its integration
across the product lifecycle. Agentic AI, characterized by autonomy,
goal-driven behavior, and multi-agent collaboration, redefines product managers
(PMs) as orchestrators of socio-technical ecosystems. Using systems theory,
co-evolutionary theory, and human-AI interaction theory, the framework maps
agentic AI capabilities in discovery, scoping, business case development,
development, testing, and launch. An integrative review of 70+ sources,
including case studies from leading tech firms, highlights PMs' evolving roles
in AI orchestration, supervision, and strategic alignment. Findings emphasize
mutual adaptation between PMs and AI, requiring skills in AI literacy,
governance, and systems thinking. Addressing gaps in traditional frameworks,
this study provides a foundation for future research and practical
implementation to ensure responsible, effective agentic AI integration in
software organizations.

</details>


### [56] [Spatially Distributed Wettability Characterization in Porous Media](https://arxiv.org/abs/2507.01617)
*Faisal Aljaberi,Hadi Belhaj,Sajjad Foroughi,Mohammed Al-Kobaisi,Martin Blunt*

Main category: cs.CE

TL;DR: 该论文提出了一种改进的微CT图片自动化接触角测量算法，揭示了润湿性空间异质性，挑战了传统均值指标，助力提升多相流预测与地下能源过程优化。


<details>
  <summary>Details</summary>
Motivation: 现有微孔接触角测量方法在准确性和揭示润湿性异质性方面存在局限，无法充分反映多孔介质中多相流行为的复杂性。

Method: 提出了一种增强型几何算法，通过稳健的流体-流体和固体-流体界面外推，实现了基于微CT图像的自动逐孔接触角测量。利用高分辨率数据，生成空间分布的接触角地图。

Result: 在分析混合润湿体系时发现，平均接触角为64.7度但有40%孔隙处于中间润湿状态（70-110度），揭示了润湿性异质性和不同孔填充机制共存。

Conclusion: 空间分辨的润湿性表征能够更真实反映多孔介质的多相流行为，为优化地下能源存储与开采提供了支持。相关开源工具促进了精确预测。

Abstract: An enhanced geometric algorithm for automated pore-by-pore contact angle
measurement from micro-CT images, is presented that achieves superior accuracy
compared to existing methods through robust fluid-fluid and solid-fluid
interface extrapolation. Using this high resolution data, we generate spatially
distributed contact angle maps that reveal previously hidden wettability
heterogeneity. Our analysis of mixed-wet systems demonstrates the severe
limitations of averaged metrics: a sample with a mean contact angle of 64.7
degrees, conventionally classified as uniformly weakly water-wet, exhibits 40%
of its pore space in the intermediate-wetting regime (70-110 degrees). This
heterogeneity explains the presence of minimal surface interfaces and
fundamentally different pore-filling mechanisms operating within the same
sample. By providing open-source tools for spatially-resolved wettability
characterization, this work enables more accurate predictions of multiphase
flow behavior in heterogeneous porous materials, essential for optimizing
subsurface energy storage and recovery processes.

</details>


### [57] [A modified Levenberg-Marquardt method for estimating the elastic material parameters of polymer waveguides using residuals between autocorrelated frequency responses](https://arxiv.org/abs/2507.01706)
*Dominik Itner,Dmitrij Dreiling,Hauke Gravenkamp,Bernd Henning,Carolin Birk*

Main category: cs.CE

TL;DR: 本文提出两项新方法改进聚合物超声弹性参数估算：采用改进的优化器和目标函数，能减少模型调用，提升运算速度，对实际应用有明显帮助。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在解决使用超声波测量聚合物频率相关弹性参数时的计算效率与参数估算准确性不足的问题。现有方法在优化过程中模型评估次数多、耗时长。

Method: 作者提出将此逆问题转化为非线性回归型优化，并专注于空心圆柱波导的模拟以提高仿真效率。方法创新包括：(1) 基于最小二乘几何解释改进Levenberg-Marquardt方法用于步长适应，(2) 基于自动相关包络改进目标函数。

Result: 所提方法在各种各向同性材料实验中，与现有优化算法比较，能够大幅减少模型评估次数，显著缩短材料参数识别总耗时。

Conclusion: 所改进的优化方法和目标函数提升了聚合物弹性参数识别的效率与可复现性，对实际测量具有推广价值。

Abstract: In this contribution, we address the estimation of the frequency-dependent
elastic parameters of polymers in the ultrasound range, which is formulated as
an inverse problem. This inverse problem is implemented as a nonlinear
regression-type optimization problem, in which the simulation signals are
fitted to the measurement signals. These signals consist of displacement
responses in waveguides, focusing on hollow cylindrical geometries to enhance
the simulation efficiency. To accelerate the optimization and reduce the number
of model evaluations and wait times, we propose two novel methods. First, we
introduce an adaptation of the Levenberg-Marquardt method derived from a
geometrical interpretation of the least-squares optimization problem. Second,
we introduce an improved objective function based on the autocorrelated
envelopes of the measurement and simulation signals. Given that this study
primarily relies on simulation data to quantify optimization convergence, we
aggregate the expected ranges of realistic material parameters and derive their
distributions to ensure the reproducibility of optimizations with proper
measurements. We demonstrate the effectiveness of our objective function
modification and step adaptation for various materials with isotropic material
symmetry by comparing them with a state-of-the-art optimization method. In all
cases, our method reduces the total number of model evaluations, thereby
shortening the time to identify the material parameters.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [58] [Can AI be Consentful?](https://arxiv.org/abs/2507.01051)
*Giada Pistilli,Bruna Trevelin*

Main category: cs.CY

TL;DR: 生成式AI让数据同意机制失效，带来范围、时效与自主权三大问题；传统法律无法保护AI生成内容下的个人权益，需法律与伦理框架创新。


<details>
  <summary>Details</summary>
Motivation: 生成式AI技术的发展，使传统基于同意的数据保护和隐私权思路遇到严峻挑战，现有同意机制难以应对AI生成内容的不确定性和扩散性，因此有必要重新审视同意在AI语境下的适用性。

Method: 采用法律和伦理分析方法，梳理个人数据被AI用于生成内容时同意机制面临的问题，并结合AI责任治理原则进行透视和讨论。

Result: 作者识别出三大同意挑战：范围问题、时效问题、自主权陷阱，形成所谓“同意缺口”；并指出现行法律未能有效解决个人自主权、身份权和社会责任等新兴风险，呼吁完善AI同意治理体系以适应技术变革。

Conclusion: 目前法律与伦理框架关于同意（consent）的规定无法充分应对AI生成内容对个人权益产生的新挑战，亟需更新相关理念与法规。

Abstract: The evolution of generative AI systems exposes the challenges of traditional
legal and ethical frameworks built around consent. This chapter examines how
the conventional notion of consent, while fundamental to data protection and
privacy rights, proves insufficient in addressing the implications of
AI-generated content derived from personal data. Through legal and ethical
analysis, we show that while individuals can consent to the initial use of
their data for AI training, they cannot meaningfully consent to the numerous
potential outputs their data might enable or the extent to which the output is
used or distributed. We identify three fundamental challenges: the scope
problem, the temporality problem, and the autonomy trap, which collectively
create what we term a ''consent gap'' in AI systems and their surrounding
ecosystem. We argue that current legal frameworks inadequately address these
emerging challenges, particularly regarding individual autonomy, identity
rights, and social responsibility, especially in cases where AI-generated
content creates new forms of personal representation beyond the scope of the
original consent. By examining how these consent limitations intersect with
broader principles of responsible AI (including fairness, transparency,
accountability, and autonomy) we demonstrate the need to evolve ethical and
legal approaches to consent.

</details>


### [59] [Epitome: Pioneering an Experimental Platform for AI-Social Science Integration](https://arxiv.org/abs/2507.01061)
*Jingjing Qu,Kejia Hu,Jun Zhu,Wenhao Li,Teng Wang,Zhiyun Chen,Yulei Ye,Chaochao Lu,Aimin Zhou,Xiangfeng Wang,James Evan*

Main category: cs.CY

TL;DR: 本文提出并实现了世界首个AI与社会科学深度融合实验平台Epitome，平台能够系统支撑多层次人-机交互实验，通过复现经典实验展示其实用性和理论价值，为社会科学研究AI影响提供了创新工具。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLMs）逐步深度融入社会科学实验，但缺乏系统的实验平台来支持跨学科多层次的人机交互研究，因此亟需创新的综合性实验平台推动AI与社会科学的实际融合及社会影响评估。

Method: 提出并实现了Epitome平台，结合管理学、传播学、社会学、心理学及伦理学等理论，通过跨学科实验构建理论支持体系，平台以七大模块涵盖基础模型、复杂应用开发和用户反馈，并内嵌社会科学经典控制-比较-因果逻辑，支持多场景多层次人机交互实验。通过复现三项经典实验，展示平台能力。

Result: 平台成功复现了三项具有代表性的社会科学实验，展示了Epitome对于高效设计、组织和执行复杂实验的能力，为AI社会影响研究提供了可靠的方法和工具，结果具备发表高水平期刊的潜力。

Conclusion: Epitome平台极大提升了AI与社会科学融合研究的效率与质量，是推动AI社会效应理解和政策制定的有力工具，助力跨学科理论与应用创新。

Abstract: The integration of Large Language Models (LLMs) into social science
experiments represents a transformative approach to understanding human-AI
interactions and their societal impacts. We introduce Epitome, the world's
first open experimental platform dedicated to the deep integration of
artificial intelligence and social science. Rooted in theoretical foundations
from management, communication studies, sociology, psychology, and ethics,
Epitome focuses on the interactive impacts of AI on individuals, organizations,
and society during its real-world deployment. It constructs a theoretical
support system through cross-disciplinary experiments. The platform offers a
one-stop comprehensive experimental solution spanning "foundation
models-complex application development-user feedback" through seven core
modules, while embedding the classical "control-comparison-comparative causal
logic" of social science experiments into multilevel human-computer interaction
environments, including dialogues, group chats, and multi-agent virtual
scenarios. With its canvas-style, user-friendly interface, Epitome enables
researchers to easily design and run complex experimental scenarios,
facilitating systematic investigations into the social impacts of AI and
exploration of integrated solutions.To demonstrate its capabilities, we
replicated three seminal social science experiments involving LLMs, showcasing
Epitome's potential to streamline complex experimental designs and produce
robust results, suitable for publishing in the top selective journals. Our
findings highlight the platform's utility in enhancing the efficiency and
quality of human-AI interactions, providing valuable insights into the societal
implications of AI technologies. Epitome thus offers a powerful tool for
advancing interdisciplinary research at the intersection of AI and social
science, with potential applications in policy-making, ...

</details>


### [60] [Quantifying Student Success with Generative AI: A Monte Carlo Simulation Informed by Systematic Review](https://arxiv.org/abs/2507.01062)
*Seyma Yaman Kayadibi*

Main category: cs.CY

TL;DR: 本研究通过系统文献综述和模拟建模，发现大学生对人工智能工具的可用性与现实用途的态度对学习成效提升作用显著，建议关注实际应用而非单纯信任或情感因素。


<details>
  <summary>Details</summary>
Motivation: 近年来，生成式人工智能（如ChatGPT）迅速发展，引发了人们对于其在高等教育中应用的兴趣，尤其是学生如何看待并使用这些技术及其对学习效果的影响。

Method: 采用混合方法，包括系统性文献综述和基于模拟的建模，从Scopus数据库中筛选了2023-2025年间的19篇实证研究，并通过主题分类综合文献中的模式，对定量信息进行概率建模，并利用Monte Carlo模拟对典型数据集进行逆方差加权分析。

Result: 研究发现，与情感或信任相关因素相比，关注可用性与实际用途的态度因素能更好地预测学生学习成就的提升。

Conclusion: 高等教育领域中，学生对生成式人工智能的可用性及其现实应用的积极看法，是提升学习成果的重要预测因子，推动了对大学中如何合理利用GenAI工具的讨论。

Abstract: The exponential development of generative artificial intelligence (GenAI)
technologies like ChatGPT has raised increasing curiosity about their use in
higher education, specifically with respect to how students view them, make use
of them, and the implications for learning outcomes. This paper employs a
hybrid methodological approach involving a systematic literature review and
simulation-based modeling to explore student perceptions of GenAI use in the
context of higher education. A total of nineteen empirical articles from 2023
through 2025 were selected from the PRISMA-based search targeting the Scopus
database. Synthesis of emerging patterns from the literature was achieved by
thematic categorization. Six of these had enough quantitative information,
i.e., item-level means and standard deviations, to permit probabilistic
modeling. One dataset, from the resulting subset, was itself selected as a
representative case with which to illustrate inverse-variance weighting by
Monte Carlo simulation, by virtue of its well-designed Likert scale format and
thematic alignment with the use of computing systems by the researcher.
  The simulation provided a composite "Success Score" forecasting the strength
of the relationship between student perceptions and learning achievements.
Findings reveal that attitude factors concerned with usability and real-world
usefulness are significantly better predictors of positive learning achievement
than affective or trust-based factors. Such an interdisciplinary perspective
provides a unique means of linking thematic results with predictive modelling,
resonating with longstanding controversies about the proper use of GenAI tools
within the university.

</details>


### [61] [A Practical SAFE-AI Framework for Small and Medium-Sized Enterprises Developing Medical Artificial Intelligence Ethics Policies](https://arxiv.org/abs/2507.01304)
*Ion Nemteanu,Adir Mancebo Jr.,Leslie Joe,Ryan Lopez,Patricia Lopez,Warren Woodrich Pettine*

Main category: cs.CY

TL;DR: 本文针对现有AI伦理框架难以落地的问题，提出了将伦理机制融入敏捷开发流程的SAFE-AI框架，兼顾企业效率与伦理监督，实现可持续、可扩展的伦理AI开发。


<details>
  <summary>Details</summary>
Motivation: 人工智能（AI）在医疗领域带来巨大潜力的同时，也引发了如偏见等重要伦理问题。现有的伦理框架虽然严谨，但往往过于复杂，难以在节奏快、资源有限的业务环境中落地。因此急需一个兼顾伦理性与业务需求、能够高效执行的框架。

Method: 作者提出了一个可扩展的敏捷AI执行伦理框架（SAFE-AI），将伦理监督嵌入到基于敏捷的产品开发流程中。该框架通过设计可测试的验收标准、公平性与透明性指标，持续监控AI生命周期，采用基于情景概率类比映射的责任指标，并对模型再训练和调整施加轻量但有效的伦理审查。

Result: SAFE-AI框架能够用最小的必要要求，推动负责任的AI开发。它既能契合企业实际，也适用于未设有专门伦理团队的组织，还能提升伦理透明度和利益相关方的信任。

Conclusion: SAFE-AI成功实现了伦理与敏捷开发的融合，为资源有限的组织提供了一种高效、可扩展且业务友好的伦理AI开发模式。其核心机制为早期建立度量标准及持续迭代，确保开发与伦理并重。

Abstract: Artificial intelligence (AI) offers incredible possibilities for patient
care, but raises significant ethical issues, such as the potential for bias.
Powerful ethical frameworks exist to minimize these issues, but are often
developed for academic or regulatory environments and tend to be comprehensive
but overly prescriptive, making them difficult to operationalize within
fast-paced, resource-constrained environments. We introduce the Scalable Agile
Framework for Execution in AI (SAFE-AI) designed to balance ethical rigor with
business priorities by embedding ethical oversight into standard Agile-based
product development workflows. The framework emphasizes the early establishment
of testable acceptance criteria, fairness metrics, and transparency metrics to
manage model uncertainty, while also promoting continuous monitoring and
re-evaluation of these metrics across the AI lifecycle. A core component of
this framework are responsibility metrics using scenario-based probability
analogy mapping designed to enhance transparency and stakeholder trust. This
ensures that retraining or tuning activities are subject to lightweight but
meaningful ethical review. By focusing on the minimum necessary requirements
for responsible development, our framework offers a scalable, business-aligned
approach to ethical AI suitable for organizations without dedicated ethics
teams.

</details>


### [62] [Penalizing Transparency? How AI Disclosure and Author Demographics Shape Human and AI Judgments About Writing](https://arxiv.org/abs/2507.01418)
*Inyoung Cheong,Alicia Guo,Mina Lee,Zhehui Liao,Kowe Kadoma,Dongyoung Go,Joseph Chee Chang,Peter Henderson,Mor Naaman,Amy X. Zhang*

Main category: cs.CY

TL;DR: 人类和AI评委对公开AI辅助的写作持负面看法。AI评委会更偏爱某些群体（女性或黑人）作者的作品，但如果披露了AI辅助，优势即消失，显示AI披露与社会身份存在复杂交互。


<details>
  <summary>Details</summary>
Motivation: 随着AI技术广泛融入人类写作，各方呼吁增加关于AI辅助写作的透明度。但如果诚实公开AI使用的成本在不同身份群体之间分布不均，‘透明’本身将带来不均衡的负担。因此，研究动机是探究AI辅助写作披露对作者的影响，特别是在不同种族和性别群体中是否存在差异。

Method: 通过一项大规模对照实验，分别让人类评委（n = 1,970）和大型语言模型（LLM）评委（n = 2,520）对唯一一篇由人类撰写的新闻文章进行评价。实验中系统地变换披露AI辅助写作的声明以及作者的种族和性别信息，以观测其对文章质量评价的影响。

Result: 结果显示，无论是人类还是LLM评委，对于公开AI辅助写作的文章都存在评分惩罚。但只有LLM评委在作者身份上出现互动效应：在没有AI披露时，更偏好归属于女性或黑人作者的文章，而一旦AI辅助事实被公开，這种优势则消失。

Conclusion: AI披露与作者身份之间存在复杂的关系，尤其在机器与人类评价模式之间表现出显著差异，这突显了AI写作披露带来的社会公正和机会平等新挑战。

Abstract: As AI integrates in various types of human writing, calls for transparency
around AI assistance are growing. However, if transparency operates on uneven
ground and certain identity groups bear a heavier cost for being honest, then
the burden of openness becomes asymmetrical. This study investigates how AI
disclosure statement affects perceptions of writing quality, and whether these
effects vary by the author's race and gender. Through a large-scale controlled
experiment, both human raters (n = 1,970) and LLM raters (n = 2,520) evaluated
a single human-written news article while disclosure statements and author
demographics were systematically varied. This approach reflects how both human
and algorithmic decisions now influence access to opportunities (e.g., hiring,
promotion) and social recognition (e.g., content recommendation algorithms). We
find that both human and LLM raters consistently penalize disclosed AI use.
However, only LLM raters exhibit demographic interaction effects: they favor
articles attributed to women or Black authors when no disclosure is present.
But these advantages disappear when AI assistance is revealed. These findings
illuminate the complex relationships between AI disclosure and author identity,
highlighting disparities between machine and human evaluation patterns.

</details>


### [63] [AI and Remote Sensing for Resilient and Sustainable Built Environments: A Review of Current Methods, Open Data and Future Directions](https://arxiv.org/abs/2507.01547)
*Ubada El Joulani,Tatiana Kalganova,Stergios-Aristoteles Mitoulis,Sotirios Argyroudis*

Main category: cs.CY

TL;DR: 本文综述了AI在交通基础设施损伤评估中的应用，发现AI结合SAR数据用于桥梁损伤检测方面存在研究空白，提出需加强该领域的研究，为AI驱动的基础设施监测提供参考。


<details>
  <summary>Details</summary>
Motivation: 交通基础设施对经济增长至关重要，但受到老化、气候变化以及自然灾害、网络攻击等多重威胁，亟需提升其韧性和功能保障。

Method: 通过系统性文献综述，梳理与总结当前人工智能（AI）技术在交通基础设施损伤评估及监测中的应用现状，重点关注桥梁损伤检测及AI与合成孔径雷达（SAR）数据的结合。

Result: 当前AI模型和数据集已在道路、桥梁等设施受灾后的损伤评估方面有所应用，但针对桥梁的研究较为有限，尤其是AI与SAR结合用于桥梁全面损伤评估的相关研究寥寥，存在显著研究空白。

Conclusion: 本综述明确了AI技术在交通基础设施损伤评估领域特别是桥梁领域的研究不足，并为未来基于AI的桥梁及其他关键交通基础设施评估和监测研究提供了理论基础和研究方向。

Abstract: Critical infrastructure, such as transport networks, underpins economic
growth by enabling mobility and trade. However, ageing assets, climate change
impacts (e.g., extreme weather, rising sea levels), and hybrid threats ranging
from natural disasters to cyber attacks and conflicts pose growing risks to
their resilience and functionality. This review paper explores how emerging
digital technologies, specifically Artificial Intelligence (AI), can enhance
damage assessment and monitoring of transport infrastructure. A systematic
literature review examines existing AI models and datasets for assessing damage
in roads, bridges, and other critical infrastructure impacted by natural
disasters. Special focus is given to the unique challenges and opportunities
associated with bridge damage detection due to their structural complexity and
critical role in connectivity. The integration of SAR (Synthetic Aperture
Radar) data with AI models is also discussed, with the review revealing a
critical research gap: a scarcity of studies applying AI models to SAR data for
comprehensive bridge damage assessment. Therefore, this review aims to identify
the research gaps and provide foundations for AI-driven solutions for assessing
and monitoring critical transport infrastructures.

</details>


### [64] [From Reports to Reality: Testing Consistency in Instagram's Digital Services Act Compliance Data](https://arxiv.org/abs/2507.01787)
*Marie-Therese Sekwenz,Ben Wagner,Hans De Bruijn*

Main category: cs.CY

TL;DR: 本文以Instagram为例，提出多层次一致性框架用于评估DSA合规性，发现应将平台视作整体生态系统考量，有助于提升监管和合规效果。


<details>
  <summary>Details</summary>
Motivation: DSA对内容审核和平台治理提出了新要求，尤其对大型平台和搜索引擎提出了强制合规机制，亟需实证研究以指导平台、监管者和研究者更好利用DSA机制。

Method: 开发并应用了一个多层次一致性框架，对Instagram遵循DSA合规要求的表现进行了评估。

Result: 一致性评估框架能够帮助发现合规失误；研究为实证基础监管提供了见解，有助于提升平台报告、执法的质量与问责性。

Conclusion: 平台应被视作相互关联的生态系统，而非孤立个体进行评估，这对于有效的DSA合规评估至关重要。

Abstract: The Digital Services Act (DSA) introduces harmonized rules for content
moderation and platform governance in the European Union, mandating robust
compliance mechanisms, particularly for very large online platforms and search
engines. This study examined compliance with DSA requirements, focusing on
Instagram as a case study. We develop and apply a multi-level consistency
framework to evaluate DSA compliance. Our findings contribute to the broader
discussion on empirically-based regulation, providing insight into how
researchers, regulators, auditors and platforms can better utilize DSA
mechanisms to improve reporting and enforcement quality and accountability.
This work underscores that consistency can help detect potential compliance
failures. It also demonstrates that platforms should be evaluated as part of an
interconnected ecosystem rather than through isolated processes, which is
crucial for effective compliance evaluation under the DSA.

</details>
