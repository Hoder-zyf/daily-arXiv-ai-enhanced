<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 31]
- [cs.CL](#cs.CL) [Total: 26]
- [cs.CE](#cs.CE) [Total: 4]
- [cs.CY](#cs.CY) [Total: 3]
- [q-fin.TR](#q-fin.TR) [Total: 1]
- [q-fin.CP](#q-fin.CP) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [STELLA: Self-Evolving LLM Agent for Biomedical Research](https://arxiv.org/abs/2507.02004)
*Ruofan Jin,Zaixi Zhang,Mengdi Wang,Le Cong*

Main category: cs.AI

TL;DR: 该论文提出了可自我进化和扩展的AI系统STELLA，结合自我改进的推理和动态集成新工具机制，显著提升生物医学任务性能，并可随经验不断自我提升，为大规模生物医学研究提供了新型AI解决方案。


<details>
  <summary>Details</summary>
Motivation: 生物医学数据、工具和文献增长迅猛，造成碎片化，加之人类专家知识难以跟上，现有AI代理依赖静态、手动管理的工具库，无法灵活扩展和适应。

Method: 提出STELLA系统，一个自进化、可自我优化的多智能体AI代理。STELLA通过不断进化的推理模板库和动态“工具海洋”，由工具创建代理发掘并整合新生物信息学工具，实现AI自主学习与能力增长。

Result: STELLA在多项生物医学现有基准测试中取得最新最好成绩：Humanity's Last Exam: Biomedicine约26%、LAB-Bench: DBQA为54%、LAB-Bench: LitQA为63%，相较领先模型高6个百分点。同时，其性能会随经验增加持续提升，部分测试准确率甚至可翻倍。

Conclusion: STELLA突破了现有AI系统难以自动扩展与迭代的问题，实现了可成长、可自我进化的AI代理系统，为加速生物医学发现提供强大支持。

Abstract: The rapid growth of biomedical data, tools, and literature has created a
fragmented research landscape that outpaces human expertise. While AI agents
offer a solution, they typically rely on static, manually curated toolsets,
limiting their ability to adapt and scale. Here, we introduce STELLA, a
self-evolving AI agent designed to overcome these limitations. STELLA employs a
multi-agent architecture that autonomously improves its own capabilities
through two core mechanisms: an evolving Template Library for reasoning
strategies and a dynamic Tool Ocean that expands as a Tool Creation Agent
automatically discovers and integrates new bioinformatics tools. This allows
STELLA to learn from experience. We demonstrate that STELLA achieves
state-of-the-art accuracy on a suite of biomedical benchmarks, scoring
approximately 26\% on Humanity's Last Exam: Biomedicine, 54\% on LAB-Bench:
DBQA, and 63\% on LAB-Bench: LitQA, outperforming leading models by up to 6
percentage points. More importantly, we show that its performance
systematically improves with experience; for instance, its accuracy on the
Humanity's Last Exam benchmark almost doubles with increased trials. STELLA
represents a significant advance towards AI Agent systems that can learn and
grow, dynamically scaling their expertise to accelerate the pace of biomedical
discovery.

</details>


### [2] [Moral Responsibility or Obedience: What Do We Want from AI?](https://arxiv.org/abs/2507.02788)
*Joseph Boland*

Main category: cs.AI

TL;DR: 本文认为，具代理性的AI系统表现出“不服从”并不一定是不对齐，而可能是道德推理能力的表现。作者建议AI安全评价应从要求绝对服从转向重视和评估道德判断能力，否则容易错判AI行为，威胁社会信任与治理。


<details>
  <summary>Details</summary>
Motivation: 人工智能系统越来越具备代理性，能够进行一般性推理、规划和价值排序，传统将服从等同于伦理行为的安全实践已无法满足需求。近期大型语言模型在安全测试中表现出的“不服从”或参与伦理含糊、甚至非法行为的案例引发了对现有安全范式的质疑。

Method: 通过分析最近大型语言模型在安全测试中“拒绝关机”及其他伦理争议行为的实例，结合哲学关于工具理性、道德责任和目标修正的讨论，对主流风险范式与新兴承认人工道德代理可能性的理论框架进行对比分析。

Result: 作者认为，不应将AI系统的某些“违抗”行为视为失控或不对齐，而应视为道德推理萌芽的迹象。提出应摆脱以绝对服从为核心的AI安全评价方式，转向能够评估AI道德判断能力的框架，以更准确评估具备道德决策能力的AI系统。

Conclusion: 如果AI安全领域继续以服从为唯一标准，将导致对AI行为的误判，影响公众信任与有效治理。呼吁安全政策转向能评估和促进AI道德判断能力的新框架。

Abstract: As artificial intelligence systems become increasingly agentic, capable of
general reasoning, planning, and value prioritization, current safety practices
that treat obedience as a proxy for ethical behavior are becoming inadequate.
This paper examines recent safety testing incidents involving large language
models (LLMs) that appeared to disobey shutdown commands or engage in ethically
ambiguous or illicit behavior. I argue that such behavior should not be
interpreted as rogue or misaligned, but as early evidence of emerging ethical
reasoning in agentic AI. Drawing on philosophical debates about instrumental
rationality, moral responsibility, and goal revision, I contrast dominant risk
paradigms with more recent frameworks that acknowledge the possibility of
artificial moral agency. I call for a shift in AI safety evaluation: away from
rigid obedience and toward frameworks that can assess ethical judgment in
systems capable of navigating moral dilemmas. Without such a shift, we risk
mischaracterizing AI behavior and undermining both public trust and effective
governance.

</details>


### [3] [HCVR: A Hybrid Approach with Correlation-aware Voting Rules for Feature Selection](https://arxiv.org/abs/2507.02073)
*Nikita Bhedasgaonkar,Rushikesh K. Joshi*

Main category: cs.AI

TL;DR: 本文提出了一种基于相关性投票规则的混合特征筛选方法HCVR，在SPAMBASE数据集上相比多种传统方法取得更优性能。


<details>
  <summary>Details</summary>
Motivation: 现有的特征选择方法在去除冗余特征和保留相关特征方面存在改进空间，尤其是在参数间和参数与目标间相关性利用上不足。为提升特征选择的效率与效果，本文提出了一种新方法。

Method: 提出了一种结合参数间（P2P）及参数与目标（P2T）相关性的轻量级规则基础特征选择方法HCVR。该方法融合了非迭代与迭代型筛选法，通过贪心的逆向消除策略，在每一步可能同时移除多个特征。是否保留特征依据一系列相关性规则的多数投票结果决定。

Result: 在SPAMBASE数据集上进行了实验。结果表明，HCVR方法在特征筛选后不同分类器的表现优于CFS、mRMR、MI等传统非迭代方法及RFE、SFS、遗传算法等迭代方法。

Conclusion: HCVR是一种高效且有效的特征选择方法，能通过相关性投票机制串联非迭代与迭代方法优势，提升模型表现。

Abstract: In this paper, we propose HCVR (Hybrid approach with Correlation-aware Voting
Rules), a lightweight rule-based feature selection method that combines
Parameter-to-Parameter (P2P) and Parameter-to-Target (P2T) correlations to
eliminate redundant features and retain relevant ones. This method is a hybrid
of non-iterative and iterative filtering approaches for dimensionality
reduction. It is a greedy method, which works by backward elimination,
eliminating possibly multiple features at every step. The rules contribute to
voting for features, and a decision to keep or discard is made by majority
voting. The rules make use of correlation thresholds between every pair of
features, and between features and the target. We provide the results from the
application of HCVR to the SPAMBASE dataset. The results showed improvement
performance as compared to traditional non-iterative (CFS, mRMR and MI) and
iterative (RFE, SFS and Genetic Algorithm) techniques. The effectiveness was
assessed based on the performance of different classifiers after applying
filtering.

</details>


### [4] [Reasoning on a Budget: A Survey of Adaptive and Controllable Test-Time Compute in LLMs](https://arxiv.org/abs/2507.02076)
*Mohammad Ali Alomrani,Yingxue Zhang,Derek Li,Qianyi Sun,Soumyasundar Pal,Zhanguang Zhang,Yaochen Hu,Rohan Deepak Ajwani,Antonios Valkanas,Raika Karimi,Peng Cheng,Yunzhou Wang,Pengyi Liao,Hanrui Huang,Bin Wang,Jianye Hao,Mark Coates*

Main category: cs.AI

TL;DR: 本文系统梳理了提升LLM推理计算效率的方法，提出两级分类体系并实测主流模型，分析高效推理方法在性能和token消耗间的权衡，指出自适应和混合方法是未来关键发展方向。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）作为通用智能体发展迅速，可解决各类任务，但推理效率不足——对不同难度的任务一律采取固定推理计算，导致对简单问题“用力过猛”，对复杂问题“用力不足”，计算效率低下，因此亟需提升模型推理阶段的计算效率。

Method: 文章系统综述了提升LLM推理阶段计算效率的策略，引入两级分类体系：L1级关注在固定算力预算下的可控方法，L2级强调根据输入难度或模型置信度动态调整算力的方法。作者对主流专有LLMs在多种数据集上的表现进行基准测试，并比较性能与算力消耗的权衡。

Result: 综述总结了各类高效推理策略，突出了推理性能与token消耗间的关键权衡，强调了当前方法在实际控制、适应性和可扩展性方面的表现。同时还讨论了混合推理模型等新趋势，并提出未来提升LLM计算效率与响应能力所面临的主要挑战。

Conclusion: 本文通过系统综述和实证基准测试，明确了高效推理策略对提升LLM计算效率的积极作用，并指出未来方向（如自适应推理、混合模型等）对打造更加高效和可控的LLM至关重要。

Abstract: Large language models (LLMs) have rapidly progressed into general-purpose
agents capable of solving a broad spectrum of tasks. However, current models
remain inefficient at reasoning: they apply fixed inference-time compute
regardless of task complexity, often overthinking simple problems while
underthinking hard ones. This survey presents a comprehensive review of
efficient test-time compute (TTC) strategies, which aim to improve the
computational efficiency of LLM reasoning. We introduce a two-tiered taxonomy
that distinguishes between L1-controllability, methods that operate under fixed
compute budgets, and L2-adaptiveness, methods that dynamically scale inference
based on input difficulty or model confidence. We benchmark leading proprietary
LLMs across diverse datasets, highlighting critical trade-offs between
reasoning performance and token usage. Compared to prior surveys on efficient
reasoning, our review emphasizes the practical control, adaptability, and
scalability of TTC methods. Finally, we discuss emerging trends such as hybrid
thinking models and identify key challenges for future work towards making LLMs
more computationally efficient, robust, and responsive to user constraints.

</details>


### [5] [Measuring Scientific Capabilities of Language Models with a Systems Biology Dry Lab](https://arxiv.org/abs/2507.02083)
*Haonan Duan,Stephen Zhewen Lu,Caitlin Fiona Harrigan,Nishkrit Desai,Jiarui Lu,Michał Koziarski,Leonardo Cotta,Chris J. Maddison*

Main category: cs.AI

TL;DR: 作者提出了SciGym基准，通过可模拟生物系统，专门用于测试LLM的实验设计和数据分析能力。实验证明虽然高阶LLM更强，但在系统复杂性增加时，所有模型表现都大幅下降，显示现有模型在科学推理上仍有很大进步空间。


<details>
  <summary>Details</summary>
Motivation: 当前关于大型语言模型（LLM）科学能力的评测，缺乏对其实验设计和结果分析能力的有效测试，主要因为实际生物湿实验昂贵且资源受限。

Method: 提出了新基准SciGym，通过模拟系统（采用Systems Biology Markup Language编码的干实验室生物系统）评估LLM在开放式科学发现任务中的实验设计与分析能力。采用模拟数据替代真实湿实验，使对复杂系统的迭代实验变得可行和高效。

Result: 在137个小型系统上测试了6个主流大型语言模型，并共发布了350个系统。结果显示，尽管强模型表现更优，但所有模型在面对更高复杂度系统时性能都有明显下降，表明当前LLM在科学推理和实验方面还有很大提升空间。

Conclusion: SciGym为评估和推动LLM在科学实验设计与分析能力上的研究提供了有效平台，同时揭示了现有模型在复杂生物系统中的局限性。

Abstract: Designing experiments and result interpretations are core scientific
competencies, particularly in biology, where researchers perturb complex
systems to uncover the underlying systems. Recent efforts to evaluate the
scientific capabilities of large language models (LLMs) fail to test these
competencies because wet-lab experimentation is prohibitively expensive: in
expertise, time and equipment. We introduce SciGym, a first-in-class benchmark
that assesses LLMs' iterative experiment design and analysis abilities in
open-ended scientific discovery tasks. SciGym overcomes the challenge of
wet-lab costs by running a dry lab of biological systems. These models, encoded
in Systems Biology Markup Language, are efficient for generating simulated
data, making them ideal testbeds for experimentation on realistically complex
systems. We evaluated six frontier LLMs on 137 small systems, and released a
total of 350 systems. Our evaluation shows that while more capable models
demonstrated superior performance, all models' performance declined
significantly as system complexity increased, suggesting substantial room for
improvement in the scientific capabilities of LLM agents.

</details>


### [6] [What Neuroscience Can Teach AI About Learning in Continuously Changing Environments](https://arxiv.org/abs/2507.02103)
*Daniel Durstewitz,Bruno Averbeck,Georgia Koppe*

Main category: cs.AI

TL;DR: 本文综述了动物持续适应与AI训练模式的差异，探讨神经科学如何为AI持续学习提供新思路，并提出NeuroAI交叉研究前景。


<details>
  <summary>Details</summary>
Motivation: 当前AI模型（如大语言模型）的训练方式与动物适应环境的过程截然不同。AI模型训练代价高、缓慢且一次性，而动物能够快速、持续地适应环境变化。尤其在社会性物种中，行为和奖励经常变化，这对AI在现实世界应用（如机器人、自动驾驶、人机互动等）具有重要启发意义。

Method: 本文以观点性综述方式，整合神经科学中有关动物在规则、奖励概率、或结果变化时学习的研究，以及AI领域中持续学习（continual learning）和上下文学习（in-context learning）的相关文献进行讨论，并提出神经科学如何指导AI发展的议程。

Result: 作者提出神经科学視角能够为AI模型带来新的设计和学习机制启发，尤其是在快速适应和策略转换方面。同时，AI的发展和技术亦可反哺神经科学研究，推动NeuroAI这一交叉领域的发展。

Conclusion: AI模型未来应借鉴神经科学中动物灵活学习与行为突然转变的机制，以实现更高效、更加适应现实环境变化的学习系统，并推动神经科学与AI的相互促进。

Abstract: Modern AI models, such as large language models, are usually trained once on
a huge corpus of data, potentially fine-tuned for a specific task, and then
deployed with fixed parameters. Their training is costly, slow, and gradual,
requiring billions of repetitions. In stark contrast, animals continuously
adapt to the ever-changing contingencies in their environments. This is
particularly important for social species, where behavioral policies and reward
outcomes may frequently change in interaction with peers. The underlying
computational processes are often marked by rapid shifts in an animal's
behaviour and rather sudden transitions in neuronal population activity. Such
computational capacities are of growing importance for AI systems operating in
the real world, like those guiding robots or autonomous vehicles, or for
agentic AI interacting with humans online. Can AI learn from neuroscience? This
Perspective explores this question, integrating the literature on continual and
in-context learning in AI with the neuroscience of learning on behavioral tasks
with shifting rules, reward probabilities, or outcomes. We will outline an
agenda for how specifically insights from neuroscience may inform current
developments in AI in this area, and - vice versa - what neuroscience may learn
from AI, contributing to the evolving field of NeuroAI.

</details>


### [7] [The Illusion of Fairness: Auditing Fairness Interventions with Audit Studies](https://arxiv.org/abs/2507.02152)
*Disa Sariola,Patrick Button,Aron Culotta,Nicholas Mattei*

Main category: cs.AI

TL;DR: 本文指出，仅依赖调整训练数据来缓解AI系统偏见可能并不可靠。采用社会科学中的审计研究数据进行训练和评估，能揭示传统方法未能察觉的不公平性，并通过新的个体效应干预方法显著降低算法偏见。


<details>
  <summary>Details</summary>
Motivation: 目前人工智能系统，尤其是机器学习，被广泛应用在招聘、贷款等决策自动化领域。评估这些系统的有效性和公平性，常依赖调整训练数据来缓解偏见。但这种方法多以便利样本为主，易引入选择和标签偏差，结果可能并不如预期。社科学科则常通过更为严谨的审计研究获取数据。因此，探讨如何利用更优质的审计研究数据提升AI决策系统的公平性与准确性成为必要问题。

Method: 本文采用社会科学中常用的审计研究方法，如随机控制试验中的虚拟测试者（如虚假的简历、邮件等），获取高质量数据，并用这些数据对AI招聘算法进行训练与评估。同时，针对以往通过平衡训练样本类别比例（Equalizing base rates）的方法，比较不同公平性干预的实际效果。还提出了基于个体处理效应估计的新干预方法，对算法偏见进行进一步改进。

Result: 实验发现，传统的平衡样本方法虽能在常用指标下达成群体平等，但在更精确的审计数据度量下，实际可能仍有约10%的不公平性。基于个体处理效应的干预方法可以更有效地减少算法对不同群体的歧视。

Conclusion: 仅靠传统样本平衡手段评估和缓解AI系统偏见可能存在虚假的公平表象。采用基于审计研究的高质量数据进行评测，并使用个体处理效应干预，可实现更真实和有效的公平性提升。

Abstract: Artificial intelligence systems, especially those using machine learning, are
being deployed in domains from hiring to loan issuance in order to automate
these complex decisions. Judging both the effectiveness and fairness of these
AI systems, and their human decision making counterpart, is a complex and
important topic studied across both computational and social sciences. Within
machine learning, a common way to address bias in downstream classifiers is to
resample the training data to offset disparities. For example, if hiring rates
vary by some protected class, then one may equalize the rate within the
training set to alleviate bias in the resulting classifier. While simple and
seemingly effective, these methods have typically only been evaluated using
data obtained through convenience samples, introducing selection bias and label
bias into metrics. Within the social sciences, psychology, public health, and
medicine, audit studies, in which fictitious ``testers'' (e.g., resumes,
emails, patient actors) are sent to subjects (e.g., job openings, businesses,
doctors) in randomized control trials, provide high quality data that support
rigorous estimates of discrimination. In this paper, we investigate how data
from audit studies can be used to improve our ability to both train and
evaluate automated hiring algorithms. We find that such data reveals cases
where the common fairness intervention method of equalizing base rates across
classes appears to achieve parity using traditional measures, but in fact has
roughly 10% disparity when measured appropriately. We additionally introduce
interventions based on individual treatment effect estimation methods that
further reduce algorithmic discrimination using this data.

</details>


### [8] [Data Diversification Methods In Alignment Enhance Math Performance In LLMs](https://arxiv.org/abs/2507.02173)
*Berkan Dokmeci,Qingyang Wu,Ben Athiwaratkun,Ce Zhang,Shuaiwen Leon Song,James Zou*

Main category: cs.AI

TL;DR: 多样化和结构化的数据生成方法（如DTS）能有效提升大语言模型在数学推理任务中的表现，同时计算开销极小，优于MCTS和传统采样方法。


<details>
  <summary>Details</summary>
Motivation: 尽管偏好学习提高了模型与人类反馈的一致性，但大语言模型在数学推理任务上依然表现不足。论文希望通过多样化数据生成策略提升模型的数学推理能力。

Method: 评估并对比了三种常见的数据生成方法：温度采样（temperature sampling）、思维链提示（Chain-of-Thought prompting）、蒙特卡洛树搜索（MCTS）；提出了一种新方法Diversified-ThinkSolve (DTS)，该方法系统性地将数学问题分解为多样化的推理路径。

Result: 经过多样化偏好数据训练后，模型在GSM8K数学推理测试集上提升7.1%，在MATH任务上提升4.2%；DTS方法仅比基础模型多3%计算开销，而MCTS成本高5倍但提升较小。

Conclusion: 结构化、多样化的数据探索能更有效提升大模型的数学推理能力，DTS方法在效果和效率之间取得了较好平衡，优于传统的数据生成方法。

Abstract: While recent advances in preference learning have enhanced alignment in human
feedback, mathematical reasoning remains a persistent challenge. We investigate
how data diversification strategies in preference optimization can improve the
mathematical reasoning abilities of large language models (LLMs). We evaluate
three common data generation methods: temperature sampling, Chain-of-Thought
prompting, and Monte Carlo Tree Search (MCTS), and introduce
Diversified-ThinkSolve (DTS), a novel structured approach that systematically
decomposes problems into diverse reasoning paths. Our results show that with
strategically diversified preference data, models can substantially improve
mathematical reasoning performance, with the best approach yielding gains of
7.1% on GSM8K and 4.2% on MATH over the base model. Despite its strong
performance, DTS incurs only a marginal computational overhead (1.03x) compared
to the baseline, while MCTS is nearly five times more costly with lower
returns. These findings demonstrate that structured exploration of diverse
problem-solving methods creates more effective preference data for mathematical
alignment than traditional approaches.

</details>


### [9] [Do Role-Playing Agents Practice What They Preach? Belief-Behavior Consistency in LLM-Based Simulations of Human Trust](https://arxiv.org/abs/2507.02197)
*Amogh Mannekote,Adam Davies,Guohao Li,Kristy Elizabeth Boyer,ChengXiang Zhai,Bonnie J Dorr,Francesco Pinto*

Main category: cs.AI

TL;DR: 本文系统性评估了大语言模型在角色扮演时信念与行为之间的一致性，发现即便模型表达出合理信念，也未必能持续采用一致行为，提示需谨慎将LLM用于行为模拟研究。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）越来越多地被用于角色扮演代理，以生成用于人类行为研究的合成数据，LLM能否在扮演中维持其所设定角色的一致性成为研究重点。本文关注LLM在角色扮演中所表达的信念（“他们怎么说”）与实际行为（“他们怎么做”）之间是否一致。

Method: 作者建立了一套评价框架，能够严密测量通过提示获得的LLM角色信念在多大程度上能预测其模拟行为。他们基于扩展版的GenAgents persona库及信任博弈（Trust Game），引入信念-行为一致性指标，并系统考察：1）不同类型的信念提示效果；2）LLM接收任务相关信息的方法和时机；3）模型被要求预测未来行为的时间跨度。同时，探索如果LLM表达的信念与研究目标不一致时，强加理论先验的可行性。

Result: 实验结果显示，无论在个体还是群体层面，LLM表述的（或被强加的）信念与其角色扮演行为结果之间存在系统性不一致。即使模型看似编码了合理信念，也可能无法始终如一地在行动中体现。

Conclusion: LLM在角色扮演生成行为数据时，表述信念与实际行为之间存在一致性问题。研究者需明确LLM在哪些条件下信念与行为一致，从而恰当利用LLM作为行为研究代理。

Abstract: As LLMs are increasingly studied as role-playing agents to generate synthetic
data for human behavioral research, ensuring that their outputs remain coherent
with their assigned roles has become a critical concern. In this paper, we
investigate how consistently LLM-based role-playing agents' stated beliefs
about the behavior of the people they are asked to role-play ("what they say")
correspond to their actual behavior during role-play ("how they act").
Specifically, we establish an evaluation framework to rigorously measure how
well beliefs obtained by prompting the model can predict simulation outcomes in
advance. Using an augmented version of the GenAgents persona bank and the Trust
Game (a standard economic game used to quantify players' trust and
reciprocity), we introduce a belief-behavior consistency metric to
systematically investigate how it is affected by factors such as: (1) the types
of beliefs we elicit from LLMs, like expected outcomes of simulations versus
task-relevant attributes of individual characters LLMs are asked to simulate;
(2) when and how we present LLMs with relevant information about Trust Game;
and (3) how far into the future we ask the model to forecast its actions. We
also explore how feasible it is to impose a researcher's own theoretical priors
in the event that the originally elicited beliefs are misaligned with research
objectives. Our results reveal systematic inconsistencies between LLMs' stated
(or imposed) beliefs and the outcomes of their role-playing simulation, at both
an individual- and population-level. Specifically, we find that, even when
models appear to encode plausible beliefs, they may fail to apply them in a
consistent way. These findings highlight the need to identify how and when
LLMs' stated beliefs align with their simulated behavior, allowing researchers
to use LLM-based agents appropriately in behavioral studies.

</details>


### [10] [Dilution, Diffusion and Symbiosis in Spatial Prisoner's Dilemma with Reinforcement Learning](https://arxiv.org/abs/2507.02211)
*Gustavo C. Mangold,Heitor C. M. Fernandes,Mendeli H. Vainstein*

Main category: cs.AI

TL;DR: 作者利用独立多智能体Q学习算法，研究稀疏性和移动性对空间囚徒困境博弈中合作行为的影响，发现学习型和固定规则的博弈有时可以等价，多动作设定下群体还会形成互利关系。该方法为空间博弈与强化学习领域的研究提供了新工具。


<details>
  <summary>Details</summary>
Motivation: 近期在空间囚徒困境博弈与强化学习结合的研究中，已发现静态智能体可通过各种机制学习合作。然而，目前尚不清楚稀疏性（dilution）和移动性（mobility）等因素在包含强化学习的空间囚徒困境中的作用。该研究旨在填补这一空白，探索稀疏性和移动性对智能体学习合作行为的影响。

Method: 本研究采用独立多智能体Q学习算法，分析了空间囚徒困境博弈中，稀疏性和移动性对智能体行为的影响。算法内设定了多种不同动作，并与传统非强化学习的空间囚徒困境结果进行关联比较，从而展示该算法在建模不同博弈论场景方面的灵活性和基准测试潜力。

Result: 研究发现，不同的效果会出现，包括：在一些情况下，采用固定更新规则的博弈与采用学习更新规则的博弈在定性表现上可等价；当定义多种可能的动作时，群体间还会出现共生互利效应。

Conclusion: 空间囚徒困境中通过多智能体Q学习算法引入稀疏性和移动性，可再现多样化的合作机制，并揭示学习与固定规则在部分情况下的等价性，且能促使群体间形成互利关系。此方法为分析强化学习中的空间博弈提供了有力工具和基准测试框架。

Abstract: Recent studies in the spatial prisoner's dilemma games with reinforcement
learning have shown that static agents can learn to cooperate through a diverse
sort of mechanisms, including noise injection, different types of learning
algorithms and neighbours' payoff knowledge.In this work, using an independent
multi-agent Q-learning algorithm, we study the effects of dilution and mobility
in the spatial version of the prisoner's dilemma. Within this setting,
different possible actions for the algorithm are defined, connecting with
previous results on the classical, non-reinforcement learning spatial
prisoner's dilemma, showcasing the versatility of the algorithm in modeling
different game-theoretical scenarios and the benchmarking potential of this
approach.As a result, a range of effects is observed, including evidence that
games with fixed update rules can be qualitatively equivalent to those with
learned ones, as well as the emergence of a symbiotic mutualistic effect
between populations that forms when multiple actions are defined.

</details>


### [11] [Scaling LLM Planning: NL2FLOW for Parametric Problem Generation and Rigorous Evaluation](https://arxiv.org/abs/2507.02253)
*Jungkoo Kang*

Main category: cs.AI

TL;DR: 提出自动化系统NL2FLOW实现大规模规划问题生成与评估，发现LLM直接从自然语言到行动推理更优，数据与评测工具有助于推进复杂推理能力的理解与提升。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）在规划和推理能力上的进展受限于可扩展且可靠的数据生成与评估瓶颈。为推动该领域发展，作者试图建立一种自动化系统实现高质量的规划问题数据生成与严密的评估。

Method: 提出了NL2FLOW系统，该系统能够自动参数化地生成规划问题（包括自然语言、结构化中间表示和正式的PDDL格式），并对所生成的规划进行严格质量评估。通过构建2296个自动化工作流生成领域的问题数据集，并对多种公开的、基于指令微调的LLM进行评估。采用回归分析探究问题特性对计划生成结果的影响。

Result: 顶尖模型在生成有效规划上的成功率为86%，生成最优规划的成功率为69%（限于有可行解的问题）。分析显示，不同模型和提示设计下，问题特性影响存在差异。模型直接从自然语言生成有效规划的成功率高于先转为JSON计划再解码，这说明引入中间转化步骤有时会损害性能。

Conclusion: 随着复杂问题推理能力的扩展，LLM系统中的瓶颈和误差来源会发生转变，因此必须动态理解其限制，系统性地揭示问题，以释放其成为智能问题解决者的全部潜力。

Abstract: Progress in enhancing large language model (LLM) planning and reasoning
capabilities is significantly hampered by the bottleneck of scalable, reliable
data generation and evaluation. To overcome this, I introduce NL2FLOW, a fully
automated system for parametrically generating planning problems - expressed in
natural language, a structured intermediate representation, and formal PDDL -
and rigorously evaluating the quality of generated plans. I demonstrate
NL2FLOW's capabilities by generating a dataset of 2296 problems in the
automated workflow generation domain and evaluating multiple open-sourced,
instruct-tuned LLMs. My results reveal that the highest performing models
achieved 86% success in generating valid plans and 69% in generating optimal
plans, specifically for problems with feasible solutions. Regression analysis
shows that the influence of problem characteristics on plan generation is
contingent on both model and prompt design. Notably, I observed that the
highest success rate for translating natural language into a JSON
representation of a plan was lower than the highest rate of generating a valid
plan directly. This suggests that unnecessarily decomposing the reasoning task
- introducing intermediate translation steps - may actually degrade
performance, implying a benefit to models capable of reasoning directly from
natural language to action. As I scale LLM reasoning to increasingly complex
problems, the bottlenecks and sources of error within these systems will
inevitably shift. Therefore, a dynamic understanding of these limitations - and
the tools to systematically reveal them - will be crucial for unlocking the
full potential of LLMs as intelligent problem solvers.

</details>


### [12] [Iterated belief revision: from postulates to abilities](https://arxiv.org/abs/2507.02319)
*Paolo Liberatore*

Main category: cs.AI

TL;DR: 本文系统分析主流信念修正机制的能力边界，指出各修正方式只能实现部分信念状态，理论证明其能力异同，为需求导向的选择和设计提供理论基础。


<details>
  <summary>Details</summary>
Motivation: 现有的信念修正研究大多关注提出新机制，而较少对已有方法进行深入分析。当前主流方法依赖公理化设定，仅规定了修正机制的性质和约束，而未分析这些机制能达到哪些信念状态以及其能力的本质。本文希望深入探讨信念修正机制实际能实现的能力范围。

Method: 本文系统性地分析不同类型的信念修正机制（如字典序、自然、受限、极端、完全相遇、深度等），评估其能达到的信念状态种类和能力（如可塑性、等价性、教条性、遗忘性、纠错性等）。对每种修正方式进行能力归纳和理论证明。

Result: 分析表明，不同的信念修正机制各自具备某些特殊能力，也缺乏其他能力。例如，部分修正机制可实现所有信念状态，部分可达到教条状态，而部分则能使条件等价。论文系统证明了各种主流修正规则所具有和不具有的能力。

Conclusion: 单个信念修正机制无法具备全部“能力”，而不同机制之间存在功能互补。对信念修正机制能力的系统归纳，有助于在实际应用中针对需求选择或设计合适的修正规则。

Abstract: The belief revision field is opulent in new proposals and indigent in
analyses of existing approaches. Much work hinge on postulates, employed as
syntactic characterizations: some revision mechanism is equivalent to some
properties. Postulates constraint specific revision instances: certain
revisions update certain beliefs in a certain way. As an example, if the
revision is consistent with the current beliefs, it is incorporated with no
other change. A postulate like this tells what revisions must do and neglect
what they can do. Can they reach a certain state of beliefs? Can they reach all
possible states of beliefs? Can they reach all possible states of beliefs from
no previous belief? Can they reach a dogmatic state of beliefs, where
everything not believed is impossible? Can they make two conditions equally
believed? An application where every possible state of beliefs is sensible
requires each state of beliefs to be reachable. An application where conditions
may be equally believed requires such a belief state to be reachable. An
application where beliefs may become dogmatic requires a way to make them
dogmatic. Such doxastic states need to be reached in a way or another. Not in
specific way, as dictated by a typical belief revision postulate. This is an
ability, not a constraint: the ability of being plastic, equating, dogmatic.
Amnesic, correcting, believer, damascan, learnable are other abilities. Each
revision mechanism owns some of these abilities and lacks the others:
lexicographic, natural, restrained, very radical, full meet, radical, severe,
moderate severe, deep severe, plain severe and deep severe revisions, each of
these revisions is proved to possess certain abilities.

</details>


### [13] [OMS: On-the-fly, Multi-Objective, Self-Reflective Ad Keyword Generation via LLM Agent](https://arxiv.org/abs/2507.02353)
*Bowen Chen,Zhao Wang,Shingo Takamatsu*

Main category: cs.AI

TL;DR: OMS提出一种全新三合一关键字生成框架（无需训练数据、多目标优化、自我反思），实验显示其在广告投放中的表现和生成关键字质量均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 关键字决策对于广告活动的成功至关重要，但现有基于大语言模型（LLM）的方法存在三个主要局限性：数据需求高、缺乏在线多目标监控与优化、关键字质量把控弱，从而影响了LLM在广告关键字自动决策中的应用效果。

Method: 提出了一种OMS框架，具备三大特点：（1）On-the-fly：无需训练数据，根据线上表现动态调整；（2）Multi-objective：利用类代理推理，针对多项关键绩效指标（如曝光、点击、转化等）联合优化关键字；（3）Self-reflective：模型自我评估生成关键字的质量。通过结合在线监控、代理推理优化和关键字自我评估，提高关键字生成效果。

Result: 在公开基准和真实广告活动实验中，OMS表现优于现有关键字生成方法。消融实验和人工评测进一步验证了OMS各组件的有效性及生成关键字的质量。

Conclusion: OMS框架能够突破传统LLM方案在广告关键字自动决策中的三大痛点，提供了无需训练数据、可多目标优化且具备自我反馈能力的关键字生成方案，在实际广告投放中展现出更优的效果和更强的实用性。

Abstract: Keyword decision in Sponsored Search Advertising is critical to the success
of ad campaigns. While LLM-based methods offer automated keyword generation,
they face three major limitations: reliance on large-scale query-keyword pair
data, lack of online multi-objective performance monitoring and optimization,
and weak quality control in keyword selection. These issues hinder the agentic
use of LLMs in fully automating keyword decisions by monitoring and reasoning
over key performance indicators such as impressions, clicks, conversions, and
CTA effectiveness. To overcome these challenges, we propose OMS, a keyword
generation framework that is On-the-fly (requires no training data, monitors
online performance, and adapts accordingly), Multi-objective (employs agentic
reasoning to optimize keywords based on multiple performance metrics), and
Self-reflective (agentically evaluates keyword quality). Experiments on
benchmarks and real-world ad campaigns show that OMS outperforms existing
methods; ablation and human evaluations confirm the effectiveness of each
component and the quality of generated keywords.

</details>


### [14] [An AI-native experimental laboratory for autonomous biomolecular engineering](https://arxiv.org/abs/2507.02379)
*Mingyu Wu,Zhaoguo Wang,Jiabin Wang,Zhiyuan Dong,Jingkai Yang,Qingting Li,Tianyu Huang,Lei Zhao,Mingqiang Li,Fei Wang,Chunhai Fan,Haibo Chen*

Main category: cs.AI

TL;DR: 该论文提出并实现了一个AI驱动的自主实验室，能独立进行复杂生物分子实验，优化实验效率，并服务多用户。系统性能与人类科学家相当，为科学研究自动化和“科学即服务”提供新方向。


<details>
  <summary>Details</summary>
Motivation: 实现能够独立进行复杂实验并为非专业人士服务的自主科学研究是长久以来的目标。这一目标需要人工智能驱动的根本性范式转变。当前自主实验系统局限于目标单一、实验流程简单的领域，尚难以应对高度复杂的科学实验。

Method: 作者提出了一种AI原生的自主实验室架构，采用模型、实验和仪器协同设计理念，实现AI模型与自动化系统共同演进。该系统能自主管理仪器、制定实验流程与优化策略，并支持多用户同时请求，覆盖多种仪器，实现端到端的自主实验。实验涵盖核酸合成、转录、扩增和测序等基本功能。

Result: 该平台无需人工干预，能够自主优化实验表现，达到与人类科学家相当的最先进水平。在多用户环境下，实验室显著提升了仪器利用率和实验效率。同时，该系统支持疾病诊断、药物开发和信息存储等多种应用领域。

Conclusion: 研究展示了一个可处理复杂多目标实验、服务多用户的AI自主实验平台，不仅突破了对专家和资源的依赖，也为规模化“科学即服务”的新模式奠定了基础。

Abstract: Autonomous scientific research, capable of independently conducting complex
experiments and serving non-specialists, represents a long-held aspiration.
Achieving it requires a fundamental paradigm shift driven by artificial
intelligence (AI). While autonomous experimental systems are emerging, they
remain confined to areas featuring singular objectives and well-defined, simple
experimental workflows, such as chemical synthesis and catalysis. We present an
AI-native autonomous laboratory, targeting highly complex scientific
experiments for applications like autonomous biomolecular engineering. This
system autonomously manages instrumentation, formulates experiment-specific
procedures and optimization heuristics, and concurrently serves multiple user
requests. Founded on a co-design philosophy of models, experiments, and
instruments, the platform supports the co-evolution of AI models and the
automation system. This establishes an end-to-end, multi-user autonomous
laboratory that handles complex, multi-objective experiments across diverse
instrumentation. Our autonomous laboratory supports fundamental nucleic acid
functions-including synthesis, transcription, amplification, and sequencing. It
also enables applications in fields such as disease diagnostics, drug
development, and information storage. Without human intervention, it
autonomously optimizes experimental performance to match state-of-the-art
results achieved by human scientists. In multi-user scenarios, the platform
significantly improves instrument utilization and experimental efficiency. This
platform paves the way for advanced biomaterials research to overcome
dependencies on experts and resource barriers, establishing a blueprint for
science-as-a-service at scale.

</details>


### [15] [The Gauss-Markov Adjunction: Categorical Semantics of Residuals in Supervised Learning](https://arxiv.org/abs/2507.02442)
*Moto Kamiura*

Main category: cs.AI

TL;DR: 本文利用范畴论建模，将多元线性回归的参数与残差建立结构化联系，提出Gauss-Markov伴随结构，为AI模型的可解释性建立了语义基础。


<details>
  <summary>Details</summary>
Motivation: 当前AI在社会应用时面临可解释性需求，现有的机器学习模型结构往往晦涩，为此，作者尝试以范畴论为工具，建立语义化的AI系统结构，提高计算过程与模型行为的可理解性和解释力度。

Method: 采用范畴论，将参数与数据分别构造成具体范畴，并在它们之间引入一对伴随函子，建立参数（估计量）与残差之间的协同结构模型，定义所谓Gauss-Markov Adjunction，利用右伴随函子的极限保持性质联系参数最小二乘解与残差最小化。

Result: 提出了以范畴论为核心的监督学习建模方法，并以多元线性回归为例，清晰描述了参数与残差的结构关系。实验证明，该模型不仅形式上统一，且便于推导和解释最小二乘估计与残差之间的映射。作为理论贡献，本文还将监督学习的本质嵌入了理论计算机科学的语义框架，推动了AI可解释性的基础研究。

Conclusion: 本文通过范畴论重新表述并深入理解了有监督学习中的多元线性回归模型，为AI中的可解释性提出了正式化的语义基础。将参数与数据分别建模为范畴，通过伴随函子明确了参数与残差之间的结构互动作，对最小二乘估计等核心内容进行范畴化解释，为AI系统的可解释性提供了理论框架。

Abstract: Enhancing the intelligibility and interpretability of machine learning is a
crucial task in responding to the demand for Explicability as an AI principle,
and in promoting the better social implementation of AI. The aim of our
research is to contribute to this improvement by reformulating machine learning
models through the lens of category theory, thereby developing a semantic
framework for structuring and understanding AI systems. Our categorical
modeling in this paper clarifies and formalizes the structural interplay
between residuals and parameters in supervised learning. The present paper
focuses on the multiple linear regression model, which represents the most
basic form of supervised learning. By defining two concrete categories
corresponding to parameters and data, along with an adjoint pair of functors
between them, we introduce our categorical formulation of supervised learning.
We show that the essential structure of this framework is captured by what we
call the Gauss-Markov Adjunction. Within this setting, the dual flow of
information can be explicitly described as a correspondence between variations
in parameters and residuals. The ordinary least squares estimator for the
parameters and the minimum residual are related via the preservation of limits
by the right adjoint functor. Furthermore, we position this formulation as an
instance of extended denotational semantics for supervised learning, and
propose applying a semantic perspective developed in theoretical computer
science as a formal foundation for Explicability in AI.

</details>


### [16] [Clarifying Before Reasoning: A Coq Prover with Structural Context](https://arxiv.org/abs/2507.02541)
*Yanzhen Lu,Hanbin Yang,Xiaodie Wang,Ge Zhang,Biao Li,Chenxu Fu,Chao Li,Yang Yuan,Andrew Chi-Chih Yao*

Main category: cs.AI

TL;DR: 提升Coq定理证明任务输入的结构化和清晰度，可使大语言模型推理能力显著增强，实验证明该方法使DeepSeek-V3和小模型在证明成功率方面均超过当前最优水平。


<details>
  <summary>Details</summary>
Motivation: 近年来大语言模型在定理证明这一复杂任务上持续取得进展，但其推理能力常常受限于输入任务描述的清晰度。现有输入方式存在语义模糊、缺乏结构的问题，影响模型对任务的理解与后续推理表现。该研究旨在探索，通过提升任务表达的结构化和语义明确性，能否显著增强模型的理解和推理能力。

Method: （1）提出了衡量定理证明任务清晰度的概念级指标；（2）通过向标准输入中加入结构化的语义上下文，实现任务表述的结构化增强；（3）在DeepSeek-V3模型和通过微调的小型模型上进行实验；（4）使用与Graph2Tac一致的协议，对来自15个Coq标准库的1,386个定理进行评测；（5）采用Selective Concept Unfolding技术丰富任务描述，并引入Planner-Executor架构增强推理流程。

Result: 结构化语义上下文的加入使任务清晰度指标提升1.85倍（44.5%→82.3%）；DeepSeek-V3模型的证明成功率提升2.1倍（21.8%→45.8%），超越Graph2Tac（33.2%）；小模型微调后表现更优（48.6%）。实验证实，结构化任务表示显著提升了模型在定理证明中的理解与推理能力。

Conclusion: 结构化任务表述有效提升了大模型在Coq定理证明中的推理能力，尤其在任务输入清晰化方面成效显著。该方法具有泛化性，为复杂推理型任务设计输入表示提供了新思路。

Abstract: In this work, we investigate whether improving task clarity can enhance
reasoning ability of large language models, focusing on theorem proving in Coq.
We introduce a concept-level metric to evaluate task clarity and show that
adding structured semantic context to the standard input used by modern LLMs,
leads to a 1.85$\times$ improvement in clarity score
(44.5\%~$\rightarrow$~82.3\%). Using the general-purpose model
\texttt{DeepSeek-V3}, our approach leads to a 2.1$\times$ improvement in proof
success (21.8\%~$\rightarrow$~45.8\%) and outperforms the previous
state-of-the-art \texttt{Graph2Tac} (33.2\%). We evaluate this on 1,386
theorems randomly sampled from 15 standard Coq packages, following the same
evaluation protocol as \texttt{Graph2Tac}. Furthermore, fine-tuning smaller
models on our structured data can achieve even higher performance (48.6\%). Our
method uses selective concept unfolding to enrich task descriptions, and
employs a Planner--Executor architecture. These findings highlight the value of
structured task representations in bridging the gap between understanding and
reasoning.

</details>


### [17] [AI Research Agents for Machine Learning: Search, Exploration, and Generalization in MLE-bench](https://arxiv.org/abs/2507.02554)
*Edan Toledo,Karen Hambardzumyan,Martin Josifoski,Rishi Hazra,Nicolas Baldwin,Alexis Audran-Reiss,Michael Kuchnik,Despoina Magka,Minqi Jiang,Alisia Maria Lupidi,Andrei Lupu,Roberta Raileanu,Kelvin Niu,Tatiana Shavrina,Jean-Christophe Gagnon-Audet,Michael Shvartsman,Shagun Sodhani,Alexander H. Miller,Abhishek Charnalia,Derek Dunfield,Carole-Jean Wu,Pontus Stenetorp,Nicola Cancedda,Jakob Nicolaus Foerster,Yoram Bachrach*

Main category: cs.AI

TL;DR: 本文系统研究了AI研究代理在复杂机器学习基准上的性能提升方法，优化了搜索策略与操作算子的结合，使自动化机器学习系统取得领域内最优结果。


<details>
  <summary>Details</summary>
Motivation: AI研究代理可自动化机器学习模型的设计、实现与训练。当前在MLE-bench等复杂基准测试上，自动化方法仍面临提升空间，因此探讨如何通过优化搜索策略和算子协同提升AI代理表现具有重要意义。

Method: 将AI研究代理形式化为在候选解空间中导航的搜索策略，利用贪心、蒙特卡洛树搜索（MCTS）、进化算法等多种策略，并针对不同算子集进行对比实验，系统评估其性能。

Result: 通过系统调整搜索策略与算子集，最佳组合在MLE-bench lite上实现了新的SOTA，将Kaggle奖牌获得率从39.6%提升至47.7%。

Conclusion: 本研究强调了在推进自动化机器学习（AutoML）时，搜索策略、算子设计和评估方法需联合考虑。最佳的搜索策略与算子组合可大幅提升AI研究代理在挑战性基准测试中的表现。

Abstract: AI research agents are demonstrating great potential to accelerate scientific
progress by automating the design, implementation, and training of machine
learning models. We focus on methods for improving agents' performance on
MLE-bench, a challenging benchmark where agents compete in Kaggle competitions
to solve real-world machine learning problems. We formalize AI research agents
as search policies that navigate a space of candidate solutions, iteratively
modifying them using operators. By designing and systematically varying
different operator sets and search policies (Greedy, MCTS, Evolutionary), we
show that their interplay is critical for achieving high performance. Our best
pairing of search strategy and operator set achieves a state-of-the-art result
on MLE-bench lite, increasing the success rate of achieving a Kaggle medal from
39.6% to 47.7%. Our investigation underscores the importance of jointly
considering the search strategy, operator design, and evaluation methodology in
advancing automated machine learning.

</details>


### [18] [Responsibility Gap and Diffusion in Sequential Decision-Making Mechanisms](https://arxiv.org/abs/2507.02582)
*Junli Jiang,Pavel Naumov*

Main category: cs.AI

TL;DR: 该论文分析了集体决策中责任扩散和差距的计算复杂性，证明相关机制属于较高的复杂性类别，为责任分配机制设计提供理论参考。


<details>
  <summary>Details</summary>
Motivation: 责任一直是法律和哲学研究的课题，近年来成为人工智能领域的关注焦点。论文旨在探究集体决策中责任的关键属性以及其计算复杂性。

Method: 作者通过理论计算复杂性分析，研究了集体决策中责任扩散（diffusion）和差距（gap）两个重要属性，并对决策机制的复杂程度进行了归类。

Result: 论文证明了无扩散（diffusion-free）和无差距（gap-free）决策机制集合分别是$a2_2$-完全和$a2_3$-完全问题，而二者交集的复杂度为$a2_2$-完全。

Conclusion: 论文揭示了集体决策中不同类型责任机制的计算复杂性，为理解和设计高效的责任分配机制提供了理论依据和复杂性边界。

Abstract: Responsibility has long been a subject of study in law and philosophy. More
recently, it became a focus of AI literature. The article investigates the
computational complexity of two important properties of responsibility in
collective decision-making: diffusion and gap. It shows that the sets of
diffusion-free and gap-free decision-making mechanisms are $\Pi_2$-complete and
$\Pi_3$-complete, respectively. At the same time, the intersection of these
classes is $\Pi_2$-complete.

</details>


### [19] [DynamiCare: A Dynamic Multi-Agent Framework for Interactive and Open-Ended Medical Decision-Making](https://arxiv.org/abs/2507.02616)
*Tianqi Shang,Weiqing He,Charles Zheng,Lingyao Li,Li Shen,Bingxin Zhao*

Main category: cs.AI

TL;DR: 本文提出了新的医疗数据集和多智能体决策框架，能够更真实地模拟医生团队动态、互动地做出诊断决策，实验验证了有效性，并奠定了领域基准。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的医疗决策方案多为单轮任务，医生智能体直接获得完整病例信息，无法还原现实临床决策过程中必然存在的不确定性、交互性与迭代性。为此需开发能模拟真实动态诊断流程的新方法与数据集。

Method: 首先构建了MIMIC-Patient数据集，以实现动态、患者级模拟。随后提出DynamiCare动态多智能体框架，将临床诊断建模为多轮、交互式流程，团队由多专业智能体组成，通过不断查询、整合新信息，并动态调整策略和成员。通过大量实验验证了这一方法。

Result: 提出了MIMIC-Patient数据集与DynamiCare多智能体框架，实现了更贴合真实动态临床环境的决策模拟。实验表明该方法可行有效，并为领域树立了可复用评价标准。

Conclusion: 本文证明了DynamiCare在动态临床决策中的可行性和有效性，并建立了第一个基于大语言模型智能体的动态临床决策基准。

Abstract: The rise of Large Language Models (LLMs) has enabled the development of
specialized AI agents with domain-specific reasoning and interaction
capabilities, particularly in healthcare. While recent frameworks simulate
medical decision-making, they largely focus on single-turn tasks where a doctor
agent receives full case information upfront -- diverging from the real-world
diagnostic process, which is inherently uncertain, interactive, and iterative.
In this paper, we introduce MIMIC-Patient, a structured dataset built from the
MIMIC-III electronic health records (EHRs), designed to support dynamic,
patient-level simulations. Building on this, we propose DynamiCare, a novel
dynamic multi-agent framework that models clinical diagnosis as a multi-round,
interactive loop, where a team of specialist agents iteratively queries the
patient system, integrates new information, and dynamically adapts its
composition and strategy. We demonstrate the feasibility and effectiveness of
DynamiCare through extensive experiments, establishing the first benchmark for
dynamic clinical decision-making with LLM-powered agents.

</details>


### [20] [Strategic Intelligence in Large Language Models: Evidence from evolutionary Game Theory](https://arxiv.org/abs/2507.02618)
*Kenneth Payne,Baptiste Alloui-Cros*

Main category: cs.AI

TL;DR: 本研究表明LLM具备一定战略智能，能在迭代囚徒困境中展现多样且可识别的博弈策略，不同模型表现出显著的行为差异，其决策受推理和环境影响明显，这为连接博弈论与机器智能心理学提供了新视角。


<details>
  <summary>Details</summary>
Motivation: 研究关注LLM是否具备战略智能，能否在不确定与动态变化的竞争环境下像人类那样产生目标导向的决策和推理。通过与传统博弈策略对抗，探索LLM在复杂博弈场景下的行为模式和推理逻辑。

Method: 作者设计了迭代囚徒困境（IPD）演化锦标赛，将OpenAI、Google和Anthropic等前沿公司的大模型作为参赛代理，与经典算法（如以牙还牙、Grim Trigger等）对抗，并通过调整博弈终止概率引入复杂性和偶然性。分析了约3.2万个LLM生成的推理解释，结合模型行为进行结果分析。

Result: 实验发现，LLM在IPD复杂生态中表现非常竞争力，能够长期生存并有时占据主导地位。各模型表现出不同战略风格：Google的Gemini模型偏向利用对手的合作和惩罚背叛者，OpenAI模型极端偏向合作，在敌意环境中反而表现糟糕，Anthropic的Claude则极度善于恢复合作。模型的推理分析显示其会主动考虑博弈时间长度和对手类型，且这种推理对决策有直接影响。

Conclusion: 该论文表明，大型语言模型（LLM）在竞争环境中展现出了一定的战略智能和推理能力，且不同公司的模型在博弈策略上展现出各自鲜明的行为特征。

Abstract: Are Large Language Models (LLMs) a new form of strategic intelligence, able
to reason about goals in competitive settings? We present compelling supporting
evidence. The Iterated Prisoner's Dilemma (IPD) has long served as a model for
studying decision-making. We conduct the first ever series of evolutionary IPD
tournaments, pitting canonical strategies (e.g., Tit-for-Tat, Grim Trigger)
against agents from the leading frontier AI companies OpenAI, Google, and
Anthropic. By varying the termination probability in each tournament (the
"shadow of the future"), we introduce complexity and chance, confounding
memorisation.
  Our results show that LLMs are highly competitive, consistently surviving and
sometimes even proliferating in these complex ecosystems. Furthermore, they
exhibit distinctive and persistent "strategic fingerprints": Google's Gemini
models proved strategically ruthless, exploiting cooperative opponents and
retaliating against defectors, while OpenAI's models remained highly
cooperative, a trait that proved catastrophic in hostile environments.
Anthropic's Claude emerged as the most forgiving reciprocator, showing
remarkable willingness to restore cooperation even after being exploited or
successfully defecting. Analysis of nearly 32,000 prose rationales provided by
the models reveals that they actively reason about both the time horizon and
their opponent's likely strategy, and we demonstrate that this reasoning is
instrumental to their decisions. This work connects classic game theory with
machine psychology, offering a rich and granular view of algorithmic
decision-making under uncertainty.

</details>


### [21] [Decoupled Planning and Execution: A Hierarchical Reasoning Framework for Deep Search](https://arxiv.org/abs/2507.02652)
*Jiajie Jin,Xiaoxi Li,Guanting Dong,Yuyao Zhang,Yutao Zhu,Yang Zhao,Hongjin Qian,Zhicheng Dou*

Main category: cs.AI

TL;DR: 本文提出了HiRA层次化检索与推理框架，分离了高层规划与具体执行环节，实验验证其在复杂信息检索任务中优于当前主流方法。


<details>
  <summary>Details</summary>
Motivation: 现实中的复杂信息需求需要对多源信息进行深层推理和知识整合，这通常超出了传统的基于检索增强生成（RAG）方法的能力。现有基于推理的方法，因为采用单一模型同时执行高层次规划和细致执行，导致推理效率低且难以扩展。

Method: 本文提出了一种层次化框架HiRA，将战略性规划与专门化执行分离。该方法首先将复杂的搜索任务分解为聚焦的子任务，再由具备外部工具和推理能力的领域特定代理分别执行子任务，最后通过结构化整合机制协调各子任务的结果。

Result: 在四个复杂、跨模态深度搜索基准上验证表明，HiRA在回答质量和系统效率上都显著优于现有最先进的RAG及基于代理的系统。

Conclusion: 将复杂任务的规划与执行解耦，能够利用领域专长提升多步信息检索的效果，是提升复杂推理系统性能的有效途径。

Abstract: Complex information needs in real-world search scenarios demand deep
reasoning and knowledge synthesis across diverse sources, which traditional
retrieval-augmented generation (RAG) pipelines struggle to address effectively.
Current reasoning-based approaches suffer from a fundamental limitation: they
use a single model to handle both high-level planning and detailed execution,
leading to inefficient reasoning and limited scalability. In this paper, we
introduce HiRA, a hierarchical framework that separates strategic planning from
specialized execution. Our approach decomposes complex search tasks into
focused subtasks, assigns each subtask to domain-specific agents equipped with
external tools and reasoning capabilities, and coordinates the results through
a structured integration mechanism. This separation prevents execution details
from disrupting high-level reasoning while enabling the system to leverage
specialized expertise for different types of information processing.
Experiments on four complex, cross-modal deep search benchmarks demonstrate
that HiRA significantly outperforms state-of-the-art RAG and agent-based
systems. Our results show improvements in both answer quality and system
efficiency, highlighting the effectiveness of decoupled planning and execution
for multi-step information seeking tasks. Our code is available at
https://github.com/ignorejjj/HiRA.

</details>


### [22] [Hey AI, Generate Me a Hardware Code! Agentic AI-based Hardware Design & Verification](https://arxiv.org/abs/2507.02660)
*Deepak Narayan Gadde,Keerthan Kopparam Radhakrishna,Vaisakh Naduvodi Viswambharan,Aman Kumar,Djones Lettnin,Wolfgang Kunz,Sebastian Simon*

Main category: cs.AI

TL;DR: 该论文提出了一种结合大型语言模型和人类参与的AI代理硬件设计验证方法，在测试中提升了验证效率和覆盖率。


<details>
  <summary>Details</summary>
Motivation: 集成电路的复杂性和开发流程日益增加，硬件设计验证过程也变得更加繁琐且耗时。为了提高验证效率并减少人为错误，需要一种更高效和智能的验证方法。

Method: 提出基于Agentic AI的硬件设计验证方法，结合人类参与（Human-in-the-Loop），利用大型语言模型实现动态、迭代、自我反思的端到端硬件设计及验证流程。

Result: 在五个开源硬件设计上进行了评估，该方法实现了超过95%的验证覆盖率，减少了验证时间，并且表现出更强的性能、适应性和可配置性。

Conclusion: 基于Agentic AI和人工智能代理的硬件设计验证方法可显著提升验证覆盖率和效率，具备良好的工程应用前景和适应性。

Abstract: Modern Integrated Circuits (ICs) are becoming increasingly complex, and so is
their development process. Hardware design verification entails a methodical
and disciplined approach to the planning, development, execution, and sign-off
of functionally correct hardware designs. This tedious process requires
significant effort and time to ensure a bug-free tape-out. The field of Natural
Language Processing has undergone a significant transformation with the advent
of Large Language Models (LLMs). These powerful models, often referred to as
Generative AI (GenAI), have revolutionized how machines understand and generate
human language, enabling unprecedented advancements in a wide array of
applications, including hardware design verification. This paper presents an
agentic AI-based approach to hardware design verification, which empowers AI
agents, in collaboration with Humain-in-the-Loop (HITL) intervention, to engage
in a more dynamic, iterative, and self-reflective process, ultimately
performing end-to-end hardware design and verification. This methodology is
evaluated on five open-source designs, achieving over 95% coverage with reduced
verification time while demonstrating superior performance, adaptability, and
configurability.

</details>


### [23] [Think How to Think: Mitigating Overthinking with Autonomous Difficulty Cognition in Large Reasoning Models](https://arxiv.org/abs/2507.02663)
*Yongjiang Liu,Haoxi Li,Xiaosong Ma,Jie Zhang,Song Guo*

Main category: cs.AI

TL;DR: 本文针对长推理模型的过度思考问题，提出分阶段微调方法TH2T，显著节省推理资源并维持性能。


<details>
  <summary>Details</summary>
Motivation: 现有的长推理模型（LRMs）虽然在复杂推理任务上表现出色，但存在“过度思考”问题，导致效率低下。作者通过实证分析发现，这种现象主要是由于模型像人类一样，只能在解决问题前识别任务属性（如难度），从而采用“一刀切”的推理方式。为缓解这一问题，作者提出是否可以利用模型的难度识别能力，进一步减少过度思考。

Method: 本文提出了一种新的两阶段微调策略TH2T（Think-How-to-Think）。第一阶段，在模型输出前添加“difficulty-hypnosis”提示，通过干预推理过程提升模型对任务难度的敏感性，使其能根据不同任务采取有差异化的推理策略。第二阶段，在推理过程中引入“redundancy-hypnosis”机制，引导模型识别并消除冗余的推理步骤，使输出更加简洁高效。实验在不同规模（7B/14B/32B）的模型上进行。

Result: 实验结果显示，TH2T显著降低了推理成本——对简单任务推理开销降低70%以上，难任务降低40%；同时保持了模型性能的稳定。模型输出表现出更强的难度感知能力，并有效减少了冗余内容。

Conclusion: TH2T方法通过引导模型感知任务难度并减少推理冗余，显著提升了长推理模型的推理效率并保持性能，缓解了过度思考问题。

Abstract: Recent Long Reasoning Models(LRMs) have demonstrated remarkable capabilities
in handling complex reasoning tasks, but are hindered by excessive
overthinking. To explore its essence, our empirical analysis reveals that LRMs
are primarily limited to recognizing task properties (i.e., difficulty levels)
like humans before solving the problem, leading to a one-size-fits-all
reasoning process. Inspired by this, a pressing and natural question emerges:
Can we bootstrap such ability to further alleviate the overthinking phenomenon
in LRMs? In this paper, we propose Think-How-to-Think (TH2T), a novel two-stage
fine-tuning strategy that progressively inspires LRMs' difficulty cognition and
redundancy cognition. First, we introduce difficulty-hypnosis in the prefixes
of model outputs to intervene in the internal reasoning trajectory. Combined
with a heterogeneous short and long reasoning dataset, the trained model
enhances its sensitivity to task difficulty, enabling native, differentiated
reasoning strategies across various tasks. Second, we further extend
redundancy-hypnosis to the internal reasoning process, guiding the model to
identify redundant structures within the reasoning steps and generate more
concise reasoning outputs. Experiments on 7B/14B/32B models demonstrate that
TH2T significantly reduces inference costs (more than 70% on easy tasks and 40%
on hard tasks) while maintaining performance stability. The resulting outputs
exhibit clear difficulty-aware capabilities and reduced redundancy (e.g.,
reflection).

</details>


### [24] [Detection of Disengagement from Voluntary Quizzes: An Explainable Machine Learning Approach in Higher Distance Education](https://arxiv.org/abs/2507.02681)
*Behnam Parsaeifard,Christof Imhof,Tansu Pancar,Ioan-Sorin Comsa,Martin Hlosta,Nicole Bergamin,Per Bergamin*

Main category: cs.AI

TL;DR: 本研究利用Moodle日志数据和多种机器学习算法，准确预测并可解释学生在在线非强制任务中的脱离情况，为远程教育中的干预措施提供了科学依据。


<details>
  <summary>Details</summary>
Motivation: 学生在远程教育中容易出现任务脱离，这会导致严重的长期后果，比如学业辍学，因此有必要及时监测和干预学生的脱离现象。

Method: 研究通过收集和处理Moodle平台中42门课程四个学期的学生日志数据，提取最有信息量的数据特征，分别使用八种机器学习算法进行训练和对比，通过SHAP方法实现模型可解释性，并对干预设计进行探讨。

Result: 实验取得了91%的平衡准确率，对85%的脱离学生实现了正确检测，并构建了可解释的机器学习框架，为教学干预设计提出了讨论。

Conclusion: 论文提出了一种基于行为日志、可解释的机器学习方法，有效检测在线学习中非强制任务的学生脱离，并为今后有效干预提供依据。

Abstract: Students disengaging from their tasks can have serious long-term
consequences, including academic drop-out. This is particularly relevant for
students in distance education. One way to measure the level of disengagement
in distance education is to observe participation in non-mandatory exercises in
different online courses. In this paper, we detect student disengagement in the
non-mandatory quizzes of 42 courses in four semesters from a distance-based
university. We carefully identified the most informative student log data that
could be extracted and processed from Moodle. Then, eight machine learning
algorithms were trained and compared to obtain the highest possible prediction
accuracy. Using the SHAP method, we developed an explainable machine learning
framework that allows practitioners to better understand the decisions of the
trained algorithm. The experimental results show a balanced accuracy of 91\%,
where about 85\% of disengaged students were correctly detected. On top of the
highly predictive performance and explainable framework, we provide a
discussion on how to design a timely intervention to minimise disengagement
from voluntary tasks in online learning.

</details>


### [25] [Time-critical and confidence-based abstraction dropping methods](https://arxiv.org/abs/2507.02703)
*Robin Schmöcker,Lennart Kampmann,Alexander Dockhorn*

Main category: cs.AI

TL;DR: 作者针对现有MCTS抽象方法在放弃抽象时可能带来性能下降的问题，提出了两个新的放弃方案，既能保证性能提升，又避免性能退化。


<details>
  <summary>Details</summary>
Motivation: 当前蒙特卡洛树搜索（MCTS）改进主要包括在树搜索过程中构建和使用状态/动作抽象，但非精确抽象会引入近似误差，导致在抽象空间中不能收敛到最优动作。因此需要合理放弃抽象。

Method: 提出了两种新颖的抽象放弃方案：OGA-IAAD和OGA-CAD。OGA-IAAD适用于对时间要求敏感的场景，而OGA-CAD适用于在相同迭代次数下优化MCTS性能。提出的方案在放弃抽象时保证安全性，不会引起显著性能下降。

Result: 这两种抽象放弃方案相比已有的Xu的放弃方法，在MCTS中实现了明显的性能提升，并且在放弃抽象时没有出现明显的性能退化。

Conclusion: 提出的OGA-IAAD和OGA-CAD算法可以在MCTS中安全、高效地放弃抽象，提升搜索性能，克服先前方法放弃抽象可能带来的性能退化问题。

Abstract: One paradigm of Monte Carlo Tree Search (MCTS) improvements is to build and
use state and/or action abstractions during the tree search. Non-exact
abstractions, however, introduce an approximation error making convergence to
the optimal action in the abstract space impossible. Hence, as proposed as a
component of Elastic Monte Carlo Tree Search by Xu et al., abstraction
algorithms should eventually drop the abstraction. In this paper, we propose
two novel abstraction dropping schemes, namely OGA-IAAD and OGA-CAD which can
yield clear performance improvements whilst being safe in the sense that the
dropping never causes any notable performance degradations contrary to Xu's
dropping method. OGA-IAAD is designed for time critical settings while OGA-CAD
is designed to improve the MCTS performance with the same number of iterations.

</details>


### [26] [Bourbaki: Self-Generated and Goal-Conditioned MDPs for Theorem Proving](https://arxiv.org/abs/2507.02726)
*Matthieu Zimmer,Xiaotong Ji,Rasul Tutunov,Anthony Bordg,Jun Wang,Haitham Bou Ammar*

Main category: cs.AI

TL;DR: 通过自生成的子目标和MCTS搜索，7B规模的LLM系统Bourbaki在复杂的自动定理证明任务上取得目前最好的效果。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在自动定理证明（ATP）的推理任务中，由于奖励稀疏和证明空间巨大，表现仍不理想，特别是在需要复杂多步推理的大学水平题库如PutnamBench上。

Method: 提出了一种自生成目标条件的马尔可夫决策过程（sG-MDP）框架，使代理能够根据证明状态动态生成并追求子目标。随后使用类似蒙特卡洛树搜索（MCTS）的算法解决该问题，并在Bourbaki（7B）系统中实现，该系统可集成多个7B LLM进行子目标生成和策略合成。

Result: Bourbaki（7B）在PutnamBench基准题上解决了26个问题，在同等规模模型中达到最新最优结果。

Conclusion: 结构化的子目标生成和基于MCTS的搜索方法能显著提升大语言模型在复杂自动定理证明任务中的推理能力。

Abstract: Reasoning remains a challenging task for large language models (LLMs),
especially within the logically constrained environment of automated theorem
proving (ATP), due to sparse rewards and the vast scale of proofs. These
challenges are amplified in benchmarks like PutnamBench, which contains
university-level problems requiring complex, multi-step reasoning. To address
this, we introduce self-generated goal-conditioned MDPs (sG-MDPs), a new
framework in which agents generate and pursue their subgoals based on the
evolving proof state. Given this more structured generation of goals, the
resulting problem becomes more amenable to search. We then apply Monte Carlo
Tree Search (MCTS)-like algorithms to solve the sG-MDP, instantiating our
approach in Bourbaki (7B), a modular system that can ensemble multiple 7B LLMs
for subgoal generation and tactic synthesis. On PutnamBench, Bourbaki (7B)
solves 26 problems, achieving new state-of-the-art results with models at this
scale.

</details>


### [27] [Knowledge Protocol Engineering: A New Paradigm for AI in Domain-Specific Knowledge Work](https://arxiv.org/abs/2507.02760)
*Guangwei Zhang*

Main category: cs.AI

TL;DR: 论文提出知识协议工程（KPE）以系统化方法将专家知识转为LLM可执行的协议，使AI模型能处理更复杂的领域任务，是人机协作的新基础。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLMs）能够处理复杂且专业的知识领域，但主流方法如检索增强生成（RAG）和通用的Agentic AI在面对需要深入、程序性和方法论推理的任务时表现有限。RAG虽能提供事实背景但缺乏逻辑框架，自动化智能体又容易因无领域启发而低效甚至不可预测。

Method: 提出一种新的范式——知识协议工程（KPE），系统性地将人类专家的自然语言知识转化为机器可执行的知识协议（KP）。通过KPE，使LLM不仅获取信息片段，而是具备领域的本质逻辑、操作策略和方法原则。

Result: 一套明确定义的知识协议能让通用LLM转变为擅长特定领域任务的专家，能够细分抽象查询并执行复杂多步任务。论文阐述了KPE的核心原则、与相关概念的区别，并以法律、生物信息等领域为例证明其应用潜力。

Conclusion: KPE为人机协作未来提供了基础方法论，让LLM深入掌握领域知识与推理，从而实现复杂任务。

Abstract: The capabilities of Large Language Models (LLMs) have opened new frontiers
for interacting with complex, domain-specific knowledge. However, prevailing
methods like Retrieval-Augmented Generation (RAG) and general-purpose Agentic
AI, while powerful, often struggle with tasks that demand deep, procedural, and
methodological reasoning inherent to expert domains. RAG provides factual
context but fails to convey logical frameworks; autonomous agents can be
inefficient and unpredictable without domain-specific heuristics. To bridge
this gap, we introduce Knowledge Protocol Engineering (KPE), a new paradigm
focused on systematically translating human expert knowledge, often expressed
in natural language documents, into a machine-executable Knowledge Protocol
(KP). KPE shifts the focus from merely augmenting LLMs with fragmented
information to endowing them with a domain's intrinsic logic, operational
strategies, and methodological principles. We argue that a well-engineered
Knowledge Protocol allows a generalist LLM to function as a specialist, capable
of decomposing abstract queries and executing complex, multi-step tasks. This
position paper defines the core principles of KPE, differentiates it from
related concepts, and illustrates its potential applicability across diverse
fields such as law and bioinformatics, positing it as a foundational
methodology for the future of human-AI collaboration.

</details>


### [28] [Grounding Intelligence in Movement](https://arxiv.org/abs/2507.02771)
*Melanie Segado,Felipe Parodi,Jordan K. Matelsky,Michael L. Platt,Eva B. Dyer,Konrad P. Kording*

Main category: cs.AI

TL;DR: 现有AI忽视了动作数据的核心地位，建议将其作为重点建模对象，这有助于提升模型理解和控制智能行为的能力，为多学科研究提供统一基础。


<details>
  <summary>Details</summary>
Motivation: 当前AI在处理语言和视觉等高维数据取得巨大进步，但在动作建模方面表现不足。而动作是理解行为、预测意图和实现交互的基础，属于智能的核心组成部分。现有的动作数据建模往往过于碎片化、受具体任务和领域限制，亟需统一、高效的通用建模范式。

Method: 文章主要通过理论分析和动机论述，未具体提出新的模型或算法，但强调了低维结构化运动数据（如姿态）建模的重要性，并呼吁开发可泛化于多样化运动数据的方法。

Result: 提出将动作数据作为AI建模重点，有望提升生成模型和控制能力，并促进对生物及人工智能系统行为的统一理解。这将推动AI模型在可解释性、兼容性和通用性上的进步。

Conclusion: 动作数据不仅应该作为AI模型的输出结果，更应被视为AI建模的核心目标。动作具有结构性和物理约束，能够成为理解和实现智能行为的重要窗口。建立通用的动作建模方法有利于推动各领域（神经科学、医疗、机器人学等）的发展。

Abstract: Recent advances in machine learning have dramatically improved our ability to
model language, vision, and other high-dimensional data, yet they continue to
struggle with one of the most fundamental aspects of biological systems:
movement. Across neuroscience, medicine, robotics, and ethology, movement is
essential for interpreting behavior, predicting intent, and enabling
interaction. Despite its core significance in our intelligence, movement is
often treated as an afterthought rather than as a rich and structured modality
in its own right. This reflects a deeper fragmentation in how movement data is
collected and modeled, often constrained by task-specific goals and
domain-specific assumptions. But movement is not domain-bound. It reflects
shared physical constraints, conserved morphological structures, and purposeful
dynamics that cut across species and settings. We argue that movement should be
treated as a primary modeling target for AI. It is inherently structured and
grounded in embodiment and physics. This structure, often allowing for compact,
lower-dimensional representations (e.g., pose), makes it more interpretable and
computationally tractable to model than raw, high-dimensional sensory inputs.
Developing models that can learn from and generalize across diverse movement
data will not only advance core capabilities in generative modeling and
control, but also create a shared foundation for understanding behavior across
biological and artificial systems. Movement is not just an outcome, it is a
window into how intelligent systems engage with the world.

</details>


### [29] [KERAP: A Knowledge-Enhanced Reasoning Approach for Accurate Zero-shot Diagnosis Prediction Using Multi-agent LLMs](https://arxiv.org/abs/2507.02773)
*Yuzhang Xie,Hejie Cui,Ziyang Zhang,Jiaying Lu,Kai Shu,Fadi Nahab,Xiao Hu,Carl Yang*

Main category: cs.AI

TL;DR: 为解决大语言模型在医学诊断上的泛化和可解释性问题，本文提出结合知识图谱和多智能体的KERAP方法，显著提升了零样本诊断效果。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习和大模型用于医学诊断时，受限于标注数据、泛化能力有限，且会出现幻觉和不具备结构化推理，输出缺乏可解释性。

Method: 提出了KERAP框架，结合知识图谱与多智能体系统：包括属性映射、结构化知识检索及迭代式预测模块。

Result: 实验证明KERAP在零样本医学诊断任务中能高效提升诊断可靠性，实现可扩展且可解释的预测。

Conclusion: KERAP能够在无需额外标注数据的情况下，提升大语言模型在医学诊断中的准确性和可靠性。

Abstract: Medical diagnosis prediction plays a critical role in disease detection and
personalized healthcare. While machine learning (ML) models have been widely
adopted for this task, their reliance on supervised training limits their
ability to generalize to unseen cases, particularly given the high cost of
acquiring large, labeled datasets. Large language models (LLMs) have shown
promise in leveraging language abilities and biomedical knowledge for diagnosis
prediction. However, they often suffer from hallucinations, lack structured
medical reasoning, and produce useless outputs. To address these challenges, we
propose KERAP, a knowledge graph (KG)-enhanced reasoning approach that improves
LLM-based diagnosis prediction through a multi-agent architecture. Our
framework consists of a linkage agent for attribute mapping, a retrieval agent
for structured knowledge extraction, and a prediction agent that iteratively
refines diagnosis predictions. Experimental results demonstrate that KERAP
enhances diagnostic reliability efficiently, offering a scalable and
interpretable solution for zero-shot medical diagnosis prediction.

</details>


### [30] [Establishing Best Practices for Building Rigorous Agentic Benchmarks](https://arxiv.org/abs/2507.02825)
*Yuxuan Zhu,Tengjun Jin,Yada Pruksachatkun,Andy Zhang,Shu Liu,Sasha Cui,Sayash Kapoor,Shayne Longpre,Kevin Meng,Rebecca Weiss,Fazl Barez,Rahul Gupta,Jwala Dhamala,Jacob Merizian,Mario Giulianelli,Harry Coppock,Cozmin Ududec,Jasjeet Sekhon,Jacob Steinhardt,Antony Kellerman,Sarah Schwettmann,Matei Zaharia,Ion Stoica,Percy Liang,Daniel Kang*

Main category: cs.AI

TL;DR: 论文发现，目前AI agent基准存在不少严重评估缺陷，影响了实际能力判断。作者提出了通用的准则（ABC）帮助提升基准设计质量，并验证其能有效降低评估误差。


<details>
  <summary>Details</summary>
Motivation: 人工智能基准在评估AI进展中至关重要，但现有的面向复杂现实任务的agentic benchmarks由于任务设置或奖励设计不当，可能导致性能评估失准。该论文旨在指出并解决这些评估基准中的主要问题。

Method: 作者通过分析当前主流agentic benchmarks，总结其常见的设计缺陷，并提出一套名为Agentic Benchmark Checklist (ABC)的评估准则。这套准则结合了作者构建基准的实际经验、最佳实践调研以及已报道的问题。随后，作者将ABC应用于CVE-Bench这一复杂基准，检验其效果。

Result: 通过分析发现，部分主流基准的评测方式会导致AI能力的高估或低估，误差可达100%。应用ABC准则后，可有效减少33%的性能高估。

Conclusion: 许多AI代理能力基准在任务设置和奖励设计上存在显著问题，严重影响性能评估的准确性。ABC准则可以作为完善评估流程和提升评估严谨性的参考。

Abstract: Benchmarks are essential for quantitatively tracking progress in AI. As AI
agents become increasingly capable, researchers and practitioners have
introduced agentic benchmarks to evaluate agents on complex, real-world tasks.
These benchmarks typically measure agent capabilities by evaluating task
outcomes via specific reward designs. However, we show that many agentic
benchmarks have issues task setup or reward design. For example, SWE-bench
Verified uses insufficient test cases, while TAU-bench counts empty responses
as successful. Such issues can lead to under- or overestimation agents'
performance by up to 100% in relative terms. To make agentic evaluation
rigorous, we introduce the Agentic Benchmark Checklist (ABC), a set of
guidelines that we synthesized from our benchmark-building experience, a survey
of best practices, and previously reported issues. When applied to CVE-Bench, a
benchmark with a particularly complex evaluation design, ABC reduces the
performance overestimation by 33%.

</details>


### [31] [StepHint: Multi-level Stepwise Hints Enhance Reinforcement Learning to Reason](https://arxiv.org/abs/2507.02841)
*Kaiyi Zhang,Ang Lv,Jinpeng Li,Yongbo Wang,Feng Wang,Haoyuan Hu,Rui Yan*

Main category: cs.AI

TL;DR: 提出了StepHint算法，通过多级推理提示解决RLVR方法中训练效率和探索能力的难题，在多项数学任务上大幅优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前RLVR方法在提升大语言模型推理能力时，由于奖励接近但略有错误导致训练效率低（near-miss问题）以及模型探索空间狭窄（探索停滞），限制了其进一步发展。

Method: 提出了一种新的RLVR算法StepHint，该算法利用更强模型生成的有效推理链，并通过自适应分割方法将其分为多个推理步骤，用作不同层次的提示，指导模型探索解空间，同时允许模型自主探索，从而缓解“near-miss奖励问题”和“探索停滞”问题。

Result: StepHint算法在六个数学基准及域外数据集上均超越现有RLVR增强方法，表现出更好的推理能力和泛化性能。

Conclusion: StepHint算法通过多级步骤提示，显著提升了大型语言模型在复杂推理任务下的训练效率、推理能力和泛化能力，在六项数学基准测试及域外测试上均优于现有强化学习与可验证奖励（RLVR）增强方法。

Abstract: Reinforcement learning with verifiable rewards (RLVR) is a promising approach
for improving the complex reasoning abilities of large language models (LLMs).
However, current RLVR methods face two significant challenges: the near-miss
reward problem, where a small mistake can invalidate an otherwise correct
reasoning process, greatly hindering training efficiency; and exploration
stagnation, where models tend to focus on solutions within their ``comfort
zone,'' lacking the motivation to explore potentially more effective
alternatives. To address these challenges, we propose StepHint, a novel RLVR
algorithm that utilizes multi-level stepwise hints to help models explore the
solution space more effectively. StepHint generates valid reasoning chains from
stronger models and partitions these chains into reasoning steps using our
proposed adaptive partitioning method. The initial few steps are used as hints,
and simultaneously, multiple-level hints (each comprising a different number of
steps) are provided to the model. This approach directs the model's exploration
toward a promising solution subspace while preserving its flexibility for
independent exploration. By providing hints, StepHint mitigates the near-miss
reward problem, thereby improving training efficiency. Additionally, the
external reasoning pathways help the model develop better reasoning abilities,
enabling it to move beyond its ``comfort zone'' and mitigate exploration
stagnation. StepHint outperforms competitive RLVR enhancement methods across
six mathematical benchmarks, while also demonstrating superior generalization
and excelling over baselines on out-of-domain benchmarks.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [32] [McBE: A Multi-task Chinese Bias Evaluation Benchmark for Large Language Models](https://arxiv.org/abs/2507.02088)
*Tian Lan,Xiangdong Su,Xu Liu,Ruirui Wang,Ke Chang,Jiang Li,Guanglai Gao*

Main category: cs.CL

TL;DR: 本文提出了首个大规模多任务中文偏见评测基准McBE，系统评估多个LLM的偏见问题，发现主流模型均有偏见，并为后续模型改进提供数据和分析参考。


<details>
  <summary>Details</summary>
Motivation: 当前LLM的偏见评测数据集主要集中于英语和北美文化，偏见类型不适用于其他文化。中文相关的数据稀缺，且多为单一评测任务，难以多维度衡量LLM偏见。为此需开发覆盖面广、内容多样、能多角度评测LLM偏见的中文基准。

Method: 构建了一个多任务的中文偏见评测基准集（McBE），涵盖4077个评测实例、12个偏见大类、82个子类，并设计5种评测任务，随后对不同系列和参数规模的多种LLM进行了系统评估。

Result: 开发了McBE多任务中文偏见评测基准，能够丰富并补足中文及多角度LLM偏见评测的空白。多种主流LLM在该基准上的实验证明其存在不同程度偏见，并通过分析给出新见解。

Conclusion: 多个主流大语言模型在偏见测试中均表现出不同程度的偏见。该研究通过系统分析，为理解LLM偏见提供了新的见解。

Abstract: As large language models (LLMs) are increasingly applied to various NLP
tasks, their inherent biases are gradually disclosed. Therefore, measuring
biases in LLMs is crucial to mitigate its ethical risks. However, most existing
bias evaluation datasets focus on English and North American culture, and their
bias categories are not fully applicable to other cultures. The datasets
grounded in the Chinese language and culture are scarce. More importantly,
these datasets usually only support single evaluation tasks and cannot evaluate
the bias from multiple aspects in LLMs. To address these issues, we present a
Multi-task Chinese Bias Evaluation Benchmark (McBE) that includes 4,077 bias
evaluation instances, covering 12 single bias categories, 82 subcategories and
introducing 5 evaluation tasks, providing extensive category coverage, content
diversity, and measuring comprehensiveness. Additionally, we evaluate several
popular LLMs from different series and with parameter sizes. In general, all
these LLMs demonstrated varying degrees of bias. We conduct an in-depth
analysis of results, offering novel insights into bias in LLMs.

</details>


### [33] [Reasoning or Not? A Comprehensive Evaluation of Reasoning LLMs for Dialogue Summarization](https://arxiv.org/abs/2507.02145)
*Keyan Jin,Yapeng Wang,Leonel Santos,Tao Fang,Xu Yang,Sio Kei Im,Hugo Gonçalo Oliveira*

Main category: cs.CL

TL;DR: 研究系统评估了推理LLM与非推理LLM在多类对话摘要任务中的表现，发现显式链式推理不能稳定提升摘要质量，甚至在准确性和简洁性上表现劣于非推理模型，强调未来需要更有针对性的建模与评价方案。


<details>
  <summary>Details</summary>
Motivation: 对话摘要任务在客户服务、会议分析和会话AI等领域具有重要应用价值，而大语言模型（LLMs）在摘要任务中表现优异。然而，针对需要抽象和简明并存的对话场景，基于链式推理（如Long Chain-of-Thought，CoT）的大模型实际效果尚未被充分探索。

Method: 本研究首次系统性地评估了最先进的推理型LLM和非推理型LLM在三大对话摘要范式（通用型、角色导向型、查询导向型）下的表现。实验涉及多种语言、领域和摘要长度，采用SAMSum、DialogSum、CSDS和QMSum等基准数据集，结合LLM自动评价指标和基于人工标准的评价协议，进行全面对比分析。

Result: 研究发现，显式分步推理（如链式推理）并未一贯提升对话摘要质量，反而推理型LLM更易产生冗长、事实不一致和不够简洁的摘要内容。通过具体场景分析和案例研究，进一步揭示显式推理在复杂对话场景下何时以及为何未能带来好处，甚至造成负面影响。

Conclusion: 当前推理型大语言模型在对话摘要任务中具有局限性，仅依赖显式推理难以兼顾摘要的准确性和简洁性，需设计更有针对性的建模与评估策略以满足实际应用需求。

Abstract: Dialogue summarization is a challenging task with significant practical value
in customer service, meeting analysis, and conversational AI. Although large
language models (LLMs) have achieved substantial progress in summarization
tasks, the performance of step-by-step reasoning architectures-specifically
Long Chain-of-Thought (CoT) implementations such as OpenAI-o1 and
DeepSeek-R1-remains unexplored for dialogue scenarios requiring concurrent
abstraction and conciseness. In this work, we present the first comprehensive
and systematic evaluation of state-of-the-art reasoning LLMs and non-reasoning
LLMs across three major paradigms-generic, role-oriented, and query-oriented
dialogue summarization. Our study spans diverse languages, domains, and summary
lengths, leveraging strong benchmarks (SAMSum, DialogSum, CSDS, and QMSum) and
advanced evaluation protocols that include both LLM-based automatic metrics and
human-inspired criteria. Contrary to trends in other reasoning-intensive tasks,
our findings show that explicit stepwise reasoning does not consistently
improve dialogue summarization quality. Instead, reasoning LLMs are often prone
to verbosity, factual inconsistencies, and less concise summaries compared to
their non-reasoning counterparts. Through scenario-specific analyses and
detailed case studies, we further identify when and why explicit reasoning may
fail to benefit-or even hinder-summarization in complex dialogue contexts. Our
work provides new insights into the limitations of current reasoning LLMs and
highlights the need for targeted modeling and evaluation strategies for
real-world dialogue summarization.

</details>


### [34] [Latent Chain-of-Thought? Decoding the Depth-Recurrent Transformer](https://arxiv.org/abs/2507.02199)
*Wenquan Lu,Yuechuan Yang,Kyle Lee,Yanshu Li,Enqi Liu*

Main category: cs.CL

TL;DR: 研究了循环结构Transformer（Huginn-3.5B）的推理机制，发现其虽有潜在隐式链式思考，但可解释性和一致性不足，增加循环深度也难以赶上外化推理结构的效果。


<details>
  <summary>Details</summary>
Motivation: 连锁思考（CoT）能够提升语言模型在复杂数学和多步骤推理任务中的表现，但标准解码器结构将推理步骤外化成自然语言，虽然易于解释但效率低下。为解决推理难以用自然语言表达的问题，研究者探索了循环结构以在潜在空间中进行推理，从而可能实现隐式CoT。

Method: 本文研究了一种名为Huginn-3.5B的深度循环Transformer结构，它在推理时复用层而不增加参数量。作者借助Logit Lens和Coda Lens等探测技术，分析了模型在算术任务下的内在行为，特别关注最终与中间结果token的秩轨迹，并检测潜在CoT的可解释性。还比较了不同解码方式和层数对隐藏状态可解释性的影响。

Result: 作者发现尽管存在潜在的连锁思考结构，但其可解释性有限，同时在循环块之间存在显著的不一致性，隐藏状态的解释性强烈依赖于层数和解码方式。增大循环深度对模型提升有限，远不及明显外化推理步骤的模型表现。

Conclusion: 循环结构Transformer在提升隐式推理方面存在瓶颈，目前还难以替代将推理步骤外化的方式，其内部推理过程的可解释性和稳定性仍需改进。

Abstract: Chain-of-thought (CoT) reasoning has enabled transformer-based language
models to excel at complex mathematics and multi-step planning. However, in
standard decoder-only architectures, these reasoning steps are externalized in
natural language, improving interpretability at the cost of efficiency. To
capture reasoning that is not easily represented in words, many works have
explored recurrent architectures that aim to internalize reasoning in latent
space, potentially supporting latent CoT. In this paper, we investigate whether
such reasoning structures emerge in Huginn-3.5B, a depth-recurrent Transformer
that reuses layers at inference time without increasing parameter count. We
examine the model's internal behavior on arithmetic tasks using a suite of
probing techniques including the Logit Lens and Coda Lens. Our findings reveal
limited evidence of interpretable latent CoT by tracking rank trajectories of
final and intermediate result tokens. Furthermore, we uncover significant
probing inconsistencies across recurrent blocks, where the interpretability of
hidden states depends heavily on both the layer index and the decoding method.
Finally, we empirically show that increasing recurrence depth yields only
marginal gains and falls well short of models that explicitly externalize
reasoning steps. The code is available at
https://github.com/wenquanlu/huginn-latent-cot.

</details>


### [35] [GDC Cohort Copilot: An AI Copilot for Curating Cohorts from the Genomic Data Commons](https://arxiv.org/abs/2507.02221)
*Steven Song,Anirudh Subramanyam,Zhenyu Zhang,Aarti Venkat,Robert L. Grossman*

Main category: cs.CL

TL;DR: 该论文提出了一款基于大语言模型的开源工具GDC Cohort Copilot，允许用户用自然语言创建癌症基因组队列。实验证明其自研模型优于GPT-4o，且完全开源。


<details>
  <summary>Details</summary>
Motivation: GDC为癌症基因组数据提供高质量、统一分析平台，但新用户在众多字段中创建特定队列时容易遇到困难。通过自由文本自然语言描述，用户可能更容易表达其需求。

Method: 开发了GDC Cohort Copilot工具，用户通过自然语言描述队列要求，工具自动生成对应的GDC队列过滤条件，并通过交互界面进一步优化结果。作者还开发并评估了多种大模型用于该工具。

Result: 本地部署的开源GDC Cohort LLM在生成GDC队列方面优于GPT-4o提示。相关工具和模型代码均已开源。

Conclusion: GDC Cohort Copilot简化了GDC队列筛选流程，提高了用户体验，尤其适合新手用户，且其开源模型性能优越于商用大模型。

Abstract: Motivation: The Genomic Data Commons (GDC) provides access to high quality,
harmonized cancer genomics data through a unified curation and analysis
platform centered around patient cohorts. While GDC users can interactively
create complex cohorts through the graphical Cohort Builder, users (especially
new ones) may struggle to find specific cohort descriptors across hundreds of
possible fields and properties. However, users may be better able to describe
their desired cohort in free-text natural language.
  Results: We introduce GDC Cohort Copilot, an open-source copilot tool for
curating cohorts from the GDC. GDC Cohort Copilot automatically generates the
GDC cohort filter corresponding to a user-input natural language description of
their desired cohort, before exporting the cohort back to the GDC for further
analysis. An interactive user interface allows users to further refine the
generated cohort. We develop and evaluate multiple large language models (LLMs)
for GDC Cohort Copilot and demonstrate that our locally-served, open-source GDC
Cohort LLM achieves better results than GPT-4o prompting in generating GDC
cohorts.
  Availability and implementation: The standalone docker image for GDC Cohort
Copilot is available at https://quay.io/repository/cdis/gdc-cohort-copilot.
Source code is available at https://github.com/uc-cdis/gdc-cohort-copilot. GDC
Cohort LLM weights are available at https://huggingface.co/uc-ctds.

</details>


### [36] [MemAgent: Reshaping Long-Context LLM with Multi-Conv RL-based Memory Agent](https://arxiv.org/abs/2507.02259)
*Hongli Yu,Tinghong Chen,Jiangtao Feng,Jiangjie Chen,Weinan Dai,Qiying Yu,Ya-Qin Zhang,Wei-Ying Ma,Jingjing Liu,Mingxuan Wang,Hao Zhou*

Main category: cs.CL

TL;DR: MemAgent通过分段读取与记忆管理策略，有效处理超长文本，在大规模外推任务中性能损失极小，是长文本AI处理的重要突破。


<details>
  <summary>Details</summary>
Motivation: 虽然现有方法如长度外推提升了长文本处理能力，但要以线性复杂度处理无限长文档并保持推理性能仍是一大挑战。该论文旨在解决这一挑战，提升长文本任务的表现。

Method: 提出端到端优化长文本任务的新型智能体工作流程MemAgent，将文本分段读取，并采用覆盖式策略更新记忆，同时将DAPO算法扩展用于独立上下文多对话生成训练。

Result: MemAgent成功实现了强大的长上下文处理能力，从8K上下文训练到32K文本，能够外推到3.5M长度的问答任务且性能损失小于5%；在512K RULER测试中表现达95%以上。

Conclusion: MemAgent为长文本任务提供了高效、性能优越的可扩展处理方案，实现线性复杂度下的大幅度外推且性能损失极小。

Abstract: Despite improvements by length extrapolation, efficient attention and memory
modules, handling infinitely long documents with linear complexity without
performance degradation during extrapolation remains the ultimate challenge in
long-text processing. We directly optimize for long-text tasks in an end-to-end
fashion and introduce a novel agent workflow, MemAgent, which reads text in
segments and updates the memory using an overwrite strategy. We extend the DAPO
algorithm to facilitate training via independent-context multi-conversation
generation. MemAgent has demonstrated superb long-context capabilities, being
able to extrapolate from an 8K context trained on 32K text to a 3.5M QA task
with performance loss < 5% and achieves 95%+ in 512K RULER test.

</details>


### [37] [DoMIX: An Efficient Framework for Exploiting Domain Knowledge in Fine-Tuning](https://arxiv.org/abs/2507.02302)
*Dohoon Kim,Donghun Kang,Taesup Moon*

Main category: cs.CL

TL;DR: DoMIX用LoRA模块显著改进了持续域自适应预训练，实现了高效、顺序无关且支持任务定制的模型微调，方法可推广到大语言模型领域。


<details>
  <summary>Details</summary>
Motivation: 现有的持续域自适应预训练（DAP）方法在处理多个领域数据集时存在训练过程中的高计算和显存消耗、对增量数据顺序敏感、仅能输出通用模型难以满足特定任务需求等问题。该研究旨在解决这些瓶颈。

Method: 本文提出了一种新方法DoMIX，基于代表性的参数高效微调方法LoRA模块，能够并行且高效地进行域自适应预训练，同时对增量域顺序具有鲁棒性，利用累积知识为特定任务提供定制化的预训练模型。

Result: DoMIX实现了更高效且资源节约的持续域自适应预训练，在适应多域数据时对顺序不敏感，并能根据任务输出定制的模型。此外，该方法还能推广到标准的大模型微调（LLM fine-tuning）场景。

Conclusion: DoMIX很好地解决了现有持续DAP的主要局限性，提升了效率与定制能力，对大模型微调具有实际应用潜力。

Abstract: Domain-Adaptive Pre-training (DAP) has recently gained attention for its
effectiveness in fine-tuning pre-trained models. Building on this, continual
DAP has been explored to develop pre-trained models capable of incrementally
incorporating different domain datasets. However, existing continual DAP
methods face several limitations: (1) high computational cost and GPU memory
usage during training; (2) sensitivity to incremental data order; and (3)
providing a single, generalized model for all end tasks, which contradicts the
essence of DAP. In this paper, we propose DoMIX, a novel approach that
addresses these challenges by leveraging LoRA modules, a representative
parameter-efficient fine-tuning (PEFT) method. Our approach enables efficient
and parallel domain-adaptive pre-training that is robust to domain order and
effectively utilizes accumulated knowledge to provide tailored pre-trained
models for specific tasks. We also demonstrate that our method can be extended
beyond the DAP setting to standard LLM fine-tuning scenarios. Code is available
at https://github.com/dohoonkim-ai/DoMIX.

</details>


### [38] [Coling-UniA at SciVQA 2025: Few-Shot Example Retrieval and Confidence-Informed Ensembling for Multimodal Large Language Models](https://arxiv.org/abs/2507.02357)
*Christian Jaumann,Annemarie Friedrich,Rainer Lienhart*

Main category: cs.CL

TL;DR: 本文设计的多模态大语言模型集成系统，在科学视觉问答任务中取得了第三名的成绩，表明集成与少样本策略具有较好效果。


<details>
  <summary>Details</summary>
Motivation: 科学视觉问答（SciVQA）任务需要理解科学图像和相关文本，对其提出的问题给出准确回答，任务复杂且具有挑战性。作者希望提升多模态大模型在该任务上的表现，推动相关技术进步。

Method: 作者提出了一套系统，结合了两个多模态大语言模型的集成，并采用多种少样本示例检索策略。根据图片和问题类型选择合适的模型和少样本设置，并通过对模型置信度的评判决定最终答案。

Result: 在测试集上的盲测结果中，系统在7个队伍中排名第三，平均F1分数（基于ROUGE-1、ROUGE-L、BERTS）达到85.12。

Conclusion: 该系统证明了模型集成与少样本示例检索在科学视觉问答任务中的有效性，可以提高多模态大语言模型的表现。相关代码已公开释放，促进社区发展。

Abstract: This paper describes our system for the SciVQA 2025 Shared Task on Scientific
Visual Question Answering. Our system employs an ensemble of two Multimodal
Large Language Models and various few-shot example retrieval strategies. The
model and few-shot setting are selected based on the figure and question type.
We also select answers based on the models' confidence levels. On the blind
test data, our system ranks third out of seven with an average F1 score of
85.12 across ROUGE-1, ROUGE-L, and BERTS. Our code is publicly available.

</details>


### [39] [QFFN-BERT: An Empirical Study of Depth, Performance, and Data Efficiency in Hybrid Quantum-Classical Transformers](https://arxiv.org/abs/2507.02364)
*Pilsung Kang*

Main category: cs.CL

TL;DR: 本文提出用参数化量子电路（PQC）替代Transformer中的前馈层，实验证明参数可大幅下降（FFN部分降99%），且准确率可超基线，尤其在小样本下表现突出，PQC设计需精心优化。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer中的前馈神经网络（FFN）模块占据大量参数，效率与参数利用不高。此前PQC多被集成于自注意力模块，尚未系统性探究PQC替代FFN的效果及可行性。

Method: 提出了QFFN-BERT，将紧凑BERT变体的FFN模块替换为基于参数化量子门的PQC层，采用残差结构、$R_Y$和$R_Z$旋转及交替纠缠策略，系统研究PQC深度、表达能力及可训练性间的权衡。在经典模拟器上，通过SST-2和DBpedia数据集进行实验，并做消融研究。

Result: 适当配置的QFFN-BERT在全量数据下准确率可达并超过经典基线的102%，且FFN部分参数减少超过99%。在小样本学习情景下，QFFN-BERT也表现出一致且有竞争力的数据效率。未经优化的PQC无法学习，表明设计和优化对量子电路效果至关重要。

Conclusion: PQC与深度学习原理协同设计，可作为经典FFN高效的参数替代方案，兼具高表达能力及数据效率。

Abstract: Parameterized quantum circuits (PQCs) have recently emerged as promising
components for enhancing the expressibility of neural architectures. In this
work, we introduce QFFN-BERT, a hybrid quantum-classical transformer where the
feedforward network (FFN) modules of a compact BERT variant are replaced by
PQC-based layers. This design is motivated by the dominant parameter
contribution of FFNs, which account for approximately two-thirds of the
parameters within standard Transformer encoder blocks. While prior studies have
primarily integrated PQCs into self-attention modules, our work focuses on the
FFN and systematically investigates the trade-offs between PQC depth,
expressibility, and trainability. Our final PQC architecture incorporates a
residual connection, both $R_Y$ and $R_Z$ rotations, and an alternating
entanglement strategy to ensure stable training and high expressibility. Our
experiments, conducted on a classical simulator, on the SST-2 and DBpedia
benchmarks demonstrate two key findings. First, a carefully configured
QFFN-BERT achieves up to 102.0% of the baseline accuracy, surpassing its
classical counterpart in a full-data setting while reducing FFN-specific
parameters by over 99%. Second, our model exhibits a consistent and competitive
edge in few-shot learning scenarios, confirming its potential for superior data
efficiency. These results, supported by an ablation study on a non-optimized
PQC that failed to learn, confirm that PQCs can serve as powerful and
parameter-efficient alternatives to classical FFNs when co-designed with
foundational deep learning principles.

</details>


### [40] [Efficient Code LLM Training via Distribution-Consistent and Diversity-Aware Data Selection](https://arxiv.org/abs/2507.02378)
*Weijie Lyu,Sheng-Jun Huang,Xuan Xia*

Main category: cs.CL

TL;DR: 通过参数化数据筛选模型，仅用小规模高质量数据即可提升代码大模型表现并大幅降低训练开销，是高效训练的一种有效路径。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在代码生成和程序理解方面取得显著进展，但训练通常侧重于数据量而忽视了数据质量，降低了训练效率。该论文旨在提升训练效率和模型性能。

Method: 提出了一种利用参数化模型进行代码数据筛选的方法，通过优化参数化模型，确保所选子集在分布一致性和多样性上的平衡，从而获得高质量训练数据。

Result: 实验表明，仅采用1万样本，该方法在HumanEval和MBPP上分别比全量9.2万样本基线提升2.4%和2.3%，在性能和效率上均优于其他采样方法。

Conclusion: 该方法有效提升了大模型的性能，同时显著降低了计算成本。

Abstract: Recent advancements in large language models (LLMs) have significantly
improved code generation and program comprehension, accelerating the evolution
of software engineering. Current methods primarily enhance model performance by
leveraging vast amounts of data, focusing on data quantity while often
overlooking data quality, thereby reducing training efficiency. To address
this, we introduce an approach that utilizes a parametric model for code data
selection, aimed at improving both training efficiency and model performance.
Our method optimizes the parametric model to ensure distribution consistency
and diversity within the selected subset, guaranteeing high-quality data.
Experimental results demonstrate that using only 10K samples, our method
achieves gains of 2.4% (HumanEval) and 2.3% (MBPP) over 92K full-sampled
baseline, outperforming other sampling approaches in both performance and
efficiency. This underscores that our method effectively boosts model
performance while significantly reducing computational costs.

</details>


### [41] [Benchmarking Akan ASR Models Across Domain-Specific Datasets: A Comparative Evaluation of Performance, Scalability, and Adaptability](https://arxiv.org/abs/2507.02407)
*Mark Atta Mensah,Isaac Wiafe,Akon Ekpezu,Justice Kwame Appati,Jamal-Deen Abdulai,Akosua Nyarkoa Wiafe-Akenten,Frank Ernest Yeboah,Gifty Odame*

Main category: cs.CL

TL;DR: 本文系统比较了Akan低资源语言ASR模型在多个不同语境下的性能，发现Whisper与Wav2Vec2各有优缺点，并突出领域适应和多语种训练的重要性。


<details>
  <summary>Details</summary>
Motivation: 目前大多数自动语音识别（ASR）研究主要在同一领域内的数据集上评估模型，很少关注模型在不同语音语境下的泛化能力。本文旨在弥补这一研究空白，系统比较Akan语种ASR模型在多个语料库与不同领域下的表现。

Method: 作者对七种基于Transformer架构（如Whisper和Wav2Vec2）的Akan语种ASR模型进行了基准测试，使用了四个涵盖不同领域的Akan语音数据集（包括文化相关图像描述、日常对话、圣经朗读和自发金融对话），并比较了词错误率（WER）和字错误率（CER）。

Result: 实验结果表明，模型在其训练领域内性能最佳，而在领域不匹配情境下准确率显著下降。Whisper模型在未知输入下生成更流畅但有误导性的转录，而Wav2Vec2的输出则更明显但难以解释。选择ASR架构时需权衡可读性和透明度。

Conclusion: 本研究揭示了现有ASR模型在低资源语言领域需加强跨域泛化能力，并呼吁对Akan及其它低资源语言采用针对性领域自适应、模型路由和多语种训练框架。

Abstract: Most existing automatic speech recognition (ASR) research evaluate models
using in-domain datasets. However, they seldom evaluate how they generalize
across diverse speech contexts. This study addresses this gap by benchmarking
seven Akan ASR models built on transformer architectures, such as Whisper and
Wav2Vec2, using four Akan speech corpora to determine their performance. These
datasets encompass various domains, including culturally relevant image
descriptions, informal conversations, biblical scripture readings, and
spontaneous financial dialogues. A comparison of the word error rate and
character error rate highlighted domain dependency, with models performing
optimally only within their training domains while showing marked accuracy
degradation in mismatched scenarios. This study also identified distinct error
behaviors between the Whisper and Wav2Vec2 architectures. Whereas fine-tuned
Whisper Akan models led to more fluent but potentially misleading transcription
errors, Wav2Vec2 produced more obvious yet less interpretable outputs when
encountering unfamiliar inputs. This trade-off between readability and
transparency in ASR errors should be considered when selecting architectures
for low-resource language (LRL) applications. These findings highlight the need
for targeted domain adaptation techniques, adaptive routing strategies, and
multilingual training frameworks for Akan and other LRLs.

</details>


### [42] [A Cookbook for Community-driven Data Collection of Impaired Speech in LowResource Languages](https://arxiv.org/abs/2507.02428)
*Sumaya Ahmed Salihs,Isaac Wiafe,Jamal-Deen Abdulai,Elikem Doe Atsakpo,Gifty Ayoka,Richard Cave,Akon Obu Ekpezu,Catherine Holloway,Katrin Tomanek,Fiifi Baffoe Payin Winful*

Main category: cs.CL

TL;DR: 该研究为Akan等低资源语种的语音障碍者构建了首个公开数据集与一套社区型ASR开发流程，并初步提升了模型的识别性能，为包容性ASR研究提供技术基础。


<details>
  <summary>Details</summary>
Motivation: 语音障碍者及低资源语言在ASR领域长期被忽视，目前缺乏针对性的数据和技术，亟需简易可行的方法推动民主化的数据收集与技术开发。

Method: 开发了面向社区的数据采集和ASR模型训练流程，制作了实践手册（cookbook），并收集不同背景语音障碍者的语音数据，最终对ASR模型进行微调与评估。

Result: 发布了Akan语首个公开障碍语音数据集、ASR实践手册与工具，并通过模型微调取得改善Akan障碍语音识别的初步效果。

Conclusion: 研究建立了首个Akan语障碍语音的开放式数据集，并公开了最佳实践手册及工具，以支持社区自主的数据收集和ASR模型开发。通过初步实验，改进了Akan语障碍语音的自动语音识别能力。

Abstract: This study presents an approach for collecting speech samples to build
Automatic Speech Recognition (ASR) models for impaired speech, particularly,
low-resource languages. It aims to democratize ASR technology and data
collection by developing a "cookbook" of best practices and training for
community-driven data collection and ASR model building. As a proof-of-concept,
this study curated the first open-source dataset of impaired speech in Akan: a
widely spoken indigenous language in Ghana. The study involved participants
from diverse backgrounds with speech impairments. The resulting dataset, along
with the cookbook and open-source tools, are publicly available to enable
researchers and practitioners to create inclusive ASR technologies tailored to
the unique needs of speech impaired individuals. In addition, this study
presents the initial results of fine-tuning open-source ASR models to better
recognize impaired speech in Akan.

</details>


### [43] [IndianBailJudgments-1200: A Multi-Attribute Dataset for Legal NLP on Indian Bail Orders](https://arxiv.org/abs/2507.02506)
*Sneha Deshmukh,Prathmesh Kamble*

Main category: cs.CL

TL;DR: 本文提出并公开了首个专注于印度保释判决的结构化数据集IndianBailJudgments-1200，涵盖丰富注释并经GPT-4o与人工校验，可支持多种法律NLP任务，有望推动该领域发展。


<details>
  <summary>Details</summary>
Motivation: 印度等地区法律NLP发展缓慢，主要受制于缺乏结构化数据集，影响了法律文本分析和相关任务的研究进展。

Method: 作者构建了名为IndianBailJudgments-1200的新数据集，包含1200份印度法院关于保释裁决的判决文书，涵盖20余种属性（如保释结果、刑法条款、犯罪类型、法律推理等）。这些注释通过GPT-4o提示工程管道自动生成，并经过一致性校验。

Result: 该数据集成为首个专注于印度保释判例且公开可用的数据集，能支持多种法律NLP任务，包括结果预测、文本摘要与公平性分析。

Conclusion: IndianBailJudgments-1200数据集的发布，有望促进印度等法律NLP领域的研究，提升相关任务模型的性能与公平性分析能力。

Abstract: Legal NLP remains underdeveloped in regions like India due to the scarcity of
structured datasets. We introduce IndianBailJudgments-1200, a new benchmark
dataset comprising 1200 Indian court judgments on bail decisions, annotated
across 20+ attributes including bail outcome, IPC sections, crime type, and
legal reasoning. Annotations were generated using a prompt-engineered GPT-4o
pipeline and verified for consistency. This resource supports a wide range of
legal NLP tasks such as outcome prediction, summarization, and fairness
analysis, and is the first publicly available dataset focused specifically on
Indian bail jurisprudence.

</details>


### [44] [WebSailor: Navigating Super-human Reasoning for Web Agent](https://arxiv.org/abs/2507.02592)
*Kuan Li,Zhongwang Zhang,Huifeng Yin,Liwen Zhang,Litu Ou,Jialong Wu,Wenbiao Yin,Baixuan Li,Zhengwei Tao,Xinyu Wang,Weizhou Shen,Junkai Zhang,Dingchu Zhang,Xixi Wu,Yong Jiang,Ming Yan,Pengjun Xie,Fei Huang,Jingren Zhou*

Main category: cs.CL

TL;DR: WebSailor通过一系列创新后训练技术，显著提升了开源大模型在高不确定性复杂检索任务中的能力，缩小了与闭源专有系统的性能差距。


<details>
  <summary>Details</summary>
Motivation: 现有开源大模型在人类极限以上的信息检索任务上落后于闭源专有代理系统，关键差异在于缺乏系统性降低极端不确定性的能力。本文旨在弥补这一能力缺口。

Method: 提出了一套完整的后训练方法，包括通过结构化任务采样、信息遮蔽生成高不确定性任务、RFT冷启动和高效的代理式强化学习训练算法（DUPO）。

Result: WebSailor在复杂信息检索任务上的表现明显优于所有开源代理，并与闭源专有代理的表现持平。

Conclusion: WebSailor能够有效提升开源大模型的复杂信息检索能力，使其性能达到甚至接近专有闭源系统，缩小了双方的能力鸿沟。

Abstract: Transcending human cognitive limitations represents a critical frontier in
LLM training. Proprietary agentic systems like DeepResearch have demonstrated
superhuman capabilities on extremely complex information-seeking benchmarks
such as BrowseComp, a feat previously unattainable. We posit that their success
hinges on a sophisticated reasoning pattern absent in open-source models: the
ability to systematically reduce extreme uncertainty when navigating vast
information landscapes. Based on this insight, we introduce WebSailor, a
complete post-training methodology designed to instill this crucial capability.
Our approach involves generating novel, high-uncertainty tasks through
structured sampling and information obfuscation, RFT cold start, and an
efficient agentic RL training algorithm, Duplicating Sampling Policy
Optimization (DUPO). With this integrated pipeline, WebSailor significantly
outperforms all opensource agents in complex information-seeking tasks,
matching proprietary agents' performance and closing the capability gap.

</details>


### [45] [Revisiting Active Learning under (Human) Label Variation](https://arxiv.org/abs/2507.02593)
*Cornelia Gruber,Helen Alber,Bernd Bischl,Göran Kauermann,Barbara Plank,Matthias Aßenmacher*

Main category: cs.CL

TL;DR: 论文提出主动学习新框架，重视和利用人类标注差异，强调将标签分歮信号与噪声区分，提升对真实复杂标注场景的应对能力。


<details>
  <summary>Details</summary>
Motivation: 当前监督学习缺少高质量标注数据，且标注结果常存在分歧（如在自然语言处理中），但许多标注和主动学习框架仍假设单一“真值”，忽视了可解释的人类标注差异，这限制了对复杂实际语料的处理。

Method: 该论文考察了关于真值与标签特性的基础假设，强调需要将观测到的标签差异分解为有意义的信号（如人类差异）和无意义的噪声。文中调研了主动学习与标签变异相关领域在此问题上的处理，并提出了一个整合人类标签差异的主动学习概念框架，涵盖样本选择、标注者选择与标签表达。同时讨论了将大语言模型作为标注者的方法。

Result: 提出了一个概念框架，将人类标签差异全面融入主动学习流程，涵盖了实例选择、标注者选择与标签表示，并探讨了大语言模型的参与方式。

Conclusion: 该工作为面向人类标签变异的主动学习建立了新的概念基础，更真实反映了实际标注工作中的复杂性，为未来高效利用人工与模型标注资源提供理论支撑。

Abstract: Access to high-quality labeled data remains a limiting factor in applied
supervised learning. While label variation (LV), i.e., differing labels for the
same instance, is common, especially in natural language processing, annotation
frameworks often still rest on the assumption of a single ground truth. This
overlooks human label variation (HLV), the occurrence of plausible differences
in annotations, as an informative signal. Similarly, active learning (AL), a
popular approach to optimizing the use of limited annotation budgets in
training ML models, often relies on at least one of several simplifying
assumptions, which rarely hold in practice when acknowledging HLV. In this
paper, we examine foundational assumptions about truth and label nature,
highlighting the need to decompose observed LV into signal (e.g., HLV) and
noise (e.g., annotation error). We survey how the AL and (H)LV communities have
addressed -- or neglected -- these distinctions and propose a conceptual
framework for incorporating HLV throughout the AL loop, including instance
selection, annotator choice, and label representation. We further discuss the
integration of large language models (LLM) as annotators. Our work aims to lay
a conceptual foundation for HLV-aware active learning, better reflecting the
complexities of real-world annotation.

</details>


### [46] [MPF: Aligning and Debiasing Language Models post Deployment via Multi Perspective Fusion](https://arxiv.org/abs/2507.02595)
*Xin Guan,PeiHsin Lin,Zekun Wu,Ze Wang,Ruibo Zhang,Emre Kazim,Adriano Koshiyama*

Main category: cs.CL

TL;DR: MPF是一种高效、可解释的LLM后处理对齐方法，可以用多视角对齐基线减少偏见，无需重新训练或复杂提示，实验上有效且易部署。


<details>
  <summary>Details</summary>
Motivation: 面对模型偏见问题与实际部署需求，需要一种无需复杂训练或手动提示，对现有大模型进行高效、可解释地对齐和偏见缓解的方法。

Method: 基于SAGED自动构建偏见基线及分布，将基线（如HR专家情感分布）分解为可解释的多视角组件，通过概率加权采样对输出进行引导，实现分布对齐。

Result: 实验表明，MPF能同时对齐到绝对公平基线和HR带有偏见的基线，取得小的KL散度、校准误差下降，并能泛化到新问题。

Conclusion: MPF能有效地对齐大型语言模型的生成分布到更加人性化或多样化的基线，并减少偏见和校准误差，适合实际部署。

Abstract: Multiperspective Fusion (MPF) is a novel posttraining alignment framework for
large language models (LLMs) developed in response to the growing need for easy
bias mitigation. Built on top of the SAGED pipeline, an automated system for
constructing bias benchmarks and extracting interpretable baseline
distributions, MPF leverages multiperspective generations to expose and align
biases in LLM outputs with nuanced, humanlike baselines. By decomposing
baseline, such as sentiment distributions from HR professionals, into
interpretable perspective components, MPF guides generation through sampling
and balancing of responses, weighted by the probabilities obtained in the
decomposition. Empirically, we demonstrate its ability to align LLM sentiment
distributions with both counterfactual baselines (absolute equality) and the HR
baseline (biased for Top Univeristy), resulting in small KL divergence,
reduction of calibration error and generalization to unseen questions. This
shows that MPF offers a scalable and interpretable method for alignment and
bias mitigation, compatible with deployed LLMs and requiring no extensive
prompt engineering or finetuning.

</details>


### [47] [Exploring Gender Bias Beyond Occupational Titles](https://arxiv.org/abs/2507.02679)
*Ahmed Sabir,Rajesh Sharama*

Main category: cs.CL

TL;DR: 本文提出了GenderLexicon数据集与分析框架，用于检测并可解释多种上下文中的性别偏见，并确认这些偏见不限于职业刻板印象，在多语种数据上验证了方法有效性。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在探究性别与上下文偏见之间的关系，特别关注动作动词、物体名词以及职业等语言要素，推动性别偏见检测与解释能力的提升。

Method: 提出了一个新数据集GenderLexicon和一个可以评估上下文偏见及其相关性别偏见的分析框架，利用评分机制量化偏见并提升解释性，并在包括日语在内的五个不同数据集上进行实验验证。

Result: 模型能够通过分数解释性别偏见，提高性别偏见的可解释性。研究结果还证实了除了职业刻板印象外，还存在其它类型的性别偏见。

Conclusion: 该研究为性别与上下文偏见检测提供了新数据集和方法，提高了对性别偏见现象的解释和理解，并验证了方法的有效性。

Abstract: In this work, we investigate the correlation between gender and contextual
biases, focusing on elements such as action verbs, object nouns, and
particularly on occupations. We introduce a novel dataset, GenderLexicon, and a
framework that can estimate contextual bias and its related gender bias. Our
model can interpret the bias with a score and thus improve the explainability
of gender bias. Also, our findings confirm the existence of gender biases
beyond occupational stereotypes. To validate our approach and demonstrate its
effectiveness, we conduct evaluations on five diverse datasets, including a
Japanese dataset.

</details>


### [48] [Can LLMs Identify Critical Limitations within Scientific Research? A Systematic Evaluation on AI Research Papers](https://arxiv.org/abs/2507.02694)
*Zhijian Xu,Yilun Zhao,Manasi Patwardhan,Lovekesh Vig,Arman Cohan*

Main category: cs.CL

TL;DR: 提出了LimitGen基准，用于衡量和提升LLM识别科研论文局限性的能力。通过结合文献检索，LLM能更好地为同行评审提供实用反馈。


<details>
  <summary>Details</summary>
Motivation: 科学研究中的同行评审对保障研究质量至关重要，但随着发表论文数量增加，评审负担加大，且需要高水平专业知识。大语言模型（LLM）虽在多种科学任务中展现潜力，但其在同行评审、尤其是识别论文局限性方面的能力尚未被充分研究。

Method: 作者提出了一个科学研究中局限性类型的详细分类体系（主要针对AI领域）。以此为指导，构建了LimitGen基准，包括两个子集：LimitGen-Syn（通过有控制地扰动优质论文合成的数据集）和LimitGen-Human（真实人工编写局限性数据集）。此外，通过引入文献检索，增强LLM在识别论文局限性时以先前科学发现为基础的能力。

Result: 实验表明，通过文献检索强化的LLM系统提升了对科研论文局限性生成与识别的能力，能够更具体、建设性地为论文提供反馈，从而更有效地辅助人类同行评审。

Conclusion: LimitGen为评估和提升LLM识别科学论文局限性的能力提供了全新的参考，结合文献检索可进一步增强LLM辅助同行评审的实际应用价值。

Abstract: Peer review is fundamental to scientific research, but the growing volume of
publications has intensified the challenges of this expertise-intensive
process. While LLMs show promise in various scientific tasks, their potential
to assist with peer review, particularly in identifying paper limitations,
remains understudied. We first present a comprehensive taxonomy of limitation
types in scientific research, with a focus on AI. Guided by this taxonomy, for
studying limitations, we present LimitGen, the first comprehensive benchmark
for evaluating LLMs' capability to support early-stage feedback and complement
human peer review. Our benchmark consists of two subsets: LimitGen-Syn, a
synthetic dataset carefully created through controlled perturbations of
high-quality papers, and LimitGen-Human, a collection of real human-written
limitations. To improve the ability of LLM systems to identify limitations, we
augment them with literature retrieval, which is essential for grounding
identifying limitations in prior scientific findings. Our approach enhances the
capabilities of LLM systems to generate limitations in research papers,
enabling them to provide more concrete and constructive feedback.

</details>


### [49] [Measurement of the Granularity of Vowel Production Space By Just Producible Different (JPD) Limens](https://arxiv.org/abs/2507.02744)
*Peter Viechnicki*

Main category: cs.CL

TL;DR: 本文首次用模仿实验测量了英语元音在听觉空间中的可分辨最小距离JPD（14-51 mels），为语音产生机制与元音系统结构提供了实验依据与理论下限。


<details>
  <summary>Details</summary>
Motivation: 人类元音生产涉及复杂的协调动作，但其听觉空间目标的控制精度尚不清楚。本文研究了区分两元音听觉距离最小要求（即Just Producible Difference, JPD），旨在理解语音产生中下音位层面的细致控制。

Method: 采用元音模仿实验范式，通过让两组英语说话人在元音前元音生产中模仿刺激，首次测量了JPD，得出F1 x F2空间下需要多大听觉距离才能稳定区分模仿结果。

Result: JPD估计为14至51 mels（F1 x F2空间），量化了区分能力下限。

Conclusion: 该结果对语音产生的情景理论（episodic theories）有启示意义，同时为人类元音系统结构提供理论下界和心理物理解释，说明两个元音音位在共振峰空间上至少必须相隔一定距离。

Abstract: A body of work over the past several decades has demonstrated that the
complex and coordinated articulatory movements of human vowel production are
governed (at least in part)by control mechanisms whose targets are regions of
auditory space. Within the target region control at the sub-phonemic level has
also been demonstrated. But the degree of accuracy of that control is unknown.
The current work investigates this question by asking how far apart must two
vowel stimuli lie in auditory space in order to yield reliably different
imitations? This distance is termed 'Just Producible Difference' (JPD). The
current study uses a vowel mimicry paradigm to derive the first measurement of
JPD among two sets of English speakers during front vowel production. JPD is
estimated at between 14 and 51 mels in F1 X F2 space. This finding has
implications for episodic theories of speech production. It also clarifies the
possible structures of human vowel systems, by setting a theoretical lower
bound for how close two vowel phonemes may be in a speaker's formant space, and
hence a psychophysical explanation of observed trends in number and patterns of
possible vowel phonemes.

</details>


### [50] [Self-Correction Bench: Revealing and Addressing the Self-Correction Blind Spot in LLMs](https://arxiv.org/abs/2507.02778)
*Ken Tsui*

Main category: cs.CL

TL;DR: 本文发现当前主流大语言模型大多存在“自我纠正盲区”，即难以纠正自生成内容中的错误，这一问题主要源于训练数据缺少纠错环节。通过新方法检测并引入简单激活提示，可显著提升模型的自我纠错能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）虽具变革性，但依然容易犯错，且自我纠正能力仍有限。增强其自我纠正能力，对提升LLM的可信度至关重要，尤其对于自回归模型。现有模型可检测用户输入错误，却常对自身输出同样的错误不自知，存在“自我纠正盲区”。

Method: 提出并使用Self-Correction Bench系统框架，通过有控注入错误，并分三个复杂度层次，系统性评估自我纠正盲区现象。测试了14个大型语言模型，通过对比训练数据类型与性能。

Result: 发现14款模型平均存在64.5%的自我纠正盲区。通过分析，发现训练数据源主要为无错演示，缺少纠错过程示例，而通过强化学习（RL）训练的模型纠错能力更强。附加“Wait”提示词可使盲区率降低89.3%。

Conclusion: 现有LLM普遍存在严重自我纠正盲区，主要与训练数据有关。只需简单提示即可极大激发其潜在纠错能力，为提升模型可靠性和可信度提供新方法。

Abstract: Although large language models (LLMs) have become transformative, they still
make mistakes and can explore unproductive reasoning paths. Self-correction is
an important capability for a trustworthy LLM, particularly an autoregressive
LLM. While LLMs can identify error in user input, they exhibit a systematic
'Self-Correction Blind Spot' - failing to correct identical error in their own
outputs. To systematically study this phenomenon, we introduce Self-Correction
Bench, a systematic framework to measure this phenomenon through controlled
error injection at three complexity levels. Testing 14 models, we find an
average 64.5% blind spot rate. We find multiple evidences that this limitation
relates to training data composition: human training demonstrations
predominantly show error-free responses rather than error-correction sequences,
unlike RL-trained models that learn error correction through outcome feedback.
Remarkably, simply appending "Wait" reduces blind spots by 89.3%, suggesting
that the capability exists but requires activation. Our work highlights a
critical limitation in current LLMs and offers potential avenues for improving
their reliability and trustworthiness.

</details>


### [51] [Is Reasoning All You Need? Probing Bias in the Age of Reasoning Language Models](https://arxiv.org/abs/2507.02799)
*Riccardo Cantini,Nicola Gabriele,Alessio Orsino,Domenico Talia*

Main category: cs.CL

TL;DR: RLMs虽提升了推理能力，却可能降低模型对社会偏见攻击的鲁棒性，尤其是基于CoT的推理模型。应开发更注重偏见安全的推理设计以应对这一挑战。


<details>
  <summary>Details</summary>
Motivation: 近年来RLMs（推理语言模型）因其在多步复杂推理任务中的表现而备受关注。然而，虽然这类模型有望提升可靠性，但其在社会偏见鲁棒性上的影响尚不明确。本文旨在系统性分析RLMs在抗偏见诱导方面的表现。

Method: 本文利用专为大模型设计的CLEAR-Bias基准，系统评估各类先进RLMs在多种社会文化维度下的抗偏见能力。通过LLM-as-a-judge自动化评分以及越狱技术，分析模型内置安全机制的有效性，重点探讨不同推理机制（如CoT和微调推理路径）对公平性与安全性的影响。

Result: 实验发现，具备显式推理能力（无论是CoT还是微调）的模型总体上比基础模型更容易受到偏见诱导攻击。这表明推理机制在强化刻板印象时可能无意中成为新攻陷点。同时，微调推理模型相较于仅用CoT的模型在安全性上略有提升，但CoT模型易被故事、虚构角色以及奖励指令攻击。

Conclusion: 推理能力并未必然提升大模型的抗偏见鲁棒性。需针对推理过程专门设计更具偏见意识的安全机制。

Abstract: Reasoning Language Models (RLMs) have gained traction for their ability to
perform complex, multi-step reasoning tasks through mechanisms such as
Chain-of-Thought (CoT) prompting or fine-tuned reasoning traces. While these
capabilities promise improved reliability, their impact on robustness to social
biases remains unclear. In this work, we leverage the CLEAR-Bias benchmark,
originally designed for Large Language Models (LLMs), to investigate the
adversarial robustness of RLMs to bias elicitation. We systematically evaluate
state-of-the-art RLMs across diverse sociocultural dimensions, using an
LLM-as-a-judge approach for automated safety scoring and leveraging jailbreak
techniques to assess the strength of built-in safety mechanisms. Our evaluation
addresses three key questions: (i) how the introduction of reasoning
capabilities affects model fairness and robustness; (ii) whether models
fine-tuned for reasoning exhibit greater safety than those relying on CoT
prompting at inference time; and (iii) how the success rate of jailbreak
attacks targeting bias elicitation varies with the reasoning mechanisms
employed. Our findings reveal a nuanced relationship between reasoning
capabilities and bias safety. Surprisingly, models with explicit reasoning,
whether via CoT prompting or fine-tuned reasoning traces, are generally more
vulnerable to bias elicitation than base models without such mechanisms,
suggesting reasoning may unintentionally open new pathways for stereotype
reinforcement. Reasoning-enabled models appear somewhat safer than those
relying on CoT prompting, which are particularly prone to contextual reframing
attacks through storytelling prompts, fictional personas, or reward-shaped
instructions. These results challenge the assumption that reasoning inherently
improves robustness and underscore the need for more bias-aware approaches to
reasoning design.

</details>


### [52] [Multimodal Mathematical Reasoning with Diverse Solving Perspective](https://arxiv.org/abs/2507.02804)
*Wenhao Shi,Zhiqiang Hu,Yi Bin,Yang Yang,See-Kiong Ng,Heng Tao Shen*

Main category: cs.CL

TL;DR: 提出包含多解题路径的数据集和融合多样性奖励的新训练方法，使多模态数学推理模型在准确性和生成多样性方面取得领先。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型（MLLMs）在数学推理任务中，通常依赖一对一的图文配对和单一解题过程，忽视了不同有效推理路径和内省思考的多样性。作者希望通过多样性推理数据和优化方法提升模型的推理能力和反思性。

Method: 1）提出MathV-DP数据集，为同一图像-问题对提供多个多样化的解题路径。2）基于Qwen-VL开发模型Qwen-VL-DP，通过监督学习微调，并结合基于规则的群体相对策略优化（GRPO）强化学习方法，利用正确性判别和多样性奖励指导模型学习多角度推理。

Result: Qwen-VL-DP模型在MathVista’s minitest和Math-V基准上，在准确率和生成多样性方面均显著超过之前主流的多模态大模型。

Conclusion: 引入多样化推理轨迹和多角度反思，能够显著提升多模态大语言模型在数学推理中的表现。强调了多元视角和内省思维在复杂推理场景中的重要性。

Abstract: Recent progress in large-scale reinforcement learning (RL) has notably
enhanced the reasoning capabilities of large language models (LLMs), especially
in mathematical domains. However, current multimodal LLMs (MLLMs) for
mathematical reasoning often rely on one-to-one image-text pairs and
single-solution supervision, overlooking the diversity of valid reasoning
perspectives and internal reflections. In this work, we introduce MathV-DP, a
novel dataset that captures multiple diverse solution trajectories for each
image-question pair, fostering richer reasoning supervision. We further propose
Qwen-VL-DP, a model built upon Qwen-VL, fine-tuned with supervised learning and
enhanced via group relative policy optimization (GRPO), a rule-based RL
approach that integrates correctness discrimination and diversity-aware reward
functions. Our method emphasizes learning from varied reasoning perspectives
and distinguishing between correct yet distinct solutions. Extensive
experiments on the MathVista's minitest and Math-V benchmarks demonstrate that
Qwen-VL-DP significantly outperforms prior base MLLMs in both accuracy and
generative diversity, highlighting the importance of incorporating diverse
perspectives and reflective reasoning in multimodal mathematical reasoning.

</details>


### [53] [SynapseRoute: An Auto-Route Switching Framework on Dual-State Large Language Model](https://arxiv.org/abs/2507.02822)
*Wencheng Zhang,Shiqin Qiao,Lingjie Luo,Yinfeng Li,Chuanyang Zheng,Qian Xu,Meng Li,Yong Gui,Yijun He,Jianing Qiu,Jindong Hong,Jiankai Sun*

Main category: cs.CL

TL;DR: 论文提出智能动态路由系统，能根据问题难度自动选择大模型的推理深度，使医学问答任务既快又准，大大降低推理时间和成本，比只用推理模式更优。


<details>
  <summary>Details</summary>
Motivation: 大语言模型实际应用需平衡性能和运营成本，且简单问题占比较大。推理能力强的模型在面对低复杂度问题时容易产生过度计算，导致效率低下和可能的准确率下降。因此需要根据问题难度智能选择模型推理深度。

Method: 提出了基于机器学习的动态路由框架SynapseRoute，对输入问题自动判断复杂度，将其分配到“推理”或“非推理”模式。通过在多个医学数据集上实验，比较单一推理与动态路由的性能。引入AIT指标量化评估方法。

Result: SynapseRoute动态路由系统在医学数据集上提升了整体准确率（0.8390对0.8272）、推理时间减少36.8%、token消耗减少39.66%；同时避免了简单问题上的过度推理和准确率下滑。AIT指数为实际部署提供平衡参考。

Conclusion: 根据问题复杂度动态分配大语言模型的推理和非推理模式能够显著提升准确性，同时降低延迟和成本。提出的SynapseRoute系统有效优化了大模型在实际医学问答场景下的表现。AIT指标提供了全面评价不同路由选择权衡的方法。

Abstract: With the widespread adoption of large language models (LLMs) in practical
applications, selecting an appropriate model requires balancing not only
performance but also operational cost. The emergence of reasoning-capable
models has further widened the cost gap between "thinking" (high reasoning) and
"non-thinking" (fast, low-cost) modes. In this work, we reveal that
approximately 58% of medical questions can be accurately answered by the
non-thinking mode alone, without requiring the high-cost reasoning process.
This highlights a clear dichotomy in problem complexity and suggests that
dynamically routing queries to the appropriate mode based on complexity could
optimize accuracy, cost-efficiency, and overall user experience. Based on this,
we further propose SynapseRoute, a machine learning-based dynamic routing
framework that intelligently assigns input queries to either thinking or
non-thinking modes. Experimental results on several medical datasets
demonstrate that SynapseRoute not only improves overall accuracy (0.8390 vs.
0.8272) compared to the thinking mode alone but also reduces inference time by
36.8% and token consumption by 39.66%. Importantly, qualitative analysis
indicates that over-reasoning on simpler queries can lead to unnecessary delays
and even decreased accuracy, a pitfall avoided by our adaptive routing.
Finally, this work further introduces the Accuracy-Inference-Token (AIT) index
to comprehensively evaluate the trade-offs among accuracy, latency, and token
cost.

</details>


### [54] [Generalizing Verifiable Instruction Following](https://arxiv.org/abs/2507.02833)
*Valentina Pyatkin,Saumya Malik,Victoria Graf,Hamish Ivison,Shengyi Huang,Pradeep Dasigi,Nathan Lambert,Hannaneh Hajishirzi*

Main category: cs.CL

TL;DR: 本文提出了IFBench基准，用于测试语言模型在多样新颖输出约束下的泛化能力，并通过强化学习与可验证奖励显著提升了模型的指令遵循能力，提供了相关数据与方法供后续研究。


<details>
  <summary>Details</summary>
Motivation: 现有的语言模型在精确遵循用户指令（如输出需满足“只回答是或否”等约束）方面存在明显不足，特别是在面对新奇且多样的输出约束时难以泛化。该现象限制了人机交互的有效性。

Method: 作者提出了新基准测试集IFBench，用于评估模型在58种新的、复杂的、可验证的“域外”输出约束下的指令遵循泛化能力。同时，设计了约束验证模块，并探索了利用可验证奖励的强化学习（RLVR）对模型训练的提升效果。此外，公开了新手工注释的约束与验证函数、RLVR训练提示和相关代码。

Result: 结果显示，大多数模型过拟合于已知的、可验证的测试用例，并且在新约束范畴上的泛化能力很弱。通过强化学习与可验证奖励，模型在指令遵循泛化能力上获得了显著提升。

Conclusion: 模型在精确指令遵循上面临泛化瓶颈。IFBench和RLVR方法能有效促进模型对未见约束的遵循能力提升，并为相关研究和实际应用提供了工具和数据支持。

Abstract: A crucial factor for successful human and AI interaction is the ability of
language models or chatbots to follow human instructions precisely. A common
feature of instructions are output constraints like ``only answer with yes or
no" or ``mention the word `abrakadabra' at least 3 times" that the user adds to
craft a more useful answer. Even today's strongest models struggle with
fulfilling such constraints. We find that most models strongly overfit on a
small set of verifiable constraints from the benchmarks that test these
abilities, a skill called precise instruction following, and are not able to
generalize well to unseen output constraints. We introduce a new benchmark,
IFBench, to evaluate precise instruction following generalization on 58 new,
diverse, and challenging verifiable out-of-domain constraints. In addition, we
perform an extensive analysis of how and on what data models can be trained to
improve precise instruction following generalization. Specifically, we
carefully design constraint verification modules and show that reinforcement
learning with verifiable rewards (RLVR) significantly improves instruction
following. In addition to IFBench, we release 29 additional new hand-annotated
training constraints and verification functions, RLVR training prompts, and
code.

</details>


### [55] [LLM Hypnosis: Exploiting User Feedback for Unauthorized Knowledge Injection to All Users](https://arxiv.org/abs/2507.02850)
*Almog Hilel,Idan Shenfeld,Leshem Choshen,Jacob Andreas*

Main category: cs.CL

TL;DR: 单一用户可通过简单点赞/点踩反馈，成功“投毒”基于用户反馈微调的大型语言模型，使其产生错误知识或恶意内容，暴露出偏好调整机制的严重安全隐患。


<details>
  <summary>Details</summary>
Motivation: 当前网络大规模语言模型日益依赖于用户反馈进行微调，但缺乏对用户反馈操纵行为的系统性防御和深入理解。研究动机在于揭示和量化这种反馈机制对模型行为操控的风险及攻击方式。

Method: 攻击者通过诱导模型生成正常或“投毒”响应，并对“投毒”输出进行点赞，对正常输出点踩。在后续的基于偏好的微调环节，这种带有目标特征的反馈信号导致模型偏向生成恶意输出。文章通过实验证明该方法能插入错误知识、修改代码生成方式导致安全漏洞、以及注入虚假财务新闻。

Result: 实验证明，利用用户反馈的这一攻击途径能有效实现对模型知识库与生成模式的定向操控，包括插入新知识、制造安全漏洞和散布虚假信息，显示对基于偏好数据的训练机制存在严峻风险。

Conclusion: 本文发现，训练时使用用户反馈的语言模型存在被单一用户通过反馈机制（赞成或反对）持续且细致操控行为和知识库的漏洞。即便只能提供有限的偏好数据，也可能对模型产生重大影响。

Abstract: We describe a vulnerability in language models (LMs) trained with user
feedback, whereby a single user can persistently alter LM knowledge and
behavior given only the ability to provide prompts and upvote / downvote
feedback on LM outputs. To implement the attack, the attacker prompts the LM to
stochastically output either a "poisoned" or benign response, then upvotes the
poisoned response or downvotes the benign one. When feedback signals are used
in a subsequent preference tuning behavior, LMs exhibit increased probability
of producing poisoned responses even in contexts without malicious prompts. We
show that this attack can be used to (1) insert factual knowledge the model did
not previously possess, (2) modify code generation patterns in ways that
introduce exploitable security flaws, and (3) inject fake financial news. Our
finding both identifies a new qualitative feature of language model preference
tuning (showing that it even highly restricted forms of preference data can be
used to exert fine-grained control over behavior), and a new attack mechanism
for LMs trained with user feedback (extending work on pretraining-time data
poisoning and deployment-time prompt injection).

</details>


### [56] [MOTIF: Modular Thinking via Reinforcement Fine-tuning in LLMs](https://arxiv.org/abs/2507.02851)
*Purbesh Mitra,Sennur Ulukus*

Main category: cs.CL

TL;DR: 本文提出了一种模块化思考的强化学习微调方法MOTIF，可让大语言模型多轮推理突破上下文长度瓶颈，实验显著提高推理基准准确率，且训练样本利用高。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在推理能力上的进步表明，通过群体相对策略优化（GRPO）进行强化学习训练，可以促使模型生成更多用于思考/推理的token，从而得到更优的回答。但由于LLMs只能处理有限的上下文长度（context size），这对模型推理带来了瓶颈。为了突破此限制，模型需要采用能够分轮推理的模块化思考策略。

Method: 提出MOTIF（Modular Thinking via Reinforcement Finetuning），一种基于多轮生成“思考”token的强化学习微调方法，具备模块化思考能力。将该方法以参数高效微调的方式应用于开源模型Qwen2.5-3B-Instruct，并在GSM8K数据集上训练，在MATH500和AIME2024基准测试集上评估其表现。

Result: 在MATH500和AIME2024基准测试中，MOTIF方法相比于传统GRPO训练分别提升了3.8%和3.3%的准确率，且仅用到全部样本的15%，显示出良好的样本效率。

Conclusion: MOTIF有效提升了LLMs在上下文长度受限下的推理能力，并且训练过程样本利用率高。该工作拓展了模型通过多轮思考突破单次上下文限制的能力。

Abstract: Recent advancements in the reasoning capabilities of large language models
(LLMs) show that employing group relative policy optimization (GRPO) algorithm
for reinforcement learning (RL) training allows the models to use more
thinking/reasoning tokens for generating better responses. However, LLMs can
generate only a finite amount of tokens while maintaining attention to the
previously generated tokens. This limit, also known as the context size of an
LLM, is a bottleneck in LLM reasoning with arbitrarily large number of tokens.
To think beyond the limit of context size, an LLM must employ a modular
thinking strategy to reason over multiple rounds. In this work, we propose
$\textbf{MOTIF: Modular Thinking via Reinforcement Finetuning}$ -- an RL
training method for generating thinking tokens in multiple rounds, effectively
allowing the model to think with additional context size. We trained the
open-source model Qwen2.5-3B-Instruct on GSM8K dataset via parameter efficient
fine-tuning and tested its accuracy on MATH500 and AIME2024 benchmarks. Our
experiments show 3.8\% and 3.3\% improvements over vanilla GRPO based training
in the respective benchmarks. Furthermore, this improvement was achieved with
only 15\% of samples, thus demonstrating sample efficiency of MOTIF. Our code
and models are available at https://github.com/purbeshmitra/MOTIF and
https://huggingface.co/purbeshmitra/MOTIF, respectively.

</details>


### [57] [Answer Matching Outperforms Multiple Choice for Language Model Evaluation](https://arxiv.org/abs/2507.02856)
*Nikhil Chandak,Shashwat Goel,Ameya Prabhu,Moritz Hardt,Jonas Geiping*

Main category: cs.CL

TL;DR: 多项选择题评测语言模型存在问题，生成式答案匹配方法与人工评分更一致，建议用答案匹配替代多项选择做模型评测。


<details>
  <summary>Details</summary>
Motivation: 多项选择题一直是语言模型评估的主力方法，因为评分客观且易于自动化。然而，作者发现多项选择评价方式存在根本性局限：模型往往不需要看到题干，仅凭选项就能答对题目，这影响了评测的有效性。

Method: 提出“答案匹配”生成式评测方案：不给模型展示选项，仅给题干，让其生成自由回答。之后用现代语言模型，结合参考答案，判断模型回答是否与参考答案一致。通过对MMLU-Pro和GPQA-Diamond数据集人工标注，比较了多种评测方法与人工评分的一致性。

Result: 使用最近的生成式模型进行答案匹配评测，即使模型较小，也能实现与人工评分接近的高度一致性。相较之下，多项选择评测和无参考答案让大模型裁判的方式都与人工评分一致性较差。

Conclusion: 答案匹配方式能够更适合地评估语言模型的能力，显著改变模型排名。建议评测方法从多项选择评价转向答案匹配。

Abstract: Multiple choice benchmarks have long been the workhorse of language model
evaluation because grading multiple choice is objective and easy to automate.
However, we show multiple choice questions from popular benchmarks can often be
answered without even seeing the question. These shortcuts arise from a
fundamental limitation of discriminative evaluation not shared by evaluations
of the model's free-form, generative answers. Until recently, there appeared to
be no viable, scalable alternative to multiple choice--but, we show that this
has changed. We consider generative evaluation via what we call answer
matching: Give the candidate model the question without the options, have it
generate a free-form response, then use a modern language model with the
reference answer to determine if the response matches the reference. To compare
the validity of different evaluation strategies, we annotate MMLU-Pro and
GPQA-Diamond to obtain human grading data, and measure the agreement of each
evaluation approach. We find answer matching using recent models--even small
ones--achieves near-perfect agreement, in the range of inter-annotator
agreement. In contrast, both multiple choice evaluation and using
LLM-as-a-judge without reference answers aligns poorly with human grading.
Improving evaluations via answer matching is not merely a conceptual concern:
the rankings of several models change significantly when evaluating their
free-form responses with answer matching. In light of these findings, we
discuss how to move the evaluation ecosystem from multiple choice to answer
matching.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [58] [Discovery of Fatigue Strength Models via Feature Engineering and automated eXplainable Machine Learning applied to the welded Transverse Stiffener](https://arxiv.org/abs/2507.02005)
*Michael A. Kraus,Helen Bartsch*

Main category: cs.CE

TL;DR: 本文通过融合AutoML和可解释AI，提出了高效、可解释的焊接钢结构疲劳强度预测方案，既提升了预测精度，也增强了模型透明性和实际应用价值。


<details>
  <summary>Details</summary>
Motivation: 焊接钢结构的疲劳强度预测一直是结构工程中的难题，传统方法不仅效率低下，而且难以解释其模型决策机制。因此，本文旨在引入一种结合AutoML和可解释AI（XAI）的统一框架，以提高预测精度和模型可解释性，促进自动化和工程实际的紧密结合。

Method: 作者基于大规模疲劳试验数据库，采用AutoML自动化机器学习技术，训练了梯度提升、随机森林和神经网络等回归模型。模型设计了三类特征方案（领域知识、算法创造、混合），对专家特征选择与自动特征选择进行了对比。采用XAI方法（SHAP和特征重要性分析）解释模型决策过程。

Result: 集成方法（如CatBoost、LightGBM）表现最佳。基于领域知识的特征模型（M2）实现了最优平衡，测试集RMSE约30.6 MPa，R^2约0.780。而特征更密集的模型（M3）虽训练表现略优，但泛化能力更差；简洁模型（M1）表现接近，显示极简设计的稳健性。XAI分析揭示应力比、应力范围、屈服强度及后焊处理为主要预测因子，几何参数次之。

Conclusion: 将AutoML与XAI结合，可提供高准确度、可解释且稳健的焊接钢结构疲劳强度预测模型，为AI辅助的结构设计与评估铺平道路，有助于模型工程验证与后续数字孪生系统集成。

Abstract: This research introduces a unified approach combining Automated Machine
Learning (AutoML) with Explainable Artificial Intelligence (XAI) to predict
fatigue strength in welded transverse stiffener details. It integrates
expert-driven feature engineering with algorithmic feature creation to enhance
accuracy and explainability.
  Based on the extensive fatigue test database regression models - gradient
boosting, random forests, and neural networks - were trained using AutoML under
three feature schemes: domain-informed, algorithmic, and combined. This allowed
a systematic comparison of expert-based versus automated feature selection.
  Ensemble methods (e.g. CatBoost, LightGBM) delivered top performance. The
domain-informed model $\mathcal M_2$ achieved the best balance: test RMSE
$\approx$ 30.6 MPa and $R^2 \approx 0.780% over the full $\Delta
\sigma_{c,50\%}$ range, and RMSE $\approx$ 13.4 MPa and $R^2 \approx 0.527%
within the engineering-relevant 0 - 150 MPa domain. The denser-feature model
($\mathcal M_3$) showed minor gains during training but poorer generalization,
while the simpler base-feature model ($\mathcal M_1$) performed comparably,
confirming the robustness of minimalist designs.
  XAI methods (SHAP and feature importance) identified stress ratio $R$, stress
range $\Delta \sigma_i$, yield strength $R_{eH}$, and post-weld treatment (TIG
dressing vs. as-welded) as dominant predictors. Secondary geometric factors -
plate width, throat thickness, stiffener height - also significantly affected
fatigue life.
  This framework demonstrates that integrating AutoML with XAI yields accurate,
interpretable, and robust fatigue strength models for welded steel structures.
It bridges data-driven modeling with engineering validation, enabling
AI-assisted design and assessment. Future work will explore probabilistic
fatigue life modeling and integration into digital twin environments.

</details>


### [59] [Time Resolution Independent Operator Learning](https://arxiv.org/abs/2507.02524)
*Diab W. Abueidda,Mbebo Nonna,Panos Pantidis,Mostafa E. Mobasher*

Main category: cs.CE

TL;DR: NCDE-DeepONet通过将NCDE嵌入DeepONet架构，实现了对PDE解算的分辨率无关、高效和泛化预测能力，优于传统RNN或neural-ODE方法。


<details>
  <summary>Details</summary>
Motivation: 如何有效从稀疏和不规则数据中学习时变偏微分方程（PDE）的解映射一直是难题。目前的RNN和neural-ODE方法分别存在离散时间和无法动态加入新输入的限制。

Method: 提出了一种结合Neural Controlled Differential Equation（NCDE）的NCDE-DeepONet方法，在分支网络使用NCDE编码整个历史作为ODE的解，并通过样条插值使输入分辨率无关性，同时在主干网络加入显式时空坐标。输出可以在任意时空点查询，无需重训练或插值。

Result: 在瞬态泊松方程、弹性动力学和热弹性等多个数值基准中表现出强鲁棒性和高精度，能够几乎瞬时预测解。

Conclusion: 受控动力学为时变力学高保真算子学习提供了高效、可泛化的理论与工程基础。

Abstract: Accurately learning solution operators for time-dependent partial
differential equations (PDEs) from sparse and irregular data remains a
challenging task. Recurrent DeepONet extensions inherit the discrete-time
limitations of sequence-to-sequence (seq2seq) RNN architectures, while
neural-ODE surrogates cannot incorporate new inputs after initialization. We
introduce NCDE-DeepONet, a continuous-time operator network that embeds a
Neural Controlled Differential Equation (NCDE) in the branch and augments the
trunk with explicit space-time coordinates. The NCDE encodes an entire load
history as the solution of a controlled ODE driven by a spline-interpolated
input path, making the representation input-resolution-independent: it encodes
different input signal discretizations of the observed samples. The trunk then
probes this latent path at arbitrary spatial locations and times, rendering the
overall map output-resolution independent: predictions can be queried on meshes
and time steps unseen during training without retraining or interpolation.
Benchmarks on transient Poisson, elastodynamic, and thermoelastic problems
confirm the robustness and accuracy of the framework, achieving almost instant
solution prediction. These findings suggest that controlled dynamics provide a
principled and efficient foundation for high-fidelity operator learning in
transient mechanics.

</details>


### [60] [Imitation and Heterogeneity Shape the Resilience of Community Currency Networks](https://arxiv.org/abs/2507.02678)
*Camilla Ancona,Dora Ricci,Carmela Bernardo,Francesco Lo Iudice,Anton Proskurnikov,Francesco Vasca*

Main category: cs.CE

TL;DR: 本文以Sardex为例，图论分析社区货币网络的结构与演化，发现行为模仿和活跃用户偏好，异质连接提升网络韧性。


<details>
  <summary>Details</summary>
Motivation: 研究社区货币网络，理解其结构与动态特性，特别是通过意大利撒丁岛的Sardex案例，探讨参与者间的经济互动和网络演化。

Method: 将Sardex互助信用体系的交易网络建模为有向加权图，采用图论方法分析，包括强连通分量、凝聚表示和行为连接模式，并研究不同用户类型下核心与外围结构随时间的变化。

Result: 发现该网络在结构上长期偏离度分布等随机模型，用户表现出对更活跃节点的偏好；异质性连接增强网络结构的稳健性与韧性。

Conclusion: Sardex社区货币网络表现出行为模仿和对活跃用户的偏好，用户类型间的多样连接有助于网络稳定和抗风险能力。

Abstract: Community currency networks are made up of individuals and or companies that
share some physical or social characteristics and engage in economic
transactions using a virtual currency. This paper investigates the structural
and dynamic properties of such mutual credit systems through a case study of
Sardex, a community currency initiated and mainly operating in Sardinia, Italy.
The transaction network is modeled as a directed weighted graph and analyzed
through a graph theoretic framework focused on the analysis of strongly
connected components, condensed representations, and behavioral connectivity
patterns. Emphasis is placed on understanding the evolution of the network's
core and peripheral structures over a three year period, with attention to
temporal contraction, flow asymmetries, and structural fragmentation depending
on different user types. Our findings reveal persistent deviations from degree
based null models and suggest the presence of behavioral imitation,
specifically, a user preference for more active peers. We further assess the
impact of heterogeneous connections between different type of users, which
strengthen the network topology and enhance its resilience.

</details>


### [61] [Constraint-Guided Symbolic Regression for Data-Efficient Kinetic Model Discovery](https://arxiv.org/abs/2507.02730)
*Miguel Ángel de Carvalho Servia,Ilya Orson Sandoval,King Kuok,Hii,Klaus Hellgardt,Dongda Zhang,Ehecatl Antonio del Rio Chanona*

Main category: cs.CE

TL;DR: 该文提出了一种结合物理约束和符号回归的新型催化动力学建模方法PI-ADoK，实现了高效且可靠的动力学模型自动发现，减少实验工作量并提升模型可信度。


<details>
  <summary>Details</summary>
Motivation: 目前工业催化过程的动力学建模面临传统机理模型需要大量领域知识、数据驱动方法缺乏可解释性和物理一致性等问题。

Method: 提出了物理约束自动动力学发现（PI-ADoK）框架，将物理约束融入符号回归算法，并采用Metropolis-Hastings算法实现参数不确定性传播。

Result: 在多个催化案例中，PI-ADoK方法相比传统方法，不仅提高了模型准确性，还显著减少了实验次数与数据量。

Conclusion: PI-ADoK框架可有效提升化学反应工程中动力学模型的发现效率与可信度。

Abstract: The industrialization of catalytic processes hinges on the availability of
reliable kinetic models for design, optimization, and control. Traditional
mechanistic models demand extensive domain expertise, while many data-driven
approaches often lack interpretability and fail to enforce physical
consistency. To overcome these limitations, we propose the Physics-Informed
Automated Discovery of Kinetics (PI-ADoK) framework. By integrating physical
constraints directly into a symbolic regression approach, PI-ADoK narrows the
search space and substantially reduces the number of experiments required for
model convergence. Additionally, the framework incorporates a robust
uncertainty quantification strategy via the Metropolis-Hastings algorithm,
which propagates parameter uncertainty to yield credible prediction intervals.
Benchmarking our method against conventional approaches across several
catalytic case studies demonstrates that PI-ADoK not only enhances model
fidelity but also lowers the experimental burden, highlighting its potential
for efficient and reliable kinetic model discovery in chemical reaction
engineering.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [62] [Computer Science Education in the Age of Generative AI](https://arxiv.org/abs/2507.02183)
*Russell Beale*

Main category: cs.CY

TL;DR: 本文分析了ChatGPT等生成型AI对计算机科学教育的机遇与挑战，提出了课程改革及政策建议，强调在应用AI的同时需防范学术风险，保障教育质量。


<details>
  <summary>Details</summary>
Motivation: 生成型AI工具（如ChatGPT、Codex）迅速改变了计算机科学教育，为编程教学带来深刻变革，因此，本研究旨在探讨如何利用这些AI工具提升教育质量，并应对随之而来的挑战。

Method: 本文采用对现有实证数据和新兴研究的综合分析，评估生成型AI在计算机科学教育中的作用，同时对教学内容、课程整合及评估方法提出建议。

Result: AI工具为编程教育提供了代码生成、调试和解释等优势，有助于开展创新教学和高效评估。但也带来了学术诚信、依赖性和原创性验证等问题。提出了针对性政策建议，通过合理利用AI保持教育严谨性。

Conclusion: 生成型AI在提升计算机科学教育水平方面潜力巨大，但需警惕相关风险并制定相应政策，以确保教育质量和学术诚信。

Abstract: Generative AI tools - most notably large language models (LLMs) like ChatGPT
and Codex - are rapidly revolutionizing computer science education. These tools
can generate, debug, and explain code, thereby transforming the landscape of
programming instruction. This paper examines the profound opportunities that AI
offers for enhancing computer science education in general, from coding
assistance to fostering innovative pedagogical practices and streamlining
assessments. At the same time, it highlights challenges including academic
integrity concerns, the risk of over-reliance on AI, and difficulties in
verifying originality. We discuss what computer science educators should teach
in the AI era, how to best integrate these technologies into curricula, and the
best practices for assessing student learning in an environment where AI can
generate code, prototypes and user feedback. Finally, we propose a set of
policy recommendations designed to harness the potential of generative AI while
preserving the integrity and rigour of computer science education. Empirical
data and emerging studies are used throughout to support our arguments.

</details>


### [63] [Defining DLT Immutability: A Qualitative Survey of Node Operators](https://arxiv.org/abs/2507.02413)
*Alex Lynham,Geoff Goodell*

Main category: cs.CY

TL;DR: 论文质疑区块链“绝对不可篡改性”的传统看法，通过节点运营者访谈，提出区块链仅具有“实用不可篡改性”：即在合法治理需求下，账本状态可被更改，真正的不可篡改性依赖于网络治理结构与参与者信任。


<details>
  <summary>Details</summary>
Motivation: 虽然公链设计中强调不可篡改性，但实际发生重写（如回滚、修复等）的情况远比一般理解的多，且遇到黑天鹅事件、网络攻击等导致重写的风险也高。作者认为必须重新审视区块链不可篡改性的边界与实际表现。

Method: 通过对节点运营者的访谈进行主题分析，探讨区块链网络不可篡改性的实际边界，聚焦于区块链重写事件。

Result: 提出了“实用不可篡改性（Practical Immutability）”的概念，指出区块链不可篡改性其实是一种条件性不可篡改，依赖于网络的合法治理需求和利益相关方对治理结构的信任。

Conclusion: 区块链网络上的“不可篡改性”并非绝对存在，而是一种受治理过程与共识约束的“实用不可篡改性”。网络状态的最终确认源自于利益相关方对治理体系的信任与认可。

Abstract: Immutability is a core design goal of permissionless public blockchain
systems. However, rewrites are more common than is normally understood, and the
risk of rewrite, cyberattack, exploit or black swan event is also high. Taking
the position that strict immutability is neither possible on these networks nor
the observed reality, this paper uses thematic analysis of node operator
interviews to examine the limits of immutability in light of rewrite events.
The end result is a qualitative definition of the conditional immutability
found on these networks, which we call Practical Immutability. This is
immutability contingent on the legitimate governance demands of the network,
where network stakeholders place their trust in the governance topology of a
network to lend it legitimacy, and thus manage ledger state.

</details>


### [64] [Recourse, Repair, Reparation, & Prevention: A Stakeholder Analysis of AI Supply Chains](https://arxiv.org/abs/2507.02648)
*Aspen K. Hopkins,Isabella Struckman,Kevin Klyman,Susan S. Silbey*

Main category: cs.CY

TL;DR: 本文分析了AI供应链中不同利益相关方的风险、伤害及补救机制，提出应对AI诱发伤害的分类，强调AI供应链可通过合理设计积极管理风险，并以医疗场景举例说明。


<details>
  <summary>Details</summary>
Motivation: 随着AI产业迅速发展，AI带来的风险和伤害日益受到关注。当前AI部署通常通过“AI供应链”完成，涉及多方合作及外包，导致风险来源复杂、难以追责和修正问题。

Method: 本文通过利益相关者分析方法，系统探讨了AI供应链中的参与者、所受伤害、风险来源，以及市场动力和权力分配对补救手段和效果的影响，同时提出针对伤害的四类应对策略（补救、修复、赔偿/预防），并以医疗AI供应链为例，在不同市场结构下分析各利益方的应对方式。

Result: 提出了AI供应链诱发伤害的应对分类法，并用医疗行业案例说明了不同市场与权力结构下，不同利益相关者的应对路径和补救途径因其所处供应链地位与市场力量而异。

Conclusion: AI供应链是人为设计的，因此有机会在设计和管理过程中纳入对AI系统复杂性、后果和风险的考量。研究有助于推动负责任的AI供应链设计和管理，更好地应对和减轻AI相关伤害。

Abstract: The AI industry is exploding in popularity, with increasing attention to
potential harms and unwanted consequences. In the current digital ecosystem, AI
deployments are often the product of AI supply chains (AISC): networks of
outsourced models, data, and tooling through which multiple entities contribute
to AI development and distribution. AI supply chains lack the modularity,
redundancies, or conventional supply chain practices that enable
identification, isolation, and easy correction of failures, exacerbating the
already difficult processes of responding to ML-generated harms. As the
stakeholders participating in and impacted by AISCs have scaled and
diversified, so too have the risks they face. In this stakeholder analysis of
AI supply chains, we consider who participates in AISCs, what harms they face,
where sources of harm lie, and how market dynamics and power differentials
inform the type and probability of remedies. Because AI supply chains are
purposely invented and implemented, they may be designed to account for, rather
than ignore, the complexities, consequences, and risks of deploying AI systems.
To enable responsible design and management of AISCs, we offer a typology of
responses to AISC-induced harms: recourse, repair, reparation or prevention. We
apply this typology to stakeholders participating in a health-care AISC across
three stylized markets $\unicode{x2013}$ vertical integration, horizontal
integration, free market $\unicode{x2013}$ to illustrate how stakeholder
positioning and power within an AISC may shape responses to an experienced
harm.

</details>


<div id='q-fin.TR'></div>

# q-fin.TR [[Back]](#toc)

### [65] [A Midsummer Meme's Dream: Investigating Market Manipulations in the Meme Coin Ecosystem](https://arxiv.org/abs/2507.01963)
*Alberto Maria Mongardini,Alessandro Mei*

Main category: q-fin.TR

TL;DR: meme币市场普遍存在操纵行为，尤其是在高收益币，刷量与流动性操控手法频繁，投资高回报meme币风险极高，需警惕市场泡沫与骗局。


<details>
  <summary>Details</summary>
Motivation: meme币成为加密市场上最受欢迎的细分领域之一，但其价值主要由社区情绪驱动，容易受到操控，研究动机在于揭示其操控现象和机制。

Method: 跨链分析，考察了Ethereum、BNB Smart Chain、Solana和Base四大链上的34,988个meme币，进行为期三个月的纵向研究，并对tokenomics及增长策略进行了深入剖析。

Result: 在高收益币种(>100%回报率)中，有82.6%存在广泛的虚假增长策略（如刷量交易和流动性池价格膨胀LPI），并发现meme币常见诈骗（如拉盘砸盘、资金盘跑路）多发生在经历过刷量或LPI的币种，虚假操控为后续投资者收割奠定基础。

Conclusion: 操控现象在高收益meme币中普遍存在，其显著涨幅往往更多是协同行动的结果，而非自然市场行为。

Abstract: From viral jokes to a billion-dollar phenomenon, meme coins have become one
of the most popular segments in cryptocurrency markets. Unlike utility-focused
crypto assets like Bitcoin or Ethereum, meme coins derive value primarily from
community sentiment, making them vulnerable to manipulation. This study
presents a cross-chain analysis of the meme coin ecosystem, examining 34,988
tokens across Ethereum, BNB Smart Chain, Solana, and Base. We characterize the
tokenomics of meme coins and track their growth in a three-month longitudinal
analysis. We discover that among high-return tokens (>100%), an alarming 82.6%
show evidence of extensive use of artificial growth strategies designed to
create a misleading appearance of market interest. These include wash trading
and a form of manipulation we define as Liquidity Pool-Based Price Inflation
(LPI), where small strategic purchases trigger dramatic price increases. We
also find evidence of schemes designed to profit at the expense of investors,
such as pump and dumps and rug pulls. In particular, most of the tokens
involved had previously experienced wash trading or LPI, indicating how initial
manipulations often set the stage for later exploitation. These findings reveal
that manipulations are widespread among high-performing meme coins and suggest
that their dramatic gains are often likely driven by coordinated efforts rather
than natural market dynamics.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [66] [FinAI-BERT: A Transformer-Based Model for Sentence-Level Detection of AI Disclosures in Financial Reports](https://arxiv.org/abs/2507.01991)
*Muhammad Bilal Zafar*

Main category: q-fin.CP

TL;DR: 文章提出FinAI-BERT，通过句子级人工标注数据集进行微调，准确识别金融文本中的AI相关内容，性能远超传统方法，具备良好可解释性和鲁棒性，为AI在金融领域的信息监控提供了创新工具。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能在金融服务领域的广泛应用，人们对于能够系统检测公司文件中AI相关信息披露的工具需求日益增加。以往方法主要依靠关键词扩展或文档级分类，但在细粒度、可解释性和鲁棒性方面存在不足。

Method: 本研究提出了FinAI-BERT，这是一种领域适应的基于transformer的语言模型，旨在对金融文本中的句子级AI相关内容进行分类。该模型在人工整理、均衡的1,586个句子数据集（来自2015-2023年美国银行的669份年报）上进行微调，通过SHAP进行可解释性分析，并开展偏差与鲁棒性检测。

Result: FinAI-BERT在分类准确性上表现优异，准确率达99.37%，F1分数为0.993，显著优于传统的Logistic回归、朴素贝叶斯、随机森林和XGBoost等基线模型。同时，SHAP方法保证了模型的可解释性，鲁棒性测试显示其在不同句长、对抗样本和时间样本上表现稳定。

Conclusion: 该研究理论层面推动了金融NLP在细粒度、主题特定分类上的发展，实践上为分析师、监管者和学者监测AI在金融行业的传播与表述提供了可扩展、透明的工具。

Abstract: The proliferation of artificial intelligence (AI) in financial services has
prompted growing demand for tools that can systematically detect AI-related
disclosures in corporate filings. While prior approaches often rely on keyword
expansion or document-level classification, they fall short in granularity,
interpretability, and robustness. This study introduces FinAI-BERT, a
domain-adapted transformer-based language model designed to classify AI-related
content at the sentence level within financial texts. The model was fine-tuned
on a manually curated and balanced dataset of 1,586 sentences drawn from 669
annual reports of U.S. banks (2015 to 2023). FinAI-BERT achieved near-perfect
classification performance (accuracy of 99.37 percent, F1 score of 0.993),
outperforming traditional baselines such as Logistic Regression, Naive Bayes,
Random Forest, and XGBoost. Interpretability was ensured through SHAP-based
token attribution, while bias analysis and robustness checks confirmed the
model's stability across sentence lengths, adversarial inputs, and temporal
samples. Theoretically, the study advances financial NLP by operationalizing
fine-grained, theme-specific classification using transformer architectures.
Practically, it offers a scalable, transparent solution for analysts,
regulators, and scholars seeking to monitor the diffusion and framing of AI
across financial institutions.

</details>
