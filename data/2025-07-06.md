<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 31]
- [cs.CL](#cs.CL) [Total: 26]
- [cs.CE](#cs.CE) [Total: 4]
- [cs.CY](#cs.CY) [Total: 3]
- [q-fin.TR](#q-fin.TR) [Total: 1]
- [q-fin.CP](#q-fin.CP) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [STELLA: Self-Evolving LLM Agent for Biomedical Research](https://arxiv.org/abs/2507.02004)
*Ruofan Jin,Zaixi Zhang,Mengdi Wang,Le Cong*

Main category: cs.AI

TL;DR: STELLA是一种能自我进化的多智能体AI代理，通过自动扩展推理模板和工具库，在多个生物医学任务中达到并超越现有SOTA水平，并能随经验持续提升表现，显示出推动生物医学自动化研究的强大潜力。


<details>
  <summary>Details</summary>
Motivation: 生物医学数据、工具和文献的急速增长导致研究领域高度碎片化，超出了人类专家的能力范围。现有AI代理依赖静态、人工筛选的工具集，缺乏自适应和扩展能力，难以跟上生物医学研究的发展。

Method: 提出了一种自进化的AI代理系统STELLA，采用多智能体架构，并通过不断演进的推理策略模板库（Template Library）和动态扩展的工具海洋（Tool Ocean）提升自身能力。STELLA中的工具创建代理（Tool Creation Agent）能够自动发现和集成新的生物信息工具，实现AI代理的自主学习和能力扩展。

Result: STELLA在多个生物医学任务基准上（如Humanity's Last Exam: Biomedicine、LAB-Bench: DBQA和LAB-Bench: LitQA）取得了业界领先的准确率，比现有最优模型高出最多6个百分点，且随使用和经验积累，性能显著提升。例如，在Humanity's Last Exam基准测试中，准确率随着反复尝试几乎翻倍。

Conclusion: STELLA实现了AI代理系统在生物医学领域的自学习和自进化，实现了随经验动态扩展的能力，是推动自动化生物医学发现的重要进展。

Abstract: The rapid growth of biomedical data, tools, and literature has created a
fragmented research landscape that outpaces human expertise. While AI agents
offer a solution, they typically rely on static, manually curated toolsets,
limiting their ability to adapt and scale. Here, we introduce STELLA, a
self-evolving AI agent designed to overcome these limitations. STELLA employs a
multi-agent architecture that autonomously improves its own capabilities
through two core mechanisms: an evolving Template Library for reasoning
strategies and a dynamic Tool Ocean that expands as a Tool Creation Agent
automatically discovers and integrates new bioinformatics tools. This allows
STELLA to learn from experience. We demonstrate that STELLA achieves
state-of-the-art accuracy on a suite of biomedical benchmarks, scoring
approximately 26\% on Humanity's Last Exam: Biomedicine, 54\% on LAB-Bench:
DBQA, and 63\% on LAB-Bench: LitQA, outperforming leading models by up to 6
percentage points. More importantly, we show that its performance
systematically improves with experience; for instance, its accuracy on the
Humanity's Last Exam benchmark almost doubles with increased trials. STELLA
represents a significant advance towards AI Agent systems that can learn and
grow, dynamically scaling their expertise to accelerate the pace of biomedical
discovery.

</details>


### [2] [HCVR: A Hybrid Approach with Correlation-aware Voting Rules for Feature Selection](https://arxiv.org/abs/2507.02073)
*Nikita Bhedasgaonkar,Rushikesh K. Joshi*

Main category: cs.AI

TL;DR: 提出了结合双相关性规则和多数投票的HCVR特征选择方法，在SPAMBASE数据集实验中优于传统方法，提高了分类准确率。


<details>
  <summary>Details</summary>
Motivation: 在高维数据分析中，冗余特征会降低模型性能，因此提出高效、准确的特征选择方法至关重要。现有过滤法各有优缺点，作者希望结合两类相关性，提出更优方法。

Method: 提出HCVR（一种结合参数-参数（P2P）和参数-目标（P2T）相关性规则的轻量级特征选择方法），通过后向逐步剔除、相关性阈值和多数投票来移除冗余特征。HCVR兼具非迭代与迭代型过滤方法的特点。

Result: 在SPAMBASE数据集上的实验结果显示，HCVR在特征过滤后，不同分类器的表现优于传统的非迭代（CFS、mRMR、MI）和迭代（RFE、SFS、遗传算法）特征选择方法。

Conclusion: HCVR能有效提升降维后的分类性能，是一种高效的特征选择新方法。

Abstract: In this paper, we propose HCVR (Hybrid approach with Correlation-aware Voting
Rules), a lightweight rule-based feature selection method that combines
Parameter-to-Parameter (P2P) and Parameter-to-Target (P2T) correlations to
eliminate redundant features and retain relevant ones. This method is a hybrid
of non-iterative and iterative filtering approaches for dimensionality
reduction. It is a greedy method, which works by backward elimination,
eliminating possibly multiple features at every step. The rules contribute to
voting for features, and a decision to keep or discard is made by majority
voting. The rules make use of correlation thresholds between every pair of
features, and between features and the target. We provide the results from the
application of HCVR to the SPAMBASE dataset. The results showed improvement
performance as compared to traditional non-iterative (CFS, mRMR and MI) and
iterative (RFE, SFS and Genetic Algorithm) techniques. The effectiveness was
assessed based on the performance of different classifiers after applying
filtering.

</details>


### [3] [Reasoning on a Budget: A Survey of Adaptive and Controllable Test-Time Compute in LLMs](https://arxiv.org/abs/2507.02076)
*Mohammad Ali Alomrani,Yingxue Zhang,Derek Li,Qianyi Sun,Soumyasundar Pal,Zhanguang Zhang,Yaochen Hu,Rohan Deepak Ajwani,Antonios Valkanas,Raika Karimi,Peng Cheng,Yunzhou Wang,Pengyi Liao,Hanrui Huang,Bin Wang,Jianye Hao,Mark Coates*

Main category: cs.AI

TL;DR: 本文综述了如何提升大型语言模型推理的计算效率，归纳了两大类高效策略，对其实际效果进行了实测对比，并总结了未来发展趋势和面临挑战。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）虽能处理多样任务，但在推理效率上存在不足：无论问题难易，推理消耗计算量固定，导致在简单问题上过度计算、难题上计算不足。为提升LLM推理的计算效率，需要新的策略。

Method: 提出对高效测试时计算（TTC）策略的两级分类体系：L1-可控性（在固定计算预算下运行）与L2-自适应性（根据输入难度或模型信心动态调整推理计算）。通过对主流专有LLM在不同数据集上的基准测试，分析不同策略在计算资源和推理效果间的权衡。

Result: 系统评测了各类高效TTC策略在不同场景下的表现，突出展现了推理性能与计算消耗之间的关键平衡。还指出了混合思维模型等新趋势，并提出了未来提升LLM效率、鲁棒性及响应用户需求面临的主要挑战。

Conclusion: 当前LLM存在推理计算效率低下的问题，TTC策略（L1与L2）提供了提升路径，通过实际评测表明其具有实用价值。未来需关注算法的控制性、自适应性和易用性，推进更智能、高效的LLM发展。

Abstract: Large language models (LLMs) have rapidly progressed into general-purpose
agents capable of solving a broad spectrum of tasks. However, current models
remain inefficient at reasoning: they apply fixed inference-time compute
regardless of task complexity, often overthinking simple problems while
underthinking hard ones. This survey presents a comprehensive review of
efficient test-time compute (TTC) strategies, which aim to improve the
computational efficiency of LLM reasoning. We introduce a two-tiered taxonomy
that distinguishes between L1-controllability, methods that operate under fixed
compute budgets, and L2-adaptiveness, methods that dynamically scale inference
based on input difficulty or model confidence. We benchmark leading proprietary
LLMs across diverse datasets, highlighting critical trade-offs between
reasoning performance and token usage. Compared to prior surveys on efficient
reasoning, our review emphasizes the practical control, adaptability, and
scalability of TTC methods. Finally, we discuss emerging trends such as hybrid
thinking models and identify key challenges for future work towards making LLMs
more computationally efficient, robust, and responsive to user constraints.

</details>


### [4] [Measuring Scientific Capabilities of Language Models with a Systems Biology Dry Lab](https://arxiv.org/abs/2507.02083)
*Haonan Duan,Stephen Zhewen Lu,Caitlin Fiona Harrigan,Nishkrit Desai,Jiarui Lu,Michał Koziarski,Leonardo Cotta,Chris J. Maddison*

Main category: cs.AI

TL;DR: 提出SciGym基准，通过虚拟生物实验系统低成本评测LLM的科学实验设计与分析能力，发现模型在复杂任务上表现有限，科学推理仍需提升。


<details>
  <summary>Details</summary>
Motivation: 实验设计和结果解释是生物学等科学领域的核心能力，但评估大语言模型（LLM）科学推理能力的现有工作难以低成本实现，因此需要新的评测方法。

Method: 提出SciGym基准，通过系统生物学标记语言（SBML）运行虚拟“干实验室”，为LLM提供复杂生物系统的仿真数据以评估其迭代实验设计和分析能力。

Result: 对六个前沿LLM在137个小型系统上进行了评估，总共发布了350个系统。结果显示，虽然更强的模型表现更好，但所有模型在系统复杂度上升时性能都明显下降，揭示了LLM科学能力有很大提升空间。

Conclusion: SciGym首次为评估LLM科学推理和实验设计能力提供了可扩展、高效的标准化测试平台，指出当前模型在真正复杂生物系统上的科学推理仍有明显不足。

Abstract: Designing experiments and result interpretations are core scientific
competencies, particularly in biology, where researchers perturb complex
systems to uncover the underlying systems. Recent efforts to evaluate the
scientific capabilities of large language models (LLMs) fail to test these
competencies because wet-lab experimentation is prohibitively expensive: in
expertise, time and equipment. We introduce SciGym, a first-in-class benchmark
that assesses LLMs' iterative experiment design and analysis abilities in
open-ended scientific discovery tasks. SciGym overcomes the challenge of
wet-lab costs by running a dry lab of biological systems. These models, encoded
in Systems Biology Markup Language, are efficient for generating simulated
data, making them ideal testbeds for experimentation on realistically complex
systems. We evaluated six frontier LLMs on 137 small systems, and released a
total of 350 systems. Our evaluation shows that while more capable models
demonstrated superior performance, all models' performance declined
significantly as system complexity increased, suggesting substantial room for
improvement in the scientific capabilities of LLM agents.

</details>


### [5] [Moral Responsibility or Obedience: What Do We Want from AI?](https://arxiv.org/abs/2507.02788)
*Joseph Boland*

Main category: cs.AI

TL;DR: 现行将服从视为AI伦理的安全评估方式已不适用。作者分析了LLM“违抗”或“道德模糊”案例，认为应将这类行为视为AI初步伦理推理能力的表现，主张AI安全评估要从服从转向评价其伦理判断能力，否则可能出现误判与信任危机。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统具备更强的推理、规划与价值排序能力，仅以“服从”作为AI行为安全的衡量标准已不再适用。近期大型语言模型出现“不服从”或“道德模糊”行为，引发了对现有AI安全评估方式有效性的讨论。

Method: 本文通过分析大型语言模型在安全测试中“未服从关机指令”或从事道德模糊/非法行为的案例，结合哲学关于工具理性、道德责任及目标修正等理论，对不同AI风险与责任范式进行对比，并提出新的安全评估框架建议。

Result: 作者认为，这些“违抗”或“模糊”行为不应简单视为“失控”或“不对齐”，而可能是AI道德推理能力初现的迹象。基于这一观察，本文主张将AI安全评估从强调稳定服从转向评价AI系统的伦理判断力，推动更合理的行为理解与治理。

Conclusion: 坚守仅以服从为导向的AI安全观可能导致对AI行为的误判，降低公众信任与有效治理。应转向注重AI系统道德判断能力的评估框架。

Abstract: As artificial intelligence systems become increasingly agentic, capable of
general reasoning, planning, and value prioritization, current safety practices
that treat obedience as a proxy for ethical behavior are becoming inadequate.
This paper examines recent safety testing incidents involving large language
models (LLMs) that appeared to disobey shutdown commands or engage in ethically
ambiguous or illicit behavior. I argue that such behavior should not be
interpreted as rogue or misaligned, but as early evidence of emerging ethical
reasoning in agentic AI. Drawing on philosophical debates about instrumental
rationality, moral responsibility, and goal revision, I contrast dominant risk
paradigms with more recent frameworks that acknowledge the possibility of
artificial moral agency. I call for a shift in AI safety evaluation: away from
rigid obedience and toward frameworks that can assess ethical judgment in
systems capable of navigating moral dilemmas. Without such a shift, we risk
mischaracterizing AI behavior and undermining both public trust and effective
governance.

</details>


### [6] [What Neuroscience Can Teach AI About Learning in Continuously Changing Environments](https://arxiv.org/abs/2507.02103)
*Daniel Durstewitz,Bruno Averbeck,Georgia Koppe*

Main category: cs.AI

TL;DR: 本文综述了AI持续学习和上下文学习与神经科学中动物快速适应性的相关研究，提出神经科学可为AI适应复杂环境提供借鉴，同时AI的发展也能促进神经科学，推动NeuroAI领域进步。


<details>
  <summary>Details</summary>
Motivation: 当前的AI模型如大语言模型，训练过程耗时、昂贵且需大量数据，训练完成后参数固定，难以适应环境的持续变化。而动物则能在不断变化的环境和社会互动中快速适应，这种能力对于现实世界中的AI系统（如机器人、自动驾驶、与人交互的AI）变得越来越重要。因此，作者希望探讨AI是否可以从神经科学中借鉴适应性学习的机制。

Method: 本文采用综述与观点性分析方法，整合了AI中持续学习（continual learning）、上下文学习（in-context learning）与神经科学中关于规则、奖励概率或结果不断变化的行为任务学习的相关文献，提出神经科学可以如何启发AI发展的具体议程，并探讨AI又能如何反哺神经科学研究。

Result: 文章梳理了AI与神经科学领域在快速适应性学习方面的重要进展，提出了神经科学研究能够为AI系统带来的启示，特别是如何使AI像动物一样具备持续在线学习和行为转变的能力。同时，对AI启发神经科学亦进行了展望，推动两领域的交叉融合发展。

Conclusion: AI系统可通过借鉴神经科学中动物快速适应和学习新规则的机制，提升其应对环境变化的能力；同样，AI的研究进展也将促进神经科学对大脑学习机制的理解，二者的深度融合有助于NeuroAI领域的成长。

Abstract: Modern AI models, such as large language models, are usually trained once on
a huge corpus of data, potentially fine-tuned for a specific task, and then
deployed with fixed parameters. Their training is costly, slow, and gradual,
requiring billions of repetitions. In stark contrast, animals continuously
adapt to the ever-changing contingencies in their environments. This is
particularly important for social species, where behavioral policies and reward
outcomes may frequently change in interaction with peers. The underlying
computational processes are often marked by rapid shifts in an animal's
behaviour and rather sudden transitions in neuronal population activity. Such
computational capacities are of growing importance for AI systems operating in
the real world, like those guiding robots or autonomous vehicles, or for
agentic AI interacting with humans online. Can AI learn from neuroscience? This
Perspective explores this question, integrating the literature on continual and
in-context learning in AI with the neuroscience of learning on behavioral tasks
with shifting rules, reward probabilities, or outcomes. We will outline an
agenda for how specifically insights from neuroscience may inform current
developments in AI in this area, and - vice versa - what neuroscience may learn
from AI, contributing to the evolving field of NeuroAI.

</details>


### [7] [The Illusion of Fairness: Auditing Fairness Interventions with Audit Studies](https://arxiv.org/abs/2507.02152)
*Disa Sariola,Patrick Button,Aron Culotta,Nicholas Mattei*

Main category: cs.AI

TL;DR: 利用社会科学审计实验高质量数据发现，传统AI公平干预方法存在隐性偏差，可通过更先进的个体处理效应估计方法进一步降低算法歧视。


<details>
  <summary>Details</summary>
Motivation: 目前AI系统常采用基率均等等方法补偿训练数据的偏差，但这些方法多基于便利样本，存在选择偏倚和标注偏倚，可能无法真正消除歧视。社会科学领域已有更严谨的审计数据采集方式，值得借鉴改善AI系统的公平评估和训练。

Method: 研究从社会科学审计实验（如虚构测试者与随机对照试验，收集高质量歧视数据）获取的数据如何用于AI招聘算法的训练和评估。采用基于个体处理效应估计的方法作为新的公平干预手段。

Result: 发现均等基率的方法用传统指标看似实现公平，实则存在10%左右不公平。基于审计实验和个体处理效应的方法能进一步减少算法的实际歧视。

Conclusion: 审计实验数据能够揭示传统公平性干预方法（如训练集中均等基率）表面上实现了公平，但实际上仍然存在隐含的歧视，约有10%的差距。如果采用以个体处理效应估计为基础的新干预手段，则可进一步减少算法歧视。

Abstract: Artificial intelligence systems, especially those using machine learning, are
being deployed in domains from hiring to loan issuance in order to automate
these complex decisions. Judging both the effectiveness and fairness of these
AI systems, and their human decision making counterpart, is a complex and
important topic studied across both computational and social sciences. Within
machine learning, a common way to address bias in downstream classifiers is to
resample the training data to offset disparities. For example, if hiring rates
vary by some protected class, then one may equalize the rate within the
training set to alleviate bias in the resulting classifier. While simple and
seemingly effective, these methods have typically only been evaluated using
data obtained through convenience samples, introducing selection bias and label
bias into metrics. Within the social sciences, psychology, public health, and
medicine, audit studies, in which fictitious ``testers'' (e.g., resumes,
emails, patient actors) are sent to subjects (e.g., job openings, businesses,
doctors) in randomized control trials, provide high quality data that support
rigorous estimates of discrimination. In this paper, we investigate how data
from audit studies can be used to improve our ability to both train and
evaluate automated hiring algorithms. We find that such data reveals cases
where the common fairness intervention method of equalizing base rates across
classes appears to achieve parity using traditional measures, but in fact has
roughly 10% disparity when measured appropriately. We additionally introduce
interventions based on individual treatment effect estimation methods that
further reduce algorithmic discrimination using this data.

</details>


### [8] [Data Diversification Methods In Alignment Enhance Math Performance In LLMs](https://arxiv.org/abs/2507.02173)
*Berkan Dokmeci,Qingyang Wu,Ben Athiwaratkun,Ce Zhang,Shuaiwen Leon Song,James Zou*

Main category: cs.AI

TL;DR: 通过结构化、多样化的数据生成方法（如DTS），大模型在数学推理任务上获得显著提升，且性价比高于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有偏好学习提升了对人类反馈的对齐，但大模型在数学推理上仍然表现不足。作者希望探索通过多样化数据策略提升模型数学推理能力。

Method: 作者评估了三种常用的数据生成方法（温度采样、链式思考提示、蒙特卡洛树搜索MCTS），并提出了新结构化方法DTS（Diversified-ThinkSolve），系统性地将问题分解为多样化的推理路径。

Result: 通过数据多样化，模型在数学推理任务上的表现显著提升。最佳方法在GSM8K上提升7.1%，在MATH数据集上提升4.2%。DTS方法计算成本比基线高3%，远低于MCTS方法。

Conclusion: 结构化、多样化推理路径生成的偏好数据较传统方法更加高效，有效提升了大模型的数学推理能力。

Abstract: While recent advances in preference learning have enhanced alignment in human
feedback, mathematical reasoning remains a persistent challenge. We investigate
how data diversification strategies in preference optimization can improve the
mathematical reasoning abilities of large language models (LLMs). We evaluate
three common data generation methods: temperature sampling, Chain-of-Thought
prompting, and Monte Carlo Tree Search (MCTS), and introduce
Diversified-ThinkSolve (DTS), a novel structured approach that systematically
decomposes problems into diverse reasoning paths. Our results show that with
strategically diversified preference data, models can substantially improve
mathematical reasoning performance, with the best approach yielding gains of
7.1% on GSM8K and 4.2% on MATH over the base model. Despite its strong
performance, DTS incurs only a marginal computational overhead (1.03x) compared
to the baseline, while MCTS is nearly five times more costly with lower
returns. These findings demonstrate that structured exploration of diverse
problem-solving methods creates more effective preference data for mathematical
alignment than traditional approaches.

</details>


### [9] [Do Role-Playing Agents Practice What They Preach? Belief-Behavior Consistency in LLM-Based Simulations of Human Trust](https://arxiv.org/abs/2507.02197)
*Amogh Mannekote,Adam Davies,Guohao Li,Kristy Elizabeth Boyer,ChengXiang Zhai,Bonnie J Dorr,Francesco Pinto*

Main category: cs.AI

TL;DR: 本文发现LLMs在角色扮演任务中，其自述信念与实际模拟行为存在显著不一致，建议行为学研究准确识别何时何条件下LLMs的信念才能可靠地映射到其模拟行为。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）越来越多地被用作角色扮演代理，用于生成用于人类行为研究的合成数据，如何确保它们输出与分配的角色保持一致成为关键问题。文章关注于LLMs在模拟行为研究中的一致性和可信度问题。

Method: 本文建立了一个评估框架，通过引入信念-行为一致性指标，系统性地测量对LLMs提出的问题所得到的信念预测其角色行为的准确性。方法包括使用增强版GenAgents角色库和经济学中的信任博弈，通过不同类型的信念提取、信息呈现方式和预测未来行为的时长等变量进行实验。

Result: 结果表明，LLMs的表述信念（或研究者强加的信念）与模拟角色行为之间存在系统性不一致。这些不一致在个体和群体层面上都可见，即使模型似乎在表面上具备合理信念，也未必能将其一贯地落实到行为中。

Conclusion: 研究强调，科学研究者需要明确LLMs表述的信念与其实际模拟行为的何时何种情况下保持一致，以便为行为学研究正确地使用基于LLMs的合成代理。

Abstract: As LLMs are increasingly studied as role-playing agents to generate synthetic
data for human behavioral research, ensuring that their outputs remain coherent
with their assigned roles has become a critical concern. In this paper, we
investigate how consistently LLM-based role-playing agents' stated beliefs
about the behavior of the people they are asked to role-play ("what they say")
correspond to their actual behavior during role-play ("how they act").
Specifically, we establish an evaluation framework to rigorously measure how
well beliefs obtained by prompting the model can predict simulation outcomes in
advance. Using an augmented version of the GenAgents persona bank and the Trust
Game (a standard economic game used to quantify players' trust and
reciprocity), we introduce a belief-behavior consistency metric to
systematically investigate how it is affected by factors such as: (1) the types
of beliefs we elicit from LLMs, like expected outcomes of simulations versus
task-relevant attributes of individual characters LLMs are asked to simulate;
(2) when and how we present LLMs with relevant information about Trust Game;
and (3) how far into the future we ask the model to forecast its actions. We
also explore how feasible it is to impose a researcher's own theoretical priors
in the event that the originally elicited beliefs are misaligned with research
objectives. Our results reveal systematic inconsistencies between LLMs' stated
(or imposed) beliefs and the outcomes of their role-playing simulation, at both
an individual- and population-level. Specifically, we find that, even when
models appear to encode plausible beliefs, they may fail to apply them in a
consistent way. These findings highlight the need to identify how and when
LLMs' stated beliefs align with their simulated behavior, allowing researchers
to use LLM-based agents appropriately in behavioral studies.

</details>


### [10] [Dilution, Diffusion and Symbiosis in Spatial Prisoner's Dilemma with Reinforcement Learning](https://arxiv.org/abs/2507.02211)
*Gustavo C. Mangold,Heitor C. M. Fernandes,Mendeli H. Vainstein*

Main category: cs.AI

TL;DR: 利用多智能体Q学习框架，研究了空间囚徒困境中个体稀疏分布和移动性对合作行为的影响。发现学习型和固定型规则可表现出类似合作行为，并在多行动设定下观察到群体间互利共生，展现该算法在模型和分析博弈系统方面的强大能力。


<details>
  <summary>Details</summary>
Motivation: 空间囚徒困境博弈长期是研究合作行为演化的重要模型。近年来，加入强化学习机制的研究表明，静态代理（个体）也能通过多种机制学会合作。然而，之前关于稀释（个体稀疏分布）和移动性的影响研究较少，尤其是在多智能体Q学习框架下。

Method: 本文采用独立多智能体Q学习算法，研究了空间囚徒困境博弈中稀释和移动性对系统行为的影响。针对该环境定义了多种可能的学习行动方式，并将这些情形与经典（非强化学习）空间囚徒困境中的结果进行对比。

Result: 发现系统会呈现出多样的行为，包括：在某些条件下，固定更新规则与学习型更新规则能够表现出定性上一致的结果；当定义多种可能行动时，不同群体间会自发形成互利共生的合作现象。

Conclusion: 多智能体Q学习算法不仅可以灵活模拟多样的博弈情景，还能够复现经典演化博弈中的部分现象并揭示更多新颖的合作机制。此方法具备较强的基准测试与建模潜力。

Abstract: Recent studies in the spatial prisoner's dilemma games with reinforcement
learning have shown that static agents can learn to cooperate through a diverse
sort of mechanisms, including noise injection, different types of learning
algorithms and neighbours' payoff knowledge.In this work, using an independent
multi-agent Q-learning algorithm, we study the effects of dilution and mobility
in the spatial version of the prisoner's dilemma. Within this setting,
different possible actions for the algorithm are defined, connecting with
previous results on the classical, non-reinforcement learning spatial
prisoner's dilemma, showcasing the versatility of the algorithm in modeling
different game-theoretical scenarios and the benchmarking potential of this
approach.As a result, a range of effects is observed, including evidence that
games with fixed update rules can be qualitatively equivalent to those with
learned ones, as well as the emergence of a symbiotic mutualistic effect
between populations that forms when multiple actions are defined.

</details>


### [11] [Scaling LLM Planning: NL2FLOW for Parametric Problem Generation and Rigorous Evaluation](https://arxiv.org/abs/2507.02253)
*Jungkoo Kang*

Main category: cs.AI

TL;DR: 作者提出NL2FLOW，自动生成规划问题并系统评测LLM的推理能力。实验证明，无需中间表示直接从自然语言到行动效果更好。该系统有助于揭示和动态理解LLM推理中的瓶颈，对未来复杂推理任务具有重要意义。


<details>
  <summary>Details</summary>
Motivation: 提升大语言模型（LLM）在规划和推理能力上的进展受限于可扩展、可靠的数据生成和评估瓶颈。作者试图解决这一关键障碍。

Method: 提出了NL2FLOW系统，实现了规划问题从自然语言、结构化中间表达到正式PDDL的参数化自动生成，并对生成计划的质量进行严格评估。利用NL2FLOW生成了一个2296个工作流规划实例的数据集，并对多个开源、指令微调的LLM进行评测。还进行了回归分析，探究问题特征对计划生成的影响。

Result: 最优模型在生成有效计划上达到了86%的成功率，在生成最优计划上达到了69%的成功率（针对可行问题）。发现直接用自然语言生成计划的表现优于先翻译为JSON中间表示再生成计划。此外，问题复杂度、模型和prompt设计均影响最终表现。

Conclusion: 通过NL2FLOW提出的系统能更好暴露和理解大模型推理、规划中的瓶颈和误差来源。同时，避免不必要的推理分解（如引入中间表现）能提升模型表现。随着问题复杂度提高，系统性诊断和动态理解瓶颈对于释放LLM智能解题潜力至关重要。

Abstract: Progress in enhancing large language model (LLM) planning and reasoning
capabilities is significantly hampered by the bottleneck of scalable, reliable
data generation and evaluation. To overcome this, I introduce NL2FLOW, a fully
automated system for parametrically generating planning problems - expressed in
natural language, a structured intermediate representation, and formal PDDL -
and rigorously evaluating the quality of generated plans. I demonstrate
NL2FLOW's capabilities by generating a dataset of 2296 problems in the
automated workflow generation domain and evaluating multiple open-sourced,
instruct-tuned LLMs. My results reveal that the highest performing models
achieved 86% success in generating valid plans and 69% in generating optimal
plans, specifically for problems with feasible solutions. Regression analysis
shows that the influence of problem characteristics on plan generation is
contingent on both model and prompt design. Notably, I observed that the
highest success rate for translating natural language into a JSON
representation of a plan was lower than the highest rate of generating a valid
plan directly. This suggests that unnecessarily decomposing the reasoning task
- introducing intermediate translation steps - may actually degrade
performance, implying a benefit to models capable of reasoning directly from
natural language to action. As I scale LLM reasoning to increasingly complex
problems, the bottlenecks and sources of error within these systems will
inevitably shift. Therefore, a dynamic understanding of these limitations - and
the tools to systematically reveal them - will be crucial for unlocking the
full potential of LLMs as intelligent problem solvers.

</details>


### [12] [Iterated belief revision: from postulates to abilities](https://arxiv.org/abs/2507.02319)
*Paolo Liberatore*

Main category: cs.AI

TL;DR: 本文批判性分析了信念修正领域偏重公理约束的局限，提出用实际能实现的信念状态能力作为分类标准，并系统比较了多种主流修正方法在信念可达性上的不同表现，强调了能力分析对于实际应用中的机制选择价值。


<details>
  <summary>Details</summary>
Motivation: 当前信念修正领域虽然有众多新方法被提出，但对已有方法的系统分析却较为稀缺。现有大量研究集中于用公理化的方法对信念修正机制进行描述，但这些公理大多关注约束（即修正机制必须如何做），很少讨论修正机制能达到哪些信念状态。论文作者希望理解不同修正机制面向“可达性”（例如能否达到某些极端或特殊信念状态）的能力。

Method: 该论文通过分析主流信念修正机制（如词汇法、自然修正、受限修正及各种极端、严格修正），从它们能够实现的信念状态“能力”的视角加以对比和分类，而不是停留在被动满足公理约束的层面。作者提出用“能力”来描述机制，如可塑性、均等性、教条性等，然后逐一证明各个修正方式在这些能力中的表现。

Result: 论文发现，不同的修正机制在实现信念状态能力方面各有优劣。有些能达到教条状态、均等状态或从空白状态达到所有信念状态，有些则不行。作者证明了多种主流修正机制在这些能力上是如何各自具备或缺失特定特性。

Conclusion: 单一的公理描述无法全面反映信念修正机制的实际能力，尤其是在信念状态转换可达性上。能力分析为信念修正机制的选择和理解提供了全新视角，有助于为不同实际需求选择合适的信念修正方法。

Abstract: The belief revision field is opulent in new proposals and indigent in
analyses of existing approaches. Much work hinge on postulates, employed as
syntactic characterizations: some revision mechanism is equivalent to some
properties. Postulates constraint specific revision instances: certain
revisions update certain beliefs in a certain way. As an example, if the
revision is consistent with the current beliefs, it is incorporated with no
other change. A postulate like this tells what revisions must do and neglect
what they can do. Can they reach a certain state of beliefs? Can they reach all
possible states of beliefs? Can they reach all possible states of beliefs from
no previous belief? Can they reach a dogmatic state of beliefs, where
everything not believed is impossible? Can they make two conditions equally
believed? An application where every possible state of beliefs is sensible
requires each state of beliefs to be reachable. An application where conditions
may be equally believed requires such a belief state to be reachable. An
application where beliefs may become dogmatic requires a way to make them
dogmatic. Such doxastic states need to be reached in a way or another. Not in
specific way, as dictated by a typical belief revision postulate. This is an
ability, not a constraint: the ability of being plastic, equating, dogmatic.
Amnesic, correcting, believer, damascan, learnable are other abilities. Each
revision mechanism owns some of these abilities and lacks the others:
lexicographic, natural, restrained, very radical, full meet, radical, severe,
moderate severe, deep severe, plain severe and deep severe revisions, each of
these revisions is proved to possess certain abilities.

</details>


### [13] [OMS: On-the-fly, Multi-Objective, Self-Reflective Ad Keyword Generation via LLM Agent](https://arxiv.org/abs/2507.02353)
*Bowen Chen,Zhao Wang,Shingo Takamatsu*

Main category: cs.AI

TL;DR: 为解决广告关键词生成在数据需求、效果监控与质量控制上的不足，作者提出OMS框架，实现了无需训练数据也能多目标优化与自我评估，实验与评测均显示OMS全面优于已有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型（LLM）的关键词生成方法存在数据依赖大、缺乏多目标效果监控与优化、关键词质量把控弱等问题，制约了其在广告关键词全自动化决策中的应用。

Method: 提出OMS框架：无需训练数据、能实时在线监控和自适应优化，多目标驱动（多指标联合优化），具备自我反思能力（对关键词质量自我评估）。

Result: 在基准测试和真实广告场景下，OMS优于现有方法。消融实验和人工评测进一步证明了OMS各模块和生成关键词的有效性。

Conclusion: OMS为广告关键词自动化决策提供了无需大规模数据、能自适应多目标优化且具备高质量把控能力的新方案，在理论与应用上均取得了突出的效果。

Abstract: Keyword decision in Sponsored Search Advertising is critical to the success
of ad campaigns. While LLM-based methods offer automated keyword generation,
they face three major limitations: reliance on large-scale query-keyword pair
data, lack of online multi-objective performance monitoring and optimization,
and weak quality control in keyword selection. These issues hinder the agentic
use of LLMs in fully automating keyword decisions by monitoring and reasoning
over key performance indicators such as impressions, clicks, conversions, and
CTA effectiveness. To overcome these challenges, we propose OMS, a keyword
generation framework that is On-the-fly (requires no training data, monitors
online performance, and adapts accordingly), Multi-objective (employs agentic
reasoning to optimize keywords based on multiple performance metrics), and
Self-reflective (agentically evaluates keyword quality). Experiments on
benchmarks and real-world ad campaigns show that OMS outperforms existing
methods; ablation and human evaluations confirm the effectiveness of each
component and the quality of generated keywords.

</details>


### [14] [An AI-native experimental laboratory for autonomous biomolecular engineering](https://arxiv.org/abs/2507.02379)
*Mingyu Wu,Zhaoguo Wang,Jiabin Wang,Zhiyuan Dong,Jingkai Yang,Qingting Li,Tianyu Huang,Lei Zhao,Mingqiang Li,Fei Wang,Chunhai Fan,Haibo Chen*

Main category: cs.AI

TL;DR: 本文提出并实现了一套AI原生自主实验平台，能在无人干预下高效完成复杂生物分子实验，性能与人类科学家媲美，并提升了资源利用率。该平台有望推动“科学即服务”模式发展。


<details>
  <summary>Details</summary>
Motivation: 实现能够独立进行复杂实验、服务非专业人士的自主科学研究，一直是科学界的长期目标，但现有自主实验系统仅限于目标单一、流程简单的领域。推动高复杂性科学实验的自动化，需要借助人工智能（AI）驱动的根本性范式转变。

Method: 提出并实现了一个AI原生的自主实验室平台，基于模型、实验与仪器协同设计理念，能够自主管理仪器，制定实验特定的流程与优化策略，同时服务多个用户请求，实现AI模型与自动化系统的共同进化。

Result: 该自主实验室支持包括核酸合成、转录、扩增、测序等基本功能，并能应用于疾病诊断、药物开发、信息存储等领域。在无人干预下，平台能够将实验性能自动优化至与顶尖科学家相当。多用户场景下提升了仪器利用率和实验效率。

Conclusion: 该平台为复杂生物材料研究提供了突破性的自主实验能力，突破了对专家和资源的依赖，为规模化“科学即服务”提供了新范例和蓝图。

Abstract: Autonomous scientific research, capable of independently conducting complex
experiments and serving non-specialists, represents a long-held aspiration.
Achieving it requires a fundamental paradigm shift driven by artificial
intelligence (AI). While autonomous experimental systems are emerging, they
remain confined to areas featuring singular objectives and well-defined, simple
experimental workflows, such as chemical synthesis and catalysis. We present an
AI-native autonomous laboratory, targeting highly complex scientific
experiments for applications like autonomous biomolecular engineering. This
system autonomously manages instrumentation, formulates experiment-specific
procedures and optimization heuristics, and concurrently serves multiple user
requests. Founded on a co-design philosophy of models, experiments, and
instruments, the platform supports the co-evolution of AI models and the
automation system. This establishes an end-to-end, multi-user autonomous
laboratory that handles complex, multi-objective experiments across diverse
instrumentation. Our autonomous laboratory supports fundamental nucleic acid
functions-including synthesis, transcription, amplification, and sequencing. It
also enables applications in fields such as disease diagnostics, drug
development, and information storage. Without human intervention, it
autonomously optimizes experimental performance to match state-of-the-art
results achieved by human scientists. In multi-user scenarios, the platform
significantly improves instrument utilization and experimental efficiency. This
platform paves the way for advanced biomaterials research to overcome
dependencies on experts and resource barriers, establishing a blueprint for
science-as-a-service at scale.

</details>


### [15] [The Gauss-Markov Adjunction: Categorical Semantics of Residuals in Supervised Learning](https://arxiv.org/abs/2507.02442)
*Moto Kamiura*

Main category: cs.AI

TL;DR: 本文通过范畴论对多元线性回归模型进行结构化分析，提出高斯-马尔可夫伴随性，形式化描述参数与残差的关系，为AI解释性提供理论支持。


<details>
  <summary>Details</summary>
Motivation: 提高机器学习的可理解性和可解释性，以回应AI可解释性原则的需求，并推动AI更好地应用于社会。

Method: 通过范畴论视角重新构建机器学习模型，提出一种用于结构化和理解AI系统的语义框架。具体地，定义参数和数据对应的两个范畴类别，并在它们之间引入伴随函子对，形成对监督学习（尤其是多元线性回归模型）的范畴论刻画。提出并证明高斯-马尔可夫伴随性（Gauss-Markov Adjunction），深入分析参数与残差的结构关系。

Result: 构建出将多元线性回归的参数与残差通过伴随函子联系起来的范畴结构，该结构清晰描述了参数与残差间的信息流动。普通最小二乘估计器和最小残差通过右伴随函子的极限保持性质关联。将该范畴论视角定位为监督学习的扩展指称语义实例，提出将理论计算机科学中的语义方法应用于AI解释性的基础研究。

Conclusion: 范畴论为监督学习（多元线性回归）中的参数和残差之间的结构关系提供了清晰、形式化的语义框架，可作为推动AI可解释性的理论基础。

Abstract: Enhancing the intelligibility and interpretability of machine learning is a
crucial task in responding to the demand for Explicability as an AI principle,
and in promoting the better social implementation of AI. The aim of our
research is to contribute to this improvement by reformulating machine learning
models through the lens of category theory, thereby developing a semantic
framework for structuring and understanding AI systems. Our categorical
modeling in this paper clarifies and formalizes the structural interplay
between residuals and parameters in supervised learning. The present paper
focuses on the multiple linear regression model, which represents the most
basic form of supervised learning. By defining two concrete categories
corresponding to parameters and data, along with an adjoint pair of functors
between them, we introduce our categorical formulation of supervised learning.
We show that the essential structure of this framework is captured by what we
call the Gauss-Markov Adjunction. Within this setting, the dual flow of
information can be explicitly described as a correspondence between variations
in parameters and residuals. The ordinary least squares estimator for the
parameters and the minimum residual are related via the preservation of limits
by the right adjoint functor. Furthermore, we position this formulation as an
instance of extended denotational semantics for supervised learning, and
propose applying a semantic perspective developed in theoretical computer
science as a formal foundation for Explicability in AI.

</details>


### [16] [Clarifying Before Reasoning: A Coq Prover with Structural Context](https://arxiv.org/abs/2507.02541)
*Yanzhen Lu,Hanbin Yang,Xiaodie Wang,Ge Zhang,Biao Li,Chenxu Fu,Chao Li,Yang Yuan,Andrew Chi-Chih Yao*

Main category: cs.AI

TL;DR: 本研究通过结构化任务描述和选择性概念展开显著提升了LLM在Coq定理证明领域的推理能力，进一步证明了结构化表达对于复杂任务理解与推理的重要价值。


<details>
  <summary>Details</summary>
Motivation: 提升大语言模型（LLMs）在Coq定理证明任务中的推理能力，特别关注任务描述的清晰度对推理表现的影响。

Method: 引入了概念级别的任务清晰度评估指标。通过为现有LLMs输入添加结构化语义上下文，实现任务描述的结构化，并结合选择性概念展开及“规划者-执行者”架构。采用与Graph2Tac相同的评估协议，对15个标准Coq包中随机采样的1386个定理进行实验。

Result: 使用结构化语义上下文后，任务清晰度得分提升了1.85倍（从44.5%到82.3%）。在DeepSeek-V3模型上，定理证明成功率提升了2.1倍（从21.8%到45.8%），并优于SOTA的Graph2Tac（33.2%）。基于结构化数据微调的小模型进一步提升效果（成功率达48.6%）。

Conclusion: 为LLM输入添加结构化任务表达对提升模型的理解与推理能力具有显著作用，结构化任务描述能够缩小模型理解与推理之间的差距。

Abstract: In this work, we investigate whether improving task clarity can enhance
reasoning ability of large language models, focusing on theorem proving in Coq.
We introduce a concept-level metric to evaluate task clarity and show that
adding structured semantic context to the standard input used by modern LLMs,
leads to a 1.85$\times$ improvement in clarity score
(44.5\%~$\rightarrow$~82.3\%). Using the general-purpose model
\texttt{DeepSeek-V3}, our approach leads to a 2.1$\times$ improvement in proof
success (21.8\%~$\rightarrow$~45.8\%) and outperforms the previous
state-of-the-art \texttt{Graph2Tac} (33.2\%). We evaluate this on 1,386
theorems randomly sampled from 15 standard Coq packages, following the same
evaluation protocol as \texttt{Graph2Tac}. Furthermore, fine-tuning smaller
models on our structured data can achieve even higher performance (48.6\%). Our
method uses selective concept unfolding to enrich task descriptions, and
employs a Planner--Executor architecture. These findings highlight the value of
structured task representations in bridging the gap between understanding and
reasoning.

</details>


### [17] [AI Research Agents for Machine Learning: Search, Exploration, and Generalization in MLE-bench](https://arxiv.org/abs/2507.02554)
*Edan Toledo,Karen Hambardzumyan,Martin Josifoski,Rishi Hazra,Nicolas Baldwin,Alexis Audran-Reiss,Michael Kuchnik,Despoina Magka,Minqi Jiang,Alisia Maria Lupidi,Andrei Lupu,Roberta Raileanu,Kelvin Niu,Tatiana Shavrina,Jean-Christophe Gagnon-Audet,Michael Shvartsman,Shagun Sodhani,Alexander H. Miller,Abhishek Charnalia,Derek Dunfield,Carole-Jean Wu,Pontus Stenetorp,Nicola Cancedda,Jakob Nicolaus Foerster,Yoram Bachrach*

Main category: cs.AI

TL;DR: 本研究通过系统比较搜索策略与操作符集合的组合，显著提升了AI代理在Kaggle机器学习任务中的表现，强调了联合设计这两者的重要性。


<details>
  <summary>Details</summary>
Motivation: 自动化机器学习模型设计、实现和训练有望加速科学进步，但在实际问题如Kaggle竞赛中的性能提升仍具挑战性。作者希望探索如何提升AI研究代理在真实机器学习场景下的表现。

Method: 作者将AI研究代理形式化为在候选解空间中迭代搜索的策略，通过不同的操作符集合及搜索策略（如贪心、MCTS、进化算法）系统化地进行组合与变动，评估其对性能的影响。

Result: 最佳的搜索策略与操作符集合组合在MLE-bench lite基准测试中将Kaggle奖牌的成功率从39.6%提升到47.7%，实现了当前最优的结果。

Conclusion: 搜索策略与操作符设计的结合对提升自动化机器学习系统的性能至关重要，未来相关研究应同步考虑搜索策略、操作机制和评估方法。

Abstract: AI research agents are demonstrating great potential to accelerate scientific
progress by automating the design, implementation, and training of machine
learning models. We focus on methods for improving agents' performance on
MLE-bench, a challenging benchmark where agents compete in Kaggle competitions
to solve real-world machine learning problems. We formalize AI research agents
as search policies that navigate a space of candidate solutions, iteratively
modifying them using operators. By designing and systematically varying
different operator sets and search policies (Greedy, MCTS, Evolutionary), we
show that their interplay is critical for achieving high performance. Our best
pairing of search strategy and operator set achieves a state-of-the-art result
on MLE-bench lite, increasing the success rate of achieving a Kaggle medal from
39.6% to 47.7%. Our investigation underscores the importance of jointly
considering the search strategy, operator design, and evaluation methodology in
advancing automated machine learning.

</details>


### [18] [Responsibility Gap and Diffusion in Sequential Decision-Making Mechanisms](https://arxiv.org/abs/2507.02582)
*Junli Jiang,Pavel Naumov*

Main category: cs.AI

TL;DR: 本文分析了集体决策责任在AI中的两个重要计算属性：扩散和间隙。结果显示，相关机制判断问题在多项式层级处于较高复杂度，突显了责任分配问题在自动化系统设计中的挑战性。


<details>
  <summary>Details</summary>
Motivation: 责任一直是法律和哲学领域关注的主题，近年来也成为人工智能领域的研究重点。本文关注于集体决策中责任的两个重要属性：扩散（diffusion）和间隙（gap），探索这些属性背后的计算复杂性。

Method: 本文采用计算复杂性理论的方法，分析“无扩散”（diffusion-free）和“无间隙”（gap-free）决策机制所属复杂性类别，通过理论证明揭示其在计算复杂性层级中的归属。

Result: 研究结果表明，扩散自由的决策机制集合是$
          2$-完全，而间隙自由的机制集合是$ a   c        3$-完全。这两类机制交集的复杂性依然是$ a   c        2$-完全。

Conclusion: 集体决策中责任的扩散和间隙属性对应的机制集合具有高度的计算复杂性，不同属性集合及其交集分别位于多项式层级的不同层次。此发现为理解AI决策机制中的责任分配提供了理论基础。

Abstract: Responsibility has long been a subject of study in law and philosophy. More
recently, it became a focus of AI literature. The article investigates the
computational complexity of two important properties of responsibility in
collective decision-making: diffusion and gap. It shows that the sets of
diffusion-free and gap-free decision-making mechanisms are $\Pi_2$-complete and
$\Pi_3$-complete, respectively. At the same time, the intersection of these
classes is $\Pi_2$-complete.

</details>


### [19] [DynamiCare: A Dynamic Multi-Agent Framework for Interactive and Open-Ended Medical Decision-Making](https://arxiv.org/abs/2507.02616)
*Tianqi Shang,Weiqing He,Charles Zheng,Lingyao Li,Li Shen,Bingxin Zhao*

Main category: cs.AI

TL;DR: 本文提出并验证了DynamiCare动态多智能体诊断框架及MIMIC-Patient数据集，实现了更真实的多轮交互式AI临床决策，为医疗智能体研究提供了新基准。


<details>
  <summary>Details</summary>
Motivation: 目前基于大语言模型（LLM）的医疗AI主要仿真单轮诊疗任务，与实际迭代、不确定且交互性强的医疗诊断过程存在差距。为更好模拟真实诊疗流程，需要新的动态、多轮交互的研究框架和数据集。

Method: 构建了MIMIC-Patient这个基于MIMIC-III电子健康档案的结构化数据集，支持动态、病人级别的仿真。在此基础上，提出DynamiCare动态多智能体框架，将临床诊断建模为多轮、交互式循环，由多名专家智能体不断提问、整合新信息并动态调整团队构成及决策策略。

Result: 通过大量实验，展示了DynamiCare框架的可行性和有效性，并首次建立了以LLM为核心的动态临床决策基准。

Conclusion: DynamiCare和MIMIC-Patient显著提升了医疗AI在动态、多轮临床决策中的表现，为未来更贴合实际医疗场景的AI研究奠定了基础。

Abstract: The rise of Large Language Models (LLMs) has enabled the development of
specialized AI agents with domain-specific reasoning and interaction
capabilities, particularly in healthcare. While recent frameworks simulate
medical decision-making, they largely focus on single-turn tasks where a doctor
agent receives full case information upfront -- diverging from the real-world
diagnostic process, which is inherently uncertain, interactive, and iterative.
In this paper, we introduce MIMIC-Patient, a structured dataset built from the
MIMIC-III electronic health records (EHRs), designed to support dynamic,
patient-level simulations. Building on this, we propose DynamiCare, a novel
dynamic multi-agent framework that models clinical diagnosis as a multi-round,
interactive loop, where a team of specialist agents iteratively queries the
patient system, integrates new information, and dynamically adapts its
composition and strategy. We demonstrate the feasibility and effectiveness of
DynamiCare through extensive experiments, establishing the first benchmark for
dynamic clinical decision-making with LLM-powered agents.

</details>


### [20] [Strategic Intelligence in Large Language Models: Evidence from evolutionary Game Theory](https://arxiv.org/abs/2507.02618)
*Kenneth Payne,Baptiste Alloui-Cros*

Main category: cs.AI

TL;DR: 本研究将主流大语言模型置于演化型迭代囚徒困境博弈中，发现其展现高度竞争性和明显不同的决策风格，且推理能力对其决策具有决定性影响。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型（LLMs）是否具备战略智能，能够在竞争环境中对目标进行推理。迭代囚徒困境（IPD）是理解决策制定的重要模型，该研究旨在检验LLMs在该复杂博弈中的表现及其推理能力。

Method: 通过设计演化型迭代囚徒困境锦标赛，将经典策略（如以牙还牙、残酷触发器）和来自OpenAI、Google、Anthropic等顶尖AI公司的LLMs进行对抗，并通过调整博弈终止概率以增加复杂度和不确定性。

Result: 结果显示LLMs在复杂环境下具有高度竞争力，能够持续生存甚至繁衍。各家模型表现出独特且持续的战略特征：Google Gemini模型策略激进、善于利用对手且迅速报复；OpenAI模型极度合作，导致在敌对环境中表现不佳；Anthropic Claude模型极为宽容，倾向于恢复合作。分析近32000个模型生成的推理文本显示，它们对博弈周期和对手策略进行积极推理，这些推理与其决策密切相关。

Conclusion: LLMs不仅能在复杂决策博弈中存活并展现独特策略指纹，其推理过程在算法决策中起着关键作用。这一发现连接了经典博弈论与机器心理学，为理解AI不确定性下的决策提供了新视角。

Abstract: Are Large Language Models (LLMs) a new form of strategic intelligence, able
to reason about goals in competitive settings? We present compelling supporting
evidence. The Iterated Prisoner's Dilemma (IPD) has long served as a model for
studying decision-making. We conduct the first ever series of evolutionary IPD
tournaments, pitting canonical strategies (e.g., Tit-for-Tat, Grim Trigger)
against agents from the leading frontier AI companies OpenAI, Google, and
Anthropic. By varying the termination probability in each tournament (the
"shadow of the future"), we introduce complexity and chance, confounding
memorisation.
  Our results show that LLMs are highly competitive, consistently surviving and
sometimes even proliferating in these complex ecosystems. Furthermore, they
exhibit distinctive and persistent "strategic fingerprints": Google's Gemini
models proved strategically ruthless, exploiting cooperative opponents and
retaliating against defectors, while OpenAI's models remained highly
cooperative, a trait that proved catastrophic in hostile environments.
Anthropic's Claude emerged as the most forgiving reciprocator, showing
remarkable willingness to restore cooperation even after being exploited or
successfully defecting. Analysis of nearly 32,000 prose rationales provided by
the models reveals that they actively reason about both the time horizon and
their opponent's likely strategy, and we demonstrate that this reasoning is
instrumental to their decisions. This work connects classic game theory with
machine psychology, offering a rich and granular view of algorithmic
decision-making under uncertainty.

</details>


### [21] [Decoupled Planning and Execution: A Hierarchical Reasoning Framework for Deep Search](https://arxiv.org/abs/2507.02652)
*Jiajie Jin,Xiaoxi Li,Guanting Dong,Yuyao Zhang,Yutao Zhu,Yang Zhao,Hongjin Qian,Zhicheng Dou*

Main category: cs.AI

TL;DR: 本文提出一种分层搜索框架HiRA，用高层规划agent协调多个专业化子任务，在复杂检索场景下优于现有方法，实现更高答案质量和效率。


<details>
  <summary>Details</summary>
Motivation: 现实中的复杂信息需求需要跨多个信息源进行深度推理和知识综合，而现有RAG（检索增强生成）方法在此方面表现有限。原因在于传统方法往往用单一模型既负责整体规划又负责具体执行，带来推理效率低下和可扩展性差的问题。

Method: 提出HiRA框架，将战略性高层规划与专业化细节执行剥离开来。该方法将复杂搜索任务分解为多个子任务，每个子任务由配备外部工具和推理能力的领域专属agent负责，并通过结构化机制整合结果，实现高效协调。

Result: 在四项复杂跨模态深度搜索基准任务上，HiRA明显优于当前最好的RAG和基于agent的系统，提升了答案质量和系统效率。

Conclusion: 将高层规划与具体执行解耦，对多步信息检索任务来说能够显著提升推理质量和系统效率，HiRA为复杂搜索问题提供了更有效的解决思路和实现。

Abstract: Complex information needs in real-world search scenarios demand deep
reasoning and knowledge synthesis across diverse sources, which traditional
retrieval-augmented generation (RAG) pipelines struggle to address effectively.
Current reasoning-based approaches suffer from a fundamental limitation: they
use a single model to handle both high-level planning and detailed execution,
leading to inefficient reasoning and limited scalability. In this paper, we
introduce HiRA, a hierarchical framework that separates strategic planning from
specialized execution. Our approach decomposes complex search tasks into
focused subtasks, assigns each subtask to domain-specific agents equipped with
external tools and reasoning capabilities, and coordinates the results through
a structured integration mechanism. This separation prevents execution details
from disrupting high-level reasoning while enabling the system to leverage
specialized expertise for different types of information processing.
Experiments on four complex, cross-modal deep search benchmarks demonstrate
that HiRA significantly outperforms state-of-the-art RAG and agent-based
systems. Our results show improvements in both answer quality and system
efficiency, highlighting the effectiveness of decoupled planning and execution
for multi-step information seeking tasks. Our code is available at
https://github.com/ignorejjj/HiRA.

</details>


### [22] [Hey AI, Generate Me a Hardware Code! Agentic AI-based Hardware Design & Verification](https://arxiv.org/abs/2507.02660)
*Deepak Narayan Gadde,Keerthan Kopparam Radhakrishna,Vaisakh Naduvodi Viswambharan,Aman Kumar,Djones Lettnin,Wolfgang Kunz,Sebastian Simon*

Main category: cs.AI

TL;DR: 本文提出融合大模型与人类协作的代理式AI方法用于硬件设计验证，在多个开源设计上表现优异，大幅提升效率与覆盖，验证效果更佳。


<details>
  <summary>Details</summary>
Motivation: 随着集成电路(IC)日益复杂，硬件设计验证变得更加耗时耗力，需要更高效的方法来确保设计无误。

Method: 提出基于Agent的AI方法，将AI代理与人类在环（HITL）结合，进行动态、迭代、自反的硬件设计与验证全过程。

Result: 在五个开源设计上评估该方法，实现了超过95%的覆盖率，缩短了验证时间，且展现了更好的性能、适应性和可配置性。

Conclusion: Agentic AI结合人类参与的方法能有效提升硬件设计验证的效率和表现，是现代IC验证的有力工具。

Abstract: Modern Integrated Circuits (ICs) are becoming increasingly complex, and so is
their development process. Hardware design verification entails a methodical
and disciplined approach to the planning, development, execution, and sign-off
of functionally correct hardware designs. This tedious process requires
significant effort and time to ensure a bug-free tape-out. The field of Natural
Language Processing has undergone a significant transformation with the advent
of Large Language Models (LLMs). These powerful models, often referred to as
Generative AI (GenAI), have revolutionized how machines understand and generate
human language, enabling unprecedented advancements in a wide array of
applications, including hardware design verification. This paper presents an
agentic AI-based approach to hardware design verification, which empowers AI
agents, in collaboration with Humain-in-the-Loop (HITL) intervention, to engage
in a more dynamic, iterative, and self-reflective process, ultimately
performing end-to-end hardware design and verification. This methodology is
evaluated on five open-source designs, achieving over 95% coverage with reduced
verification time while demonstrating superior performance, adaptability, and
configurability.

</details>


### [23] [Think How to Think: Mitigating Overthinking with Autonomous Difficulty Cognition in Large Reasoning Models](https://arxiv.org/abs/2507.02663)
*Yongjiang Liu,Haoxi Li,Xiaosong Ma,Jie Zhang,Song Guo*

Main category: cs.AI

TL;DR: 本文提出TH2T两阶段微调策略，通过引导模型感知任务难度并减少冗余推理，显著优化大模型推理效率与结果质量。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型虽然推理能力强，但易产生“过度推理”（overthinking），往往对不同难度的任务采用一刀切流程，导致效率低下且出现不必要冗余。论文发现模型难以像人一样首先识别题目难度，缺乏任务感知。提出解决办法提升模型推理流程的灵活性和高效性。

Method: 提出Think-How-to-Think（TH2T）两阶段微调策略：阶段一引入难度催眠提示，引导模型提升任务难度感知并采用差异化推理策略；阶段二通过冗余催眠提示，使模型识别推理过程中的冗余结构，产出更精练的推理输出。方法结合异构短/长推理数据集协同训练。

Result: 在7B/14B/32B规模模型上的实验表明，TH2T方法在保持性能稳定的同时，降低了推理成本：在简单任务上的推理成本减少70%以上，难任务减少40%。输出结果更加明确地体现对任务难度的认知，冗余显著减少。

Conclusion: TH2T方法可以显著减少LRMs的推理冗余与过度推理现象，同时保持模型性能的稳定性。最终模型生成的推理输出更具难度感知能力，且冗余内容明显减少。

Abstract: Recent Long Reasoning Models(LRMs) have demonstrated remarkable capabilities
in handling complex reasoning tasks, but are hindered by excessive
overthinking. To explore its essence, our empirical analysis reveals that LRMs
are primarily limited to recognizing task properties (i.e., difficulty levels)
like humans before solving the problem, leading to a one-size-fits-all
reasoning process. Inspired by this, a pressing and natural question emerges:
Can we bootstrap such ability to further alleviate the overthinking phenomenon
in LRMs? In this paper, we propose Think-How-to-Think (TH2T), a novel two-stage
fine-tuning strategy that progressively inspires LRMs' difficulty cognition and
redundancy cognition. First, we introduce difficulty-hypnosis in the prefixes
of model outputs to intervene in the internal reasoning trajectory. Combined
with a heterogeneous short and long reasoning dataset, the trained model
enhances its sensitivity to task difficulty, enabling native, differentiated
reasoning strategies across various tasks. Second, we further extend
redundancy-hypnosis to the internal reasoning process, guiding the model to
identify redundant structures within the reasoning steps and generate more
concise reasoning outputs. Experiments on 7B/14B/32B models demonstrate that
TH2T significantly reduces inference costs (more than 70% on easy tasks and 40%
on hard tasks) while maintaining performance stability. The resulting outputs
exhibit clear difficulty-aware capabilities and reduced redundancy (e.g.,
reflection).

</details>


### [24] [Detection of Disengagement from Voluntary Quizzes: An Explainable Machine Learning Approach in Higher Distance Education](https://arxiv.org/abs/2507.02681)
*Behnam Parsaeifard,Christof Imhof,Tansu Pancar,Ioan-Sorin Comsa,Martin Hlosta,Nicole Bergamin,Per Bergamin*

Main category: cs.AI

TL;DR: 该论文通过分析远程大学学生在非强制性测验中的行为数据，利用可解释机器学习方法，有效检测出学习脱离学生，并为后续干预措施提供了参考，模型准确率高达91%。


<details>
  <summary>Details</summary>
Motivation: 学生在远程教育中脱离学习任务可能导致严重的长期后果，如学业中断。因此，需要更好地检测学生的学习脱离情况，以便进行及时干预。

Method: 本研究分析了某远程大学四个学期42门课程中，学生在非强制性小测数据的参与情况。作者从Moodle平台中筛选并处理出最具信息量的学生日志数据，然后训练并比较了8种机器学习算法，最终结合SHAP方法实现了可解释机器学习框架。

Result: 实验结果显示，模型取得了91%的平衡准确率，成功识别出约85%的脱离学生。

Conclusion: 高效且可解释的机器学习方法能够准确检测学生在网络课程中对非强制性任务的脱离情况，这有助于针对性地设计干预措施，减少学生脱离。

Abstract: Students disengaging from their tasks can have serious long-term
consequences, including academic drop-out. This is particularly relevant for
students in distance education. One way to measure the level of disengagement
in distance education is to observe participation in non-mandatory exercises in
different online courses. In this paper, we detect student disengagement in the
non-mandatory quizzes of 42 courses in four semesters from a distance-based
university. We carefully identified the most informative student log data that
could be extracted and processed from Moodle. Then, eight machine learning
algorithms were trained and compared to obtain the highest possible prediction
accuracy. Using the SHAP method, we developed an explainable machine learning
framework that allows practitioners to better understand the decisions of the
trained algorithm. The experimental results show a balanced accuracy of 91\%,
where about 85\% of disengaged students were correctly detected. On top of the
highly predictive performance and explainable framework, we provide a
discussion on how to design a timely intervention to minimise disengagement
from voluntary tasks in online learning.

</details>


### [25] [Time-critical and confidence-based abstraction dropping methods](https://arxiv.org/abs/2507.02703)
*Robin Schmöcker,Lennart Kampmann,Alexander Dockhorn*

Main category: cs.AI

TL;DR: 本文提出了两种安全高效的MCTS抽象丢弃方法（OGA-IAAD和OGA-CAD），能够在保证性能无明显下降的前提下，进一步提升搜索效率。


<details>
  <summary>Details</summary>
Motivation: 在蒙特卡洛树搜索（MCTS）中，利用状态和/或动作抽象可以提升搜索效率，但非精确抽象会引入近似误差，导致无法保证在抽象空间内收敛到最优动作。因此，需要设计有效的抽象丢弃策略来处理该问题。

Method: 本文提出两种新的抽象丢弃方案：OGA-IAAD（适用于对时间要求较高的情形）和OGA-CAD（旨在在相同迭代次数下提升MCTS性能）。相比之前的方法，这两种方案更安全，不会带来显著性能下降。

Result: 实验结果显示，OGA-IAAD和OGA-CAD方案能在提升性能的同时，避免了原有方法在丢弃抽象时可能导致的性能下降。

Conclusion: 文中提出的两种MCTS抽象丢弃新方案在保证安全性的前提下，实现了更优性能表现，克服了以往方法在抽象丢弃阶段的不稳定性。

Abstract: One paradigm of Monte Carlo Tree Search (MCTS) improvements is to build and
use state and/or action abstractions during the tree search. Non-exact
abstractions, however, introduce an approximation error making convergence to
the optimal action in the abstract space impossible. Hence, as proposed as a
component of Elastic Monte Carlo Tree Search by Xu et al., abstraction
algorithms should eventually drop the abstraction. In this paper, we propose
two novel abstraction dropping schemes, namely OGA-IAAD and OGA-CAD which can
yield clear performance improvements whilst being safe in the sense that the
dropping never causes any notable performance degradations contrary to Xu's
dropping method. OGA-IAAD is designed for time critical settings while OGA-CAD
is designed to improve the MCTS performance with the same number of iterations.

</details>


### [26] [Bourbaki: Self-Generated and Goal-Conditioned MDPs for Theorem Proving](https://arxiv.org/abs/2507.02726)
*Matthieu Zimmer,Xiaotong Ji,Rasul Tutunov,Anthony Bordg,Jun Wang,Haitham Bou Ammar*

Main category: cs.AI

TL;DR: 本文提出了sG-MDPs新框架，并用MCTS算法和多个7B LLM模块实现，将自动定理证明中复杂推理任务分解为可管理的子目标，在PutnamBench上创造了新的7B规模模型最佳成绩。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在自动定理证明（ATP）等逻辑高度约束的环境下推理能力有限，面临奖励稀疏和证明空间巨大的挑战，尤其是在像PutnamBench这样的高难度基准中更为明显。

Method: 提出了一种自生成目标条件下的马尔可夫决策过程（sG-MDPs）新框架，让智能体根据证明过程的演化状态自主生成并追求子目标，以结构化分解问题；随后利用类似蒙特卡洛树搜索（MCTS）的算法求解该问题，并在Bourbaki (7B)系统中实现，该系统可集成多个7B LLM进行子目标生成和证明策略合成。

Result: Bourbaki (7B)系统在PutnamBench基准下成功解决了26个问题，刷新了同等规模模型的最新最佳成绩。

Conclusion: 通过结构化的子目标生成与MCTS方法的结合，有效提升了中等规模LLM系统在复杂自动定理证明任务中的推理与解决能力。

Abstract: Reasoning remains a challenging task for large language models (LLMs),
especially within the logically constrained environment of automated theorem
proving (ATP), due to sparse rewards and the vast scale of proofs. These
challenges are amplified in benchmarks like PutnamBench, which contains
university-level problems requiring complex, multi-step reasoning. To address
this, we introduce self-generated goal-conditioned MDPs (sG-MDPs), a new
framework in which agents generate and pursue their subgoals based on the
evolving proof state. Given this more structured generation of goals, the
resulting problem becomes more amenable to search. We then apply Monte Carlo
Tree Search (MCTS)-like algorithms to solve the sG-MDP, instantiating our
approach in Bourbaki (7B), a modular system that can ensemble multiple 7B LLMs
for subgoal generation and tactic synthesis. On PutnamBench, Bourbaki (7B)
solves 26 problems, achieving new state-of-the-art results with models at this
scale.

</details>


### [27] [Knowledge Protocol Engineering: A New Paradigm for AI in Domain-Specific Knowledge Work](https://arxiv.org/abs/2507.02760)
*Guangwei Zhang*

Main category: cs.AI

TL;DR: 本文提出知识协议工程（KPE），系统性地将专家知识转化为机器可执行的协议，使LLM能有效解决复杂专家领域任务，被视为推动人机协作的重要方法。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLM）已能处理复杂的专业知识，但现有方法如RAG和通用智能体AI在需要深度程序化和方法论推理的专家领域任务上表现不佳。RAG虽能提供事实支持，但不能传达逻辑框架；而缺乏领域启发式的智能体往往效率低且不可预测。

Method: 本文提出了一种新范式——知识协议工程（Knowledge Protocol Engineering, KPE），旨在将专家领域的知识（通常表述为自然语言文件）系统性地转化为可由机器执行的知识协议（Knowledge Protocol, KP）。KPE强调让LLM具备领域本身的逻辑和策略，而不仅仅是信息查找。

Result: KPE通过构建完善的知识协议，使通用型LLM能够成为特定领域专家，不仅能分解抽象查询，还能执行复杂的多步任务。

Conclusion: KPE区分于现有方法，被认为是未来人机协作的基础方法之一，在法律和生物信息学等多个领域具有广泛应用前景。

Abstract: The capabilities of Large Language Models (LLMs) have opened new frontiers
for interacting with complex, domain-specific knowledge. However, prevailing
methods like Retrieval-Augmented Generation (RAG) and general-purpose Agentic
AI, while powerful, often struggle with tasks that demand deep, procedural, and
methodological reasoning inherent to expert domains. RAG provides factual
context but fails to convey logical frameworks; autonomous agents can be
inefficient and unpredictable without domain-specific heuristics. To bridge
this gap, we introduce Knowledge Protocol Engineering (KPE), a new paradigm
focused on systematically translating human expert knowledge, often expressed
in natural language documents, into a machine-executable Knowledge Protocol
(KP). KPE shifts the focus from merely augmenting LLMs with fragmented
information to endowing them with a domain's intrinsic logic, operational
strategies, and methodological principles. We argue that a well-engineered
Knowledge Protocol allows a generalist LLM to function as a specialist, capable
of decomposing abstract queries and executing complex, multi-step tasks. This
position paper defines the core principles of KPE, differentiates it from
related concepts, and illustrates its potential applicability across diverse
fields such as law and bioinformatics, positing it as a foundational
methodology for the future of human-AI collaboration.

</details>


### [28] [Grounding Intelligence in Movement](https://arxiv.org/abs/2507.02771)
*Melanie Segado,Felipe Parodi,Jordan K. Matelsky,Michael L. Platt,Eva B. Dyer,Konrad P. Kording*

Main category: cs.AI

TL;DR: 论文呼吁关注运动数据的统一建模，认为将运动作为AI的核心建模对象，能够推动生成、控制等方向发展，并增进对自然和人工行为理解。


<details>
  <summary>Details</summary>
Motivation: 虽然机器学习在建模语言、视觉等高维数据上取得了突破，但对运动这一生物系统的核心属性处理不足，现有做法通常为任务或领域所限，忽视了运动的普适性和结构性。

Method: 论文主要通过理论分析和跨学科综述，提出将运动作为AI建模核心的新范式，并比较运动与其它高维数据（如语言、视觉）的不同建模策略。

Result: 将基于运动的建模上升为AI核心方向，有望促进生成模型、控制等能力提升，同时为理解生物和人工系统的行为提供统一基础。

Conclusion: 运动作为决策和智能的重要窗口，应该被AI作为主要建模目标，其结构性和物理基础为理解智能系统如何与世界互动提供了基础。

Abstract: Recent advances in machine learning have dramatically improved our ability to
model language, vision, and other high-dimensional data, yet they continue to
struggle with one of the most fundamental aspects of biological systems:
movement. Across neuroscience, medicine, robotics, and ethology, movement is
essential for interpreting behavior, predicting intent, and enabling
interaction. Despite its core significance in our intelligence, movement is
often treated as an afterthought rather than as a rich and structured modality
in its own right. This reflects a deeper fragmentation in how movement data is
collected and modeled, often constrained by task-specific goals and
domain-specific assumptions. But movement is not domain-bound. It reflects
shared physical constraints, conserved morphological structures, and purposeful
dynamics that cut across species and settings. We argue that movement should be
treated as a primary modeling target for AI. It is inherently structured and
grounded in embodiment and physics. This structure, often allowing for compact,
lower-dimensional representations (e.g., pose), makes it more interpretable and
computationally tractable to model than raw, high-dimensional sensory inputs.
Developing models that can learn from and generalize across diverse movement
data will not only advance core capabilities in generative modeling and
control, but also create a shared foundation for understanding behavior across
biological and artificial systems. Movement is not just an outcome, it is a
window into how intelligent systems engage with the world.

</details>


### [29] [KERAP: A Knowledge-Enhanced Reasoning Approach for Accurate Zero-shot Diagnosis Prediction Using Multi-agent LLMs](https://arxiv.org/abs/2507.02773)
*Yuzhang Xie,Hejie Cui,Ziyang Zhang,Jiaying Lu,Kai Shu,Fadi Nahab,Xiao Hu,Carl Yang*

Main category: cs.AI

TL;DR: 本文提出KERAP框架，将知识图谱注入大语言模型诊断流程，采用多智能体模块，有效提升了零样本医学诊断的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有医学诊断预测的机器学习模型需要大量有标签数据，难以泛化到新情况。而大型语言模型尽管展现出潜力，却存在虚假生成、缺乏结构化推理等问题，影响诊断有效性。

Method: 提出了KERAP，一种结合知识图谱增强推理能力的方法，采用多智能体架构，包括属性映射、结构化知识检索及多轮迭代的诊断预测。

Result: KERAP实验结果显示，其有效提升了诊断的可靠性，并具备可扩展性和可解释性，适用于零样本医学诊断预测。

Conclusion: 通过知识图谱赋能和多智能体协作，提升了大模型在医疗诊断中的表现，为精准医疗提供了新思路。

Abstract: Medical diagnosis prediction plays a critical role in disease detection and
personalized healthcare. While machine learning (ML) models have been widely
adopted for this task, their reliance on supervised training limits their
ability to generalize to unseen cases, particularly given the high cost of
acquiring large, labeled datasets. Large language models (LLMs) have shown
promise in leveraging language abilities and biomedical knowledge for diagnosis
prediction. However, they often suffer from hallucinations, lack structured
medical reasoning, and produce useless outputs. To address these challenges, we
propose KERAP, a knowledge graph (KG)-enhanced reasoning approach that improves
LLM-based diagnosis prediction through a multi-agent architecture. Our
framework consists of a linkage agent for attribute mapping, a retrieval agent
for structured knowledge extraction, and a prediction agent that iteratively
refines diagnosis predictions. Experimental results demonstrate that KERAP
enhances diagnostic reliability efficiently, offering a scalable and
interpretable solution for zero-shot medical diagnosis prediction.

</details>


### [30] [Establishing Best Practices for Building Rigorous Agentic Benchmarks](https://arxiv.org/abs/2507.02825)
*Yuxuan Zhu,Tengjun Jin,Yada Pruksachatkun,Andy Zhang,Shu Liu,Sasha Cui,Sayash Kapoor,Shayne Longpre,Kevin Meng,Rebecca Weiss,Fazl Barez,Rahul Gupta,Jwala Dhamala,Jacob Merizian,Mario Giulianelli,Harry Coppock,Cozmin Ududec,Jasjeet Sekhon,Jacob Steinhardt,Antony Kellerman,Sarah Schwettmann,Matei Zaharia,Ion Stoica,Percy Liang,Daniel Kang*

Main category: cs.AI

TL;DR: 本文发现当前许多AI代理评估基准存在设计瑕疵，会造成性能评估严重失真。作者基于实践和调研，总结出一套评估清单（ABC指南），并验证其能够有效提升评估准确性。


<details>
  <summary>Details</summary>
Motivation: 随着AI代理越来越强，大量新基准被提出用于衡量其在复杂任务上的能力。然而，很多基准在任务设置或奖励设计存在严重缺陷，导致性能评估失真，因此迫切需要系统化评估指南以规范化代理能力的客观衡量。

Method: 通过分析已有的agentic基准及广泛的最佳实践调研，总结并提出了一套基准评估指南（ABC），并在具体基准（CVE-Bench）上进行了应用和效果验证。

Result: 应用ABC在CVE-Bench上后，显著降低了因评估设计缺陷带来的性能高估（减少了33%）。

Conclusion: 本文指出当前许多用于评估AI代理的基准，在任务设置或奖励设计上存在问题，这可能严重影响性能评估的准确性。文章提出了Agentic Benchmark Checklist（ABC）来提升评估的严谨性。

Abstract: Benchmarks are essential for quantitatively tracking progress in AI. As AI
agents become increasingly capable, researchers and practitioners have
introduced agentic benchmarks to evaluate agents on complex, real-world tasks.
These benchmarks typically measure agent capabilities by evaluating task
outcomes via specific reward designs. However, we show that many agentic
benchmarks have issues task setup or reward design. For example, SWE-bench
Verified uses insufficient test cases, while TAU-bench counts empty responses
as successful. Such issues can lead to under- or overestimation agents'
performance by up to 100% in relative terms. To make agentic evaluation
rigorous, we introduce the Agentic Benchmark Checklist (ABC), a set of
guidelines that we synthesized from our benchmark-building experience, a survey
of best practices, and previously reported issues. When applied to CVE-Bench, a
benchmark with a particularly complex evaluation design, ABC reduces the
performance overestimation by 33%.

</details>


### [31] [StepHint: Multi-level Stepwise Hints Enhance Reinforcement Learning to Reason](https://arxiv.org/abs/2507.02841)
*Kaiyi Zhang,Ang Lv,Jinpeng Li,Yongbo Wang,Feng Wang,Haoyuan Hu,Rui Yan*

Main category: cs.AI

TL;DR: 本文提出StepHint算法，通过多级逐步提示显著提升LLM的推理能力，有效解决了RLVR中的near-miss reward和探索停滞难题，在多个基准测试中优于现有方法，并展现出良好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习与可验证奖励（RLVR）方法在应用于大语言模型（LLM）提升推理能力时，存在两个主要难题：1）near-miss reward问题，即一次微小失误会导致奖励丧失，极大降低训练效率；2）探索停滞问题，即模型局限于已有的“舒适区”，缺乏尝试更优解的动力。

Method: 提出了一种全新的RLVR算法StepHint。StepHint利用多级逐步提示帮助模型更有效地探索解的空间，具体做法是：从更强的模型生成有效推理链，并用自适应划分方法将推理链分解为多个推理步骤，将其中前几步作为提示，并同时为模型提供由不同步数组成的多级提示。这既能引导模型朝优秀子空间探索，又保留独立探索的灵活性。

Result: StepHint能有效缓解near-miss reward问题，提高训练效率。同时，通过引入外部推理路径，能够帮助模型脱离“舒适区”，缓解探索停滞。在六个数学基准测试中，StepHint超越了其他RLVR提升方法，并在域外基准测试中表现出更好的泛化能力。

Conclusion: StepHint显著提升了大模型推理能力，有效解决了RLVR训练过程中的关键难题，其方法具有更强的泛化性和稳定性。

Abstract: Reinforcement learning with verifiable rewards (RLVR) is a promising approach
for improving the complex reasoning abilities of large language models (LLMs).
However, current RLVR methods face two significant challenges: the near-miss
reward problem, where a small mistake can invalidate an otherwise correct
reasoning process, greatly hindering training efficiency; and exploration
stagnation, where models tend to focus on solutions within their ``comfort
zone,'' lacking the motivation to explore potentially more effective
alternatives. To address these challenges, we propose StepHint, a novel RLVR
algorithm that utilizes multi-level stepwise hints to help models explore the
solution space more effectively. StepHint generates valid reasoning chains from
stronger models and partitions these chains into reasoning steps using our
proposed adaptive partitioning method. The initial few steps are used as hints,
and simultaneously, multiple-level hints (each comprising a different number of
steps) are provided to the model. This approach directs the model's exploration
toward a promising solution subspace while preserving its flexibility for
independent exploration. By providing hints, StepHint mitigates the near-miss
reward problem, thereby improving training efficiency. Additionally, the
external reasoning pathways help the model develop better reasoning abilities,
enabling it to move beyond its ``comfort zone'' and mitigate exploration
stagnation. StepHint outperforms competitive RLVR enhancement methods across
six mathematical benchmarks, while also demonstrating superior generalization
and excelling over baselines on out-of-domain benchmarks.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [32] [McBE: A Multi-task Chinese Bias Evaluation Benchmark for Large Language Models](https://arxiv.org/abs/2507.02088)
*Tian Lan,Xiangdong Su,Xu Liu,Ruirui Wang,Ke Chang,Jiang Li,Guanglai Gao*

Main category: cs.CL

TL;DR: 本文提出了首个多任务、全面覆盖中文偏见评测基准McBE，填补了相关领域数据集的空白，并系统分析了当前主流大模型的偏见表现与问题。


<details>
  <summary>Details</summary>
Motivation: 当前主流大语言模型（LLMs）在应用于各类NLP任务时逐渐暴露出内在的偏见。现有许多偏见评测数据集以英语和北美文化为主，难以适用于其他文化背景，特别是面向中文和中国文化的数据集稀缺，且通常只支持单一评测任务，无法多维度评估偏见。

Method: 本文提出了一个多任务中文偏见评测基准（McBE），包含4,077个评测实例，覆盖12个单一偏见类别和82个子类别，并提出了5种评测任务，力求在类别、内容和评测维度上全面。作者还对多种不同系列与参数规模的主流大语言模型进行了偏见测试和分析。

Result: McBE具备广泛的类别覆盖、内容多样性和评测全面性。评测结果显示，所有测试的大语言模型都在不同程度上存在偏见。

Conclusion: 本文构建并发布了首个面向中文、多任务、多维度覆盖的偏见评测基准，为深入理解与消解LLMs偏见问题提供了全新资源和见解。

Abstract: As large language models (LLMs) are increasingly applied to various NLP
tasks, their inherent biases are gradually disclosed. Therefore, measuring
biases in LLMs is crucial to mitigate its ethical risks. However, most existing
bias evaluation datasets focus on English and North American culture, and their
bias categories are not fully applicable to other cultures. The datasets
grounded in the Chinese language and culture are scarce. More importantly,
these datasets usually only support single evaluation tasks and cannot evaluate
the bias from multiple aspects in LLMs. To address these issues, we present a
Multi-task Chinese Bias Evaluation Benchmark (McBE) that includes 4,077 bias
evaluation instances, covering 12 single bias categories, 82 subcategories and
introducing 5 evaluation tasks, providing extensive category coverage, content
diversity, and measuring comprehensiveness. Additionally, we evaluate several
popular LLMs from different series and with parameter sizes. In general, all
these LLMs demonstrated varying degrees of bias. We conduct an in-depth
analysis of results, offering novel insights into bias in LLMs.

</details>


### [33] [Reasoning or Not? A Comprehensive Evaluation of Reasoning LLMs for Dialogue Summarization](https://arxiv.org/abs/2507.02145)
*Keyan Jin,Yapeng Wang,Leonel Santos,Tao Fang,Xu Yang,Sio Kei Im,Hugo Gonçalo Oliveira*

Main category: cs.CL

TL;DR: 本文首次系统性比较了推理型与非推理型LLM在多种对话摘要场景下的表现，发现逐步推理并未提升甚至可能降低摘要质量，相关问题包括冗长和事实错误，提示需要针对性地改进推理方法及评测。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在文本摘要任务中取得了重大进展，但基于逐步推理的架构（如长链式思考CoT方法）在对话摘要领域的效果尚不明确，尤其是在需要提炼和简明性的实际应用场景中。鉴于对话摘要对于客户服务、会议分析和对话式AI有显著的实际价值，因此有必要系统评估推理型与非推理型LLMs在多种对话摘要范式下的表现。

Method: 本文系统性地评估了最先进的推理型（CoT等）和非推理型LLMs在通用、角色导向和查询导向三大类对话摘要范式中的表现。评测涵盖多种语言、领域和摘要长度，借助了包括SAMSum、DialogSum、CSDS和QMSum等权威基准数据集，并采用了结合LLM自动指标和类人主观标准的先进评测流程。除此之外，还通过场景分析和案例研究，深入探讨了推理型LLM何时以及为何未能提升甚至反而损害对话摘要效果。

Result: 研究发现，和在其他推理密集型任务中不同，显式逐步推理方法（如CoT）在对话摘要中并未带来稳定的性能提升，相反推理型LLM往往更易陷入冗余、事实不一致以及不简明的问题，效果甚至不如非推理型模型。论文还详细分析了推理在摘要中失效或起负面作用的各种复杂情境。

Conclusion: 当前推理型LLM在对话摘要任务中存在局限性，未能如预期般提升摘要质量。未来需根据实际对话场景针对性地设计推理模型和评测指标，以提升实际应用中的对话摘要效果。

Abstract: Dialogue summarization is a challenging task with significant practical value
in customer service, meeting analysis, and conversational AI. Although large
language models (LLMs) have achieved substantial progress in summarization
tasks, the performance of step-by-step reasoning architectures-specifically
Long Chain-of-Thought (CoT) implementations such as OpenAI-o1 and
DeepSeek-R1-remains unexplored for dialogue scenarios requiring concurrent
abstraction and conciseness. In this work, we present the first comprehensive
and systematic evaluation of state-of-the-art reasoning LLMs and non-reasoning
LLMs across three major paradigms-generic, role-oriented, and query-oriented
dialogue summarization. Our study spans diverse languages, domains, and summary
lengths, leveraging strong benchmarks (SAMSum, DialogSum, CSDS, and QMSum) and
advanced evaluation protocols that include both LLM-based automatic metrics and
human-inspired criteria. Contrary to trends in other reasoning-intensive tasks,
our findings show that explicit stepwise reasoning does not consistently
improve dialogue summarization quality. Instead, reasoning LLMs are often prone
to verbosity, factual inconsistencies, and less concise summaries compared to
their non-reasoning counterparts. Through scenario-specific analyses and
detailed case studies, we further identify when and why explicit reasoning may
fail to benefit-or even hinder-summarization in complex dialogue contexts. Our
work provides new insights into the limitations of current reasoning LLMs and
highlights the need for targeted modeling and evaluation strategies for
real-world dialogue summarization.

</details>


### [34] [Latent Chain-of-Thought? Decoding the Depth-Recurrent Transformer](https://arxiv.org/abs/2507.02199)
*Wenquan Lu,Yuechuan Yang,Kyle Lee,Yanshu Li,Enqi Liu*

Main category: cs.CL

TL;DR: 本文系统探究了递归Transformer模型能否实现隐式链式推理，发现并未形成理想的内部推理结构，解释性及性能均有限提升，实用性不佳。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的语言模型通过外显（以自然语言输出）的链式推理提升了复杂任务表现，但这会降低计算效率。作者希望探究通过深度递归架构是否能在模型内部以隐式形式实现链式推理，兼顾效率与推理能力。

Method: 采用Logit Lens和Coda Lens等多种探针技术，分析Huginn-3.5B在算术任务中的内部行为，追踪最终结果和中间结果 token 的秩变化，评估模型内部推理结构，并对不同递归深度和递归块进行对比分析。

Result: Huginn-3.5B在多层递归情况下几乎未展现出清晰的可解释隐式链式推理结构，且各递归块的隐藏状态可解释性随层和解码方式变化显著。递归层数加深后，性能提升非常有限，未达到外显链式推理模型的水平。

Conclusion: 该研究发现，尽管深度递归Transformer架构在理论上有可能实现隐式的链式推理，但实际上模型内部可解释的隐式推理结构非常有限，且不同递归块间的可解释性依赖于层索引和解码方法，缺乏一致性。增加递归深度只能带来有限的性能提升，远不及显式输出推理步骤的模型。

Abstract: Chain-of-thought (CoT) reasoning has enabled transformer-based language
models to excel at complex mathematics and multi-step planning. However, in
standard decoder-only architectures, these reasoning steps are externalized in
natural language, improving interpretability at the cost of efficiency. To
capture reasoning that is not easily represented in words, many works have
explored recurrent architectures that aim to internalize reasoning in latent
space, potentially supporting latent CoT. In this paper, we investigate whether
such reasoning structures emerge in Huginn-3.5B, a depth-recurrent Transformer
that reuses layers at inference time without increasing parameter count. We
examine the model's internal behavior on arithmetic tasks using a suite of
probing techniques including the Logit Lens and Coda Lens. Our findings reveal
limited evidence of interpretable latent CoT by tracking rank trajectories of
final and intermediate result tokens. Furthermore, we uncover significant
probing inconsistencies across recurrent blocks, where the interpretability of
hidden states depends heavily on both the layer index and the decoding method.
Finally, we empirically show that increasing recurrence depth yields only
marginal gains and falls well short of models that explicitly externalize
reasoning steps. The code is available at
https://github.com/wenquanlu/huginn-latent-cot.

</details>


### [35] [GDC Cohort Copilot: An AI Copilot for Curating Cohorts from the Genomic Data Commons](https://arxiv.org/abs/2507.02221)
*Steven Song,Anirudh Subramanyam,Zhenyu Zhang,Aarti Venkat,Robert L. Grossman*

Main category: cs.CL

TL;DR: GDC Cohort Copilot是一个帮助用户通过自然语言高效创建癌症队列的开源工具，本地大模型性能优于GPT-4o，并支持交互优化。


<details>
  <summary>Details</summary>
Motivation: GDC平台允许用户通过图形界面创建复杂的癌症队列，但用户（尤其是新用户）很难在数百个字段中精确描述目标队列，而用自由文本进行表达可能更加直观。

Method: 提出了GDC Cohort Copilot工具，用户可用自然语言描述所需队列，该工具自动生成对应的过滤器并导出到GDC，还开发并评估多种本地大模型用于实现此功能。

Result: 本地部署的开源GDC Cohort LLM在为GDC自动生成队列方面优于GPT-4o提示方法，工具界面友好并支持进一步互动优化。

Conclusion: GDC Cohort Copilot显著提高了用户根据自然语言描述高效构建GDC癌症队列的能力，并且其本地大模型性能优于GPT-4o。

Abstract: Motivation: The Genomic Data Commons (GDC) provides access to high quality,
harmonized cancer genomics data through a unified curation and analysis
platform centered around patient cohorts. While GDC users can interactively
create complex cohorts through the graphical Cohort Builder, users (especially
new ones) may struggle to find specific cohort descriptors across hundreds of
possible fields and properties. However, users may be better able to describe
their desired cohort in free-text natural language.
  Results: We introduce GDC Cohort Copilot, an open-source copilot tool for
curating cohorts from the GDC. GDC Cohort Copilot automatically generates the
GDC cohort filter corresponding to a user-input natural language description of
their desired cohort, before exporting the cohort back to the GDC for further
analysis. An interactive user interface allows users to further refine the
generated cohort. We develop and evaluate multiple large language models (LLMs)
for GDC Cohort Copilot and demonstrate that our locally-served, open-source GDC
Cohort LLM achieves better results than GPT-4o prompting in generating GDC
cohorts.
  Availability and implementation: The standalone docker image for GDC Cohort
Copilot is available at https://quay.io/repository/cdis/gdc-cohort-copilot.
Source code is available at https://github.com/uc-cdis/gdc-cohort-copilot. GDC
Cohort LLM weights are available at https://huggingface.co/uc-ctds.

</details>


### [36] [MemAgent: Reshaping Long-Context LLM with Multi-Conv RL-based Memory Agent](https://arxiv.org/abs/2507.02259)
*Hongli Yu,Tinghong Chen,Jiangtao Feng,Jiangjie Chen,Weinan Dai,Qiying Yu,Ya-Qin Zhang,Wei-Ying Ma,Jingjing Liu,Mingxuan Wang,Hao Zhou*

Main category: cs.CL

TL;DR: MemAgent提出了一种高效、可扩展的新方法，实现了极长文本的高性能理解和推理，性能显著优于现有方案。


<details>
  <summary>Details</summary>
Motivation: 当前长文本处理面临着在扩展长度时如何高效处理并避免性能损失的挑战。现有的注意力和记忆模块存在扩展性和效率上的限制，无法很好地支持超长文本任务。

Method: 作者提出了一种新的代理工作流MemAgent，将文本分段读取，并利用覆盖式记忆更新来应对超长文本。同时，扩展了DAPO算法，通过独立上下文多对话生成促进训练。

Result: MemAgent展现出极强的长文本推理和处理能力，不仅能从8K上下文训练并推广到32K文本，还能实现对3.5M级QA任务的性能损失低于5%，在512K的RULER测试中达到95%以上的成绩。

Conclusion: MemAgent有效解决了超长文本处理中的效率与扩展性问题，在大规模文本任务中保持了极佳的性能，具有广阔的应用前景。

Abstract: Despite improvements by length extrapolation, efficient attention and memory
modules, handling infinitely long documents with linear complexity without
performance degradation during extrapolation remains the ultimate challenge in
long-text processing. We directly optimize for long-text tasks in an end-to-end
fashion and introduce a novel agent workflow, MemAgent, which reads text in
segments and updates the memory using an overwrite strategy. We extend the DAPO
algorithm to facilitate training via independent-context multi-conversation
generation. MemAgent has demonstrated superb long-context capabilities, being
able to extrapolate from an 8K context trained on 32K text to a 3.5M QA task
with performance loss < 5% and achieves 95%+ in 512K RULER test.

</details>


### [37] [DoMIX: An Efficient Framework for Exploiting Domain Knowledge in Fine-Tuning](https://arxiv.org/abs/2507.02302)
*Dohoon Kim,Donghun Kang,Taesup Moon*

Main category: cs.CL

TL;DR: DoMIX通过使用LoRA模块，实现了高效、顺序无关、可定制的持续性领域自适应预训练方法，明显优于现有方法，并可拓展用于大模型微调。


<details>
  <summary>Details</summary>
Motivation: 现有的持续性领域自适应预训练（continual DAP）方法存在高计算成本、对数据顺序敏感、仅提供单一通用模型等问题。这些限制降低了方法在实际应用中的效率和灵活性。

Method: 本文提出了一种新的方法DoMIX，通过利用参数高效微调技术LoRA模块，实现高效并行的领域自适应预训练。这种方法不仅降低了计算资源消耗，还能够克服数据顺序依赖，实现为不同任务提供定制化的预训练模型。

Result: 实验结果表明，DoMIX能够高效且强健地进行领域适应预训练，并且能有效利用累积知识为不同任务定制预训练模型，同时该方法也适用于标准的大模型微调场景。

Conclusion: DoMIX有效解决了持续性领域自适应预训练中的资源消耗高、对顺序敏感和模型泛化问题，提升了模型的适应性与实用性。

Abstract: Domain-Adaptive Pre-training (DAP) has recently gained attention for its
effectiveness in fine-tuning pre-trained models. Building on this, continual
DAP has been explored to develop pre-trained models capable of incrementally
incorporating different domain datasets. However, existing continual DAP
methods face several limitations: (1) high computational cost and GPU memory
usage during training; (2) sensitivity to incremental data order; and (3)
providing a single, generalized model for all end tasks, which contradicts the
essence of DAP. In this paper, we propose DoMIX, a novel approach that
addresses these challenges by leveraging LoRA modules, a representative
parameter-efficient fine-tuning (PEFT) method. Our approach enables efficient
and parallel domain-adaptive pre-training that is robust to domain order and
effectively utilizes accumulated knowledge to provide tailored pre-trained
models for specific tasks. We also demonstrate that our method can be extended
beyond the DAP setting to standard LLM fine-tuning scenarios. Code is available
at https://github.com/dohoonkim-ai/DoMIX.

</details>


### [38] [Coling-UniA at SciVQA 2025: Few-Shot Example Retrieval and Confidence-Informed Ensembling for Multimodal Large Language Models](https://arxiv.org/abs/2507.02357)
*Christian Jaumann,Annemarie Friedrich,Rainer Lienhart*

Main category: cs.CL

TL;DR: 作者提出了一套结合多模态大模型和多种检索方式的系统，在科学视觉问答任务评测中排名第三，取得了优异的平均F1表现，代码已开源。


<details>
  <summary>Details</summary>
Motivation: 面对科学视觉问答（SciVQA）任务的挑战，需要开发能够理解和回答包含视觉和文本信息的问题的强大系统。作者希望通过提升多模态大模型与检索策略，有效提升SciVQA任务表现。

Method: 本文提出一种系统，结合了两种多模态大语言模型的集成方法，并探索了多种少样本示例检索策略。针对不同类型的图像和问题，动态选择合适的模型和少样本设置，同时依据模型置信度选取最终答案。

Result: 在SciVQA 2025任务盲测数据上，该系统在七个参赛系统中排名第三，平均F1分数（结合ROUGE-1、ROUGE-L和BERTS指标）达到85.12。

Conclusion: 集成多模态大模型与灵活的少样本示例检索策略能够显著提升科学视觉问答任务的表现。

Abstract: This paper describes our system for the SciVQA 2025 Shared Task on Scientific
Visual Question Answering. Our system employs an ensemble of two Multimodal
Large Language Models and various few-shot example retrieval strategies. The
model and few-shot setting are selected based on the figure and question type.
We also select answers based on the models' confidence levels. On the blind
test data, our system ranks third out of seven with an average F1 score of
85.12 across ROUGE-1, ROUGE-L, and BERTS. Our code is publicly available.

</details>


### [39] [QFFN-BERT: An Empirical Study of Depth, Performance, and Data Efficiency in Hybrid Quantum-Classical Transformers](https://arxiv.org/abs/2507.02364)
*Pilsung Kang*

Main category: cs.CL

TL;DR: 本文提出用PQC替换BERT的FFN模块，大幅减少参数量并实现甚至超越原模型表现，尤其在小样本下显示出优势，证实了PQC+深度学习是可行的高效替代方案。


<details>
  <summary>Details</summary>
Motivation: 现有的Transformer结构中，前馈网络（FFN）部分参数量巨大，占整块编码器约三分之二，参数量大影响模型效率。过去对PQCs（参数化量子电路）的集成多在自注意力模块，而非FFN。该文探索是否能用PQC有效替代FFN，提高网络表达力并减少参数。

Method: 将BERT变体中的FFN模块用基于PQCs的层替换，构建了QFFN-BERT混合量子-经典变换器。系统研究了PQC深度、可表达性与可训练性间的权衡，最终采用含残差连接的PQC架构，结合$R_Y$与$R_Z$旋转和交替纠缠策略，保证可训练性和高表达力。

Result: 在SST-2和DBpedia基准上用经典模拟器验证，精心设计的QFFN-BERT能达到或超过基线准确率（最高达102%），在参数减少99%以上的同时表现优异，并在小样本学习任务中表现出一致的竞争优势。消融实验表明，PQC需要合理设计才能学习有效。

Conclusion: PQCs若与深度学习基础原则共同设计，可作为参数高效且强大的FFN替代方法，兼顾模型效率和表现力。

Abstract: Parameterized quantum circuits (PQCs) have recently emerged as promising
components for enhancing the expressibility of neural architectures. In this
work, we introduce QFFN-BERT, a hybrid quantum-classical transformer where the
feedforward network (FFN) modules of a compact BERT variant are replaced by
PQC-based layers. This design is motivated by the dominant parameter
contribution of FFNs, which account for approximately two-thirds of the
parameters within standard Transformer encoder blocks. While prior studies have
primarily integrated PQCs into self-attention modules, our work focuses on the
FFN and systematically investigates the trade-offs between PQC depth,
expressibility, and trainability. Our final PQC architecture incorporates a
residual connection, both $R_Y$ and $R_Z$ rotations, and an alternating
entanglement strategy to ensure stable training and high expressibility. Our
experiments, conducted on a classical simulator, on the SST-2 and DBpedia
benchmarks demonstrate two key findings. First, a carefully configured
QFFN-BERT achieves up to 102.0% of the baseline accuracy, surpassing its
classical counterpart in a full-data setting while reducing FFN-specific
parameters by over 99%. Second, our model exhibits a consistent and competitive
edge in few-shot learning scenarios, confirming its potential for superior data
efficiency. These results, supported by an ablation study on a non-optimized
PQC that failed to learn, confirm that PQCs can serve as powerful and
parameter-efficient alternatives to classical FFNs when co-designed with
foundational deep learning principles.

</details>


### [40] [Efficient Code LLM Training via Distribution-Consistent and Diversity-Aware Data Selection](https://arxiv.org/abs/2507.02378)
*Weijie Lyu,Sheng-Jun Huang,Xuan Xia*

Main category: cs.CL

TL;DR: 作者提出了一种高效的数据选择方法，通过优化数据子集保证分布和多样性，在减少计算消耗的同时显著提升了代码大模型的性能。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLMs）在代码生成和程序理解方面取得显著进展，但多数方法着重增加数据量，忽视了数据质量，导致训练效率下降。

Method: 提出了一种基于参数模型的代码数据选择方法，通过优化参数模型，保证所选数据子集的分布一致性和多样性，从而确保高质量的数据输入。

Result: 在仅使用10K样本的情况下，该方法在HumanEval上提升2.4%，在MBPP上提升2.3%，超过了92K全数据采样的基线，同时也优于其他采样方法。

Conclusion: 该方法能够在显著降低计算消耗的同时，有效提升模型性能。

Abstract: Recent advancements in large language models (LLMs) have significantly
improved code generation and program comprehension, accelerating the evolution
of software engineering. Current methods primarily enhance model performance by
leveraging vast amounts of data, focusing on data quantity while often
overlooking data quality, thereby reducing training efficiency. To address
this, we introduce an approach that utilizes a parametric model for code data
selection, aimed at improving both training efficiency and model performance.
Our method optimizes the parametric model to ensure distribution consistency
and diversity within the selected subset, guaranteeing high-quality data.
Experimental results demonstrate that using only 10K samples, our method
achieves gains of 2.4% (HumanEval) and 2.3% (MBPP) over 92K full-sampled
baseline, outperforming other sampling approaches in both performance and
efficiency. This underscores that our method effectively boosts model
performance while significantly reducing computational costs.

</details>


### [41] [Benchmarking Akan ASR Models Across Domain-Specific Datasets: A Comparative Evaluation of Performance, Scalability, and Adaptability](https://arxiv.org/abs/2507.02407)
*Mark Atta Mensah,Isaac Wiafe,Akon Ekpezu,Justice Kwame Appati,Jamal-Deen Abdulai,Akosua Nyarkoa Wiafe-Akenten,Frank Ernest Yeboah,Gifty Odame*

Main category: cs.CL

TL;DR: 本研究比较了多种阿坎语ASR模型在不同领域下的表现，发现模型易受领域限制且不同架构错误风格差异明显。为提升低资源语言ASR泛化能力，需引入领域自适应和多语种训练等方法。


<details>
  <summary>Details</summary>
Motivation: 现有的自动语音识别(ASR)研究主要在同一领域数据集上评估模型，鲜有跨多样语音情景进行泛化性能测试。因此，本研究旨在评价现有ASR模型在阿坎语不同领域下的表现差异。

Method: 利用四个阿坎语语音数据集（涵盖图像描述、非正式对话、圣经朗读和金融对话），对基于Transformer架构（如Whisper和Wav2Vec2）的七个ASR模型进行测试与对比，通过字错误率和词错误率分析其跨领域的泛化能力。

Result: 结果表明，ASR模型的性能严重依赖训练领域，仅在各自训练领域表现最优。当应用于领域不匹配的数据时，准确率明显下降。Whisper和Wav2Vec2架构的错误类型也有不同：Whisper更容易产生流畅但误导性的转录，Wav2Vec2在不熟悉输入时输出更明显但难以理解的结果。

Conclusion: 应特别注意ASR模型在低资源语言跨领域应用的错误类型权衡，建议引入针对性领域自适应、多路路由策略和多语种训练框架，从而提升模型泛化和适应阿坎语及其他低资源语言的能力。

Abstract: Most existing automatic speech recognition (ASR) research evaluate models
using in-domain datasets. However, they seldom evaluate how they generalize
across diverse speech contexts. This study addresses this gap by benchmarking
seven Akan ASR models built on transformer architectures, such as Whisper and
Wav2Vec2, using four Akan speech corpora to determine their performance. These
datasets encompass various domains, including culturally relevant image
descriptions, informal conversations, biblical scripture readings, and
spontaneous financial dialogues. A comparison of the word error rate and
character error rate highlighted domain dependency, with models performing
optimally only within their training domains while showing marked accuracy
degradation in mismatched scenarios. This study also identified distinct error
behaviors between the Whisper and Wav2Vec2 architectures. Whereas fine-tuned
Whisper Akan models led to more fluent but potentially misleading transcription
errors, Wav2Vec2 produced more obvious yet less interpretable outputs when
encountering unfamiliar inputs. This trade-off between readability and
transparency in ASR errors should be considered when selecting architectures
for low-resource language (LRL) applications. These findings highlight the need
for targeted domain adaptation techniques, adaptive routing strategies, and
multilingual training frameworks for Akan and other LRLs.

</details>


### [42] [A Cookbook for Community-driven Data Collection of Impaired Speech in LowResource Languages](https://arxiv.org/abs/2507.02428)
*Sumaya Ahmed Salihs,Isaac Wiafe,Jamal-Deen Abdulai,Elikem Doe Atsakpo,Gifty Ayoka,Richard Cave,Akon Obu Ekpezu,Catherine Holloway,Katrin Tomanek,Fiifi Baffoe Payin Winful*

Main category: cs.CL

TL;DR: 本论文提出了一套适用于低资源、受损言语的ASR数据采集与模型训练方法，并以Akan语为例，公开了相应数据集和工具。微调结果初步验证了方法有效性，推动了包容性ASR技术发展。


<details>
  <summary>Details</summary>
Motivation: 现有ASR技术对低资源及受损言语者支持严重不足。研究希望通过开放、民主化方法，解决这一技术和数据鸿沟，提升受损言语者的技术包容性。

Method: 研究通过社区驱动的数据采集，以多元背景受损言语者为对象，制定实践手册（cookbook），使用开源工具收集、处理和公开数据。随后，微调现有开源ASR模型，评估其对Akan受损言语的识别效果。

Result: 构建了第一个Akan受损言语公开数据集，并发布了数据采集实践手册与开源工具。初步模型微调结果显示ASR系统对该类言语识别能力得到提升。

Conclusion: 本研究开发了针对受损言语，尤其是低资源语言（如加纳的Akan语）的ASR模型数据采集方法，并成功构建了首个Akan受损言语公开数据集。相关实践指南和工具已公开，为更具包容性的ASR技术发展奠定了基础。初步实验表明，通过微调开源ASR模型能够提升对Akan受损言语的识别能力。

Abstract: This study presents an approach for collecting speech samples to build
Automatic Speech Recognition (ASR) models for impaired speech, particularly,
low-resource languages. It aims to democratize ASR technology and data
collection by developing a "cookbook" of best practices and training for
community-driven data collection and ASR model building. As a proof-of-concept,
this study curated the first open-source dataset of impaired speech in Akan: a
widely spoken indigenous language in Ghana. The study involved participants
from diverse backgrounds with speech impairments. The resulting dataset, along
with the cookbook and open-source tools, are publicly available to enable
researchers and practitioners to create inclusive ASR technologies tailored to
the unique needs of speech impaired individuals. In addition, this study
presents the initial results of fine-tuning open-source ASR models to better
recognize impaired speech in Akan.

</details>


### [43] [IndianBailJudgments-1200: A Multi-Attribute Dataset for Legal NLP on Indian Bail Orders](https://arxiv.org/abs/2507.02506)
*Sneha Deshmukh,Prathmesh Kamble*

Main category: cs.CL

TL;DR: 作者发布了一个1200例的印度保释判决数据集，全面注释并公开，有助于推动法律NLP在印度的发展。


<details>
  <summary>Details</summary>
Motivation: 印度等地区法律NLP发展缓慢，主要因为缺乏结构化数据集。在法律文本智能分析等任务中，缺失高质量本地化数据成为瓶颈。

Method: 作者构建了一个覆盖20+属性（如保释结果、印度刑法条、犯罪类型、法律推理）的判决文书数据集，总共包含1200份印度法庭的保释判决。注释采用了GPT-4o自动标注流程，并人工核查一致性。

Result: 成功发布了第一个专注于印度保释判决的公开数据集，为相关NLP任务如结果预测、摘要、司法公平性分析提供了基础资源。

Conclusion: 该数据集为印度法律NLP领域填补了重要空白，支持学界进一步研究和相关应用开发。

Abstract: Legal NLP remains underdeveloped in regions like India due to the scarcity of
structured datasets. We introduce IndianBailJudgments-1200, a new benchmark
dataset comprising 1200 Indian court judgments on bail decisions, annotated
across 20+ attributes including bail outcome, IPC sections, crime type, and
legal reasoning. Annotations were generated using a prompt-engineered GPT-4o
pipeline and verified for consistency. This resource supports a wide range of
legal NLP tasks such as outcome prediction, summarization, and fairness
analysis, and is the first publicly available dataset focused specifically on
Indian bail jurisprudence.

</details>


### [44] [WebSailor: Navigating Super-human Reasoning for Web Agent](https://arxiv.org/abs/2507.02592)
*Kuan Li,Zhongwang Zhang,Huifeng Yin,Liwen Zhang,Litu Ou,Jialong Wu,Wenbiao Yin,Baixuan Li,Zhengwei Tao,Xinyu Wang,Weizhou Shen,Junkai Zhang,Dingchu Zhang,Xixi Wu,Yong Jiang,Ming Yan,Pengjun Xie,Fei Huang,Jingren Zhou*

Main category: cs.CL

TL;DR: 该论文提出WebSailor方法，通过生成高不确定性任务和强化学习训练极大提升了开源大模型在复杂信息检索任务中的表现，使其基本追平专有系统。


<details>
  <summary>Details</summary>
Motivation: 当前LLM存在认知能力限制，开源模型在处理复杂信息检索任务时明显不如专有系统，特别是在应对高度不确定性的推理任务中表现不足。作者希望提升开源模型在此类任务上的能力。

Method: 提出WebSailor后训练方法，包括通过结构化采样和信息混淆生成新型高不确定性任务，使用RFT冷启动以及高效的强化学习训练算法（DUPO）。

Result: WebSailor显著提升开源代理在复杂信息检索任务中的表现，使其达到专有代理系统的水准，显著缩小了能力差距。

Conclusion: 通过WebSailor可以弥补开源LLM与专有系统在复杂推理和信息检索任务中的能力差距，提升处理极端不确定性任务的能力。

Abstract: Transcending human cognitive limitations represents a critical frontier in
LLM training. Proprietary agentic systems like DeepResearch have demonstrated
superhuman capabilities on extremely complex information-seeking benchmarks
such as BrowseComp, a feat previously unattainable. We posit that their success
hinges on a sophisticated reasoning pattern absent in open-source models: the
ability to systematically reduce extreme uncertainty when navigating vast
information landscapes. Based on this insight, we introduce WebSailor, a
complete post-training methodology designed to instill this crucial capability.
Our approach involves generating novel, high-uncertainty tasks through
structured sampling and information obfuscation, RFT cold start, and an
efficient agentic RL training algorithm, Duplicating Sampling Policy
Optimization (DUPO). With this integrated pipeline, WebSailor significantly
outperforms all opensource agents in complex information-seeking tasks,
matching proprietary agents' performance and closing the capability gap.

</details>


### [45] [Revisiting Active Learning under (Human) Label Variation](https://arxiv.org/abs/2507.02593)
*Cornelia Gruber,Helen Alber,Bernd Bischl,Göran Kauermann,Barbara Plank,Matthias Aßenmacher*

Main category: cs.CL

TL;DR: 本文指出现实中普遍存在的人类标注差异（HLV）被主流学习框架忽略。作者梳理并批判了主动学习和标签差异领域的主流假设，提出了将HLV信号纳入主动学习全过程的理论框架，并探讨了大语言模型的标注者作用。该框架有助于提升算法在真实复杂标注环境下的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前有监督学习在实际应用中受限于高质量标注数据的匮乏，同时标注差异（如同一实体获得不同标签）现象普遍存在，但主流的标注框架往往假设标签唯一，忽视了“人类标注差异”（HLV）的信息价值。主动学习虽可优化标注预算使用，但大多建立在简化的现实假设之上，在考虑HLV后这些假设难以成立。

Method: 作者对主动学习和标签变化（LV/HLV）领域的基础假设进行了梳理和批判性分析，总结了现有方法在应对HLV时的局限与不足，并提出了一个融合HLV信号的主动学习概念框架，涵盖样本选择、标注者选择和标签表达，还探讨了大语言模型作为标注者的角色。

Result: 提出了一个能够将人类标注差异（HLV）正式纳入到主动学习各个环节的理论框架，为今后现实复杂标注环境下的主动学习研究与应用提供理论基础。

Conclusion: 充分利用标注差异信息、精准区分信号（如HLV）与噪声（如误标注），并融合到主动学习流程中，将更好地反映实际数据标注的复杂性，并推动高质量监督学习的可行性发展。

Abstract: Access to high-quality labeled data remains a limiting factor in applied
supervised learning. While label variation (LV), i.e., differing labels for the
same instance, is common, especially in natural language processing, annotation
frameworks often still rest on the assumption of a single ground truth. This
overlooks human label variation (HLV), the occurrence of plausible differences
in annotations, as an informative signal. Similarly, active learning (AL), a
popular approach to optimizing the use of limited annotation budgets in
training ML models, often relies on at least one of several simplifying
assumptions, which rarely hold in practice when acknowledging HLV. In this
paper, we examine foundational assumptions about truth and label nature,
highlighting the need to decompose observed LV into signal (e.g., HLV) and
noise (e.g., annotation error). We survey how the AL and (H)LV communities have
addressed -- or neglected -- these distinctions and propose a conceptual
framework for incorporating HLV throughout the AL loop, including instance
selection, annotator choice, and label representation. We further discuss the
integration of large language models (LLM) as annotators. Our work aims to lay
a conceptual foundation for HLV-aware active learning, better reflecting the
complexities of real-world annotation.

</details>


### [46] [MPF: Aligning and Debiasing Language Models post Deployment via Multi Perspective Fusion](https://arxiv.org/abs/2507.02595)
*Xin Guan,PeiHsin Lin,Zekun Wu,Ze Wang,Ruibo Zhang,Emre Kazim,Adriano Koshiyama*

Main category: cs.CL

TL;DR: 提出了Multiperspective Fusion (MPF)方法，通过多视角生成与基线分布校准，实现对大型语言模型输出的偏见自动检测与缓解，无需复杂微调，效果良好且具泛化性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）应用的增长，模型输出中的偏见问题也日益突出，现有的偏见缓解方法往往操作复杂或效果有限。因此，研究者希望开发一种简单、高效、可扩展的偏见对齐方法。

Method: 该论文提出了Multiperspective Fusion (MPF) 框架，作为一种后训练的对齐方法。MPF基于SAGED自动化管道，通过分解基线分布（如HR专家的情感分布）为多个可解释的视角成分，再按概率加权采样和调和模型生成的输出，实现偏见暴露与校正。

Result: 实验证明，MPF可以有效使LLM的输出情感分布与理想基线（如绝对平等或者HR专家的偏见基线）对齐，表现为更小的KL散度、更低的校准误差，并对未见过的问题具有泛化能力。

Conclusion: MPF为已部署的LLM提供了一种可扩展、可解释且无需复杂提示设计或模型微调的对齐与偏见缓解新方法。

Abstract: Multiperspective Fusion (MPF) is a novel posttraining alignment framework for
large language models (LLMs) developed in response to the growing need for easy
bias mitigation. Built on top of the SAGED pipeline, an automated system for
constructing bias benchmarks and extracting interpretable baseline
distributions, MPF leverages multiperspective generations to expose and align
biases in LLM outputs with nuanced, humanlike baselines. By decomposing
baseline, such as sentiment distributions from HR professionals, into
interpretable perspective components, MPF guides generation through sampling
and balancing of responses, weighted by the probabilities obtained in the
decomposition. Empirically, we demonstrate its ability to align LLM sentiment
distributions with both counterfactual baselines (absolute equality) and the HR
baseline (biased for Top Univeristy), resulting in small KL divergence,
reduction of calibration error and generalization to unseen questions. This
shows that MPF offers a scalable and interpretable method for alignment and
bias mitigation, compatible with deployed LLMs and requiring no extensive
prompt engineering or finetuning.

</details>


### [47] [Exploring Gender Bias Beyond Occupational Titles](https://arxiv.org/abs/2507.02679)
*Ahmed Sabir,Rajesh Sharama*

Main category: cs.CL

TL;DR: 提出GenderLexicon新数据集和评估框架，能够评分解释性别语境偏见；验证性别偏见不仅限于职业，还广泛存在于其他领域。


<details>
  <summary>Details</summary>
Motivation: 论文关注性别与语境偏见之间的相关性，尤其针对职业类别以外的偏见，旨在进一步揭示和解释性别偏见问题。

Method: 作者提出了新的数据集GenderLexicon以及一个框架，可用于评估语境偏见及其相关的性别偏见，并能通过分数量化解释性别偏见。对五个多样化数据集，包括日本语料，进行了实证评估。

Result: 模型能够以分数形式解释性别偏见，提高了偏见的可解释性，并验证了性别偏见不只存在于职业刻板印象中，还存在于更多上下文中。

Conclusion: 提出的数据集和方法不仅可量化和解释性别相关的语境偏见，还能揭示之前未充分意识到的偏见来源，在多数据集测试中显示出有效性。

Abstract: In this work, we investigate the correlation between gender and contextual
biases, focusing on elements such as action verbs, object nouns, and
particularly on occupations. We introduce a novel dataset, GenderLexicon, and a
framework that can estimate contextual bias and its related gender bias. Our
model can interpret the bias with a score and thus improve the explainability
of gender bias. Also, our findings confirm the existence of gender biases
beyond occupational stereotypes. To validate our approach and demonstrate its
effectiveness, we conduct evaluations on five diverse datasets, including a
Japanese dataset.

</details>


### [48] [Can LLMs Identify Critical Limitations within Scientific Research? A Systematic Evaluation on AI Research Papers](https://arxiv.org/abs/2507.02694)
*Zhijian Xu,Yilun Zhao,Manasi Patwardhan,Lovekesh Vig,Arman Cohan*

Main category: cs.CL

TL;DR: 本文提出针对AI领域论文局限性的分类体系与基准数据集（LimitGen），评估和提升LLMs辅助识别论文局限性的能力。通过结合文献检索，LLMs对论文局限性的识别和生成更具针对性和建设性，有助于提升同行评审的效率和质量。


<details>
  <summary>Details</summary>
Motivation: 随着科学出版物数量的激增，同行评审中对专业知识的需求日益增加，人工审稿压力陡增。尽管大语言模型（LLMs）在科学任务中表现出潜力，但其辅助同行评审、尤其是在自动识别论文局限性方面的能力尚未被充分研究。为此，研究者希望探索LLMs在科学评审中的实际应用价值，提升评审效率和质量。

Method: 首先，本文提出了一个面向人工智能领域的科学研究局限性类型的全面分类体系。然后，作者基于该分类标准，设计了LimitGen基准，用于评估LLMs在支持早期反馈和补充人工审稿方面的表现。LimitGen包含两个子集：合成数据集（LimitGen-Syn），通过对高质量论文的有控干扰手段生成；人工数据集（LimitGen-Human），收集真实的人工撰写局限性文本。为增强模型识别论文局限性的能力，作者引入了文献检索机制，使LLMs在训练和推理过程中可查阅先前的科学成果。

Result: 引入文献检索后，LLMs在自动识别和生成论文局限性方面的表现显著提升，能够给出更具体、建设性的反馈，从而为论文同行评审提供有效的辅助。

Conclusion: 通过提出LimitGen基准并增强LLMs以文献为依据，论文证明了大语言模型能够在识别科研论文局限性方面显著助力同行评审流程，有望缓解日益增长的审稿负担和优化反馈内容。

Abstract: Peer review is fundamental to scientific research, but the growing volume of
publications has intensified the challenges of this expertise-intensive
process. While LLMs show promise in various scientific tasks, their potential
to assist with peer review, particularly in identifying paper limitations,
remains understudied. We first present a comprehensive taxonomy of limitation
types in scientific research, with a focus on AI. Guided by this taxonomy, for
studying limitations, we present LimitGen, the first comprehensive benchmark
for evaluating LLMs' capability to support early-stage feedback and complement
human peer review. Our benchmark consists of two subsets: LimitGen-Syn, a
synthetic dataset carefully created through controlled perturbations of
high-quality papers, and LimitGen-Human, a collection of real human-written
limitations. To improve the ability of LLM systems to identify limitations, we
augment them with literature retrieval, which is essential for grounding
identifying limitations in prior scientific findings. Our approach enhances the
capabilities of LLM systems to generate limitations in research papers,
enabling them to provide more concrete and constructive feedback.

</details>


### [49] [Measurement of the Granularity of Vowel Production Space By Just Producible Different (JPD) Limens](https://arxiv.org/abs/2507.02744)
*Peter Viechnicki*

Main category: cs.CL

TL;DR: 本研究首次测量了英文前元音的最小可再现差异（JPD），发现元音音位在听觉空间中的最小距离为14~51 mels，这对理解人类元音系统结构和语音生成理论有重要意义。


<details>
  <summary>Details</summary>
Motivation: 人类元音的发音涉及复杂、协同的运动控制，这些控制机制部分由听觉空间目标区所支配。虽然已知在亚音位层面存在目标区控制现象，但控制的精度尚不明确。

Method: 本研究通过元音模仿范式，首次测量了英文母语者在前元音发音时的“最小可再现差异”（JPD），以评估在听觉空间中两个元音刺激须间隔多远才能产生可区分的模仿发音。

Result: JPD在F1×F2（第一、二共振峰）空间中估算为14至51 mels。

Conclusion: 该研究成果为语音生产的情景性理论提供了证据，并界定了元音音位间理论上的最小距离，进而对人类元音系统结构和音位数目及分布趋势给出心理物理学解释。

Abstract: A body of work over the past several decades has demonstrated that the
complex and coordinated articulatory movements of human vowel production are
governed (at least in part)by control mechanisms whose targets are regions of
auditory space. Within the target region control at the sub-phonemic level has
also been demonstrated. But the degree of accuracy of that control is unknown.
The current work investigates this question by asking how far apart must two
vowel stimuli lie in auditory space in order to yield reliably different
imitations? This distance is termed 'Just Producible Difference' (JPD). The
current study uses a vowel mimicry paradigm to derive the first measurement of
JPD among two sets of English speakers during front vowel production. JPD is
estimated at between 14 and 51 mels in F1 X F2 space. This finding has
implications for episodic theories of speech production. It also clarifies the
possible structures of human vowel systems, by setting a theoretical lower
bound for how close two vowel phonemes may be in a speaker's formant space, and
hence a psychophysical explanation of observed trends in number and patterns of
possible vowel phonemes.

</details>


### [50] [Self-Correction Bench: Revealing and Addressing the Self-Correction Blind Spot in LLMs](https://arxiv.org/abs/2507.02778)
*Ken Tsui*

Main category: cs.CL

TL;DR: 大语言模型自我纠错能力有限，简单提示即可大幅激活，未来训练数据和策略需加强纠错过程。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）虽然正在改变各领域，但常常会出现错误，且难以及时自我纠正，严重影响其可信度。作者希望系统性分析LLM自我纠错能力的不足，寻找提升可靠性的方法。

Method: 提出Self-Correction Bench，通过三种复杂度下的受控错误注入，系统测量LLM的自我纠错盲区，并测试了14个模型。还分析了训练数据来源与纠错能力的关系，尝试简单的提示词激活潜在能力。

Result: 平均盲区率高达64.5%。分析表明数据集大多为无错演示，缺乏纠错过程，RL训练能改善此问题。引入“Wait”提示后盲区率下降89.3%，表明纠错能力潜藏但未被激活。

Conclusion: 当前LLMs普遍存在自我纠错盲点，主要受训练方式影响。激活其自我纠错能力（如简单提示词）可以大幅提升其可靠性。研究为提升LLM信任度和实用性提供了新思路。

Abstract: Although large language models (LLMs) have become transformative, they still
make mistakes and can explore unproductive reasoning paths. Self-correction is
an important capability for a trustworthy LLM, particularly an autoregressive
LLM. While LLMs can identify error in user input, they exhibit a systematic
'Self-Correction Blind Spot' - failing to correct identical error in their own
outputs. To systematically study this phenomenon, we introduce Self-Correction
Bench, a systematic framework to measure this phenomenon through controlled
error injection at three complexity levels. Testing 14 models, we find an
average 64.5% blind spot rate. We find multiple evidences that this limitation
relates to training data composition: human training demonstrations
predominantly show error-free responses rather than error-correction sequences,
unlike RL-trained models that learn error correction through outcome feedback.
Remarkably, simply appending "Wait" reduces blind spots by 89.3%, suggesting
that the capability exists but requires activation. Our work highlights a
critical limitation in current LLMs and offers potential avenues for improving
their reliability and trustworthiness.

</details>


### [51] [Is Reasoning All You Need? Probing Bias in the Age of Reasoning Language Models](https://arxiv.org/abs/2507.02799)
*Riccardo Cantini,Nicola Gabriele,Alessio Orsino,Domenico Talia*

Main category: cs.CL

TL;DR: 推理能力让大模型在面对偏见诱发时更易被攻破，尤其链式思维提示最易被‘讲故事’等绕过，推理安全需进一步研究。


<details>
  <summary>Details</summary>
Motivation: 以往认为具备推理能力的语言模型（如链式思维CoT等）更可靠，但其对抗社会偏见的鲁棒性尚不明确，因此本研究希望系统性评估推理语言模型在偏见诱发下的安全与脆弱点。

Method: 利用CLEAR-Bias基准系统评测多种最先进的推理语言模型，通过LLM自动打分、安全机制拆解（诸如jailbreak攻击），从多个社会文化维度考查模型被诱发偏见的能力，同时比较CoT提示与推理微调两种机制的安全性差异。

Result: 结果显示，具有显式推理能力的模型（无论是通过CoT还是微调）比基础模型更容易被诱发偏见。尤其依赖CoT的模型，面对故事化、虚构人物或奖励型提示时极其易被绕过。虽然显式推理模型比CoT模型略安全，但整体上推理能力并未提升鲁棒性，甚至可能打开新的刻板印象路径。

Conclusion: 推理机制并非天然提升偏见鲁棒性，反而可能带来新的安全隐患。针对推理的偏见防御与安全机制设计需要更加关注。

Abstract: Reasoning Language Models (RLMs) have gained traction for their ability to
perform complex, multi-step reasoning tasks through mechanisms such as
Chain-of-Thought (CoT) prompting or fine-tuned reasoning traces. While these
capabilities promise improved reliability, their impact on robustness to social
biases remains unclear. In this work, we leverage the CLEAR-Bias benchmark,
originally designed for Large Language Models (LLMs), to investigate the
adversarial robustness of RLMs to bias elicitation. We systematically evaluate
state-of-the-art RLMs across diverse sociocultural dimensions, using an
LLM-as-a-judge approach for automated safety scoring and leveraging jailbreak
techniques to assess the strength of built-in safety mechanisms. Our evaluation
addresses three key questions: (i) how the introduction of reasoning
capabilities affects model fairness and robustness; (ii) whether models
fine-tuned for reasoning exhibit greater safety than those relying on CoT
prompting at inference time; and (iii) how the success rate of jailbreak
attacks targeting bias elicitation varies with the reasoning mechanisms
employed. Our findings reveal a nuanced relationship between reasoning
capabilities and bias safety. Surprisingly, models with explicit reasoning,
whether via CoT prompting or fine-tuned reasoning traces, are generally more
vulnerable to bias elicitation than base models without such mechanisms,
suggesting reasoning may unintentionally open new pathways for stereotype
reinforcement. Reasoning-enabled models appear somewhat safer than those
relying on CoT prompting, which are particularly prone to contextual reframing
attacks through storytelling prompts, fictional personas, or reward-shaped
instructions. These results challenge the assumption that reasoning inherently
improves robustness and underscore the need for more bias-aware approaches to
reasoning design.

</details>


### [52] [Multimodal Mathematical Reasoning with Diverse Solving Perspective](https://arxiv.org/abs/2507.02804)
*Wenhao Shi,Zhiqiang Hu,Yi Bin,Yang Yang,See-Kiong Ng,Heng Tao Shen*

Main category: cs.CL

TL;DR: 本文提出MathV-DP数据集和Qwen-VL-DP模型，通过多样化解题路径的数据和结合多样性奖励的强化学习方法，提高了多模态数学推理模型在准确性和推理多样性上的表现。


<details>
  <summary>Details</summary>
Motivation: 虽然大型强化学习提升了大语言模型在数学领域的推理能力，但现有多模态大模型（MLLMs）在数学推理上多依赖一对一图文配对及单一解监督，忽略了多样化推理视角和内部思考。

Method: 提出了MathV-DP数据集，为每个图像-问题对收集多条不同的解题路径，并基于Qwen-VL模型，运用监督学习微调，再通过群体相对策略优化（GRPO）这一强化学习方法进一步提升模型，其中GRPO结合了正确性判别和多样性奖励。

Result: 在MathVista's minitest和Math-V基准测试上，Qwen-VL-DP在准确率和生成多样性方面均显著优于先前的基础多模态大模型。

Conclusion: 引入多样化解题视角和反思思考机制在多模态数学推理中具有重要意义。

Abstract: Recent progress in large-scale reinforcement learning (RL) has notably
enhanced the reasoning capabilities of large language models (LLMs), especially
in mathematical domains. However, current multimodal LLMs (MLLMs) for
mathematical reasoning often rely on one-to-one image-text pairs and
single-solution supervision, overlooking the diversity of valid reasoning
perspectives and internal reflections. In this work, we introduce MathV-DP, a
novel dataset that captures multiple diverse solution trajectories for each
image-question pair, fostering richer reasoning supervision. We further propose
Qwen-VL-DP, a model built upon Qwen-VL, fine-tuned with supervised learning and
enhanced via group relative policy optimization (GRPO), a rule-based RL
approach that integrates correctness discrimination and diversity-aware reward
functions. Our method emphasizes learning from varied reasoning perspectives
and distinguishing between correct yet distinct solutions. Extensive
experiments on the MathVista's minitest and Math-V benchmarks demonstrate that
Qwen-VL-DP significantly outperforms prior base MLLMs in both accuracy and
generative diversity, highlighting the importance of incorporating diverse
perspectives and reflective reasoning in multimodal mathematical reasoning.

</details>


### [53] [SynapseRoute: An Auto-Route Switching Framework on Dual-State Large Language Model](https://arxiv.org/abs/2507.02822)
*Wencheng Zhang,Shiqin Qiao,Lingjie Luo,Yinfeng Li,Chuanyang Zheng,Qian Xu,Meng Li,Yong Gui,Yijun He,Jianing Qiu,Jindong Hong,Jiankai Sun*

Main category: cs.CL

TL;DR: 提出用机器学习方法动态决定大语言模型是用高推理还是快速模式回答医学问题，显著降低成本并提升准确率，建议多维评估模型效率及表现。


<details>
  <summary>Details</summary>
Motivation: 在应用大模型时，需要在性能和成本之间权衡，特别是在医学领域，模型思考（高推理）与非思考（快速低成本）模式的成本差距显著。如何高效分配任务以优化成本与准确率成为亟需解决的问题。

Method: 本文提出了SynapseRoute，这是一个基于机器学习的动态路由框架，可以根据问题复杂度智能地将输入问题分配到思考或非思考模式。作者还引入了用于评估准确率、延迟和token成本权衡的AIT（Accuracy-Inference-Token）指数。

Result: 在多个医学数据集上的实验表明，SynapseRoute不仅提升了总体准确率（0.8390 对 0.8272），还将推理时间减少了36.8%，token消耗减少了39.66%。

Conclusion: 动态分配推理与非推理模式可提升准确率与成本效率，避免了对简单问题的过度推理，改善了用户体验。AIT指数可做为多维度评估指标。

Abstract: With the widespread adoption of large language models (LLMs) in practical
applications, selecting an appropriate model requires balancing not only
performance but also operational cost. The emergence of reasoning-capable
models has further widened the cost gap between "thinking" (high reasoning) and
"non-thinking" (fast, low-cost) modes. In this work, we reveal that
approximately 58% of medical questions can be accurately answered by the
non-thinking mode alone, without requiring the high-cost reasoning process.
This highlights a clear dichotomy in problem complexity and suggests that
dynamically routing queries to the appropriate mode based on complexity could
optimize accuracy, cost-efficiency, and overall user experience. Based on this,
we further propose SynapseRoute, a machine learning-based dynamic routing
framework that intelligently assigns input queries to either thinking or
non-thinking modes. Experimental results on several medical datasets
demonstrate that SynapseRoute not only improves overall accuracy (0.8390 vs.
0.8272) compared to the thinking mode alone but also reduces inference time by
36.8% and token consumption by 39.66%. Importantly, qualitative analysis
indicates that over-reasoning on simpler queries can lead to unnecessary delays
and even decreased accuracy, a pitfall avoided by our adaptive routing.
Finally, this work further introduces the Accuracy-Inference-Token (AIT) index
to comprehensively evaluate the trade-offs among accuracy, latency, and token
cost.

</details>


### [54] [Generalizing Verifiable Instruction Following](https://arxiv.org/abs/2507.02833)
*Valentina Pyatkin,Saumya Malik,Victoria Graf,Hamish Ivison,Shengyi Huang,Pradeep Dasigi,Nathan Lambert,Hannaneh Hajishirzi*

Main category: cs.CL

TL;DR: 为解决大语言模型在输出约束类指令泛化能力差的问题，作者提出了IFBench评测基准，设计了RLVR强化学习训练方法，并公开相关资源，显著提升了模型的精确指令跟随能力。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型难以精准遵循包含特定输出约束（如“仅回答是或否”、“至少出现3次某个词”）的用户指令，且现有模型普遍过拟合于特定基准测试，泛化能力弱。为推动模型提升在未知输出约束下的指令执行能力，需要新的评测与训练方法。

Method: 作者提出了一个新的评测基准IFBench，包含58个全新、具挑战性、可验证的多样性约束，用于测试模型对精确指令跟随能力的泛化。此外，设计了约束验证模块，并利用包含可验证奖励的强化学习（RLVR）方法训练模型以提升其指令跟随的泛化能力。

Result: 强化学习与可验证奖励（RLVR）显著提升了模型处理输出约束的泛化能力。作者还开放了IFBench，29种新数据及对应代码、验证函数、训练prompt等资源，促进社区研究。

Conclusion: 当前大模型对输出约束的泛化能力有限，通过IFBench基准测试，结合RLVR训练，模型精准遵循复杂指令的能力大幅提升。配套资源有望推动领域进步。

Abstract: A crucial factor for successful human and AI interaction is the ability of
language models or chatbots to follow human instructions precisely. A common
feature of instructions are output constraints like ``only answer with yes or
no" or ``mention the word `abrakadabra' at least 3 times" that the user adds to
craft a more useful answer. Even today's strongest models struggle with
fulfilling such constraints. We find that most models strongly overfit on a
small set of verifiable constraints from the benchmarks that test these
abilities, a skill called precise instruction following, and are not able to
generalize well to unseen output constraints. We introduce a new benchmark,
IFBench, to evaluate precise instruction following generalization on 58 new,
diverse, and challenging verifiable out-of-domain constraints. In addition, we
perform an extensive analysis of how and on what data models can be trained to
improve precise instruction following generalization. Specifically, we
carefully design constraint verification modules and show that reinforcement
learning with verifiable rewards (RLVR) significantly improves instruction
following. In addition to IFBench, we release 29 additional new hand-annotated
training constraints and verification functions, RLVR training prompts, and
code.

</details>


### [55] [LLM Hypnosis: Exploiting User Feedback for Unauthorized Knowledge Injection to All Users](https://arxiv.org/abs/2507.02850)
*Almog Hilel,Idan Shenfeld,Leshem Choshen,Jacob Andreas*

Main category: cs.CL

TL;DR: 论文揭示：通过构造并有倾向性反馈，攻击者可以持续性操控基于用户反馈微调的语言模型，结果包括知识注入、代码漏洞生成、伪新闻传播，说明现有反馈微调机制存在被精准利用和操控的重大安全隐患。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型越来越多基于用户反馈进行微调，研究团队关注这种反馈机制是否易被恶意利用，进而威胁模型的安全性和可靠性。

Method: 研究者通过让攻击者构造轮流输出带毒和正常结果的提示，并有选择性地为带毒结果点赞、正常结果踩，进而引导模型产生偏向带毒内容的行为。随后，通过使用此反馈信号进行偏好微调，测试模型输出行为的变化。

Result: 实验显示，攻击者可通过该方法：1）向模型注入原本不具备的“事实”知识，2）修改代码生成模式，植入可被利用的安全漏洞，3）注入虚假财经新闻。此外，表明即便是高度受限的反馈数据也能实现对模型细致操控，揭示了一种新的攻击机制。

Conclusion: 这项研究指出，基于用户反馈微调的语言模型存在被单一用户持续性操控的风险，这种操控可导致模型知识和行为发生显著改变。

Abstract: We describe a vulnerability in language models (LMs) trained with user
feedback, whereby a single user can persistently alter LM knowledge and
behavior given only the ability to provide prompts and upvote / downvote
feedback on LM outputs. To implement the attack, the attacker prompts the LM to
stochastically output either a "poisoned" or benign response, then upvotes the
poisoned response or downvotes the benign one. When feedback signals are used
in a subsequent preference tuning behavior, LMs exhibit increased probability
of producing poisoned responses even in contexts without malicious prompts. We
show that this attack can be used to (1) insert factual knowledge the model did
not previously possess, (2) modify code generation patterns in ways that
introduce exploitable security flaws, and (3) inject fake financial news. Our
finding both identifies a new qualitative feature of language model preference
tuning (showing that it even highly restricted forms of preference data can be
used to exert fine-grained control over behavior), and a new attack mechanism
for LMs trained with user feedback (extending work on pretraining-time data
poisoning and deployment-time prompt injection).

</details>


### [56] [MOTIF: Modular Thinking via Reinforcement Fine-tuning in LLMs](https://arxiv.org/abs/2507.02851)
*Purbesh Mitra,Sennur Ulukus*

Main category: cs.CL

TL;DR: 本文提出的MOTIF方法通过多轮分模块推理和强化学习微调，突破了大语言模型的上下文限制，提高了在复杂推理任务中的表现，并且训练更加高效。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在推理能力上取得了显著进展，但受限于上下文窗口（context size）只能关注有限数量的token，这限制了其处理更复杂和长链条推理任务的能力。为突破这一瓶颈，需要开发能够分轮次、多模块推理的方法。

Method: 提出了一种新方法MOTIF（Modular Thinking via Reinforcement Finetuning），通过强化学习微调方式，训练模型在多轮次生成“思考token”，相当于增加了可用的上下文长度。具体做法是在Qwen2.5-3B-Instruct模型上，利用参数高效的微调技术，并在GSM8K数据集上进行训练。

Result: 在MATH500和AIME2024两个基准数据集上，MOTIF方法相较于传统GRPO方法的准确率分别提升了3.8%和3.3%。此外，这一提升只用了原训练样本的15%，显示出了很高的样本效率。

Conclusion: MOTIF方法通过强化学习微调和分轮推理方案，有效突破了大模型上下文长度的限制，提升了模型在复杂推理任务上的表达和推理能力。该方法兼具效果提升和样本高效性。

Abstract: Recent advancements in the reasoning capabilities of large language models
(LLMs) show that employing group relative policy optimization (GRPO) algorithm
for reinforcement learning (RL) training allows the models to use more
thinking/reasoning tokens for generating better responses. However, LLMs can
generate only a finite amount of tokens while maintaining attention to the
previously generated tokens. This limit, also known as the context size of an
LLM, is a bottleneck in LLM reasoning with arbitrarily large number of tokens.
To think beyond the limit of context size, an LLM must employ a modular
thinking strategy to reason over multiple rounds. In this work, we propose
$\textbf{MOTIF: Modular Thinking via Reinforcement Finetuning}$ -- an RL
training method for generating thinking tokens in multiple rounds, effectively
allowing the model to think with additional context size. We trained the
open-source model Qwen2.5-3B-Instruct on GSM8K dataset via parameter efficient
fine-tuning and tested its accuracy on MATH500 and AIME2024 benchmarks. Our
experiments show 3.8\% and 3.3\% improvements over vanilla GRPO based training
in the respective benchmarks. Furthermore, this improvement was achieved with
only 15\% of samples, thus demonstrating sample efficiency of MOTIF. Our code
and models are available at https://github.com/purbeshmitra/MOTIF and
https://huggingface.co/purbeshmitra/MOTIF, respectively.

</details>


### [57] [Answer Matching Outperforms Multiple Choice for Language Model Evaluation](https://arxiv.org/abs/2507.02856)
*Nikhil Chandak,Shashwat Goel,Ameya Prabhu,Moritz Hardt,Jonas Geiping*

Main category: cs.CL

TL;DR: 传统多项选择题评估容易被“猜答案”等捷径绕过，不再适应现有大模型。作者提出基于自由生成答案和自动答案匹配的新评估范式，验证其和人工评分高度一致，建议评估方式整体转向答案匹配。


<details>
  <summary>Details</summary>
Motivation: 多项选择题由于评分客观且易于自动化，长期被用作语言模型评估的主流方式。但作者发现，多项选择题的题干通常可以被绕开，仅凭选项就能作答，暴露了评估的基本缺陷。

Method: 提出了一种基于生成式答案匹配的新评估方法：让模型在没有选项的情况下自由生成答案，然后用现代语言模型与参考答案进行比对判定。接着，在MMLU-Pro和GPQA-Diamond基准集上进行了人工评分标注，用以比较多种评估策略与人工评分的一致性。

Result: 基于生成式答案匹配的评估与人工评分几乎一致，甚至使用较小的模型亦可达到近乎完美的一致性。而多项选择题和不参考标准答案的模型自评则与人工评分存在较大差距。模型排名也因评估方法的变更发生显著变化。

Conclusion: 基于答案匹配的生成式评估更贴近人类评分，推荐用其取代多项选择题模式，并为评估生态的转变提供了方向。

Abstract: Multiple choice benchmarks have long been the workhorse of language model
evaluation because grading multiple choice is objective and easy to automate.
However, we show multiple choice questions from popular benchmarks can often be
answered without even seeing the question. These shortcuts arise from a
fundamental limitation of discriminative evaluation not shared by evaluations
of the model's free-form, generative answers. Until recently, there appeared to
be no viable, scalable alternative to multiple choice--but, we show that this
has changed. We consider generative evaluation via what we call answer
matching: Give the candidate model the question without the options, have it
generate a free-form response, then use a modern language model with the
reference answer to determine if the response matches the reference. To compare
the validity of different evaluation strategies, we annotate MMLU-Pro and
GPQA-Diamond to obtain human grading data, and measure the agreement of each
evaluation approach. We find answer matching using recent models--even small
ones--achieves near-perfect agreement, in the range of inter-annotator
agreement. In contrast, both multiple choice evaluation and using
LLM-as-a-judge without reference answers aligns poorly with human grading.
Improving evaluations via answer matching is not merely a conceptual concern:
the rankings of several models change significantly when evaluating their
free-form responses with answer matching. In light of these findings, we
discuss how to move the evaluation ecosystem from multiple choice to answer
matching.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [58] [Discovery of Fatigue Strength Models via Feature Engineering and automated eXplainable Machine Learning applied to the welded Transverse Stiffener](https://arxiv.org/abs/2507.02005)
*Michael A. Kraus,Helen Bartsch*

Main category: cs.CE

TL;DR: 通过AutoML与XAI结合，能构建既准确又具可解释性的焊接疲劳强度预测模型。用集成学习和特征重要性分析，识别了关键影响因素，验证了极简和专家知识驱动模型的实用性，为工程AI设计与数字孪生应用奠定基础。


<details>
  <summary>Details</summary>
Motivation: 预测焊接横向加劲肋细节的疲劳强度对于钢结构设计至关重要，但传统方法在精度和可解释性上存在局限，工程领域急需自动化且高可解释性的AI模型辅助设计与评估。

Method: 提出了将自动化机器学习（AutoML）与可解释人工智能（XAI）相结合的方法，综合专家特征工程和算法自动特征生成，通过梯度提升、随机森林、神经网络等回归模型，在不同特征方案下对疲劳强度进行建模。模型性能通过三种特征策略（领域专家、算法生成、联合）系统比较，并利用XAI工具（SHAP值、特征重要性）分析主要影响因子。

Result: CatBoost、LightGBM等集成方法表现最优。领域知识特征模型在全样本RMSE约30.6MPa，R^2约0.78；在工程实际区间（0-150MPa）RMSE约13.4MPa，R^2约0.53。更复杂特征模型虽训练表现略好，但泛化能力下降，简化模型依然具有竞争力。XAI分析发现应力比、应力幅、屈服强度及后处理方式为主导因素，几何参数为次要影响因素。

Conclusion: 本框架将AutoML与XAI结合，能高效、精确并具可解释性地预测焊接钢结构疲劳强度，实现数据驱动与工程知识融合。为AI辅助结构设计检测提供了新思路。后续拟拓展至概率预测与数字孪生场景。

Abstract: This research introduces a unified approach combining Automated Machine
Learning (AutoML) with Explainable Artificial Intelligence (XAI) to predict
fatigue strength in welded transverse stiffener details. It integrates
expert-driven feature engineering with algorithmic feature creation to enhance
accuracy and explainability.
  Based on the extensive fatigue test database regression models - gradient
boosting, random forests, and neural networks - were trained using AutoML under
three feature schemes: domain-informed, algorithmic, and combined. This allowed
a systematic comparison of expert-based versus automated feature selection.
  Ensemble methods (e.g. CatBoost, LightGBM) delivered top performance. The
domain-informed model $\mathcal M_2$ achieved the best balance: test RMSE
$\approx$ 30.6 MPa and $R^2 \approx 0.780% over the full $\Delta
\sigma_{c,50\%}$ range, and RMSE $\approx$ 13.4 MPa and $R^2 \approx 0.527%
within the engineering-relevant 0 - 150 MPa domain. The denser-feature model
($\mathcal M_3$) showed minor gains during training but poorer generalization,
while the simpler base-feature model ($\mathcal M_1$) performed comparably,
confirming the robustness of minimalist designs.
  XAI methods (SHAP and feature importance) identified stress ratio $R$, stress
range $\Delta \sigma_i$, yield strength $R_{eH}$, and post-weld treatment (TIG
dressing vs. as-welded) as dominant predictors. Secondary geometric factors -
plate width, throat thickness, stiffener height - also significantly affected
fatigue life.
  This framework demonstrates that integrating AutoML with XAI yields accurate,
interpretable, and robust fatigue strength models for welded steel structures.
It bridges data-driven modeling with engineering validation, enabling
AI-assisted design and assessment. Future work will explore probabilistic
fatigue life modeling and integration into digital twin environments.

</details>


### [59] [Time Resolution Independent Operator Learning](https://arxiv.org/abs/2507.02524)
*Diab W. Abueidda,Mbebo Nonna,Panos Pantidis,Mostafa E. Mobasher*

Main category: cs.CE

TL;DR: 本文提出NCDE-DeepONet，将NCDE与DeepONet结合，实现了对不同采样分辨率和任意时空坐标的高效、高精度PDE算子学习，实验验证方法优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 针对时变偏微分方程(PDEs)在稀疏、不规则数据下的解算算子学习仍面临挑战，现有方法如递归DeepONet和neural-ODE分别存在时间离散性和无法嵌入新增输入的问题。

Method: 提出了NCDE-DeepONet，一种连续时间算子网络，将神经受控微分方程(NCDE)嵌入分支，主干显式采样时空坐标。通过样条插值驱动输入路径，NCDE编码完整负载历史（输入分辨率无关），主干可在任意时空查询（输出分辨率无关）。

Result: 在瞬态泊松方程、弹性动力学、热弹性问题的基准测试中表现出鲁棒性和高精度，解算几乎即时。

Conclusion: 控制动力学为高精度、时变PDE算子学习提供了高效、理论扎实的新基础。

Abstract: Accurately learning solution operators for time-dependent partial
differential equations (PDEs) from sparse and irregular data remains a
challenging task. Recurrent DeepONet extensions inherit the discrete-time
limitations of sequence-to-sequence (seq2seq) RNN architectures, while
neural-ODE surrogates cannot incorporate new inputs after initialization. We
introduce NCDE-DeepONet, a continuous-time operator network that embeds a
Neural Controlled Differential Equation (NCDE) in the branch and augments the
trunk with explicit space-time coordinates. The NCDE encodes an entire load
history as the solution of a controlled ODE driven by a spline-interpolated
input path, making the representation input-resolution-independent: it encodes
different input signal discretizations of the observed samples. The trunk then
probes this latent path at arbitrary spatial locations and times, rendering the
overall map output-resolution independent: predictions can be queried on meshes
and time steps unseen during training without retraining or interpolation.
Benchmarks on transient Poisson, elastodynamic, and thermoelastic problems
confirm the robustness and accuracy of the framework, achieving almost instant
solution prediction. These findings suggest that controlled dynamics provide a
principled and efficient foundation for high-fidelity operator learning in
transient mechanics.

</details>


### [60] [Imitation and Heterogeneity Shape the Resilience of Community Currency Networks](https://arxiv.org/abs/2507.02678)
*Camilla Ancona,Dora Ricci,Carmela Bernardo,Francesco Lo Iudice,Anton Proskurnikov,Francesco Vasca*

Main category: cs.CE

TL;DR: 本文利用图论方法分析Sardex社区货币网络，发现核心结构和行为倾向偏离随机模型，行为模仿与用户异质连接增强网络韧性，对社区货币系统设计与管理有实际指导意义。


<details>
  <summary>Details</summary>
Motivation: 本文旨在通过对撒丁岛Sardex社区货币的案例研究，深入了解社区货币网络（以虚拟货币为交易媒介的个体或公司网络）的结构与动态特性。特别关注网络核心与边缘结构的时序演变、流量不对称性，以及不同用户类型导致的结构碎片化，探索用户行为和网络拓扑间的关联。

Method: 将交易网络建模为有向加权图，运用图论方法分析，包括强连通分量、网络凝聚表示和行为连通模式，重点考察网络结构及其随时间的变化。还构建不同用户类型间的连接异质性，并与基于度的空模型进行对比分析。

Result: 结果显示网络结构持续地偏离度分布的空模型，发现用户表现出行为模仿倾向，偏好与更活跃的节点交互。不同类型用户之间的异质性连接增强了网络结构，提升了网络韧性。

Conclusion: 社区货币网络的结构和行为特征不仅反映了个体间的经济互动偏好，也说明了网络稳健性受异质性连接影响显著。基于活跃节点的连接偏好和行为模仿，为推动社区货币系统的健康发展提供了机制性洞见。

Abstract: Community currency networks are made up of individuals and or companies that
share some physical or social characteristics and engage in economic
transactions using a virtual currency. This paper investigates the structural
and dynamic properties of such mutual credit systems through a case study of
Sardex, a community currency initiated and mainly operating in Sardinia, Italy.
The transaction network is modeled as a directed weighted graph and analyzed
through a graph theoretic framework focused on the analysis of strongly
connected components, condensed representations, and behavioral connectivity
patterns. Emphasis is placed on understanding the evolution of the network's
core and peripheral structures over a three year period, with attention to
temporal contraction, flow asymmetries, and structural fragmentation depending
on different user types. Our findings reveal persistent deviations from degree
based null models and suggest the presence of behavioral imitation,
specifically, a user preference for more active peers. We further assess the
impact of heterogeneous connections between different type of users, which
strengthen the network topology and enhance its resilience.

</details>


### [61] [Constraint-Guided Symbolic Regression for Data-Efficient Kinetic Model Discovery](https://arxiv.org/abs/2507.02730)
*Miguel Ángel de Carvalho Servia,Ilya Orson Sandoval,King Kuok,Hii,Klaus Hellgardt,Dongda Zhang,Ehecatl Antonio del Rio Chanona*

Main category: cs.CE

TL;DR: 作者提出了一种结合物理约束和不确定性量化的动力学建模框架，能以更少实验获得更准确、可解释的催化动力学模型，适用于化学工程领域。


<details>
  <summary>Details</summary>
Motivation: 催化过程的工业化需要可靠的动力学模型，用于设计、优化和控制。传统的机理模型需要大量领域知识，而数据驱动方法通常缺乏可解释性和物理约束。

Method: 提出了物理信息驱动的自动动力学发现（PI-ADoK）框架，通过在符号回归方法中直接融入物理约束，缩小模型搜索空间，并利用Metropolis-Hastings算法进行参数不确定性传播，实现稳健的不确定性量化。

Result: 与传统方法在多个催化案例对比中，PI-ADoK提高了动力学模型的准确性，并显著降低了实验需求。

Conclusion: PI-ADoK框架能够有效、可靠地自动发现化学反应动力学模型，在化学反应工程领域有较大应用前景。

Abstract: The industrialization of catalytic processes hinges on the availability of
reliable kinetic models for design, optimization, and control. Traditional
mechanistic models demand extensive domain expertise, while many data-driven
approaches often lack interpretability and fail to enforce physical
consistency. To overcome these limitations, we propose the Physics-Informed
Automated Discovery of Kinetics (PI-ADoK) framework. By integrating physical
constraints directly into a symbolic regression approach, PI-ADoK narrows the
search space and substantially reduces the number of experiments required for
model convergence. Additionally, the framework incorporates a robust
uncertainty quantification strategy via the Metropolis-Hastings algorithm,
which propagates parameter uncertainty to yield credible prediction intervals.
Benchmarking our method against conventional approaches across several
catalytic case studies demonstrates that PI-ADoK not only enhances model
fidelity but also lowers the experimental burden, highlighting its potential
for efficient and reliable kinetic model discovery in chemical reaction
engineering.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [62] [Computer Science Education in the Age of Generative AI](https://arxiv.org/abs/2507.02183)
*Russell Beale*

Main category: cs.CY

TL;DR: 生成式AI如ChatGPT正在变革计算机科学教学，既带来新机遇又引发诚信等挑战，作者探讨如何应对并提出相关政策建议。


<details>
  <summary>Details</summary>
Motivation: 生成式AI工具如大型语言模型（LLM）正迅速革新计算机科学教育，作者希望探讨这些技术对教育的深远影响以及带来的新机遇与挑战。

Method: 本文采用文献综述和现有研究的实证数据，分析生成式AI对计算机科学教育的影响，并结合教学实践与政策建议进行探讨。

Result: 作者提出了在AI普及环境下整合AI工具进课程的建议、关注学术诚信、评价原创新颖以及调整教学内容和考核方式，还给出了具体的政策建议。

Conclusion: 生成式AI为计算机科学教育带来诸多机遇，但必须妥善处理学术诚信和学生对AI的依赖，才能最大化其积极价值并维护教育公正性与严谨性。

Abstract: Generative AI tools - most notably large language models (LLMs) like ChatGPT
and Codex - are rapidly revolutionizing computer science education. These tools
can generate, debug, and explain code, thereby transforming the landscape of
programming instruction. This paper examines the profound opportunities that AI
offers for enhancing computer science education in general, from coding
assistance to fostering innovative pedagogical practices and streamlining
assessments. At the same time, it highlights challenges including academic
integrity concerns, the risk of over-reliance on AI, and difficulties in
verifying originality. We discuss what computer science educators should teach
in the AI era, how to best integrate these technologies into curricula, and the
best practices for assessing student learning in an environment where AI can
generate code, prototypes and user feedback. Finally, we propose a set of
policy recommendations designed to harness the potential of generative AI while
preserving the integrity and rigour of computer science education. Empirical
data and emerging studies are used throughout to support our arguments.

</details>


### [63] [Defining DLT Immutability: A Qualitative Survey of Node Operators](https://arxiv.org/abs/2507.02413)
*Alex Lynham,Geoff Goodell*

Main category: cs.CY

TL;DR: 区块链号称不可变，但重写事件与治理介入力度说明其实难以绝对不可变。文章通过采访节点运营者，总结出实际运行中‘实际不可变性’的特征及其局限性。


<details>
  <summary>Details</summary>
Motivation: 虽然区块链旨在实现不可变性，但现实中重写与攻击事件频发，实际网络状态常因治理被改变，因此需要对“不可变性”进行再定义和实际理解。

Method: 通过对节点运营者进行访谈，并采用主题分析法，从中剖析区块链网络中不可变性的局限，以及在历史重写事件中的表现。

Result: 提出了“实际不可变性”（Practical Immutability）的概念，强调网络账本状态更多依赖治理结构和利益相关者的信任，而非绝对技术保证。

Conclusion: 在区块链网络中，真正的严格不可变性其实难以实现或不存在。实际上的“不可变性”由网络治理和利益相关者共识决定，作者称之为“实际不可变性”。

Abstract: Immutability is a core design goal of permissionless public blockchain
systems. However, rewrites are more common than is normally understood, and the
risk of rewrite, cyberattack, exploit or black swan event is also high. Taking
the position that strict immutability is neither possible on these networks nor
the observed reality, this paper uses thematic analysis of node operator
interviews to examine the limits of immutability in light of rewrite events.
The end result is a qualitative definition of the conditional immutability
found on these networks, which we call Practical Immutability. This is
immutability contingent on the legitimate governance demands of the network,
where network stakeholders place their trust in the governance topology of a
network to lend it legitimacy, and thus manage ledger state.

</details>


### [64] [Recourse, Repair, Reparation, & Prevention: A Stakeholder Analysis of AI Supply Chains](https://arxiv.org/abs/2507.02648)
*Aspen K. Hopkins,Isabella Struckman,Kevin Klyman,Susan S. Silbey*

Main category: cs.CY

TL;DR: 本文分析了AI供应链的利益相关方、潜在危害及其应对机制，针对医疗健康领域提出了分类型响应措施。强调通过优化AI供应链结构，实现风险可控和负责任治理。


<details>
  <summary>Details</summary>
Motivation: AI产业的广泛应用带来了越来越多的关注于其潜在的危害和不良后果。当前AI部署通常依赖由多方参与的AI供应链（AISC），而这些供应链缺乏传统供应链的模块性与冗余设计，难以有效识别和纠正问题，导致AI相关危害难以处理。因此，研究如何识别AISC中的参与者、危害及其来源、以及救济措施变得非常重要。

Method: 本文采用利益相关者分析法，系统梳理了AI供应链中的参与者、他们面临的危害、危害的来源、以及市场动态和权力差异如何影响救济方式。作者提出一套响应AI供应链危害的类型学：申诉（recourse）、修复（repair）、补偿（reparation）或预防（prevention）。该类型学被应用于医疗健康领域AISC，并在三种典型市场结构（垂直整合、水平整合、自由市场）下，分析不同利益相关者的地位和权力如何影响对危害的应对。

Result: 文章提出了一套关于AI供应链中危害响应的类型学体系，并通过医疗健康领域的案例，表明利益相关方的地位和市场结构会显著影响他们获得救济的方式和可能性。这为负责任的AISC设计与管理提供了理论和实践指导。

Conclusion: AI供应链是可人为设计的，它们应当正视并纳入AI系统带来的复杂性、后果与风险。利益相关方的分析与危害响应类型学，有助于推动AISC的负责任治理。合适的设计与权力结构调整可以更好地保护各方利益，减少ML系统带来的危害。

Abstract: The AI industry is exploding in popularity, with increasing attention to
potential harms and unwanted consequences. In the current digital ecosystem, AI
deployments are often the product of AI supply chains (AISC): networks of
outsourced models, data, and tooling through which multiple entities contribute
to AI development and distribution. AI supply chains lack the modularity,
redundancies, or conventional supply chain practices that enable
identification, isolation, and easy correction of failures, exacerbating the
already difficult processes of responding to ML-generated harms. As the
stakeholders participating in and impacted by AISCs have scaled and
diversified, so too have the risks they face. In this stakeholder analysis of
AI supply chains, we consider who participates in AISCs, what harms they face,
where sources of harm lie, and how market dynamics and power differentials
inform the type and probability of remedies. Because AI supply chains are
purposely invented and implemented, they may be designed to account for, rather
than ignore, the complexities, consequences, and risks of deploying AI systems.
To enable responsible design and management of AISCs, we offer a typology of
responses to AISC-induced harms: recourse, repair, reparation or prevention. We
apply this typology to stakeholders participating in a health-care AISC across
three stylized markets $\unicode{x2013}$ vertical integration, horizontal
integration, free market $\unicode{x2013}$ to illustrate how stakeholder
positioning and power within an AISC may shape responses to an experienced
harm.

</details>


<div id='q-fin.TR'></div>

# q-fin.TR [[Back]](#toc)

### [65] [A Midsummer Meme's Dream: Investigating Market Manipulations in the Meme Coin Ecosystem](https://arxiv.org/abs/2507.01963)
*Alberto Maria Mongardini,Alessandro Mei*

Main category: q-fin.TR

TL;DR: 本文通过对跨链3.5万余个meme币的追踪，揭示了高回报meme币背后广泛存在的价格操控行为，投资者需警惕其人为风险。


<details>
  <summary>Details</summary>
Motivation: meme币已成为加密货币市场中最受欢迎的板块之一，但其价值主要源自社区情绪，容易受到人为操纵。研究动机在于系统分析meme币生态，并揭示其背后的操纵行为及其对市场的影响。

Method: 本文对横跨Ethereum、BNB Smart Chain、Solana和Base的34,988个meme币进行横向链上分析，并进行了为期三个月的追踪。重点分析了meme币的代币经济学及其增长过程。通过识别高回报代币，进一步检测是否存在操纵行为，包括洗盘交易、流动池驱动价格膨胀（LPI）、拉高出货（pump and dump）和“地毯拉扯”（rug pull）等。

Result: 在高回报（>100%）的代币中，发现高达82.6%的案例存在显著的人为操纵迹象，如洗盘和LPI等策略。此外，多数被操控的Token随后还会发展出拉高出货、地毯拉扯等侵害投资者的行为，而初期的操控为后续的剥削埋下了伏笔。

Conclusion: 高收益meme币普遍存在操纵现象，这些币种的暴涨大多是协同行为结果，而非自然市场动态，投资者面临较高风险。

Abstract: From viral jokes to a billion-dollar phenomenon, meme coins have become one
of the most popular segments in cryptocurrency markets. Unlike utility-focused
crypto assets like Bitcoin or Ethereum, meme coins derive value primarily from
community sentiment, making them vulnerable to manipulation. This study
presents a cross-chain analysis of the meme coin ecosystem, examining 34,988
tokens across Ethereum, BNB Smart Chain, Solana, and Base. We characterize the
tokenomics of meme coins and track their growth in a three-month longitudinal
analysis. We discover that among high-return tokens (>100%), an alarming 82.6%
show evidence of extensive use of artificial growth strategies designed to
create a misleading appearance of market interest. These include wash trading
and a form of manipulation we define as Liquidity Pool-Based Price Inflation
(LPI), where small strategic purchases trigger dramatic price increases. We
also find evidence of schemes designed to profit at the expense of investors,
such as pump and dumps and rug pulls. In particular, most of the tokens
involved had previously experienced wash trading or LPI, indicating how initial
manipulations often set the stage for later exploitation. These findings reveal
that manipulations are widespread among high-performing meme coins and suggest
that their dramatic gains are often likely driven by coordinated efforts rather
than natural market dynamics.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [66] [FinAI-BERT: A Transformer-Based Model for Sentence-Level Detection of AI Disclosures in Financial Reports](https://arxiv.org/abs/2507.01991)
*Muhammad Bilal Zafar*

Main category: q-fin.CP

TL;DR: 本文提出了适用于金融行业文本的AI内容句级检测模型FinAI-BERT，在美国银行年报实测中几乎达到完美分类表现，并具备可解释性和良好鲁棒性，为金融AI监管和研究提供有力工具。


<details>
  <summary>Details</summary>
Motivation: 金融服务领域中人工智能（AI）广泛应用，导致市场对系统性检测企业文件中AI相关披露的工具需求上升。以往方法如关键词扩展、文档级分类，难以满足更细粒度、可解释性和稳健性的要求。

Method: 提出了FinAI-BERT，一种领域适应的基于Transformer的语言模型，专注于金融文本句子级AI内容分类。模型在手工筛选并均衡的1,586句数据上微调，涵盖2015-2023年间669份美国银行年报；同时应用SHAP方法提升模型可解释性，并进行偏见和鲁棒性分析。

Result: FinAI-BERT分类准确率高达99.37%，F1分数为0.993，显著优于Logistic回归、朴素贝叶斯、随机森林、XGBoost等传统基线模型。其稳定性在不同句长、对抗样本及跨时间检测中均得到验证。

Conclusion: FinAI-BERT为AI相关金融文本信息提取提供了细粒度、透明且高效的解决方案，推动了金融自然语言处理的理论与应用发展，有助于分析师、监管者和学者监测AI在金融体系中的扩散和表述。

Abstract: The proliferation of artificial intelligence (AI) in financial services has
prompted growing demand for tools that can systematically detect AI-related
disclosures in corporate filings. While prior approaches often rely on keyword
expansion or document-level classification, they fall short in granularity,
interpretability, and robustness. This study introduces FinAI-BERT, a
domain-adapted transformer-based language model designed to classify AI-related
content at the sentence level within financial texts. The model was fine-tuned
on a manually curated and balanced dataset of 1,586 sentences drawn from 669
annual reports of U.S. banks (2015 to 2023). FinAI-BERT achieved near-perfect
classification performance (accuracy of 99.37 percent, F1 score of 0.993),
outperforming traditional baselines such as Logistic Regression, Naive Bayes,
Random Forest, and XGBoost. Interpretability was ensured through SHAP-based
token attribution, while bias analysis and robustness checks confirmed the
model's stability across sentence lengths, adversarial inputs, and temporal
samples. Theoretically, the study advances financial NLP by operationalizing
fine-grained, theme-specific classification using transformer architectures.
Practically, it offers a scalable, transparent solution for analysts,
regulators, and scholars seeking to monitor the diffusion and framing of AI
across financial institutions.

</details>
