<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 2]
- [cs.CE](#cs.CE) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Subjective Evaluation Profile Analysis of Science Fiction Short Stories and its Critical-Theoretical Significance](https://arxiv.org/abs/2507.11582)
*Kazuyoshi Otsuka*

Main category: cs.CL

TL;DR: 大模型在文学评价上的主观性较强，不同模型间表现出独特风格和倾向，类似人类批评流派。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型（LLMs）在文学评价中的主观性和偏好，考察其是否拥有类似人类批评流派的独特价值体系，而非单纯中立的评判工具。

Method: 将十部日语科幻短篇翻译为英文，让六种先进的LLM在七个不同时间段独立打分。通过主成分分析和聚类方法分析一致性和评价模式，并利用TF-IDF分析不同模型的用语特征。

Result: （1）LLM间评价一致性差异显著（α从1.00到0.35）；（2）找出五种不同的评价模式；（3）不同作品的评价方差高达4.5倍；（4）每个模型的评价语言具有独特性。

Conclusion: LLM在文学批评中的表现显示出类似人类批评学派的个体评价特征，不是完全中立的评判者，其价值体系受RLHF及训练过程影响显著。

Abstract: This study positions large language models (LLMs) as "subjective literary
critics" to explore aesthetic preferences and evaluation patterns in literary
assessment. Ten Japanese science fiction short stories were translated into
English and evaluated by six state-of-the-art LLMs across seven independent
sessions. Principal component analysis and clustering techniques revealed
significant variations in evaluation consistency ({\alpha} ranging from 1.00 to
0.35) and five distinct evaluation patterns. Additionally, evaluation variance
across stories differed by up to 4.5-fold, with TF-IDF analysis confirming
distinctive evaluation vocabularies for each model. Our seven-session
within-day protocol using an original Science Fiction corpus strategically
minimizes external biases, allowing us to observe implicit value systems shaped
by RLHF and their influence on literary judgment. These findings suggest that
LLMs may possess individual evaluation characteristics similar to human
critical schools, rather than functioning as neutral benchmarkers.

</details>


### [2] [MapIQ: Benchmarking Multimodal Large Language Models for Map Question Answering](https://arxiv.org/abs/2507.11625)
*Varun Srivastava,Fan Lei,Srija Mukhopadhyay,Vivek Gupta,Ross Maciejewski*

Main category: cs.CL

TL;DR: 本研究提出包含多地图类型与丰富主题的MapIQ基准，用于全面评估MLLMs在地图视觉问答任务中的能力，发现模型在不同任务与地图设计上的表现不一，揭示了改进空间。


<details>
  <summary>Details</summary>
Motivation: 以往的地图视觉问答（Map-VQA）研究过于聚焦于晕染地图，未能覆盖更多类型的地图与更丰富的主题及分析任务，对MLLMs在更广泛地图视觉分析任务中的能力缺乏全面评估。

Method: 作者提出了MapIQ基准数据集，包含14706组涵盖晕染地图、变形地图和比例符号地图三种类型、横跨六大主题的问题-答案对。通过六种视觉分析任务，评估多种MLLMs的表现，与人类基准进行对比，并通过改变量图设计（如配色、图例、元素移除）实验，分析其鲁棒性、地理知识依赖及改进方向。

Result: 多个MLLMs在不同类型地图及视觉分析任务中的表现有较大差异，与人类基准仍有差距。地图设计的变化显著影响了模型的鲁棒性及解题能力。研究揭示了当前MLLMs在处理多样地图及复杂视觉任务上的不足及依赖内在地理知识的情况。

Conclusion: 该工作通过提出多类型、跨主题的地图视觉问答基准，有效弥补了现有Map-VQA研究的局限，为未来提升MLLM地图感知与问答能力提供了新方向与数据资源。

Abstract: Recent advancements in multimodal large language models (MLLMs) have driven
researchers to explore how well these models read data visualizations, e.g.,
bar charts, scatter plots. More recently, attention has shifted to visual
question answering with maps (Map-VQA). However, Map-VQA research has primarily
focused on choropleth maps, which cover only a limited range of thematic
categories and visual analytical tasks. To address these gaps, we introduce
MapIQ, a benchmark dataset comprising 14,706 question-answer pairs across three
map types: choropleth maps, cartograms, and proportional symbol maps spanning
topics from six distinct themes (e.g., housing, crime). We evaluate multiple
MLLMs using six visual analytical tasks, comparing their performance against
one another and a human baseline. An additional experiment examining the impact
of map design changes (e.g., altered color schemes, modified legend designs,
and removal of map elements) provides insights into the robustness and
sensitivity of MLLMs, their reliance on internal geographic knowledge, and
potential avenues for improving Map-VQA performance.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [3] [Quantifying data needs in surrogate modeling for flow fields in 2D stirred tanks with physics-informed neural networks (PINNs)](https://arxiv.org/abs/2507.11640)
*Veronika Trávníková,Eric von Lieres,Marek Behr*

Main category: cs.CE

TL;DR: 在CFD计算代价高昂、数据获取困难的背景下，PINNs只需极少数据就能高精度建模搅拌罐流场，适合高效工程优化。


<details>
  <summary>Details</summary>
Motivation: 搅拌罐在化工和生物工艺过程中非常重要，但用CFD模拟流动虽然准确，但计算开销大，尤其是在需要多次查询用于过程设计和优化时。因此，开发高效的数据驱动代理模型变得必要。然而，获得足够大型的数据集又很昂贵。为此，寻找一种既能减少数据需求又能保持精度的新方法成为主要动机。

Method: 本文采用物理信息神经网络（PINNs）作为核心方法，通过将物理规律嵌入神经网络的训练过程来降低对数据规模的需求。研究量化了PINNs作为搅拌罐二维流场代理模型的数据需求，并与传统监督神经网络及边界信息神经网络（BINNs）进行了对比。部分实验还尝试用速度剖面的近似值代替真实标签数据进行训练。

Result: 使用仅六个数据点，PINN代理模型在Reynolds数50至5000范围内的预测误差约为3%；若用速度剖面近似值替换真实标签，预测误差还能降至2.5%左右。这说明即使数据有限或不精准，PINNs依然能获得高精度的代理模型。

Conclusion: 物理信息神经网络（PINNs）能够以极少的数据量训练出高精度的搅拌罐流场代理模型，即便数据为近似值也能保证精度。相较于传统神经网络，PINNs在数据受限场景下尤其有效。

Abstract: Stirred tanks are vital in chemical and biotechnological processes,
particularly as bioreactors. Although computational fluid dynamics (CFD) is
widely used to model the flow in stirred tanks, its high computational
cost$-$especially in multi-query scenarios for process design and
optimization$-$drives the need for efficient data-driven surrogate models.
However, acquiring sufficiently large datasets can be costly. Physics-informed
neural networks (PINNs) offer a promising solution to reduce data requirements
while maintaining accuracy by embedding underlying physics into neural network
(NN) training. This study quantifies the data requirements of vanilla PINNs for
developing surrogate models of a flow field in a 2D stirred tank. We compare
these requirements with classical supervised neural networks and
boundary-informed neural networks (BINNs). Our findings demonstrate that
surrogate models can achieve prediction errors around 3% across Reynolds
numbers from 50 to 5000 using as few as six datapoints. Moreover, employing an
approximation of the velocity profile in place of real data labels leads to
prediction errors of around 2.5%. These results indicate that even with limited
or approximate datasets, PINNs can be effectively trained to deliver high
accuracy comparable to high-fidelity data.

</details>
