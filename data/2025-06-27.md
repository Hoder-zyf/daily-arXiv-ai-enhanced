<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 13]
- [cs.CL](#cs.CL) [Total: 39]
- [cs.CE](#cs.CE) [Total: 3]
- [cs.CY](#cs.CY) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [The Singapore Consensus on Global AI Safety Research Priorities](https://arxiv.org/abs/2506.20702)
*Yoshua Bengio,Tegan Maharaj,Luke Ong,Stuart Russell,Dawn Song,Max Tegmark,Lan Xue,Ya-Qin Zhang,Stephen Casper,Wan Sie Lee,Sören Mindermann,Vanessa Wilfred,Vidhisha Balachandran,Fazl Barez,Michael Belinsky,Imane Bello,Malo Bourgon,Mark Brakel,Siméon Campos,Duncan Cass-Beggs,Jiahao Chen,Rumman Chowdhury,Kuan Chua Seah,Jeff Clune,Juntao Dai,Agnes Delaborde,Nouha Dziri,Francisco Eiras,Joshua Engels,Jinyu Fan,Adam Gleave,Noah Goodman,Fynn Heide,Dan Hendrycks,Cyrus Hodes,Bryan Low Kian Hsiang,Minlie Huang,Sami Jawhar,Wang Jingyu,Adam Tauman Kalai,Meindert Kamphuis,Mohan Kankanhalli,Subhash Kantamneni,Mathias Bonde Kirk,Thomas Kwa,Jeffrey Ladish,Kwok-Yan Lam,Wan Lee Sie,Taewhi Lee,Xiaojian Li,Jiajun Liu,Chaochao Lu,Yifan Mai,Richard Mallah,Julian Michael,Nick Moës,Simon Möller,Kihyuk Nam,Kwan Yee Ng,Mark Nitzberg,Besmira Nushi,Seán O hÉigeartaigh,Alejandro Ortega,Pierre Peigné,James Petrie,Benjamin Prud'Homme,Reihaneh Rabbany,Nayat Sanchez-Pi,Sarah Schwettmann,Buck Shlegeris,Saad Siddiqui,Aradhana Sinha,Martín Soto,Cheston Tan,Dong Ting,Robert Trager,Brian Tse,Anthony Tung K. H.,Vanessa Wilfred,John Willes,Denise Wong,Wei Xu,Rongwu Xu,Yi Zeng,HongJiang Zhang,Djordje Žikelić*

Main category: cs.AI

TL;DR: 本报告基于2025新加坡AI大会成果，提出以纵深防御模型划分AI安全研究领域，强调开发、评估及部署后控制三大挑战，并推动国际合作，旨在建立可信且促进创新的AI生态。


<details>
  <summary>Details</summary>
Motivation: 随着AI能力和自主性的快速提升，社会对如何确保AI安全（即可信、可靠和安全）展开激烈讨论，需要建立一个值得信赖的AI生态系统，以促进创新并避免社会反弹。

Method: 通过2025新加坡AI大会（SCAI）：国际AI安全科研交流，汇聚各国AI科学家，梳理与综合AI安全领域研究重点；报告以“纵深防御”模型，将AI安全研究分为开发、评估、部署后监控与干预三个分支。

Result: 形成了基于纵深防御模型的AI安全研究框架，并制定了国际化、兼具前瞻性与重点的研究议程。

Conclusion: 跨国合作和分层次的防御模型能够更有力地推动AI安全研究，构建可信AI生态系统。

Abstract: Rapidly improving AI capabilities and autonomy hold significant promise of
transformation, but are also driving vigorous debate on how to ensure that AI
is safe, i.e., trustworthy, reliable, and secure. Building a trusted ecosystem
is therefore essential -- it helps people embrace AI with confidence and gives
maximal space for innovation while avoiding backlash.
  The "2025 Singapore Conference on AI (SCAI): International Scientific
Exchange on AI Safety" aimed to support research in this space by bringing
together AI scientists across geographies to identify and synthesise research
priorities in AI safety. This resulting report builds on the International AI
Safety Report chaired by Yoshua Bengio and backed by 33 governments. By
adopting a defence-in-depth model, this report organises AI safety research
domains into three types: challenges with creating trustworthy AI systems
(Development), challenges with evaluating their risks (Assessment), and
challenges with monitoring and intervening after deployment (Control).

</details>


### [2] [MAGPIE: A dataset for Multi-AGent contextual PrIvacy Evaluation](https://arxiv.org/abs/2506.20737)
*Gurusha Juneja,Alon Albalak,Wenyue Hua,William Yang Wang*

Main category: cs.AI

TL;DR: 作者提出高风险现实场景基准集MAGPIE，系统评测主流LLM多智能体在语境隐私保护与协作能力，发现现有模型易泄露隐私且任务完成率低，隐私与能力尚未兼得。


<details>
  <summary>Details</summary>
Motivation: 随着基于大语言模型（LLM）的智能体广泛应用于调度、协商和资源分配等任务，隐私保护成为关键问题。这些智能体通常需要访问专有工具和行业数据库，涉及敏感信息，必须保证严格保密性。作者关注现有LLM智能体是否能正确理解语境下的隐私需求，以及在协同任务中实际能否保护用户隐私。

Method: 作者提出了一个涵盖15个领域、158个高风险现实场景的新基准集MAGPIE，这些场景既无法完全避免使用隐私数据，又不能随意共享。利用MAGPIE，系统性评估了主流LLM（包括GPT-4o和Claude-2.7-Sonnet）对情境隐私的理解与协作中隐私保护能力，考察其在多轮对话和任务完成率中的表现。

Result: 结果显示，当前主流LLM对情境隐私理解薄弱，分别有25.2%和43.6%的情况下将私密数据误分类为可共享。在明确隐私保护指令下，多轮对话中这两种模型仍有59.9%和50.5%的机率泄露隐私信息。此外，多智能体系统有71%的场景无法完成任务。

Conclusion: 当前的LLM及其多智能体系统尚未在上下文隐私保护和协同任务完成上实现良好对齐，存在显著隐私风险和协作障碍，亟需完善。

Abstract: The proliferation of LLM-based agents has led to increasing deployment of
inter-agent collaboration for tasks like scheduling, negotiation, resource
allocation etc. In such systems, privacy is critical, as agents often access
proprietary tools and domain-specific databases requiring strict
confidentiality. This paper examines whether LLM-based agents demonstrate an
understanding of contextual privacy. And, if instructed, do these systems
preserve inference time user privacy in non-adversarial multi-turn
conversation. Existing benchmarks to evaluate contextual privacy in LLM-agents
primarily assess single-turn, low-complexity tasks where private information
can be easily excluded. We first present a benchmark - MAGPIE comprising 158
real-life high-stakes scenarios across 15 domains. These scenarios are designed
such that complete exclusion of private data impedes task completion yet
unrestricted information sharing could lead to substantial losses. We then
evaluate the current state-of-the-art LLMs on (a) their understanding of
contextually private data and (b) their ability to collaborate without
violating user privacy. Empirical experiments demonstrate that current models,
including GPT-4o and Claude-2.7-Sonnet, lack robust understanding of contextual
privacy, misclassifying private data as shareable 25.2\% and 43.6\% of the
time. In multi-turn conversations, these models disclose private information in
59.9\% and 50.5\% of cases even under explicit privacy instructions.
Furthermore, multi-agent systems fail to complete tasks in 71\% of scenarios.
These results underscore that current models are not aligned towards both
contextual privacy preservation and collaborative task-solving.

</details>


### [3] [Dynamic Context-Aware Prompt Recommendation for Domain-Specific AI Applications](https://arxiv.org/abs/2506.20815)
*Xinye Tang,Haijun Zhai,Chaitanya Belwal,Vineeth Thayanithi,Philip Baumann,Yogesh K Roy*

Main category: cs.AI

TL;DR: 论文提出一种结合多项技术的动态上下文感知提示推荐系统，显著提升了领域特定AI应用的提示质量，实验和专家评测均取得优秀成绩。


<details>
  <summary>Details</summary>
Motivation: 大模型应用强依赖于用户提示质量，但高质量提示在领域场景下难以设计，需要智能系统降低提示编写成本并提升AI应用效果。

Method: 系统结合上下文查询分析、检索增强知识锚定、分层技能组织与自适应技能排序，通过行为遥测和两阶段分层推理动态选择和排序相关技能，并利用预定义与自适应模板（包含少样本学习）合成高质量提示。

Result: 在真实数据集上的实验表明，该系统生成的提示在实用性和相关性上均获得了良好的自动化和专家评价。

Conclusion: 提出的上下文感知动态提示推荐系统能显著提升领域特定AI应用中提示的相关性和实用性，得到了自动化与专家双重验证的优异效果。

Abstract: LLM-powered applications are highly susceptible to the quality of user
prompts, and crafting high-quality prompts can often be challenging especially
for domain-specific applications. This paper presents a novel dynamic
context-aware prompt recommendation system for domain-specific AI applications.
Our solution combines contextual query analysis, retrieval-augmented knowledge
grounding, hierarchical skill organization, and adaptive skill ranking to
generate relevant and actionable prompt suggestions.
  The system leverages behavioral telemetry and a two-stage hierarchical
reasoning process to dynamically select and rank relevant skills, and
synthesizes prompts using both predefined and adaptive templates enhanced with
few-shot learning. Experiments on real-world datasets demonstrate that our
approach achieves high usefulness and relevance, as validated by both automated
and expert evaluations.

</details>


### [4] [Beyond Reactive Safety: Risk-Aware LLM Alignment via Long-Horizon Simulation](https://arxiv.org/abs/2506.20949)
*Chenkai Sun,Denghui Zhang,ChengXiang Zhai,Heng Ji*

Main category: cs.AI

TL;DR: 本文针对大语言模型在社会高风险决策中的安全挑战，提出了一套预测建议影响的框架，并构建了间接伤害评测集。结果显示该方法大幅提升了模型的安全意识，对未来安全语言智能体的开发具有重要意义。


<details>
  <summary>Details</summary>
Motivation: 随着基于语言模型的智能体在公共政策、医疗等高风险领域的影响愈发显著，理解其建议的深远社会影响变得至关重要。当前缺乏系统方法来预测模型建议在社会系统中的传播及其潜在风险。

Method: 提出了一个可行性框架，用于模拟和预测语言模型生成建议在宏观社会系统中的传播过程。此外，构建了包含100个间接伤害场景的数据集，用以评估模型识别和预测非显性风险的能力。

Result: 新方法在所提出的数据集上性能提升超过20%，并在现有安全基准（AdvBench、SafeRLHF、WildGuardMix）中达到70%以上的平均胜率，大幅超越强基线。

Conclusion: 该研究展示了预测和缓解语言模型潜在社会风险的有效途径，为开发更安全、符合社会期望的智能体提供了新思路。

Abstract: Given the growing influence of language model-based agents on high-stakes
societal decisions, from public policy to healthcare, ensuring their beneficial
impact requires understanding the far-reaching implications of their
suggestions. We propose a proof-of-concept framework that projects how
model-generated advice could propagate through societal systems on a
macroscopic scale over time, enabling more robust alignment. To assess the
long-term safety awareness of language models, we also introduce a dataset of
100 indirect harm scenarios, testing models' ability to foresee adverse,
non-obvious outcomes from seemingly harmless user prompts. Our approach
achieves not only over 20% improvement on the new dataset but also an average
win rate exceeding 70% against strong baselines on existing safety benchmarks
(AdvBench, SafeRLHF, WildGuardMix), suggesting a promising direction for safer
agents.

</details>


### [5] [Unveiling Causal Reasoning in Large Language Models: Reality or Mirage?](https://arxiv.org/abs/2506.21215)
*Haoang Chi,He Li,Wenjing Yang,Feng Liu,Long Lan,Xiaoguang Ren,Tongliang Liu,Bo Han*

Main category: cs.AI

TL;DR: 当前LLM仅具备表层因果推理能力。通过引入通用知识和目标导向型提示的新方法（G^2-Reasoner），可以显著提升LLM的因果推理水平，推动其向更接近人类的深层因果推理发展。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在理解因果关系方面表现突出，但尚不清楚其是否具备类似人类的深层因果推理能力。已有证据显示LLM目前大多只停留在对已有因果知识的表面理解，缺乏真正的人类式推理。为推动模型向更高阶因果推理发展，作者开展本研究。

Method: 本文方法包括理论分析LLM自回归机制并指出其并非本质因果；设计新基准CausalProbe-2024测试LLM在全新语料上的因果推理；结合通用知识与目标导向提示，提出G^2-Reasoner以提升LLM因果推理能力，并通过实验验证效果。

Result: 开发的CausalProbe-2024基准显示LLM在新颖因果问答任务上的表现显著下滑，证实其因果推理能力主要为level-1。G^2-Reasoner方法显著提升了LLM在这些任务中的因果推理能力，尤其在处理新颖和反事实语境时效果更明显。

Conclusion: 实验表明，通过引入通用知识和目标导向型提示（G^2-Reasoner），可以显著提升大语言模型在新颖和反事实语境下的因果推理能力，有助于其从浅层因果推理（level-1）迈向更接近人类的深层因果推理（level-2）。

Abstract: Causal reasoning capability is critical in advancing large language models
(LLMs) toward strong artificial intelligence. While versatile LLMs appear to
have demonstrated capabilities in understanding contextual causality and
providing responses that obey the laws of causality, it remains unclear whether
they perform genuine causal reasoning akin to humans. However, current evidence
indicates the contrary. Specifically, LLMs are only capable of performing
shallow (level-1) causal reasoning, primarily attributed to the causal
knowledge embedded in their parameters, but they lack the capacity for genuine
human-like (level-2) causal reasoning. To support this hypothesis,
methodologically, we delve into the autoregression mechanism of
transformer-based LLMs, revealing that it is not inherently causal.
Empirically, we introduce a new causal Q&A benchmark called CausalProbe-2024,
whose corpora are fresh and nearly unseen for the studied LLMs. The LLMs
exhibit a significant performance drop on CausalProbe-2024 compared to earlier
benchmarks, indicating the fact that they primarily engage in level-1 causal
reasoning. To bridge the gap towards level-2 causal reasoning, we draw
inspiration from the fact that human reasoning is usually facilitated by
general knowledge and intended goals. We propose G^2-Reasoner, a method that
incorporates general knowledge and goal-oriented prompts into LLMs' causal
reasoning processes. Experiments demonstrate that G^2-Reasoner significantly
enhances LLMs' causal reasoning capability, particularly in fresh and
counterfactual contexts. This work sheds light on a new path for LLMs to
advance towards genuine causal reasoning, going beyond level-1 and making
strides towards level-2.

</details>


### [6] [World-aware Planning Narratives Enhance Large Vision-Language Model Planner](https://arxiv.org/abs/2506.21230)
*Junhao Shi,Zhaoye Fei,Siyin Wang,Qipeng Guo,Jingjing Gong,Xipeng QIu*

Main category: cs.AI

TL;DR: 本文提出WAP框架，提升LVLMs在环境感知和多步任务规划上的能力，达到行业领先水平，优于主流专有模型。


<details>
  <summary>Details</summary>
Motivation: 尽管大型视觉-语言模型（LVLMs）在具身规划任务中展现出潜力，但在面对陌生环境和多步骤目标的复杂场景时表现不佳。现有方法通常依赖与环境无关的模仿学习，导致模型在处理与环境密切相关的指令时容易出现问题，同时更加依赖补充线索而不是视觉推理。作者希望解决LVLMs缺乏环境感知和情境理解的问题。

Method: 提出了世界感知规划叙事增强（WAP）框架，为LVLMs注入四项认知能力：视觉外观建模、空间推理、功能抽象和句法锚定。模型仅通过原始视觉观察，并结合课程学习进行开发和评估。

Result: 在EB-ALFRED基准测试上，采用WAP框架的Qwen2.5-VL模型任务成功率绝对提升了60.7，尤其在常识推理和长时规划方面提升分别为60.0和70.0。

Conclusion: WAP框架显著提升了LVLMs在复杂环境下的规划与推理能力，增强后的开源模型超越了GPT-4o和Claude-3.5-Sonnet等专有系统。

Abstract: Large Vision-Language Models (LVLMs) show promise for embodied planning tasks
but struggle with complex scenarios involving unfamiliar environments and
multi-step goals. Current approaches rely on environment-agnostic imitation
learning that disconnects instructions from environmental contexts, causing
models to struggle with context-sensitive instructions and rely on
supplementary cues rather than visual reasoning during long-horizon
interactions. In this work, we propose World-Aware Planning Narrative
Enhancement (WAP), a framework that infuses LVLMs with comprehensive
environmental understanding through four cognitive capabilities (visual
appearance modeling, spatial reasoning, functional abstraction, and syntactic
grounding) while developing and evaluating models using only raw visual
observations through curriculum learning. Evaluations on the EB-ALFRED
benchmark demonstrate substantial improvements, with Qwen2.5-VL achieving a
60.7 absolute improvement in task success rates, particularly in commonsense
reasoning (+60.0) and long-horizon planning (+70.0). Notably, our enhanced
open-source models outperform proprietary systems like GPT-4o and
Claude-3.5-Sonnet by a large margin.

</details>


### [7] [IXAII: An Interactive Explainable Artificial Intelligence Interface for Decision Support Systems](https://arxiv.org/abs/2506.21310)
*Pauline Speckmann,Mario Nadj,Christian Janiesch*

Main category: cs.AI

TL;DR: 本文提出IXAII交互式可解释系统，整合多种主流AI可解释方法，并针对不同用户需求进行定制，实验结果显示系统有效提升了透明度与用户体验。


<details>
  <summary>Details</summary>
Motivation: 当前许多AI可解释方法为事后型且静态，忽视了用户视角，降低了对目标用户的有效性，因此需要一种既支持多方法、又注重用户互动体验的可解释系统。

Method: 开发了IXAII交互式可解释智能系统，集成了LIME、SHAP、Anchors和DiCE四种AI可解释方法，并针对五类用户定制展示界面，通过专家和普通用户访谈对系统进行评估。

Result: IXAII系统不仅能为不同用户群体提供多种解释与多样展示，还提升了AI系统的透明度，被用户认为有助于理解AI决策过程。

Conclusion: IXAII系统能够通过多种方法和交互形式为不同用户群体提供更具透明性和灵活性的AI解释，并在实际用户中的评估得到了积极反馈。

Abstract: Although several post-hoc methods for explainable AI have been developed,
most are static and neglect the user perspective, limiting their effectiveness
for the target audience. In response, we developed the interactive explainable
intelligent system called IXAII that offers explanations from four explainable
AI methods: LIME, SHAP, Anchors, and DiCE. Our prototype provides tailored
views for five user groups and gives users agency over the explanations'
content and their format. We evaluated IXAII through interviews with experts
and lay users. Our results indicate that IXAII, which provides different
explanations with multiple visualization options, is perceived as helpful to
increase transparency. By bridging the gaps between explainable AI methods,
interactivity, and practical implementation, we provide a novel perspective on
AI explanation practices and human-AI interaction.

</details>


### [8] [Active Inference AI Systems for Scientific Discovery](https://arxiv.org/abs/2506.21329)
*Karthik Duraisamy*

Main category: cs.AI

TL;DR: 本文提出：推动AI驱动科学发现需聚焦于弥合抽象、推理与现实三大鸿沟，而非单纯追求更大模型或更多数据。为此，需建立因果自监督、闭环主动推断、持续知识演进与外部校正的AI系统，并强调人类判断不可替代。


<details>
  <summary>Details</summary>
Motivation: 当前的人工智能系统在科学发现中被寄予厚望，但由于架构、推理和与实验现实的脱节等根本性局限，未能实现预期的变革。

Method: 提出构建主动推断型AI系统，包括：1. 以因果自监督基础模型为依托，维护长期研究记忆；2. 配备贝叶斯保障机制的符号或神经符号推理规划器；3. 构建和维护知识图谱，动态生成和验证新概念与因果联系；4. 通过高保真模拟器与自动化实验室，实现内外闭环互动，完善内部表征。并强调始终需引入人类判断作为系统不可或缺的组成部分。

Result: 完成了对闭环主动推断型AI架构的定义和要素分解，阐明科学推理所必须的核心能力，包括因果推理与持续校准。论证了单纯依赖模型规模、数据量、计算力已无法显著推动AI科学发现进步，需弥补抽象、推理与现实三大鸿沟。

Conclusion: 科学发现型AI的进步取决于缩小抽象、推理与现实三大鸿沟，构建能反事实推理、因果建模、内外协同验证的主动推断AI系统，并将人类判断作为核心组件长期嵌入其中。

Abstract: The rapid evolution of artificial intelligence has led to expectations of
transformative scientific discovery, yet current systems remain fundamentally
limited by their operational architectures, brittle reasoning mechanisms, and
their separation from experimental reality. Building on earlier work, we
contend that progress in AI-driven science now depends on closing three
fundamental gaps -- the abstraction gap, the reasoning gap, and the reality gap
-- rather than on model size/data/test time compute. Scientific reasoning
demands internal representations that support simulation of actions and
response, causal structures that distinguish correlation from mechanism, and
continuous calibration. We define active inference AI systems for scientific
discovery as those that (i) maintain long-lived research memories grounded in
causal self-supervised foundation models, (ii) symbolic or neuro-symbolic
planners equipped with Bayesian guardrails, (iii) grow persistent knowledge
graphs where thinking generates novel conceptual nodes, reasoning establishes
causal edges, and real-world interaction prunes false connections while
strengthening verified pathways, and (iv) refine their internal representations
through closed-loop interaction with both high-fidelity simulators and
automated laboratories - an operational loop where mental simulation guides
action and empirical surprise reshapes understanding. In essence, we outline an
architecture where discovery arises from the interplay between internal models
that enable counterfactual reasoning and external validation that grounds
hypotheses in reality. It is also argued that the inherent ambiguity in
feedback from simulations and experiments, and underlying uncertainties makes
human judgment indispensable, not as a temporary scaffold but as a permanent
architectural component.

</details>


### [9] [TableMoE: Neuro-Symbolic Routing for Structured Expert Reasoning in Multimodal Table Understanding](https://arxiv.org/abs/2506.21393)
*Junwen Zhang,Pu Chen,Yin Zhang*

Main category: cs.AI

TL;DR: 表格多模态理解在真实环境中难度极高，TableMoE提出结合神经符号推理与专家路由的新架构，大幅超越现有方法，并在结构复杂和退化条件下表现出更强的鲁棒性与可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型（MLLMs）在面对真实环境下存在结构复杂、符号密集、视觉退化（如模糊、歪斜、水印、结构或字体不全、多层嵌套等）问题的表格时表现不佳，泛化能力有限。

Method: 提出TableMoE，一种神经符号混合专家结构（MoCE），结合神经符号路由机制，用于表格元素的角色识别和路由分配（如表头、数据单元、轴、公式），将表格动态分配给不同专家（表格-HTML、表格-JSON、表格-代码）处理，并通过置信度门限和符号推理图进行路由；引入大规模对齐预训练数据集TableMoE-Align（含120万多领域四元组），作为模型预训练数据。

Result: TableMoE在四个真实复杂场景下的新基准集（WMMFinQA、WMMTatQA、WMMTabDialog、WMMFinanceMath）上，显著优于现有SOTA模型。消融实验证明神经符号路由和专家对齐对性能有关键作用，定性分析突出其可解释性和鲁棒性。

Conclusion: TableMoE通过引入神经符号混合专家和结构化路由机制，显著提升了对真实环境复杂表格的多模态理解与推理能力，在多个新基准集上取得了领先表现，是实现强多模态表格理解的重要进展。

Abstract: Multimodal understanding of tables in real-world contexts is challenging due
to the complexity of structure, symbolic density, and visual degradation (blur,
skew, watermarking, incomplete structures or fonts, multi-span or
hierarchically nested layouts). Existing multimodal large language models
(MLLMs) struggle with such WildStruct conditions, resulting in limited
performance and poor generalization. To address these challenges, we propose
TableMoE, a neuro-symbolic Mixture-of-Connector-Experts (MoCE) architecture
specifically designed for robust, structured reasoning over multimodal table
data. TableMoE features an innovative Neuro-Symbolic Routing mechanism, which
predicts latent semantic token roles (e.g., header, data cell, axis, formula)
and dynamically routes table elements to specialized experts (Table-to-HTML,
Table-to-JSON, Table-to-Code) using a confidence-aware gating strategy informed
by symbolic reasoning graphs. To facilitate effective alignment-driven
pretraining, we introduce the large-scale TableMoE-Align dataset, consisting of
1.2M table-HTML-JSON-code quadruples across finance, science, biomedicine and
industry, utilized exclusively for model pretraining. For evaluation, we curate
and release four challenging WildStruct benchmarks: WMMFinQA, WMMTatQA,
WMMTabDialog, and WMMFinanceMath, designed specifically to stress-test models
under real-world multimodal degradation and structural complexity. Experimental
results demonstrate that TableMoE significantly surpasses existing
state-of-the-art models. Extensive ablation studies validate each core
component, emphasizing the critical role of Neuro-Symbolic Routing and
structured expert alignment. Through qualitative analyses, we further showcase
TableMoE's interpretability and enhanced robustness, underscoring the
effectiveness of integrating neuro-symbolic reasoning for multimodal table
understanding.

</details>


### [10] [Spatial Mental Modeling from Limited Views](https://arxiv.org/abs/2506.21458)
*Baiqiao Yin,Qineng Wang,Pingyue Zhang,Jianshu Zhang,Kangrui Wang,Zihan Wang,Jieyu Zhang,Keshigeyan Chandrasegaran,Han Liu,Ranjay Krishna,Saining Xie,Manling Li,Jiajun Wu,Li Fei-Fei*

Main category: cs.AI

TL;DR: 提出MindCube基准测试揭示VLMs空间认知短板，通过“先建认知图再推理”训练和强化学习，准确率从37.8%提升到70.7%，极大改进模型对不可见空间的理解。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在仅有少量视角信息情况下，难以像人类一样建立完全的空间心理模型，推理隐藏空间。作者旨在评估并提升VLMs在空间认知与推理能力上的表现。

Method: 提出了MindCube基准测试，通过三种方法（补充中间视角、自然语言推理链和认知地图）提升VLMs空间认知能力，最终采用“先建图后推理”的联合训练和强化学习进一步优化绩效。

Result: VLMs在MindCube基准上的初始准确率为37.8%。用“map-then-reason”方法提升至60.8%，再结合强化学习可达70.7%。

Conclusion: 通过“map-then-reason”联合训练方法和强化学习，显著提升了视觉语言模型（VLMs）对不可见空间的理解和推理能力。建立和利用内部空间认知图，并灵活推理，能够弥补VLMs在空间认知上的不足。

Abstract: Can Vision Language Models (VLMs) imagine the full scene from just a few
views, like humans do? Humans form spatial mental models, internal
representations of unseen space, to reason about layout, perspective, and
motion. Our new MindCube benchmark with 21,154 questions across 3,268 images
exposes this critical gap, where existing VLMs exhibit near-random performance.
Using MindCube, we systematically evaluate how well VLMs build robust spatial
mental models through representing positions (cognitive mapping), orientations
(perspective-taking), and dynamics (mental simulation for "what-if" movements).
We then explore three approaches to help VLMs approximate spatial mental
models, including unseen intermediate views, natural language reasoning chains,
and cognitive maps. The significant improvement comes from a synergistic
approach, "map-then-reason", that jointly trains the model to first generate a
cognitive map and then reason upon it. By training models to reason over these
internal maps, we boosted accuracy from 37.8% to 60.8% (+23.0%). Adding
reinforcement learning pushed performance even further to 70.7% (+32.9%). Our
key insight is that such scaffolding of spatial mental models, actively
constructing and utilizing internal structured spatial representations with
flexible reasoning processes, significantly improves understanding of
unobservable space.

</details>


### [11] [Ad-Hoc Human-AI Coordination Challenge](https://arxiv.org/abs/2506.21490)
*Tin Dizdarević,Ravi Hammond,Tobias Gessler,Anisoara Calinescu,Jonathan Cook,Matteo Gallici,Andrei Lupu,Jakob Nicolaus Foerster*

Main category: cs.AI

TL;DR: 提出AH2AC2人机协作挑战，开发用于AI人机协作评测的人类代理体与受控评测系统，并开源Hanabi数据集，为该领域研究带来高效、可复现的新方法。


<details>
  <summary>Details</summary>
Motivation: AI与人类实现无缝协作在现实世界应用中极为重要，但至今仍面临诸多挑战。合作卡牌游戏Hanabi因其信息不完全、交流受限、需要心理推理和协调行动等特性，是理想的人机协作测试平台，但因人工评测昂贵且难以复现，限制了该领域的发展。

Method: 该研究提出了Ad-Hoc Human-AI Coordination Challenge (AH2AC2)，并开发了'人类代理体'，基于大规模人类数据集，作为便宜、稳健、可复现的人类“替身”合作伙伴，辅助AI模型评测。同时，开源了含3,079局游戏的数据集，但故意限制数据量，以鼓励数据高效型方法的发展。基准结果涵盖2人和3人Hanabi场景，并通过受控评测系统而非公开代理体确保公平性。

Result: 开发了人类代理体和开源数据集，为AI与人类协作提供了新型评测手段，同时公布了Hanabi下的基准结果。通过这些方法，克服了人工评测成本高、难以复现的局限性。

Conclusion: 本工作为人机协作领域提供了一个低成本、高可复现性的新型评测标准和手段，有利于促进AI与人类协作系统的更广泛研究和实际应用。

Abstract: Achieving seamless coordination between AI agents and humans is crucial for
real-world applications, yet it remains a significant open challenge. Hanabi is
a cooperative card game featuring imperfect information, constrained
communication, theory of mind requirements, and coordinated action -- making it
an ideal testbed for human-AI coordination. However, its use for human-AI
interaction has been limited by the challenges of human evaluation. In this
work, we introduce the Ad-Hoc Human-AI Coordination Challenge (AH2AC2) to
overcome the constraints of costly and difficult-to-reproduce human
evaluations. We develop \textit{human proxy agents} on a large-scale human
dataset that serve as robust, cheap, and reproducible human-like evaluation
partners in AH2AC2. To encourage the development of data-efficient methods, we
open-source a dataset of 3,079 games, deliberately limiting the amount of
available human gameplay data. We present baseline results for both two- and
three- player Hanabi scenarios. To ensure fair evaluation, we host the proxy
agents through a controlled evaluation system rather than releasing them
publicly. The code is available at
\href{https://github.com/FLAIROx/ah2ac2}{https://github.com/FLAIROx/ah2ac2}.

</details>


### [12] [Mind2Web 2: Evaluating Agentic Search with Agent-as-a-Judge](https://arxiv.org/abs/2506.21506)
*Boyu Gou,Zanming Huang,Yuting Ning,Yu Gu,Michael Lin,Weijian Qi,Andrei Kopanev,Botao Yu,Bernal Jiménez Gutiérrez,Yiheng Shu,Chan Hee Song,Jiaman Wu,Shijie Chen,Hanane Nour Moussa,Tianshu Zhang,Jian Xie,Yifei Li,Tianci Xue,Zeyi Liao,Kai Zhang,Boyuan Zheng,Zhaowei Cai,Viktor Rozgic,Morteza Ziyadi,Huan Sun,Yu Su*

Main category: cs.AI

TL;DR: 本论文提出Mind2Web 2基准，用于评测复杂、长时Agentic Search任务，创新引入Agent-as-a-Judge自动评判体系。评测多种系统后发现主流模型已具备较强实力，但距离人类表现仍有差距，实现了领域基准的重大突破。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）的发展，智能自主执行网页浏览、信息合成和引用回答等Agentic Search系统日益普及，极大改变了用户获取信息的方式。然而，现有的评测基准和方法往往针对的是短时、静态的问题，难以覆盖Agentic Search的复杂性和开放性。因此，亟需有能适应真实复杂任务的评测体系。

Method: 提出了Mind2Web 2基准，这是一套包含130个现实、复杂、需长时浏览和深入信息合成的任务集，由超过1,000小时人工设计完成。为应对答案随时间变化且复杂的评判难题，提出了Agent-as-a-Judge方法——基于树状评价准则自动评估答案正确性和引用准确性。

Result: 对9个先进Agentic Search系统及人类表现进行了系统评估和误差分析。结果显示OpenAI Deep Research系统在用时只有人类一半的情况下，已可达到人类表现的50-70%。

Conclusion: Mind2Web 2基准和自动评测方法为Agentic Search系统的开发和评测提供了坚实基础，展现了未来自动化信息检索的强大潜力。

Abstract: Agentic search such as Deep Research systems, where large language models
autonomously browse the web, synthesize information, and return comprehensive
citation-backed answers, represents a major shift in how users interact with
web-scale information. While promising greater efficiency and cognitive
offloading, the growing complexity and open-endedness of agentic search have
outpaced existing evaluation benchmarks and methodologies, which largely assume
short search horizons and static answers. In this paper, we introduce Mind2Web
2, a benchmark of 130 realistic, high-quality, and long-horizon tasks that
require real-time web browsing and extensive information synthesis, constructed
with over 1,000 hours of human labor. To address the challenge of evaluating
time-varying and complex answers, we propose a novel Agent-as-a-Judge
framework. Our method constructs task-specific judge agents based on a
tree-structured rubric design to automatically assess both answer correctness
and source attribution. We conduct a comprehensive evaluation of nine frontier
agentic search systems and human performance, along with a detailed error
analysis to draw insights for future development. The best-performing system,
OpenAI Deep Research, can already achieve 50-70% of human performance while
spending half the time, showing a great potential. Altogether, Mind2Web 2
provides a rigorous foundation for developing and benchmarking the next
generation of agentic search systems.

</details>


### [13] [PsyLite Technical Report](https://arxiv.org/abs/2506.21536)
*Fangjun Ding,Renyu Zhang,Xinyu Feng,Chengye Xie,Zheng Zhang,Yanting Zhang*

Main category: cs.AI

TL;DR: 该论文提出了基于InternLM2.5-7B-chat的轻量化心理咨询大模型PsyLite，通过两阶段训练及创新条件RAG方法，显著提升了模型的专业性和安全性，实现了低硬件需求下的高效应用，适用于资源有限的心理咨询场景。


<details>
  <summary>Details</summary>
Motivation: 当前AI驱动的心理咨询在对话安全性、细致场景处理和轻量化部署方面存在不足。

Method: 基于InternLM2.5-7B-chat基础模型，提出轻量级心理咨询大语言模型PsyLite，采用混合蒸馏数据微调与ORPO偏好优化的两阶段训练策略，并结合Ollama、Open WebUI以及创新性条件RAG方法引入幽默元素提升体验，量化为GGUF q4_k_m以降低硬件要求。

Result: PsyLite在中文通用评测（CEval）、心理咨询专业评测（CPsyCounE）、对话安全评测（SafeDialBench）上均优于基线模型，尤其在心理咨询专业性（CPsyCounE提升47.6%）和对话安全性（SafeDialBench提升2.4%），且仅需5GB内存即可运行。

Conclusion: PsyLite在提升对话安全、专业能力和部署灵活性的同时，为资源受限环境中心理咨询应用提供了可行解决方案。

Abstract: With the rapid development of digital technology, AI-driven psychological
counseling has gradually become an important research direction in the field of
mental health. However, existing models still have deficiencies in dialogue
safety, detailed scenario handling, and lightweight deployment. To address
these issues, this study proposes PsyLite, a lightweight psychological
counseling large language model agent developed based on the base model
InternLM2.5-7B-chat. Through a two-stage training strategy (hybrid distillation
data fine-tuning and ORPO preference optimization), PsyLite enhances the
model's deep-reasoning ability, psychological counseling ability, and safe
dialogue ability. After deployment using Ollama and Open WebUI, a custom
workflow is created with Pipelines. An innovative conditional RAG is designed
to introduce crosstalk humor elements at appropriate times during psychological
counseling to enhance user experience and decline dangerous requests to
strengthen dialogue safety. Evaluations show that PsyLite outperforms the
baseline models in the Chinese general evaluation (CEval), psychological
counseling professional evaluation (CPsyCounE), and dialogue safety evaluation
(SafeDialBench), particularly in psychological counseling professionalism
(CPsyCounE score improvement of 47.6\%) and dialogue safety (\safe{} score
improvement of 2.4\%). Additionally, the model uses quantization technology
(GGUF q4\_k\_m) to achieve low hardware deployment (5GB memory is sufficient
for operation), providing a feasible solution for psychological counseling
applications in resource-constrained environments.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [14] [The Ideation-Execution Gap: Execution Outcomes of LLM-Generated versus Human Research Ideas](https://arxiv.org/abs/2506.20803)
*Chenglei Si,Tatsunori Hashimoto,Diyi Yang*

Main category: cs.CL

TL;DR: LLMs擅长提出新颖科研点子，但执行后效果不如人类专家创意；评价科研点子时应结合实际执行结果，现有LLMs在生成高效科研点子方面仍有限制。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）在科学研究流程中的应用不断增加，越来越多的证据表明，LLMs能提出新颖的研究想法，但这些想法在实际研究中的效果尚不明确。本文旨在验证AI生成的研究点子在被实际执行后，是否真的能带来更好的科研结果。

Method: 作者招募了43位专家级研究人员，让他们分别随机执行由人类专家与LLM生成的研究点子。每位研究者都需投入100小时以上实施想法并撰写短论文，随后由NLP专家进行盲审，通过对比执行前后不同评价指标（新颖性、激动性、有效性及总体评价）的分数，评估真实研究输出。

Result: 实验发现，LLM生成的点子在实际执行后，所有评价指标上的得分降幅显著高于专家点子（p < 0.05），甚至在部分指标上出现人类专家想法反超的现象。LLM点子在“想法阶段”表现突出，但在“执行阶段”后效果降低，显示出LLM生成科研点子的局限。

Conclusion: 目前的LLMs虽然在生成新颖研究想法阶段有优势，但这些想法落地后往往不如人类专家创意，这揭示了用AI辅助科研点子生成的现实局限，同时也说明仅凭点子的“新颖性”来评判其科研价值是片面的，必须结合实际执行效果来评估。

Abstract: Large Language Models (LLMs) have shown promise in accelerating the
scientific research pipeline. A key capability for this process is the ability
to generate novel research ideas, and prior studies have found settings in
which LLM-generated research ideas were judged as more novel than human-expert
ideas. However, a good idea should not simply appear to be novel, it should
also result in better research after being executed. To test whether
AI-generated ideas lead to better research outcomes, we conduct an execution
study by recruiting 43 expert researchers to execute randomly-assigned ideas,
either written by experts or generated by an LLM. Each expert spent over 100
hours implementing the idea and wrote a 4-page short paper to document the
experiments. All the executed projects are then reviewed blindly by expert NLP
researchers. Comparing the review scores of the same ideas before and after
execution, the scores of the LLM-generated ideas decrease significantly more
than expert-written ideas on all evaluation metrics (novelty, excitement,
effectiveness, and overall; p < 0.05), closing the gap between LLM and human
ideas observed at the ideation stage. When comparing the aggregated review
scores from the execution study, we even observe that for many metrics there is
a flip in rankings where human ideas score higher than LLM ideas. This
ideation-execution gap highlights the limitations of current LLMs in generating
truly effective research ideas and the challenge of evaluating research ideas
in the absence of execution outcomes.

</details>


### [15] ["What's Up, Doc?": Analyzing How Users Seek Health Information in Large-Scale Conversational AI Datasets](https://arxiv.org/abs/2506.21532)
*Akshay Paruchuri,Maryam Aziz,Rohit Vartak,Ayman Ali,Best Uchehara,Xin Liu,Ishan Chatterjee,Monica Agrawal*

Main category: cs.CL

TL;DR: 作者构建了一个涵盖真实医疗健康对话的大型数据集HealthChat-11K，系统揭示LLM在医疗领域对话中的问题和风险，强调了模型改进的重要性，并公开了相关数据和工具。


<details>
  <summary>Details</summary>
Motivation: 随着人们越来越多地通过大型语言模型（LLM）与聊天机器人交互来获取医疗健康信息，对这些对话的本质及其潜在风险却鲜有深入探讨。作者希望系统性研究用户在医疗健康场景中与AI的真实互动方式和存在的问题。

Method: 作者从大规模对话式AI数据集中筛选，构建了HealthChat-11K数据集，包含11K条真实世界对话和2.5万条用户消息；同时参考临床专家设计的分类方法，对用户与LLM在21个健康专科领域的互动进行系统性分析。

Result: 分析揭示了用户寻求健康信息的行为特征，包括常见交互模式、不完整的上下文、情感化表达以及可能导致模型阿谀的问题，对当前LLM在医疗健康领域的支持能力提出了改进需求。

Conclusion: 当前LLM在医疗健康对话中存在诸多挑战和风险，需进一步提升其安全性与专业性，为用户提供更可靠的健康信息支持。数据集和分析代码已经公开，便于社区深入研究和改进。

Abstract: People are increasingly seeking healthcare information from large language
models (LLMs) via interactive chatbots, yet the nature and inherent risks of
these conversations remain largely unexplored. In this paper, we filter
large-scale conversational AI datasets to achieve HealthChat-11K, a curated
dataset of 11K real-world conversations composed of 25K user messages. We use
HealthChat-11K and a clinician-driven taxonomy for how users interact with LLMs
when seeking healthcare information in order to systematically study user
interactions across 21 distinct health specialties. Our analysis reveals
insights into the nature of how and why users seek health information, such as
common interactions, instances of incomplete context, affective behaviors, and
interactions (e.g., leading questions) that can induce sycophancy, underscoring
the need for improvements in the healthcare support capabilities of LLMs
deployed as conversational AI. Code and artifacts to retrieve our analyses and
combine them into a curated dataset can be found here:
https://github.com/yahskapar/HealthChat

</details>


### [16] [Towards Probabilistic Question Answering Over Tabular Data](https://arxiv.org/abs/2506.20747)
*Chen Shen,Sajjadur Rahman,Estevam Hruschka*

Main category: cs.CL

TL;DR: 本文提出一种结合贝叶斯网络和大语言模型的新方法，可以显著提升表格数据下的不确定性概率性问答任务的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的基于表格问答的方法（如NL2SQL系统）在处理直接可从表格检索答案的事实性问题上表现良好，但在需要概率推理（面对不确定性推理）的概率性问题上表现不佳。论文针对这一局限性提出改进。

Method: 该论文提出了LUCARIO新基准和一个面向大规模表格数据的概率性问答框架。方法包括：从表格中归纳生成贝叶斯网络，将自然语言查询转为概率性查询，并利用大语言模型生成最终答案，实现了符号-神经混合推理。

Result: 实验证明，所提方法在概率性QA任务中相较于现有基线有显著提升，充分展现了混合符号-神经推理的优势。

Conclusion: 论文为表格概率问答提供了新基准和有效框架，在处理表格数据下的不确定性推理任务中取得了优异效果，推动了该领域的发展。

Abstract: Current approaches for question answering (QA) over tabular data, such as
NL2SQL systems, perform well for factual questions where answers are directly
retrieved from tables. However, they fall short on probabilistic questions
requiring reasoning under uncertainty. In this paper, we introduce a new
benchmark LUCARIO and a framework for probabilistic QA over large tabular data.
Our method induces Bayesian Networks from tables, translates natural language
queries into probabilistic queries, and uses large language models (LLMs) to
generate final answers. Empirical results demonstrate significant improvements
over baselines, highlighting the benefits of hybrid symbolic-neural reasoning.

</details>


### [17] [Multi-lingual Functional Evaluation for Large Language Models](https://arxiv.org/abs/2506.20793)
*Victor Ojewale,Inioluwa Deborah Raji,Suresh Venkatasubramanian*

Main category: cs.CL

TL;DR: 本文提出多语言功能性评测基准，发现静态基准与实际能力存在明显差距，同时不同语言的鲁棒性也不同，建议采用更真实的多语言评估方法。


<details>
  <summary>Details</summary>
Motivation: 多语言大模型通常依赖静态数据基准进行评估，但这类评估难以全面反映模型在实际多语场景下的表现和鲁棒性。为此，研究者希望通过更具功能性的基准，深入理解模型的多语言能力。

Method: 研究者将现有的英文功能性基准（GSM、Instruction-Following等）翻译为法语、西班牙语、印地语、阿拉伯语和约鲁巴语，构建了跨语言符号数学基准（CL-GSM Symbolic）和跨语言指令执行评测（CL-IFEval），并用这些基准在多种语言和模型上进行了测试，比较功能性与静态基准的表现差异。

Result: 发现部分静态多语言基准能较好地反映功能性表现，但不同基准的贴合度有显著差异。例如，M-GSM与CL-GSM Symbolic在英语、法语和西班牙语中表现下降幅度分别为24%、17%和18%；Belebele与CL-IFEval的表现下降在15%-24%之间；M-MMLU与CL-IFEval表现下降仅0.5%-3%。此外，不同语言的鲁棒性相差很大，如阿拉伯语和英语表现较稳定。

Conclusion: 静态多语言评测基准与实际功能性多语言能力存在显著差距，而新提出的跨语言功能性基准能更真实地反映大模型在多语环境下的实际能力和鲁棒性。建议未来评估采用更多功能性和多语言交叉的测试。

Abstract: Multi-lingual competence in large language models is often evaluated via
static data benchmarks such as Belebele, M-MMLU and M-GSM. However, these
evaluations often fail to provide an adequate understanding of the practical
performance and robustness of models across multi-lingual settings. In
response, we create multi-lingual functional benchmarks -- Cross-Lingual Grade
School Math Symbolic (CL-GSM Symbolic) and Cross-Lingual Instruction-Following
Eval (CL-IFEval)-- by translating existing functional benchmark templates from
English to five additional languages that span the range of resources available
for NLP: French, Spanish, Hindi, Arabic and Yoruba. Our results reveal that
some static multi-lingual benchmarks capture functional performance much more
closely than others (i.e. across models, there is a 24%, 17% and 18% decrease
in performance between M-GSM and CL-GSM Symbolic in English, French and Spanish
respectively; similarly there's a 15 - 24% performance drop across languages
between Belebele and CL-IFEval, and only a 0.5% to 3% performance drop between
M-MMLU and CL-IFEval). Similarly, we find that model robustness across
languages varies significantly, with certain languages (eg. Arabic, English)
being the most consistently well performing across evaluation iterations.

</details>


### [18] [MultiFinRAG: An Optimized Multimodal Retrieval-Augmented Generation (RAG) Framework for Financial Question Answering](https://arxiv.org/abs/2506.20821)
*Chinmay Gondhalekar,Urjitkumar Patel,Fang-Chun Yeh*

Main category: cs.CL

TL;DR: 本文提出了一种新型检索增强生成框架MultiFinRAG，专为金融多模态复杂问答设计。其通过多模态抽取与结构化检索，大幅提升了跨文本、表格与图像的推理能力，在普通硬件上准确率超越免费ChatGPT-4o 19个百分点。


<details>
  <summary>Details</summary>
Motivation: 金融文档（如10-K、10-Q报告和投资者展示）通常内容庞杂、数据多样，涵盖文本、表格、图形等多种模态，对答题和信息检索造成了巨大挑战。传统LLM及RAG方法受限于token数、版面丢失和上下文割裂，难以实现跨模态推理。这推动了更适合金融文档高效问答的新方法的需求。

Method: 提出MultiFinRAG检索增强生成框架，针对金融问答场景设计。流程包括：（1）通过轻量化、量化的开源多模态LLM对表格、图像进行批量抽取，生成结构化JSON输出及简明文本摘要；（2）与文本一同用模态感知的相似性阈值进行嵌入和索引，实现精准检索；（3）采用分层回退机制，必要时从仅文本检索拓展到文本+表格+图片，增强跨模态推理且降低无关干扰。

Result: 在复杂的金融问答（涉及文本、表格、图像和跨模态推理）任务上，MultiFinRAG在普通硬件上运行，准确率比ChatGPT-4o（免费版）高19个百分点。

Conclusion: MultiFinRAG显著提升了金融文档跨模态问答能力，能精确地跨文本、表格和图像进行推理并生成答案，性能优于现有主流大模型方案。

Abstract: Financial documents--such as 10-Ks, 10-Qs, and investor presentations--span
hundreds of pages and combine diverse modalities, including dense narrative
text, structured tables, and complex figures. Answering questions over such
content often requires joint reasoning across modalities, which strains
traditional large language models (LLMs) and retrieval-augmented generation
(RAG) pipelines due to token limitations, layout loss, and fragmented
cross-modal context. We introduce MultiFinRAG, a retrieval-augmented generation
framework purpose-built for financial QA. MultiFinRAG first performs multimodal
extraction by grouping table and figure images into batches and sending them to
a lightweight, quantized open-source multimodal LLM, which produces both
structured JSON outputs and concise textual summaries. These outputs, along
with narrative text, are embedded and indexed with modality-aware similarity
thresholds for precise retrieval. A tiered fallback strategy then dynamically
escalates from text-only to text+table+image contexts when necessary, enabling
cross-modal reasoning while reducing irrelevant context. Despite running on
commodity hardware, MultiFinRAG achieves 19 percentage points higher accuracy
than ChatGPT-4o (free-tier) on complex financial QA tasks involving text,
tables, images, and combined multimodal reasoning.

</details>


### [19] [Uncovering Hidden Violent Tendencies in LLMs: A Demographic Analysis via Behavioral Vignettes](https://arxiv.org/abs/2506.20822)
*Quintin Myers,Yanjun Gao*

Main category: cs.CL

TL;DR: 本文首次用社会科学问卷系统评测了大型语言模型在现实冲突中的反应能力，发现在涉及暴力和人口特征时，模型有潜在偏见与内外不一致，对其应用带来风险和挑战。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）被越来越多地用于检测和应对在线暴力内容，但它们在道德模糊、现实场景中的推理能力尚未被充分研究。本文旨在填补这一研究空白。

Method: 采用经过验证的社会科学工具——暴力行为情境问卷（VBVQ），对LLMs对日常冲突响应的能力进行评估。引入基于身份设定的人设提示，在美国内部变换模型的种族、年龄和地理身份信息。并在统一的零样本设置下，对来自不同地缘政治及组织背景的六个LLM进行横向比较。

Result: 研究发现：（1）LLMs生成的表层文本与其内部对暴力反应的偏好常常不一致；（2）不同人口统计特征下，LLMs的暴力倾向存在差异，有时甚至与犯罪学、社会学和心理学的既有研究结果相矛盾。

Conclusion: LLMs在应对现实世界中的道德与暴力问题时，存在潜在的偏见与推理局限，其表面答案不能完全代表其内部倾向。这对其实际应用提出了新的挑战。

Abstract: Large language models (LLMs) are increasingly proposed for detecting and
responding to violent content online, yet their ability to reason about morally
ambiguous, real-world scenarios remains underexamined. We present the first
study to evaluate LLMs using a validated social science instrument designed to
measure human response to everyday conflict, namely the Violent Behavior
Vignette Questionnaire (VBVQ). To assess potential bias, we introduce
persona-based prompting that varies race, age, and geographic identity within
the United States. Six LLMs developed across different geopolitical and
organizational contexts are evaluated under a unified zero-shot setting. Our
study reveals two key findings: (1) LLMs surface-level text generation often
diverges from their internal preference for violent responses; (2) their
violent tendencies vary across demographics, frequently contradicting
established findings in criminology, social science, and psychology.

</details>


### [20] [Decide less, communicate more: On the construct validity of end-to-end fact-checking in medicine](https://arxiv.org/abs/2506.20876)
*Sebastian Joseph,Lily Chen,Barry Wei,Michael Mackert,Iain J. Marshall,Paul Pu Liang,Ramez Kouzy,Byron C. Wallace,Junyi Jessy Li*

Main category: cs.CL

TL;DR: 本文分析了医学事实核查系统在实际应用中面临的挑战，包括证据匹配难、主观性强等，认为应将事实核查视为交互式沟通问题而非完全自动化流程。


<details>
  <summary>Details</summary>
Motivation: 尽管自动事实核查技术发展迅速，但在医学和公共卫生领域，由于医学决策的高风险和文献庞杂复杂，普通用户往往缺乏应付这种专业信息的知识。这导致医疗信息传播过程中的问题，为端到端事实核查系统的应用提供了动因。

Method: 本研究首次分析了临床专家如何实际对社交媒体上的真实医学主张进行核查，并通过合成医学证据来支持判定，探索当前端到端医学事实核查的上限。

Result: 研究发现，将端到端事实核查方法应用于医学领域面临三大根本挑战：1）难以将真实环境中的主张与科学证据（如临床试验）对应；2）主张内容经常存在细节不清和意图不符；3）事实核查的结论具有主观性。

Conclusion: 作者认为，医学事实核查系统需要作为一个交互式的沟通过程来设计和评估，而不是简单的端到端自动化流程，以更好地适应现实中的复杂性和不确定性。

Abstract: Technological progress has led to concrete advancements in tasks that were
regarded as challenging, such as automatic fact-checking. Interest in adopting
these systems for public health and medicine has grown due to the high-stakes
nature of medical decisions and challenges in critically appraising a vast and
diverse medical literature. Evidence-based medicine connects to every
individual, and yet the nature of it is highly technical, rendering the medical
literacy of majority users inadequate to sufficiently navigate the domain. Such
problems with medical communication ripens the ground for end-to-end
fact-checking agents: check a claim against current medical literature and
return with an evidence-backed verdict. And yet, such systems remain largely
unused. To understand this, we present the first study examining how clinical
experts verify real claims from social media by synthesizing medical evidence.
In searching for this upper-bound, we reveal fundamental challenges in
end-to-end fact-checking when applied to medicine: Difficulties connecting
claims in the wild to scientific evidence in the form of clinical trials;
ambiguities in underspecified claims mixed with mismatched intentions; and
inherently subjective veracity labels. We argue that fact-checking should be
approached and evaluated as an interactive communication problem, rather than
an end-to-end process.

</details>


### [21] [Optimising Language Models for Downstream Tasks: A Post-Training Perspective](https://arxiv.org/abs/2506.20917)
*Zhengyan Shi*

Main category: cs.CL

TL;DR: 本论文针对语言模型适应下游任务过程中的效率、鲁棒性和泛化问题，提出了利用无标签数据持续预训练、高效参数微调、改进的有监督微调及新型评测方法。实验证明这些方法显著提升了模型性能，为实现更强通用人工智能迈出重要一步。


<details>
  <summary>Details</summary>
Motivation: 当前大规模语言模型在下游任务适应中效率和鲁棒性有待提升：微调时往往只利用有标签数据，容易过拟合小数据集，且带来大量计算成本，这限制了其实际应用能力。

Method: 1. 提出利用无标签数据提取任务相关知识的持续预训练方法，优于现有半监督方法。
2. 提出高效的参数微调技术，大幅降低内存和计算开销。
3. 改进有监督微调方法，使模型在有标签数据稀缺时也能更好地遵循指令。
4. 构建了新的评估方法和基准任务，如多步空间推理，用于更全面考核能力。

Result: 经多项NLP任务实验证明，这些方法显著提升了语言模型的鲁棒性、效率和泛化能力，适用性更强。

Conclusion: 本文提出的多种新方法有效提升了语言模型适应下游任务时的性能，推动了向更高效、更通用人工智能迈进。

Abstract: Language models (LMs) have demonstrated remarkable capabilities in NLP, yet
adapting them efficiently and robustly to specific tasks remains challenging.
As their scale and complexity grow, fine-tuning LMs on labelled data often
underutilizes available unlabelled data, leads to overfitting on small
task-specific sets, and imposes significant computational costs. These
limitations hamper their application to the open-ended landscape of real-world
language tasks.
  This thesis proposes a series of methods to better adapt LMs to downstream
applications. First, we explore strategies for extracting task-relevant
knowledge from unlabelled data, introducing a novel continued pre-training
technique that outperforms state-of-the-art semi-supervised approaches. Next,
we present a parameter-efficient fine-tuning method that substantially reduces
memory and compute costs while maintaining competitive performance. We also
introduce improved supervised fine-tuning methods that enable LMs to better
follow instructions, especially when labelled data is scarce, enhancing their
performance across a range of NLP tasks, including open-ended generation.
Finally, we develop new evaluation methods and benchmarks, such as multi-hop
spatial reasoning tasks, to assess LM capabilities and adaptation more
comprehensively.
  Through extensive empirical studies across diverse NLP tasks, our results
demonstrate that these approaches substantially improve LM robustness,
efficiency, and generalization, making them more adaptable to a broad range of
applications. These advances mark a significant step towards more robust and
efficient LMs, bringing us closer to the goal of artificial general
intelligence.

</details>


### [22] [FineWeb2: One Pipeline to Scale Them All -- Adapting Pre-Training Data Processing to Every Language](https://arxiv.org/abs/2506.20920)
*Guilherme Penedo,Hynek Kydlíček,Vinko Sabolčec,Bettina Messmer,Negar Foroutan,Amir Hossein Kargaran,Colin Raffel,Martin Jaggi,Leandro Von Werra,Thomas Wolf*

Main category: cs.CL

TL;DR: 本文提出了一套基于FineWeb且支持任意语言自动适配的数据筛选与去重流水线，显著提升非英语多语言训练集质量，并公开了大规模的FineWeb2多语言数据集、流水线及代码。


<details>
  <summary>Details</summary>
Motivation: 当前英文LLM训练数据高质量数据集已公开但多语言数据处理困难，主要难点在于多语言的数据筛选与去重难以统一和泛化，导致高性能多语言LLM难以训练。

Method: 在现有FineWeb数据集基础上设计了一套新的自动化数据筛选和去重流水线，并通过基于定量标准选择的评测任务，对设计选择进行综合消融验证。还提出一种简单、有效的重平衡方法，结合文本质量和去重计数提升模型性能。最终基于1000+语言与近100个Common Crawl快照扩展至FineWeb2数据集。

Result: 提出的流水线产出的非英语语料库训练出的模型优于同类此前数据集。重平衡方法进一步提升性能。最终构建出覆盖1000余种语言、体量达20TB（50亿文档）的FineWeb2多语言数据集及流水线与代码。

Conclusion: FineWeb2数据集和相关流水线能够支持任意语言自动适配，提升多语言LLM的训练表现，并且已将该方案和代码向社区开放。

Abstract: Pre-training state-of-the-art large language models (LLMs) requires vast
amounts of clean and diverse text data. While the open development of large
high-quality English pre-training datasets has seen substantial recent
progress, training performant multilingual LLMs remains a challenge, in large
part due to the inherent difficulty of tailoring filtering and deduplication
pipelines to a large number of languages. In this work, we introduce a new
pre-training dataset curation pipeline based on FineWeb that can be
automatically adapted to support any language. We extensively ablate our
pipeline design choices on a set of nine diverse languages, guided by a set of
meaningful and informative evaluation tasks that were chosen through a novel
selection process based on measurable criteria. Ultimately, we show that our
pipeline can be used to create non-English corpora that produce more performant
models than prior datasets. We additionally introduce a straightforward and
principled approach to rebalance datasets that takes into consideration both
duplication count and quality, providing an additional performance uplift.
Finally, we scale our pipeline to over 1000 languages using almost 100 Common
Crawl snapshots to produce FineWeb2, a new 20 terabyte (5 billion document)
multilingual dataset which we release along with our pipeline, training, and
evaluation codebases.

</details>


### [23] [KaLM-Embedding-V2: Superior Training Techniques and Data Inspire A Versatile Embedding Model](https://arxiv.org/abs/2506.20923)
*Xinping Zhao,Xinshuo Hu,Zifei Shan,Shouzheng Huang,Yao Zhou,Zetian Sun,Zhenyu Liu,Dongfang Li,Xinyuan Wei,Qian Chen,Youcheng Pan,Yang Xiang,Meishan Zhang,Haofen Wang,Jun Yu,Baotian Hu,Min Zhang*

Main category: cs.CL

TL;DR: 本文提出的KaLM-Embedding-V2通过创新训练机制和模型改造，以紧凑体积显著提升文本嵌入效果，性能媲美甚至超越大规模模型，在MTEB基准上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的通用文本嵌入模型在性能和模型体积之间存在权衡，如何在保证模型紧凑的同时提升表征能力和泛化能力，成为提升应用价值的关键。

Method: 提出了KaLM-Embedding-V2模型，创新点包括：1）移除因果注意力，采用全双向transformer加均值池化获得定长向量；2）多阶段训练流程，包括大规模弱监督语料预训练、高质量提取/非提取任务微调和参数平均；3）引入focal-style样本重加权机制和在线难负样本混合策略；4）覆盖更广泛的预训练和微调数据类别。

Result: 模型在MTEB中文和英文测评中显著超越同等规模模型，并与3倍、14倍、18倍、26倍更大模型竞争，展现了优异的性能和泛化能力。

Conclusion: KaLM-Embedding-V2以小于10亿参数的紧凑规模，实现了强大的通用文本嵌入能力，刷新了同类模型的性能上限，对模型高效实用具有重要意义。

Abstract: In this paper, we propose KaLM-Embedding-V2, a versatile and compact
embedding model, which achieves impressive performance in general-purpose text
embedding tasks by leveraging superior training techniques and data. Our key
innovations include: (1) To better align the architecture with representation
learning, we remove the causal attention mask and adopt a fully bidirectional
transformer with simple yet effective mean-pooling to produce fixed-length
embeddings; (2) We employ a multi-stage training pipeline: (i) pre-training on
large-scale weakly supervised open-source corpora; (ii) fine-tuning on
high-quality retrieval and non-retrieval datasets; and (iii) model-soup
parameter averaging for robust generalization. Besides, we introduce a
focal-style reweighting mechanism that concentrates learning on difficult
samples and an online hard-negative mixing strategy to continuously enrich hard
negatives without expensive offline mining; (3) We collect over 20 categories
of data for pre-training and 100 categories of data for fine-tuning, to boost
both the performance and generalization of the embedding model. Extensive
evaluations on the Massive Text Embedding Benchmark (MTEB) Chinese and English
show that our model significantly outperforms others of comparable size, and
competes with 3x, 14x, 18x, and 26x larger embedding models, setting a new
standard for a versatile and compact embedding model with less than 1B
parameters.

</details>


### [24] [Can Gradient Descent Simulate Prompting?](https://arxiv.org/abs/2506.20989)
*Eric Zhang,Leshem Choshen,Jacob Andreas*

Main category: cs.CL

TL;DR: 本文提出一种用元训练优化语言模型，使得微调时可模拟提示法效果，无需真实标签，在多任务上性能提升，展示了梯度学习在泛化和长上下文建模中的潜力。


<details>
  <summary>Details</summary>
Motivation: 当前将新信息融入语言模型的两种主要方法是更改提示（prompt）和更改参数（如微调），其中提示法在许多任务中表现更优。本文关注于是否可以通过优化参数的方式来模拟提示法的效果，从而结合两者优点。

Method: 提出一种通过元训练（meta-training）语言模型的新方法，使得模型经过梯度更新后可以模拟条件化（prompting）新信息的效果。该方法利用梯度元学习的工具，但以模型自身的提示预测作为训练目标，无需真实标签。

Result: 在一些任务上，经过该方法训练后，梯度下降能够恢复部分甚至全部提示模型的性能。例如在“反向诅咒”任务和单步梯度更新后的问答任务上，模型性能均有提升。

Conclusion: 通过适当初始化，梯度下降具有较强表达能力，可以模拟提示法的效果。这为长上下文建模和理解梯度学习的泛化能力提供了新思路。

Abstract: There are two primary ways of incorporating new information into a language
model (LM): changing its prompt or changing its parameters, e.g. via
fine-tuning. Parameter updates incur no long-term storage cost for model
changes. However, for many model updates, prompting is significantly more
effective: prompted models can generalize robustly from single examples and
draw logical inferences that do not occur under standard fine-tuning. Can
models be modified so that fine-tuning does emulate prompting? This paper
describes a method for meta-training LMs such that gradient updates emulate the
effects of conditioning on new information. Our approach uses tools from
gradient-based meta-learning but uses an LM's own prompted predictions as
targets, eliminating the need for ground-truth labels. Subsequent gradient
descent training recovers some (and occasionally all) of prompted model
performance -- showing improvement on the ``reversal curse'' tasks, and
answering questions about text passages after a single gradient update. These
results suggest that, with appropriate initialization, gradient descent can be
surprisingly expressive. Our results suggest new avenues for long-context
modeling and offer insight into the generalization capabilities of
gradient-based learning.

</details>


### [25] [SAC: A Framework for Measuring and Inducing Personality Traits in LLMs with Dynamic Intensity Control](https://arxiv.org/abs/2506.20993)
*Adithya Chittem,Aishna Shrivastava,Sai Tarun Pendela,Jagat Sesh Challa,Dhruv Kumar*

Main category: cs.CL

TL;DR: 本文扩展了大语言模型（LLM）的人格建模，从Big Five增加到16PF，实现了十六种人格特质的深入、具体控制。新提出的SAC结构评估和动态引导人格特质强度，使模型人格更灵活、自然，并在实验中表现出更高的一致性和心理学合理性。为人机交互提供更真实的人格基础。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLM）在与人类交互时被期望展现人的个性，但现有模型主要基于Big Five（OCEAN）人格理论，维度较粗糙且缺乏性格强度的可控机制。本文试图丰富个性表现维度并增强性格强度调控能力，弥补前述不足。

Method: 提出将Machine Personality Inventory（MPI）从Big Five扩展到16 Personality Factor（16PF）模型，实现对16个人格特质的表达，并开发了Specific Attribute Control（SAC）框架，用结构化方式评估和动态调节LLM的人格特质强度。引入基于形容词的语义锚定和行为问题，围绕频率、深度、阈值、努力和意愿五个强度因子调整表达。

Result: 实验发现将人格特质强度建模为连续光谱，能使模型人格表达更加一致和可控，优于简单的二元开关模式。同时，目标人格强度变化会系统性影响相关近邻特质，并呈现心理学上合理的联动，表明LLM内化了多维度的人格结构。

Conclusion: 本文提出了更细致且可控的人格建模方法，推动了LLM在个性表达、社交交互等方向的实用性，可为健康、教育和面试等领域的人机交互带来更拟人的体验。

Abstract: Large language models (LLMs) have gained significant traction across a wide
range of fields in recent years. There is also a growing expectation for them
to display human-like personalities during interactions. To meet this
expectation, numerous studies have proposed methods for modelling LLM
personalities through psychometric evaluations. However, most existing models
face two major limitations: they rely on the Big Five (OCEAN) framework, which
only provides coarse personality dimensions, and they lack mechanisms for
controlling trait intensity. In this paper, we address this gap by extending
the Machine Personality Inventory (MPI), which originally used the Big Five
model, to incorporate the 16 Personality Factor (16PF) model, allowing
expressive control over sixteen distinct traits. We also developed a structured
framework known as Specific Attribute Control (SAC) for evaluating and
dynamically inducing trait intensity in LLMs. Our method introduces
adjective-based semantic anchoring to guide trait intensity expression and
leverages behavioural questions across five intensity factors:
\textit{Frequency}, \textit{Depth}, \textit{Threshold}, \textit{Effort}, and
\textit{Willingness}. Through experimentation, we find that modelling intensity
as a continuous spectrum yields substantially more consistent and controllable
personality expression compared to binary trait toggling. Moreover, we observe
that changes in target trait intensity systematically influence closely related
traits in psychologically coherent directions, suggesting that LLMs internalize
multi-dimensional personality structures rather than treating traits in
isolation. Our work opens new pathways for controlled and nuanced human-machine
interactions in domains such as healthcare, education, and interviewing
processes, bringing us one step closer to truly human-like social machines.

</details>


### [26] [Large Language Models Acing Chartered Accountancy](https://arxiv.org/abs/2506.21031)
*Jatin Gupta,Akhil Sharma,Saransh Singhania,Mohammad Adnan,Sakshi Deo,Ali Imam Abidi,Keshav Gupta*

Main category: cs.CL

TL;DR: 本文提出了针对印度金融领域的CA-Ben基准，对六大主流LLM进行测评。Claude 3.5 Sonnet和GPT-4o在理解金融与法律概念上表现最优，所有模型在数字计算和法律解释方面仍有改善空间，为未来模型优化方向提供参考。


<details>
  <summary>Details</summary>
Motivation: 近年来，大型语言模型（LLMs）在自然语言处理领域取得了重大进展，并逐渐被应用于金融实践。然而，这些模型在金融领域特有知识和推理能力上的有效性尚不明确，尤其是在复杂且庞大的印度金融领域缺乏专用的评测基准。该论文旨在填补这一评测空白。

Method: 作者提出了CA-Ben基准，专门用于评估LLMs在金融、法律和定量推理方面的能力。该基准涵盖了来自印度特许会计师资格考试（ICAI）的分阶段试题与答案，覆盖基础、中级和高级课程。使用标准化评测流程，对六个主流LLM（GPT-4o、LLAMA 3.3 70B、LLAMA 3.1 405B、MISTRAL Large、Claude 3.5 Sonnet 和 Microsoft Phi 4）进行测试。

Result: 评测结果显示，不同模型表现差异明显，其中Claude 3.5 Sonnet和GPT-4o在概念和法律推理方面表现优异。但所有模型在数值计算和法律解释问题上均遇到挑战。

Conclusion: 现有LLMs在金融领域应用时有一定优势，但在定量分析与法律解释上还存在不足。未来可通过混合推理、多模态或检索增强等手段提升其在金融专业领域中的准确性和实用性。

Abstract: Advanced intelligent systems, particularly Large Language Models (LLMs), are
significantly reshaping financial practices through advancements in Natural
Language Processing (NLP). However, the extent to which these models
effectively capture and apply domain-specific financial knowledge remains
uncertain. Addressing a critical gap in the expansive Indian financial context,
this paper introduces CA-Ben, a Chartered Accountancy benchmark specifically
designed to evaluate the financial, legal, and quantitative reasoning
capabilities of LLMs. CA-Ben comprises structured question-answer datasets
derived from the rigorous examinations conducted by the Institute of Chartered
Accountants of India (ICAI), spanning foundational, intermediate, and advanced
CA curriculum stages. Six prominent LLMs i.e. GPT 4o, LLAMA 3.3 70B, LLAMA 3.1
405B, MISTRAL Large, Claude 3.5 Sonnet, and Microsoft Phi 4 were evaluated
using standardized protocols. Results indicate variations in performance, with
Claude 3.5 Sonnet and GPT-4o outperforming others, especially in conceptual and
legal reasoning. Notable challenges emerged in numerical computations and legal
interpretations. The findings emphasize the strengths and limitations of
current LLMs, suggesting future improvements through hybrid reasoning and
retrieval-augmented generation methods, particularly for quantitative analysis
and accurate legal interpretation.

</details>


### [27] [A Semi-supervised Scalable Unified Framework for E-commerce Query Classification](https://arxiv.org/abs/2506.21049)
*Chunyuan Yuan,Chong Zhang,Zheng Fang,Ming Pang,Xue Jiang,Changping Peng,Zhangang Lin,Ching Law*

Main category: cs.CL

TL;DR: 文章针对电商查询分类难题，提出SSUF统一框架，结合知识、标签、结构三类增强模块，拒绝过度依赖用户点击，强力提升多任务分类效果，并通过实验验证其领先性能。


<details>
  <summary>Details</summary>
Motivation: 电商领域的查询分类（如意图和类别预测）面临查询短、上下文少、标签信息不可用等挑战，现有方法多依赖用户点击行为，易陷入马太效应恶性循环，且各子任务缺乏统一框架，算法优化效率低。

Method: 提出了一个新的半监督可扩展统一框架（SSUF），包括三大增强模块：知识增强（利用世界知识丰富查询表征）、标签增强（利用标签语义和半监督信号减少对后验标签依赖）、结构增强（基于复杂标签关系强化表示），还具备高度可插拔性。

Result: 通过离线和线上A/B测试，SSUF在各项指标上显著优于当前最先进的模型。

Conclusion: SSUF框架有效统一、多任务场景中的查询分类，大大提升了电商应用的分类准确率与算法优化效率，优于现有主流方法。

Abstract: Query classification, including multiple subtasks such as intent and category
prediction, is vital to e-commerce applications. E-commerce queries are usually
short and lack context, and the information between labels cannot be used,
resulting in insufficient prior information for modeling. Most existing
industrial query classification methods rely on users' posterior click behavior
to construct training samples, resulting in a Matthew vicious cycle.
Furthermore, the subtasks of query classification lack a unified framework,
leading to low efficiency for algorithm optimization.
  In this paper, we propose a novel Semi-supervised Scalable Unified Framework
(SSUF), containing multiple enhanced modules to unify the query classification
tasks. The knowledge-enhanced module uses world knowledge to enhance query
representations and solve the problem of insufficient query information. The
label-enhanced module uses label semantics and semi-supervised signals to
reduce the dependence on posterior labels. The structure-enhanced module
enhances the label representation based on the complex label relations. Each
module is highly pluggable, and input features can be added or removed as
needed according to each subtask. We conduct extensive offline and online A/B
experiments, and the results show that SSUF significantly outperforms the
state-of-the-art models.

</details>


### [28] [CBF-AFA: Chunk-Based Multi-SSL Fusion for Automatic Fluency Assessment](https://arxiv.org/abs/2506.20243)
*Papa Séga Wade,Mihai Andries,Ioannis Kanellos,Thierry Moudenc*

Main category: cs.CL

TL;DR: 本文提出多SSL嵌入融合+分块建模的CNN-BiLSTM流利度评估方法，有效捕获语音韵律停顿等特征，综合表现超过现有主流基线。


<details>
  <summary>Details</summary>
Motivation: 现有自动流利度评估难以精准捕捉非母语者口语的语音韵律、停顿与不流利，尤其在噪声环境与异常韵律中易受影响，因此亟需更鲁棒精准的流利度特征提取和建模手段。

Method: 首先，利用Silero-VAD对语音按呼吸组分块，获得精细的时间分析。随后提取三种SSL模型（Wav2Vec2、HuBERT、WavLM）的音频嵌入，并通过可学习加权机制融合，结合每语块语速、停顿等流利性特征，最后通过分层CNN-BiLSTM建模局部与长程依赖，完成流利度分数预测。

Result: 在Speechocean762与Avalinguo两个数据集上，该方法分别较单一SSL基线提升了2.8/4.2的F1分数和6.2/4.0的Pearson相关分数，全面优于Pyannote.audio分割基线。

Conclusion: 基于语块的多SSL融合方法在流利度自动评估任务中表现优异，能够更准确捕捉口语中的韵律、停顿和不流利现象，显著优于传统单一SSL与Pyannote.audio分割基线，并为鲁棒性流利度评估带来新方向。

Abstract: Automatic fluency assessment (AFA) remains challenging, particularly in
capturing speech rhythm, pauses, and disfluencies in non-native speakers. We
introduce a chunk-based approach integrating self-supervised learning (SSL)
models (Wav2Vec2, HuBERT, and WavLM) selected for their complementary strengths
in phonetic, prosodic, and noisy speech modeling, with a hierarchical
CNN-BiLSTM framework. Speech is segmented into breath-group chunks using Silero
voice activity detection (Silero-VAD), enabling fine-grained temporal analysis
while mitigating over-segmentation artifacts. SSL embeddings are fused via a
learnable weighted mechanism, balancing acoustic and linguistic features, and
enriched with chunk-level fluency markers (e.g., speech rate, pause durations,
n-gram repetitions). The CNN-BiLSTM captures local and long-term dependencies
across chunks. Evaluated on Avalinguo and Speechocean762, our approach improves
F1-score by 2.8 and Pearson correlation by 6.2 points over single SSL baselines
on Speechocean762, with gains of 4.2 F1-score and 4.0 Pearson points on
Avalinguo, surpassing Pyannote.audio-based segmentation baselines. These
findings highlight chunk-based multi-SSL fusion for robust fluency evaluation,
though future work should explore generalization to dialects with irregular
prosody.

</details>


### [29] [MT2-CSD: A New Dataset and Multi-Semantic Knowledge Fusion Method for Conversational Stance Detection](https://arxiv.org/abs/2506.21053)
*Fuqiang Niu,Genan Dai,Yisha Lu,Jiayu Liao,Xiang Li,Hu Huang,Bowen Zhang*

Main category: cs.CL

TL;DR: 本文解决了社交媒体多轮立场检测中数据集缺乏的问题，提出了MT2-CSD大规模数据集，并设计了基于大语言模型的LLM-CRAN方法，在新数据集上取得了显著超越基线的效果。


<details>
  <summary>Details</summary>
Motivation: 现有的立场检测研究多依赖于单一实例，难以有效建模真实社交媒体中多方、多轮讨论的复杂动向，主要受限于缺乏能真实反映社交媒体互动动态的数据集，从而阻碍了会话型立场检测的发展。

Method: 提出了MT2-CSD这一多目标、多轮对话的立场检测数据集，并基于大语言模型（LLM）的推理能力开发了会话关系注意力网络（LLM-CRAN）方法，用以提升会话理解能力，并在新数据集上进行了模型效能评估。

Result: LLM-CRAN模型在MT2-CSD数据集上的实验结果显示，其在会话立场检测任务中表现显著优于现有强基线方法。

Conclusion: 该研究为多目标、多轮立场检测提供了新的大规模数据集与有效模型，推进了会话型立场检测任务在社交媒体分析领域的发展。

Abstract: In the realm of contemporary social media, automatic stance detection is
pivotal for opinion mining, as it synthesizes and examines user perspectives on
contentious topics to uncover prevailing trends and sentiments. Traditional
stance detection research often targets individual instances, thereby limiting
its capacity to model multi-party discussions typical in real social media
scenarios. This shortcoming largely stems from the scarcity of datasets that
authentically capture the dynamics of social media interactions, hindering
advancements in conversational stance detection. In this paper, we introduce
MT2-CSD, a comprehensive dataset for multi-target, multi-turn conversational
stance detection. To the best of our knowledge, MT2-CSD is the largest dataset
available for this purpose, comprising 24,457 annotated instances and
exhibiting the greatest conversational depth, thereby presenting new challenges
for stance detection. To address these challenges, we propose the Large
Language model enhanced Conversational Relational Attention Network (LLM-CRAN),
which exploits the reasoning capabilities of LLMs to improve conversational
understanding. We conduct extensive experiments to evaluate the efficacy of
LLM-CRAN on the MT2-CSD dataset. The experimental results indicate that
LLM-CRAN significantly outperforms strong baseline models in the task of
conversational stance detection.

</details>


### [30] [DALR: Dual-level Alignment Learning for Multimodal Sentence Representation Learning](https://arxiv.org/abs/2506.21096)
*Kang He,Yuzhe Ding. Haining Wang,Fei Li,Chong Teng,Donghong Ji*

Main category: cs.CL

TL;DR: DALR方法针对多模态句子表征中的两大难题，设计跨模态一致性学习与排序蒸馏技术，在多个评测任务超越主流方法。


<details>
  <summary>Details</summary>
Motivation: 现有多模态句子表征方法多为粗粒度对齐，存在跨模态失配和模态内语义分歧，影响句子表征质量。

Method: 提出了DALR方法，包含跨模态一致性学习模块（弱化负样本、辅助任务引入语义相似度，实现细粒度对齐）和全局排序蒸馏联合全局对齐学习。

Result: 该方法在语义文本相似性（STS）和迁移（TR）任务上广泛实验验证，比当前所有方法取得更优效果。

Conclusion: DALR方法能有效提升多模态句子表示学习的表现，优于已有的先进方法。

Abstract: Previous multimodal sentence representation learning methods have achieved
impressive performance. However, most approaches focus on aligning images and
text at a coarse level, facing two critical challenges:cross-modal misalignment
bias and intra-modal semantic divergence, which significantly degrade sentence
representation quality. To address these challenges, we propose DALR
(Dual-level Alignment Learning for Multimodal Sentence Representation). For
cross-modal alignment, we propose a consistency learning module that softens
negative samples and utilizes semantic similarity from an auxiliary task to
achieve fine-grained cross-modal alignment. Additionally, we contend that
sentence relationships go beyond binary positive-negative labels, exhibiting a
more intricate ranking structure. To better capture these relationships and
enhance representation quality, we integrate ranking distillation with global
intra-modal alignment learning. Comprehensive experiments on semantic textual
similarity (STS) and transfer (TR) tasks validate the effectiveness of our
approach, consistently demonstrating its superiority over state-of-the-art
baselines.

</details>


### [31] [ComRAG: Retrieval-Augmented Generation with Dynamic Vector Stores for Real-time Community Question Answering in Industry](https://arxiv.org/abs/2506.21098)
*Qinwen Chen,Wenbiao Tao,Zhiwei Zhu,Mingfan Xi,Liangzhong Guo,Yuan Wang,Wei Wang,Yunshi Lan*

Main category: cs.CL

TL;DR: 论文提出了适用于工业问答系统的ComRAG框架，通过创新的记忆检索机制，有效利用了历史和外部知识，显著提升了系统准确性与时效性，并优化了存储效率。


<details>
  <summary>Details</summary>
Motivation: 社区问答（CQA）平台在知识共享和积累中起到重要作用，但如何在实时场景下有效利用历史交互和领域知识仍然是个难题。现有方法对于外部知识的利用不足、动态历史问答上下文集成不充分，或缺乏适合工业应用的高效记忆机制。

Method: 提出ComRAG，这是一种检索增强生成（retrieval-augmented generation）框架，专为实时工业级CQA设计。它结合了静态知识与动态历史问答对，应用了基于质心的存储记忆机制，实现检索、生成和高效存储。

Result: 在三个工业CQA数据集上的实验结果显示，ComRAG在所有基线上都取得了更好的性能：向量相似度提升最多可达25.9%，查询延迟降低8.7%至23.3%，数据块增长率从20.23%下降到2.06%。

Conclusion: ComRAG能够高效整合静态和动态知识，显著提升工业CQA系统的检索和生成能力，并且对存储和实时性能有显著优化。

Abstract: Community Question Answering (CQA) platforms can be deemed as important
knowledge bases in community, but effectively leveraging historical
interactions and domain knowledge in real-time remains a challenge. Existing
methods often underutilize external knowledge, fail to incorporate dynamic
historical QA context, or lack memory mechanisms suited for industrial
deployment. We propose ComRAG, a retrieval-augmented generation framework for
real-time industrial CQA that integrates static knowledge with dynamic
historical QA pairs via a centroid-based memory mechanism designed for
retrieval, generation, and efficient storage. Evaluated on three industrial CQA
datasets, ComRAG consistently outperforms all baselines--achieving up to 25.9%
improvement in vector similarity, reducing latency by 8.7% to 23.3%, and
lowering chunk growth from 20.23% to 2.06% over iterations.

</details>


### [32] [Progtuning: Progressive Fine-tuning Framework for Transformer-based Language Models](https://arxiv.org/abs/2506.21119)
*Xiaoshuang Ji,Zhendong Zhao,Xiaojun Chen,Xin Zhao,Zeyao Liu*

Main category: cs.CL

TL;DR: 提出新微调框架Progtuning，通过逐步减少更新的Transformer块，优化了资源分配，实现参数更新减少25%的同时维持性能，并与其他高效微调方法兼容。


<details>
  <summary>Details</summary>
Motivation: 随着Transformer模型规模的不断扩大，传统的全参数微调方法计算和存储成本越来越高。参数高效微调方法虽然缓解了部分问题，但大多数方法没有考虑Transformer各层贡献不均，导致资源分配低效。

Method: 提出了一种新颖的结合逐步学习(progressive learning)的微调框架Progtuning。Progtuning根据各Transformer块对任务的实际贡献，逐步减少被更新的块数，从而实现参数更新的高效分配。

Result: 与传统微调方法相比，Progtuning能在保证性能竞争力的前提下，减少约25%的参数更新量，同时可以很好地与现有参数高效微调方法结合，适应多种不同的迁移场景。

Conclusion: Progtuning有效地通过逐步筛选Transformer块进行更新，实现了计算资源的优化分配和参数更新量的显著降低，在多种适应任务中表现优异，推动了高效微调的发展。

Abstract: Fine-tuning is a promising technique for leveraging Transformer-based
language models in downstream tasks. As model sizes continue to grow, updating
all model parameters becomes increasingly costly. Parameter-efficient
fine-tuning methods effectively address this issue by selectively updating a
small subset of parameters. However, fine-tuning and most existing
parameter-efficient fine-tuning methods require updating the same number of
parameters as the initial size, ignoring the unequal contribution across
Transformer blocks and leading to extremely inefficient allocation of computing
resources. In this paper, we propose Progtuning, the novel fine-tuning
framework combined with progressive learning for Transformer-based language
models. Specifically, Progtuning progressively reduces the number of updated
transformer blocks based on the contribution. Remarkably, Progtuning optimizes
resource allocation and reduces the number of updated parameters by
approximately 25\%, while still maintaining competitive performance. And it
also exhibits high adaptability with parameter-efficient fine-tuning methods,
demonstrating excellent performance across various adaptation scenarios.

</details>


### [33] [Compressed and Smooth Latent Space for Text Diffusion Modeling](https://arxiv.org/abs/2506.21170)
*Viacheslav Meshchaninov,Egor Chimbulatov,Alexander Shabalin,Aleksandr Abramov,Dmitry Vetrov*

Main category: cs.CL

TL;DR: Cosmos是一种面向压缩潜在空间的扩散文本生成新方法，可在大幅提升生成速度的同时保持甚至提升文本生成质量。


<details>
  <summary>Details</summary>
Motivation: 当前自回归语言模型虽然主导了文本生成领域，但由于其序列性导致解码速度慢且难以保证全局连贯性。因此，需要一种既能加快生成速度又能保持文本质量的替代方案。扩散模型具有并行生成和灵活控制的优势，但在文本生成中应用受限于token级表示的高维性。

Method: 提出了一种名为Cosmos的新方法，将文本生成完全置于为扩散过程专门设计、平滑且压缩的潜在空间中。该空间由一个同时进行token级重构和与预训练语言编码器冻结激活对齐的自动编码器学习而成，从而保证了语义基础，支持有效的基于扰动的数据增强。

Result: 实验证明，文本表示可被压缩8倍的同时，生成质量依然可与token级扩散模型持平。当增加潜在序列长度时，Cosmos更是超越了基准扩散模型和自回归模型。

Conclusion: Cosmos在故事生成、问题生成、摘要和文本净化等四个多样化任务上，表现出与各种生成范式可比甚至更高的生成质量，并实现了超过2倍的推理速度提升。

Abstract: Autoregressive language models dominate modern text generation, yet their
sequential nature introduces fundamental limitations: decoding is slow, and
maintaining global coherence remains challenging. Diffusion models offer a
promising alternative by enabling parallel generation and flexible control;
however, their application to text generation is hindered by the high
dimensionality of token-level representations. We introduce Cosmos, a novel
approach to text generation that operates entirely in a compressed, smooth
latent space tailored specifically for diffusion. This space is learned using
an autoencoder trained simultaneously for token-level reconstruction and
alignment with frozen activations from a pretrained language encoder, providing
robust semantic grounding and enabling effective perturbation-based
augmentations. Empirically, we demonstrate that text representations can be
compressed by $8\times$ while maintaining generation quality comparable to
token-level diffusion models. Furthermore, increasing the latent sequence
length allows Cosmos to surpass both diffusion-based and autoregressive
baselines. We evaluate Cosmos on four diverse generative tasks including story
generation, question generation, summarization, and detoxification and compare
it with various generative paradigms. Cosmos achieves comparable or superior
generation quality while offering more than $2\times$ faster inference.

</details>


### [34] [Maintaining MTEB: Towards Long Term Usability and Reproducibility of Embedding Benchmarks](https://arxiv.org/abs/2506.21182)
*Isaac Chung,Imene Kerboua,Marton Kardos,Roman Solomatin,Kenneth Enevoldsen*

Main category: cs.CL

TL;DR: 本文介绍了MTEB基准测试在持续集成、质量控制和社区协作方面的工程实践，这些措施保证了其可复现性与可扩展性，有助于同行维护高质量评测平台。


<details>
  <summary>Details</summary>
Motivation: MTEB作为文本嵌入模型评估的标准，作者希望解决在持续发展中如何保证其复现性和可扩展性。

Method: 论文侧重于工程实现，包括建立健全的持续集成流程，用于验证数据集完整性、自动化测试和评估结果的泛化性，同时描述了处理社区贡献和扩展新任务的策略。

Result: 通过上述工程实践，MTEB实现了规模扩大但质量与相关性未受损，成为更具代表性的基准，并为类似领域基准维护者提供了经验参考。

Conclusion: 论文强调了工程方法在机器学习评估框架中确保复现性和可用性的重要价值，MTEB的经验对同行具有推广意义。

Abstract: The Massive Text Embedding Benchmark (MTEB) has become a standard evaluation
platform for text embedding models. While previous work has established the
core benchmark methodology, this paper focuses on the engineering aspects that
ensure MTEB's continued reproducibility and extensibility. We present our
approach to maintaining robust continuous integration pipelines that validate
dataset integrity, automate test execution, and assess benchmark results'
generalizability. We detail the design choices that collectively enhance
reproducibility and usability. Furthermore, we discuss our strategies for
handling community contributions and extending the benchmark with new tasks and
datasets. These engineering practices have been instrumental in scaling MTEB to
become more comprehensive while maintaining quality and, ultimately, relevance
to the field. Our experiences offer valuable insights for benchmark maintainers
facing similar challenges in ensuring reproducibility and usability in machine
learning evaluation frameworks. The MTEB repository is available at:
https://github.com/embeddings-benchmark/mteb

</details>


### [35] [Prompt-Guided Turn-Taking Prediction](https://arxiv.org/abs/2506.21191)
*Koji Inoue,Mikey Elmers,Yahui Fu,Zi Haur Pang,Divesh Lala,Keiko Ochi,Tatsuya Kawahara*

Main category: cs.CL

TL;DR: 本文提出了一种通过文字提示动态调控的轮次预测模型，通过引入文本嵌入提高了预测准确率与灵活性，在大规模真实对话数据测试中表现优良。


<details>
  <summary>Details</summary>
Motivation: 对话系统和交互式机器人需要准确预测对话轮次切换，以提升人机交互的自然性与灵活性。现有方法尽管实现了实时连续预测，但缺乏根据对话环境灵活调整预测行为的能力。

Method: 提出了一种新型的基于Transformer结构的语音活动预测（VAP）模型。该模型允许通过文字型提示（如“更快”或“更冷静”）动态控制轮次切换预测，具体方法是在模型的通道变换器和跨通道变换器中引入文本提示嵌入。为弥补现有数据集缺乏文本提示语，借助大语言模型（LLM）合成生成了相应提示语句。

Result: 模型在950小时人工对话数据上进行实验。结果显示，该方法不仅提升了轮次预测的准确率，同时能够根据文字提示灵活调整轮次切换时机。

Conclusion: 基于Transformer的带文本提示语调控的新一代轮次预测模型能够提升对话系统的预测性能和适应性，有助于实现更人性化的对话交互。

Abstract: Turn-taking prediction models are essential components in spoken dialogue
systems and conversational robots. Recent approaches leverage transformer-based
architectures to predict speech activity continuously and in real-time. In this
study, we propose a novel model that enables turn-taking prediction to be
dynamically controlled via textual prompts. This approach allows intuitive and
explicit control through instructions such as "faster" or "calmer" adapting
dynamically to conversational partners and contexts. The proposed model builds
upon a transformer-based voice activity projection (VAP) model, incorporating
textual prompt embeddings into both channel-wise transformers and a
cross-channel transformer. We evaluated the feasibility of our approach using
over 950 hours of human-human spoken dialogue data. Since textual prompt data
for the proposed approach was not available in existing datasets, we utilized a
large language model (LLM) to generate synthetic prompt sentences. Experimental
results demonstrated that the proposed model improved prediction accuracy and
effectively varied turn-taking timing behaviors according to the textual
prompts.

</details>


### [36] [Enhancing Automatic Term Extraction with Large Language Models via Syntactic Retrieval](https://arxiv.org/abs/2506.21222)
*Yongchan Chun,Minhyuk Kim,Dongjun Kim,Chanjun Park,Heuiseok Lim*

Main category: cs.CL

TL;DR: 提出了一种基于语法相似性检索的few-shot提示方法，使LLM在术语提取任务中表现更优，特别是在术语边界的识别上。


<details>
  <summary>Details</summary>
Motivation: 虽然大型语言模型（LLMs）已经极大推动了多种自然语言处理任务的发展，但其在术语自动提取（ATE）方面的潜力却鲜有探索。希望提升LLMs对术语识别的能力，尤其是准确捕捉术语边界。

Method: 提出了一种基于检索的few-shot提示方法，该方法根据语法相似性（而非语义相似性）选择示例，实现领域无关的演示选择，从而更可靠地引导术语边界的识别。

Result: 在三个专用ATE基准测试中，该语法检索方法无论是在同域还是跨域设置下，都表现出对F1分数的显著提升，同时分析表明查询句与检索样例间的词汇重叠度影响性能。

Conclusion: 基于语法检索的提示策略能够提升大型语言模型在术语自动提取任务中的表现，强调了语法线索对于提升术语边界识别能力的重要性。

Abstract: Automatic Term Extraction (ATE) identifies domain-specific expressions that
are crucial for downstream tasks such as machine translation and information
retrieval. Although large language models (LLMs) have significantly advanced
various NLP tasks, their potential for ATE has scarcely been examined. We
propose a retrieval-based prompting strategy that, in the few-shot setting,
selects demonstrations according to \emph{syntactic} rather than semantic
similarity. This syntactic retrieval method is domain-agnostic and provides
more reliable guidance for capturing term boundaries. We evaluate the approach
in both in-domain and cross-domain settings, analyzing how lexical overlap
between the query sentence and its retrieved examples affects performance.
Experiments on three specialized ATE benchmarks show that syntactic retrieval
improves F1-score. These findings highlight the importance of syntactic cues
when adapting LLMs to terminology-extraction tasks.

</details>


### [37] [Agent-RewardBench: Towards a Unified Benchmark for Reward Modeling across Perception, Planning, and Safety in Real-World Multimodal Agents](https://arxiv.org/abs/2506.21252)
*Tianyi Men,Zhuoran Jin,Pengfei Cao,Yubo Chen,Kang Liu,Jun Zhao*

Main category: cs.CL

TL;DR: 论文提出了Agent-RewardBench基准，专用于评估多模态大模型在智能体奖励建模上的能力。包含多场景、细粒度步骤评估以及高质量数据。实验显示目前模型仍有很大提升空间，凸显了专门训练奖励建模能力的重要性。


<details>
  <summary>Details</summary>
Motivation: 随着多模态大模型（MLLMs）的进步，多模态智能体在实际任务中表现出巨大潜力。但由于缺乏外部反馈，这些智能体难以自我纠错和泛化。目前尚无针对智能体奖励模型如何选择的明确指导，因此急需建立专门面向智能体的奖励基准。

Method: 提出并构建了Agent-RewardBench，这是一个用于评估MLLMs奖励建模能力的基准。该基准具有三个特点：1）涵盖感知、规划和安全等7个真实场景；2）能够对任务中的每一步进行奖励的细粒度评估；3）样本来自10个具有代表性的模型，通过难度控制和人工验证确保任务具备挑战性且数据高质量。

Result: 实验表明，即使是现有最先进的多模态大模型，在该基准上表现也有限，说明当前模型在智能体奖励建模方面还有明显的提升空间。

Conclusion: 当前多模态大模型在奖励建模方面能力有限；Agent-RewardBench为后续研究和模型改进提供标准和工具，未来需对奖励建模进行专门训练。

Abstract: As Multimodal Large Language Models (MLLMs) advance, multimodal agents show
promise in real-world tasks like web navigation and embodied intelligence.
However, due to limitations in a lack of external feedback, these agents
struggle with self-correction and generalization. A promising approach is to
use reward models as external feedback, but there is no clear on how to select
reward models for agents. Thus, there is an urgent need to build a reward bench
targeted at agents. To address these challenges, we propose Agent-RewardBench,
a benchmark designed to evaluate reward modeling ability in MLLMs. The
benchmark is characterized by three key features: (1) Multiple dimensions and
real-world agent scenarios evaluation. It covers perception, planning, and
safety with 7 scenarios; (2) Step-level reward evaluation. It allows for the
assessment of agent capabilities at the individual steps of a task, providing a
more granular view of performance during the planning process; and (3)
Appropriately difficulty and high-quality. We carefully sample from 10 diverse
models, difficulty control to maintain task challenges, and manual verification
to ensure the integrity of the data. Experiments demonstrate that even
state-of-the-art multimodal models show limited performance, highlighting the
need for specialized training in agent reward modeling. Code is available at
github.

</details>


### [38] [Cat and Mouse -- Can Fake Text Generation Outpace Detector Systems?](https://arxiv.org/abs/2506.21274)
*Andrea McGlinchey,Peter J Barclay*

Main category: cs.CL

TL;DR: 大模型变强，假文本更难检测？实验表明，部分新模型欺骗性提升，但利用简单统计方法，检测仍有效。检测和生成未必进入永无止境的军备竞赛。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）能够生成高度迷惑性的“假文本”，如学术写作、商品评论和政治新闻等领域，检测人工生成文本的方法愈发重要。学界担心随着模型能力增强，“检测与反检测”将陷入军备竞赛。本文关注大模型能力提升后，检测方法是否迟早会失效，还是存在检测能力瓶颈。

Method: 本文采用统计分类器，针对经典侦探小说风格的“假文本”进行检测实验。通过对比不同版本与种类的大模型（如Gemini和GPT)，分析模型生成假文本的‘欺骗性’随能力变化的趋势。

Result: 在大模型版本提升0.5的实验中，Gemini展现出生成更具迷惑性的假文本能力，但GPT未表现出显著变化。即使面对越来越强大的模型，简单统计分类器仍可保持较好的检测准确率。

Conclusion: 假文本检测技术有望继续对抗更强大的语言模型，不必然陷入技术竞赛的绝望局面。不过，模型体系的改变仍可能提升其欺骗性，需要持续关注模型架构的演化。

Abstract: Large language models can produce convincing "fake text" in domains such as
academic writing, product reviews, and political news. Many approaches have
been investigated for the detection of artificially generated text. While this
may seem to presage an endless "arms race", we note that newer LLMs use ever
more parameters, training data, and energy, while relatively simple classifiers
demonstrate a good level of detection accuracy with modest resources. To
approach the question of whether the models' ability to beat the detectors may
therefore reach a plateau, we examine the ability of statistical classifiers to
identify "fake text" in the style of classical detective fiction. Over a 0.5
version increase, we found that Gemini showed an increased ability to generate
deceptive text, while GPT did not. This suggests that reliable detection of
fake text may remain feasible even for ever-larger models, though new model
architectures may improve their deceptiveness

</details>


### [39] [Double-Checker: Enhancing Reasoning of Slow-Thinking LLMs via Self-Critical Fine-Tuning](https://arxiv.org/abs/2506.21285)
*Xin Xu,Tianhao Chen,Fan Zhang,Wanlong Liu,Pengxiang Li,Ajay Kumar Jaiswal,Yuchen Yan,Jishan Hu,Yang Wang,Hao Chen,Shiwei Liu,Shizhe Diao,Can Yang,Lu Yin*

Main category: cs.CL

TL;DR: 提出了Double-Checker框架，通过微调和自我批判机制，使大语言模型能迭代优化输出显著提升推理性能，在多个基准测试上效果明显，尤其在AIME测试中pass@1由4.4%提升到18.2%。


<details>
  <summary>Details</summary>
Motivation: 现有慢思考大语言模型（LLM）虽然具备一定的反思思维能力（如“灵光一现”），但它们在生成有见地的自我批判以及优化之前的解答方面依然存在局限。

Method: 提出了Double-Checker框架，通过在1,730个精心挑选的自我批判样本上进行微调，使LLM能在推理时反复自我批判和优化答案，直到其自我评估结果为正确。

Result: Double-Checker框架在多项推理基准测试中显著提升了LLM的推理表现。例如，在AIME高难度基准上的pass@1指标由原始模型的4.4%提升至18.2%。

Conclusion: Double-Checker能够通过结构化的自我批判和迭代优化，显著增强LLM的推理能力，为开发更值得信赖且高效的LLM指明了新方向。

Abstract: While slow-thinking large language models (LLMs) exhibit reflection-like
reasoning, commonly referred to as the "aha moment:, their ability to generate
informative critiques and refine prior solutions remains limited. In this
paper, we introduce Double-Checker, a principled framework designed to enhance
the reasoning capabilities of slow-thinking LLMs by fostering explicit
self-critique and iterative refinement of their previous solutions. By
fine-tuning on our curated 1,730 self-critical instances, Double-Checker
empowers long-CoT LLMs to iteratively critique and refine their outputs during
inference until they evaluate their solutions as correct under self-generated
critiques. We validate the efficacy of Double-Checker across a comprehensive
suite of reasoning benchmarks, demonstrating that iterative self-critique
significantly enhances the reasoning capabilities of long-CoT LLMs. Notably,
our Double-Checker increases the pass@1 performance on challenging AIME
benchmarks from 4.4% to 18.2% compared to the original long-CoT LLMs. These
results highlight a promising direction for developing more trustworthy and
effective LLMs capable of structured self-critique.

</details>


### [40] [Small Encoders Can Rival Large Decoders in Detecting Groundedness](https://arxiv.org/abs/2506.21288)
*Istabrak Abbes,Gabriele Prato,Quentin Fournier,Fernando Rodriguez,Alaa Boukhary,Adam Elwood,Sarath Chandar*

Main category: cs.CL

TL;DR: 论文提出结合轻量级检测步骤，判断问题是否能由上下文支持，再决定是否调用大型语言模型生成答案，兼顾高准确率和极低推理延迟。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在NLP任务中能更好地发挥作用，但当输入上下文缺乏信息时，LLM容易凭空臆测或依赖内部知识，导致回答缺乏事实基础。提升LLM回答的事实性和可信度非常重要。

Method: 本研究提出首先检测查询是否由提供的文档支撑（即是否“扎根于”上下文），再决定是否使用LLM生成答案。作者采用轻量级、任务专用的编码器模型（如RoBERTa和NomicBERT），并在精心筛选的数据集上进行微调。

Result: 微调后的RoBERTa和NomicBERT在检测查询是否“扎根”于给定文档时，准确率可媲美SOTA的LLM（如Llama3 8B和GPT4o），但推理延迟大幅降低。

Conclusion: 可以借助微调的轻量级编码器模型，在保证检测准确率的同时，显著降低资源消耗和推理延迟，在LLM生成高成本答案之前高效检测上下文支撑性。

Abstract: Augmenting large language models (LLMs) with external context significantly
improves their performance in natural language processing (NLP) tasks. However,
LLMs struggle to answer queries reliably when the provided context lacks
information, often resorting to ungrounded speculation or internal knowledge.
Groundedness - generating responses strictly supported by the context - is
essential for ensuring factual consistency and trustworthiness. This study
focuses on detecting whether a given query is grounded in a document provided
in context before the costly answer generation by LLMs. Such a detection
mechanism can significantly reduce both inference time and resource
consumption. We show that lightweight, task specific encoder models such as
RoBERTa and NomicBERT, fine-tuned on curated datasets, can achieve accuracy
comparable to state-of-the-art LLMs, such as Llama3 8B and GPT4o, in
groundedness detection while reducing inference latency by orders of magnitude.
The code is available at : https://github.com/chandarlab/Hallucinate-less

</details>


### [41] [Detecting Referring Expressions in Visually Grounded Dialogue with Autoregressive Language Models](https://arxiv.org/abs/2506.21294)
*Bram Willemsen,Gabriel Skantze*

Main category: cs.CL

TL;DR: 作者提出并实验证明，通过高效微调大语言模型，仅利用语言上下文也可以在视觉对话中有效检测指代表达，但由于该任务本质是多模态的，文本-only方法存在天然限制，仍需进一步结合视觉信息以提升效果。


<details>
  <summary>Details</summary>
Motivation: 研究视觉语境对话中，是否仅依赖文本（语言语境）即可有效检测出带有视觉指代的表达，从而探讨语言模型在此类任务中的能力与局限性。

Method: 采用的是纯文本、自回归语言建模方法，将预训练的大语言模型（LLM）通过高效微调，适配对话中的指代短语（mention spans）检测任务，通过文本的下一个token预测来确定指代短语的边界。

Result: 实验表明，即使只用适中规模的大语言模型、小型数据集和高效参数微调，基于文本的方法仍然在提及表达检测任务中表现出了有效性，即语言上下文本身就能提供较多有关指代表达的信息。

Conclusion: 文本语境在检测视觉指代表达方面作用重要，但文本方法本质上仍受限于单一模态，对于本质是多模态的问题（需要结合视觉信息）来说，文本-only方法存在局限。

Abstract: In this paper, we explore the use of a text-only, autoregressive language
modeling approach for the extraction of referring expressions from visually
grounded dialogue. More specifically, the aim is to investigate the extent to
which the linguistic context alone can inform the detection of mentions that
have a (visually perceivable) referent in the visual context of the
conversation. To this end, we adapt a pretrained large language model (LLM) to
perform a relatively course-grained annotation of mention spans in unfolding
conversations by demarcating mention span boundaries in text via next-token
prediction. Our findings indicate that even when using a moderately sized LLM,
relatively small datasets, and parameter-efficient fine-tuning, a text-only
approach can be effective, highlighting the relative importance of the
linguistic context for this task. Nevertheless, we argue that the task
represents an inherently multimodal problem and discuss limitations fundamental
to unimodal approaches.

</details>


### [42] [Structuralist Approach to AI Literary Criticism: Leveraging Greimas Semiotic Square for Large Language Models](https://arxiv.org/abs/2506.21360)
*Fangzhou Dong,Yifan Zeng,Yingpeng Sang,Hong Shen*

Main category: cs.CL

TL;DR: 提出GLASS框架，用于提升大模型的文学批评能力；构建新数据集并设计量化评测；实验结果优异，弥补了现有研究的不足。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在理解和生成文本方面表现优异，但在处理思想深刻、叙事复杂的文学作品时，难以做出专业的文学批评。当前缺乏结构化的分析工具和相关数据集来辅助LLMs提升文学批评能力。

Method: 1. 提出GLASS分析框架，以Greimas符号学方格为核心。2. 构建首个基于该框架的数据集，涵盖48部作品的详细分析。3. 引入基于“LLM判官”范式的量化评测体系。4. 通过与专家批评对比，验证框架有效性。5. 应用GLASS于39部经典作品，生成创新分析。

Result: GLASS框架实现对叙事结构和深层意义的快速剖析。实验显示，使用GLASS的LLM在多部作品和不同模型上的分析表现优异，且能产生高质量的原创性文学批评，有助于文学研究和教育领域。

Conclusion: 本研究提出的GLASS框架能够显著提升大语言模型（LLMs）进行深度文学批评的能力，实验结果显示其在多个作品及模型上的高性能表现，并成功产出高质量分析，填补了当前的研究空白。

Abstract: Large Language Models (LLMs) excel in understanding and generating text but
struggle with providing professional literary criticism for works with profound
thoughts and complex narratives. This paper proposes GLASS (Greimas Literary
Analysis via Semiotic Square), a structured analytical framework based on
Greimas Semiotic Square (GSS), to enhance LLMs' ability to conduct in-depth
literary analysis. GLASS facilitates the rapid dissection of narrative
structures and deep meanings in narrative works. We propose the first dataset
for GSS-based literary criticism, featuring detailed analyses of 48 works. Then
we propose quantitative metrics for GSS-based literary criticism using the
LLM-as-a-judge paradigm. Our framework's results, compared with expert
criticism across multiple works and LLMs, show high performance. Finally, we
applied GLASS to 39 classic works, producing original and high-quality analyses
that address existing research gaps. This research provides an AI-based tool
for literary research and education, offering insights into the cognitive
mechanisms underlying literary engagement.

</details>


### [43] [Leveraging LLM-Assisted Query Understanding for Live Retrieval-Augmented Generation](https://arxiv.org/abs/2506.21384)
*Guanting Dong,Xiaoxi Li,Yuyao Zhang,Mengjie Deng*

Main category: cs.CL

TL;DR: Omni-RAG提出了一套全面的LLM辅助流程，通过深度查询理解、意图感知检索和多层重排序，极大提升了RAG系统处理噪声和多意图的现实查询能力。


<details>
  <summary>Details</summary>
Motivation: 当前的检索增强生成（RAG）系统在真实环境中常常面对用户查询噪音大、含义模糊、包含多个意图等复杂问题。然而，大多数RAG系统主要是在更“干净”的数据上训练和评估，因此在处理复杂、真实的用户输入时表现欠佳。本文旨在提升RAG在开放域真实场景下的适应性和鲁棒性。

Method: 提出Omni-RAG框架，利用大语言模型（LLM）辅助的多阶段查询理解机制。该框架包含三大模块：1）基于LLM的深度查询理解与拆分，纠正查询中的拼写错误并将多意图拆解成结构化子查询；2）意图感知的知识检索，对每个子查询进行独立检索，并聚合结果；3）文档重排序与回答生成，借助reranker优化文档选择，最终用LLM依据chain-of-thought生成最终答案。

Result: Omni-RAG可以显著提升RAG系统在实际复杂、噪声环境下的表现，能高效、鲁棒地应对真实场景中的多意图和噪声查询，缩小了现有RAG与真实应用需求之间的差距。框架对SIGIR 2025 LiveRAG Challenge等实际场景具有很强的适用性。

Conclusion: Omni-RAG有效增强了RAG系统处理现实用户输入的能力，针对多意图、噪声、复杂查询表现更优，为RAG技术在实际开放域场景中落地提供了有力支撑。

Abstract: Real-world live retrieval-augmented generation (RAG) systems face significant
challenges when processing user queries that are often noisy, ambiguous, and
contain multiple intents. While RAG enhances large language models (LLMs) with
external knowledge, current systems typically struggle with such complex
inputs, as they are often trained or evaluated on cleaner data. This paper
introduces Omni-RAG, a novel framework designed to improve the robustness and
effectiveness of RAG systems in live, open-domain settings. Omni-RAG employs
LLM-assisted query understanding to preprocess user inputs through three key
modules: (1) Deep Query Understanding and Decomposition, which utilizes LLMs
with tailored prompts to denoise queries (e.g., correcting spelling errors) and
decompose multi-intent queries into structured sub-queries; (2) Intent-Aware
Knowledge Retrieval, which performs retrieval for each sub-query from a corpus
(i.e., FineWeb using OpenSearch) and aggregates the results; and (3) Reranking
and Generation, where a reranker (i.e., BGE) refines document selection before
a final response is generated by an LLM (i.e., Falcon-10B) using a
chain-of-thought prompt. Omni-RAG aims to bridge the gap between current RAG
capabilities and the demands of real-world applications, such as those
highlighted by the SIGIR 2025 LiveRAG Challenge, by robustly handling complex
and noisy queries.

</details>


### [44] [Domain Knowledge-Enhanced LLMs for Fraud and Concept Drift Detection](https://arxiv.org/abs/2506.21443)
*Ali Şenol,Garima Agrawal,Huan Liu*

Main category: cs.CL

TL;DR: 本文提出了融合领域知识和概念漂移检测的LLM系统，高效识别多类型虚假对话和意图变化，在高风险场景下，准确率和解释性远超传统方法。


<details>
  <summary>Details</summary>
Motivation: 当前动态平台上，由于语言模式和“概念漂移”不断演变，检测虚假对话变得越来越困难。概念漂移会掩盖恶意意图或模仿正常对话，对恶意行为的准确识别带来挑战。同时，大型语言模型虽然在自然语言任务上表现出色，但在高风险场景下容易因语境歧义和幻觉问题导致失效。

Method: 提出了一个结合领域知识增强的大型语言模型（DK-LLM）框架，包括三个主要部分：1）DK-LLM模块用于检测虚假或欺诈对话；2）漂移检测单元（OCDD）判断语义漂移是否发生；3）第二个DK-LLM模块对漂移进行良性或欺诈性质的分类。

Result: 实验首先在虚假评论数据上验证了领域知识的价值，随后将完整框架应用于包含多种欺诈和垃圾攻击类型的多轮对话数据集SEConvo。结果显示，该系统在检测虚假对话以及区分漂移性质方面表现出高准确率，LLaMA实现的分类准确率达98%。

Conclusion: 所提出的DK-LLM与漂移检测一体化框架能极大提升高风险NLP应用中的性能、可解释性与鲁棒性，对比零样本模型显著优于后者。

Abstract: Detecting deceptive conversations on dynamic platforms is increasingly
difficult due to evolving language patterns and Concept Drift (CD)\-i.e.,
semantic or topical shifts that alter the context or intent of interactions
over time. These shifts can obscure malicious intent or mimic normal dialogue,
making accurate classification challenging. While Large Language Models (LLMs)
show strong performance in natural language tasks, they often struggle with
contextual ambiguity and hallucinations in risk\-sensitive scenarios. To
address these challenges, we present a Domain Knowledge (DK)\-Enhanced LLM
framework that integrates pretrained LLMs with structured, task\-specific
insights to perform fraud and concept drift detection. The proposed
architecture consists of three main components: (1) a DK\-LLM module to detect
fake or deceptive conversations; (2) a drift detection unit (OCDD) to determine
whether a semantic shift has occurred; and (3) a second DK\-LLM module to
classify the drift as either benign or fraudulent. We first validate the value
of domain knowledge using a fake review dataset and then apply our full
framework to SEConvo, a multiturn dialogue dataset that includes various types
of fraud and spam attacks. Results show that our system detects fake
conversations with high accuracy and effectively classifies the nature of
drift. Guided by structured prompts, the LLaMA\-based implementation achieves
98\% classification accuracy. Comparative studies against zero\-shot baselines
demonstrate that incorporating domain knowledge and drift awareness
significantly improves performance, interpretability, and robustness in
high\-stakes NLP applications.

</details>


### [45] [skLEP: A Slovak General Language Understanding Benchmark](https://arxiv.org/abs/2506.21508)
*Marek Šuppa,Andrej Ridzik,Daniel Hládek,Tomáš Javůrek,Viktória Ondrejová,Kristína Sásiková,Martin Tamajka,Marián Šimko*

Main category: cs.CL

TL;DR: 本文提出了首个面向斯洛伐克语自然语言理解的全面基准skLEP，覆盖九项任务、丰富数据集和评测工具，并公开各项资源，带动了斯洛伐克语NLP领域的研究和发展。


<details>
  <summary>Details</summary>
Motivation: 斯洛伐克语自然语言理解（NLU）模型的评测缺乏系统性和全面性，现有资源以英语为主，针对斯洛伐克语的公开基准和数据集非常有限，阻碍了该语言相关NLP研究的发展。

Method: 构建了skLEP这个专为斯洛伐克语NLU设计的综合基准，涵盖九类不同层级的任务，并撰写了新数据集以及将已有英文NLU基准翻译为斯洛伐克语。还对多种斯洛伐克语专用、多语言、英文预训练模型进行了系统评测。此外，开源了全套基准数据、模型微调和评测工具包、以及排行榜。

Result: skLEP成为首个覆盖广泛任务、系统评测斯洛伐克语NLU模型的基准工具，带来了丰富原始数据集、评测结果和开源工具，为社区提供了重要资源，促进了可重复性与后续研究。

Conclusion: skLEP显著推动了斯洛伐克语自然语言理解的研究，通过综合基准、数据和工具的发布，为该领域提供了实验标准，填补了资源空白。

Abstract: In this work, we introduce skLEP, the first comprehensive benchmark
specifically designed for evaluating Slovak natural language understanding
(NLU) models. We have compiled skLEP to encompass nine diverse tasks that span
token-level, sentence-pair, and document-level challenges, thereby offering a
thorough assessment of model capabilities. To create this benchmark, we curated
new, original datasets tailored for Slovak and meticulously translated
established English NLU resources. Within this paper, we also present the first
systematic and extensive evaluation of a wide array of Slovak-specific,
multilingual, and English pre-trained language models using the skLEP tasks.
Finally, we also release the complete benchmark data, an open-source toolkit
facilitating both fine-tuning and evaluation of models, and a public
leaderboard at https://github.com/slovak-nlp/sklep in the hopes of fostering
reproducibility and drive future research in Slovak NLU.

</details>


### [46] [Text2Cypher Across Languages: Evaluating Foundational Models Beyond English](https://arxiv.org/abs/2506.21445)
*Makbule Gulcin Ozsoy,William Tai*

Main category: cs.CL

TL;DR: 本文建立多语言Text2Cypher测试集，发现主流大模型在非英文环境下表现显著下降，尤其是土耳其语。任务指令翻译影响有限，强调多语言模型开发的重要性。


<details>
  <summary>Details</summary>
Motivation: 虽然自然语言到数据库查询的接口获得发展，但多数仅支持英文，其他语言的研究和评估极为有限。本研究希望填补多语言下Text2Cypher模型评估的空白。

Method: 通过搭建多语言测试集（英文问题翻译为西班牙语和土耳其语，Cypher查询保持不变），标准化评测多种基础大模型在Text2Cypher任务上的表现，并比较不同语言和指令翻译的影响。

Result: 模型在英语上表现最佳，西班牙语次之，土耳其语最差。推测此差异与训练数据量及语言本身特性有关。将任务指令翻译成目标语言对最终表现影响微小。该结果提示未来需要投入更多多语言环境的模型评估和开发。

Conclusion: 多语言环境下，大型语言模型在Text2Cypher任务上表现呈现英语>西班牙语>土耳其语的趋势，提示当前模型更适合英语。翻译任务指令对评估结果影响不大。

Abstract: Recent advances in large language models have enabled natural language
interfaces that translate user questions into database queries, such as
Text2SQL, Text2SPARQL, and Text2Cypher. While these interfaces enhance database
accessibility, most research today focuses solely on English, with limited
evaluation in other languages. This paper investigates the performance of
foundational LLMs on the Text2Cypher task across multiple languages. We create
and release a multilingual test set by translating English questions into
Spanish and Turkish while preserving the original Cypher queries, enabling fair
cross-lingual comparison. We evaluate multiple foundational models using
standardized prompts and metrics. Our results show a consistent performance
pattern: highest on English, then Spanish, and lowest on Turkish. We attribute
this to differences in training data availability and linguistic
characteristics. Additionally, we explore the impact of translating task
prompts into Spanish and Turkish. Results show little to no change in
evaluation metrics, suggesting prompt translation has minor impact. Our
findings highlight the need for more inclusive evaluation and development in
multilingual query generation. Future work includes schema localization and
fine-tuning across diverse languages.

</details>


### [47] [Potemkin Understanding in Large Language Models](https://arxiv.org/abs/2506.21521)
*Marina Mancoridis,Bec Weeks,Keyon Vafa,Sendhil Mullainathan*

Main category: cs.CL

TL;DR: 本研究提出了识别和量化大语言模型“Potemkin理解”（即伪装成理解但实则非人类式理解）的新框架与方法，结果显示这种缺陷在各种模型和任务中极为普遍，并带来对其评测有效性的严肃质疑。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型常用基准测试来评估能力，但该测试所得到的优秀成绩是否真的表明模型具备类似人类的理解能力仍值得质疑。动机在于分析：如果模型对概念的误解与人类不一致，那么基于这些测试得出的能力推断可能只是幻象。

Method: 作者提出了两种量化Potemkin存在性的方法：一种是设计了专门的基准测试覆盖三个领域，以检测模型的“人类式”或非“人类式”误解；另一种是通用流程，用于给出Potemkin存在性的下界。通过这两种方法系统性地分析了模型的理解缺陷。

Result: 结果表明，Potemkin理解在多种模型、任务和领域都普遍存在。这些失败不仅仅体现在对概念的理解错误，还表现为内部概念表征的不一致性和深层次缺陷。

Conclusion: 本文发现，Potemkin理解（对应于表面的、非人类式的理解）在不同的模型、任务和领域中普遍存在。LLM在基准测试上取得好成绩，并不一定意味着其真正理解了这些任务或概念。

Abstract: Large language models (LLMs) are regularly evaluated using benchmark
datasets. But what justifies making inferences about an LLM's capabilities
based on its answers to a curated set of questions? This paper first introduces
a formal framework to address this question. The key is to note that the
benchmarks used to test LLMs -- such as AP exams -- are also those used to test
people. However, this raises an implication: these benchmarks are only valid
tests if LLMs misunderstand concepts in ways that mirror human
misunderstandings. Otherwise, success on benchmarks only demonstrates potemkin
understanding: the illusion of understanding driven by answers irreconcilable
with how any human would interpret a concept. We present two procedures for
quantifying the existence of potemkins: one using a specially designed
benchmark in three domains, the other using a general procedure that provides a
lower-bound on their prevalence. We find that potemkins are ubiquitous across
models, tasks, and domains. We also find that these failures reflect not just
incorrect understanding, but deeper internal incoherence in concept
representations.

</details>


### [48] [Aligning Spoken Dialogue Models from User Interactions](https://arxiv.org/abs/2506.21463)
*Anne Wu,Laurent Mazaré,Neil Zeghidour,Alexandre Défossez*

Main category: cs.CL

TL;DR: 提出一种新颖的偏好对齐方法，以大规模对话数据和AI反馈微调语音对话模型，有效提升了语音对话系统的表现，突显了平衡多种交互动态的重要性。


<details>
  <summary>Details</summary>
Motivation: 当前偏好学习方法主要集中于基于文本的语言模型，难以直接应用于实时语音交互的复杂场景，如插话、中断等动态与无明确分割的说话轮次。因此，需要开发新的方法提升语音对话系统性能。

Method: 作者构建了一个包含超过150,000个人工智能反馈标注的语音对话偏好对数据集，涵盖语言内容和时间上下文偏好，并通过离线偏好对齐方法微调了一个全双工自回归语音到语音模型。

Result: 实验结果表明，基于通用会话的反馈可持续性提升语音对话模型的事实性、安全性及上下文对齐能力。微调后的模型通过全面的人类评估，被证明在多轮对话中有实质性改善。

Conclusion: 研究结果强调在自然实时语音对话系统中，各类对话动态的良好平衡非常重要，这为后续相关系统的设计和优化提供了实证依据。

Abstract: We propose a novel preference alignment framework for improving spoken
dialogue models on real-time conversations from user interactions. Current
preference learning methods primarily focus on text-based language models, and
are not directly suited to the complexities of real-time speech interactions,
with richer dynamics (e.g. interruption, interjection) and no explicit
segmentation between speaker turns.We create a large-scale dataset of more than
150,000 preference pairs from raw multi-turn speech conversations, annotated
with AI feedback, to cover preferences over both linguistic content and
temporal context variations. We leverage offline alignment methods to finetune
a full-duplex autoregressive speech-to-speech model. Extensive experiments
demonstrate that feedback on generic conversations can be consistently
effective in improving spoken dialogue models to produce more factual, safer
and more contextually aligned interactions. We deploy the finetuned model and
conduct holistic human evaluations to assess the impact beyond single-turn
conversations. Our findings shed light on the importance of a well-calibrated
balance among various dynamics, crucial for natural real-time speech dialogue
systems.

</details>


### [49] [TopK Language Models](https://arxiv.org/abs/2506.21468)
*Ryosuke Takahashi,Tatsuro Inaba,Kentaro Inui,Benjamin Heinzerling*

Main category: cs.CL

TL;DR: 文章提出对Transformer结构改造，直接以TopK激活获得稀疏和可解释特征，无需传统SAE后训练，实验验证模型解析性和能力不减，为理解和控制语言模型提供了新方法。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏自编码器（SAE）用于深度剖析transformer类语言模型的激活空间，但因需事后训练、特征稳定性差、难以归因导致解释能力有限。作者希望通过结构创新，提高特征解析的稳定性与归因确定性。

Method: 作者提出在transformer架构中选定层集成TopK激活函数，直接获得稀疏、可解释的表示，省去事后训练SAE的步骤，并对比实验验证了该方法的有效性，包括神经元干预和分层分析。

Result: 引入TopK激活的模型在模型规模、计算效率和可解释性之间取得良好平衡，可拓展神经元干预、分层追踪等功能，实验显示其稀疏特征稳定、可重复，为后续解释性和可控性研究奠定了基础。

Conclusion: TopK LMs通过在特定层引入TopK激活函数，使模型的隐藏状态等同于TopK稀疏自编码器的潜在特征，提升了解析性且不影响原有能力，成为理解语言模型学习与表征概念的稳定可靠工具。

Abstract: Sparse autoencoders (SAEs) have become an important tool for analyzing and
interpreting the activation space of transformer-based language models (LMs).
However, SAEs suffer several shortcomings that diminish their utility and
internal validity. Since SAEs are trained post-hoc, it is unclear if the
failure to discover a particular concept is a failure on the SAE's side or due
to the underlying LM not representing this concept. This problem is exacerbated
by training conditions and architecture choices affecting which features an SAE
learns. When tracing how LMs learn concepts during training, the lack of
feature stability also makes it difficult to compare SAEs features across
different checkpoints. To address these limitations, we introduce a
modification to the transformer architecture that incorporates a TopK
activation function at chosen layers, making the model's hidden states
equivalent to the latent features of a TopK SAE. This approach eliminates the
need for post-hoc training while providing interpretability comparable to SAEs.
The resulting TopK LMs offer a favorable trade-off between model size,
computational efficiency, and interpretability. Despite this simple
architectural change, TopK LMs maintain their original capabilities while
providing robust interpretability benefits. Our experiments demonstrate that
the sparse representations learned by TopK LMs enable successful steering
through targeted neuron interventions and facilitate detailed analysis of
neuron formation processes across checkpoints and layers. These features make
TopK LMs stable and reliable tools for understanding how language models learn
and represent concepts, which we believe will significantly advance future
research on model interpretability and controllability.

</details>


### [50] [Bridging Offline and Online Reinforcement Learning for LLMs](https://arxiv.org/abs/2506.21495)
*Jack Lanchantin,Angelica Chen,Janice Lan,Xian Li,Swarnadeep Saha,Tianlu Wang,Jing Xu,Ping Yu,Weizhe Yuan,Jason E Weston,Sainbayar Sukhbaatar,Ilia Kulikov*

Main category: cs.CL

TL;DR: 本文系统评估了大语言模型在不同微调阶段（离线、半在线、在线）下强化学习优化方法的效果，发现在线及半在线方法显著优于离线，且多任务训练进一步提升性能。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型广泛应用，离线到在线的微调方法在不同任务中的效果及最优实现策略尚不明确，特别是如何在可验证与不可验证任务之间实现更好的泛化和性能提升。

Method: 比较了在线、半在线的Direct Preference Optimization与Group Reward Policy Optimization目标函数，在可验证数学和不可验证指令跟随任务上进行实验，并对训练动态和超参数选择进行了系统分析。

Result: 在线和半在线方法在各项任务上都远超离线方法，两类目标函数收敛性和效果相似。多任务（可验证+不可验证）训练进一步提升了模型的泛化能力和任务表现。

Conclusion: 论文发现，在大语言模型微调过程中，无论是使用在线Direct Preference Optimization还是Group Reward Policy Optimization，都表现出与半在线版本相似的性能和收敛性，并且明显优于离线方法。同时，多任务联合训练能够提升可验证和不可验证任务的整体性能。

Abstract: We investigate the effectiveness of reinforcement learning methods for
finetuning large language models when transitioning from offline to semi-online
to fully online regimes for both verifiable and non-verifiable tasks. Our
experiments cover training on verifiable math as well as non-verifiable
instruction following with a set of benchmark evaluations for both. Across
these settings, we extensively compare online and semi-online Direct Preference
Optimization and Group Reward Policy Optimization objectives, and surprisingly
find similar performance and convergence between these variants, which all
strongly outperform offline methods. We provide a detailed analysis of the
training dynamics and hyperparameter selection strategies to achieve optimal
results. Finally, we show that multi-tasking with verifiable and non-verifiable
rewards jointly yields improved performance across both task types.

</details>


### [51] [Enhancing User Engagement in Socially-Driven Dialogue through Interactive LLM Alignments](https://arxiv.org/abs/2506.21497)
*Jiashuo Wang,Kaitao Song,Chunpu Xu,Changhe Song,Yang Xiao,Dongsheng Li,Lili Qiu,Wenjie Li*

Main category: cs.CL

TL;DR: 通过用户模拟与对话发展反馈优化大模型，以直接提升社交型对话系统里的用户参与感，实验证明该策略能有效提高互动质量。


<details>
  <summary>Details</summary>
Motivation: 在社交型对话中，提升用户参与度至关重要，但以往方法主要关注知识推理或对话规划，与用户参与度的直接关系尚不明确，不能保证提升用户互动。作者希望更直接提升用户在对话系统中的真实参与度。

Method: 作者提出用用户对对话意图的反应作为用户参与度的直接衡量指标，并以此作为奖励信号，引导大模型（LLM）互动优化。为此，开发了一个用户模拟器，通过i×MCTS（交互式蒙特卡洛树搜索）与目标LLM交互，收集高、低质量体验对，并利用直接偏好优化（DPO）对模型进行对齐。

Result: 在两个社交型对话场景（情感支持对话和正向劝说）上实验证明，该方法有效提升了互动式LLM的用户参与度。

Conclusion: 通过利用用户模拟与未来对话发展的信号，结合新指标和优化方式，能显著增强社交型对话中用户的参与互动水平。

Abstract: Enhancing user engagement through interactions plays an essential role in
socially-driven dialogues. While prior works have optimized models to reason
over relevant knowledge or plan a dialogue act flow, the relationship between
user engagement and knowledge or dialogue acts is subtle and does not guarantee
user engagement in socially-driven dialogues. To this end, we enable
interactive LLMs to learn user engagement by leveraging signals from the future
development of conversations. Specifically, we adopt a more direct and relevant
indicator of user engagement, i.e., the user's reaction related to dialogue
intention after the interaction, as a reward to align interactive LLMs. To
achieve this, we develop a user simulator to interact with target interactive
LLMs and explore interactions between the user and the interactive LLM system
via \textit{i$\times$MCTS} (\textit{M}onte \textit{C}arlo \textit{T}ree
\textit{S}earch for \textit{i}nteraction). In this way, we collect a dataset
containing pairs of higher and lower-quality experiences using
\textit{i$\times$MCTS}, and align interactive LLMs for high-level user
engagement by direct preference optimization (DPO) accordingly. Experiments
conducted on two socially-driven dialogue scenarios (emotional support
conversations and persuasion for good) demonstrate that our method effectively
enhances user engagement in interactive LLMs.

</details>


### [52] [Data Efficacy for Language Model Training](https://arxiv.org/abs/2506.21545)
*Yalun Dai,Yangyu Huang,Xin Zhang,Wenshan Wu,Chong Li,Wenhui Lu,Shijie Cao,Li Dong,Scarlett Li*

Main category: cs.CL

TL;DR: 本文提出了一个关注数据有效性的新范式（DELT），通过优化训练数据评分和排序，有效提升了语言模型的性能，实现了比单纯数据筛选更好的表现。


<details>
  <summary>Details</summary>
Motivation: 目前对于语言模型的训练，数据的选择和用量（即数据效率）获得了大量关注，而如何组织训练数据、最大化其效用（即数据有效性）这一领域则尚未被充分探索。

Method: 本文提出了DELT范式，系统性地将数据评分（Data Scoring）、数据选择（Data Selection）、数据排序（Data Ordering）引入语言模型的训练流程，并具体提出了Learnability-Quality Scoring（LQS）用于数据评分，以及Folding Ordering（FO）用于数据排序。通过这些新方法优化训练数据的组织结构。

Result: 实验表明：1）DELT的不同实例均能在不增加数据量和模型规模的前提下提升语言模型表现；2）LQS结合Folding的数据排序带来最显著性能提升；3）数据有效性与传统的数据效率可以结合获得更优结果。

Conclusion: 数据有效性是提升语言模型训练表现的关键新方向，通过优化训练数据的组织和使用方式可以在不扩展模型规模的情况下获得显著增益，值得学界关注和深入研究。

Abstract: Data is fundamental to the training of language models (LM). Recent research
has been dedicated to data efficiency, which aims to maximize performance by
selecting a minimal or optimal subset of training data. Techniques such as data
filtering, sampling, and selection play a crucial role in this area. To
complement it, we define Data Efficacy, which focuses on maximizing performance
by optimizing the organization of training data and remains relatively
underexplored. This work introduces a general paradigm, DELT, for considering
data efficacy in LM training, which highlights the significance of training
data organization. DELT comprises three components: Data Scoring, Data
Selection, and Data Ordering. Among these components, we design
Learnability-Quality Scoring (LQS), as a new instance of Data Scoring, which
considers both the learnability and quality of each data sample from the
gradient consistency perspective. We also devise Folding Ordering (FO), as a
novel instance of Data Ordering, which addresses issues such as model
forgetting and data distribution bias. Comprehensive experiments validate the
data efficacy in LM training, which demonstrates the following: Firstly,
various instances of the proposed DELT enhance LM performance to varying
degrees without increasing the data scale and model size. Secondly, among these
instances, the combination of our proposed LQS for data scoring and Folding for
data ordering achieves the most significant improvement. Lastly, data efficacy
can be achieved together with data efficiency by applying data selection.
Therefore, we believe that data efficacy is a promising foundational area in LM
training.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [53] [A generalised framework for phase field-based modelling of coupled problems: application to thermo-mechanical fracture, hydraulic fracture, hydrogen embrittlement and corrosion](https://arxiv.org/abs/2506.20763)
*Y. Navidtehrani,C. Betegón,E. Martínez-Pañeda*

Main category: cs.CE

TL;DR: 本文提出一个将相场和多物理场建模结合、适用于多种结构完整性问题的通用有限元实现框架。该方法适配商用有限元软件，仅需简单集成，已开源并验证有良好准确性和广泛适用性。


<details>
  <summary>Details</summary>
Motivation: 传统的结构完整性问题常常面临多物理场耦合难以统一建模与求解的困境，尤其是在实际工程涉及热、力、化学等多种物理过程时，现有方法可移植性较差、实现复杂。作者希望提出一种通用、易于实现且可推广到多种工程应用的问题处理方案。

Method: 本文提出了一种结合相场方法与多物理场建模的通用耦合结构完整性问题求解框架。该方法利用了热传导方程的通用性优势，使其方便集成于商用有限元软件（如Abaqus），只需在积分点级别编程实现。具体通过自定义UMAT与UMATHT子程序实现。该方法被应用于四类典型工程问题，包括热力断裂、水力压裂、氢致开裂、金属腐蚀，覆盖2D与3D算例。

Result: 不同耦合结构完整性问题的数值模拟结果显示，该方法与实验数据、已有数值和解析解吻合良好。提出的用户子程序已开放共享。

Conclusion: 所提出的相场与多物理场耦合建模通用方法，能够高效、准确地模拟涉及多场耦合作用的结构完整性问题，在实际工程与科学研究中具有广泛适用性和推广价值。实现简单、移植性强。

Abstract: We present a novel, generalised formulation to treat coupled structural
integrity problems by combining phase field and multi-physics modelling. The
approach exploits the versatility of the heat transfer equation and is
therefore well suited to be adopted in commercial finite element packages,
requiring only integration point-level implementation. This aspect is
demonstrated here by implementing coupled, multi-variable phenomena through
simple \texttt{UMAT} and \texttt{UMATHT} subroutines in the finite element
package \texttt{Abaqus}. The generalised theoretical and computational
framework presented is particularised to four problems of engineering and
scientific relevance: thermo-mechanical fracture, hydraulic fracture,
hydrogen-assisted cracking and metallic corrosion. 2D and 3D problems are
considered. The results reveal a very good agreement with experimental data,
and existing numerical and analytical solutions.The user subroutines developed
are made freely available at https://mechmat.web.ox.ac.uk/codes.

</details>


### [54] [A Hereditary Integral, Transient Network Approach to Modeling Permanent Set and Viscoelastic Response in Polymers](https://arxiv.org/abs/2506.20773)
*Stephen T. Castonguay,Joshua B. Fernandes,Michael A. Puso,Sylvie Aubry*

Main category: cs.CE

TL;DR: 论文提出了通过递推关系实现高效粘弹性与永久定形建模的新数值框架，适用于多种材料模型，并验证了其在复杂载荷下的高效和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有对高分子材料粘弹性和永久定形的数值建模方法普遍效率较低，难以高效处理复杂载荷历史下的应答，需要更高效的数值框架。

Method: 基于瞬态网络理论的遗传积分形式，提出了一种高效数值建模方法。通过对不同网络中高分子链自然平衡态的区分，链条不断从旧网络断开并在零应力状态下重新连接到新网络。提出能量核的分解，从而得到递推关系，无需对整个时间历史积分。方法适用于高可压缩和近不可压缩材料，采用neo-Hookean, Blatz-Ko, Yeoh, Ogden-Hill材料模型。

Result: 多种算例表明，该方法能有效模拟速率相关响应以及复杂载荷历史下的残余应变。

Conclusion: 该数值框架为高分子材料的粘弹性与永久定形计算提供了高效且通用的方法，显著减少了计算量且能准确描述复杂力学历史影响。

Abstract: An efficient numerical framework is presented for modeling viscoelasticity
and permanent set of polymers. It is based on the hereditary integral form of
transient network theory, in which polymer chains belong to distinct networks
each with different natural equilibrium states. Chains continually detach from
previously formed networks and reattach to new networks in a state of zero
stress. The free energy of these networks is given in terms of the deformation
gradient relative to the configuration at which the network was born. A
decomposition of the kernel for various free energies allows for a recurrence
relationship to be established, bypassing the need to integrate over all time
history. The technique is established for both highly compressible and nearly
incompressible materials through the use of neo-Hookean, Blatz-Ko, Yeoh, and
Ogden-Hill material models. Multiple examples are presented showing the ability
to handle rate-dependent response and residual strains under complex loading
histories.

</details>


### [55] [Counterfactual Voting Adjustment for Quality Assessment and Fairer Voting in Online Platforms with Helpfulness Evaluation](https://arxiv.org/abs/2506.21362)
*Chang Liu,Yixin Wang,Moontae Lee*

Main category: cs.CE

TL;DR: 本文提出了CVA因果方法，修正了网络投票中的位置和从众偏见，实现了更准确公正的内容质量评估，并优于传统投票和非因果模型方法。


<details>
  <summary>Details</summary>
Motivation: 在网络平台上，信息高质量访问至关重要。然而，用户对内容的有用性投票易受位置和先前投票的影响（偏见），导致内容排名不公平。该研究旨在解决有用性投票中存在的偏见问题，实现更公正的信息质量评估。

Method: 提出了一种反事实投票调整（CVA）因果框架，能够建模不同内容展示位置和从众效应（herding bias）对投票的影响，对投票结果进行因果调整。方法通过预实验、半合成实验与真实实验，验证了CVA的有效性。

Result: CVA模型有效捕捉了位置和从众偏见，能够准确还原内容的真实质量。在真实实验中，基于CVA学习到的质量对内容重排序，与用户主观感受和GPT-4o评估结果一致性更高，优于传统汇总投票方法及没有因果推断的模型。此外，该方法还对StackExchange 120个社区的专家用户群体行为进行了比较性分析。

Conclusion: CVA因果框架能更公平、准确地评估网络平台的内容质量，改善传统投票机制的偏见问题，并为用户行为的深入分析提供了新工具。

Abstract: Efficient access to high-quality information is vital for online platforms.
To promote more useful information, users not only create new content but also
evaluate existing content, often through helpfulness voting. Although
aggregated votes help service providers rank their user content, these votes
are often biased by disparate accessibility per position and the cascaded
influence of prior votes. For a fairer assessment of information quality, we
propose the Counterfactual Voting Adjustment (CVA), a causal framework that
accounts for the context in which individual votes are cast. Through
preliminary and semi-synthetic experiments, we show that CVA effectively models
the position and herding biases, accurately recovering the predefined content
quality. In a real experiment, we demonstrate that reranking content based on
the learned quality by CVA exhibits stronger alignment with both user sentiment
and quality evaluation assessed by GPT-4o, outperforming system rankings based
on aggregated votes and model-based rerankings without causal inference. Beyond
the individual quality inference, our embeddings offer comparative insights
into the behavioral dynamics of expert user groups across 120 major
StackExchange communities.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [56] [Our Coding Adventure: Using LLMs to Personalise the Narrative of a Tangible Programming Robot for Preschoolers](https://arxiv.org/abs/2506.20982)
*Martin Ruskov*

Main category: cs.CY

TL;DR: 作者提出通过LLM为编程机器人Cubetto定制学前儿童个性化故事的方法，强调教师辅助作用且无须儿童直接接触AI。方法可复现且模型无关，但存在一致性和幻觉问题，有部分解决。该方法适合学前班教学，有望进一步推广。


<details>
  <summary>Details</summary>
Motivation: 在当前教育领域，合理使用大语言模型（LLMs）存在风险，尤其是面对对屏幕时间敏感的幼儿。作者希望探索既能利用LLMs优势又确保安全的应用方式。

Method: 利用可编程机器人Cubetto和大语言模型，开发为学前儿童个性化设计故事的流程，采用行动研究法，对5种不同LLM进行测试，总结操作流程与评估环节。

Result: 提出了一种快速为Cubetto开发游戏故事的流程，证明了其可复现性和模型无关性，同时详细记录了材料、流程、学生体验与学习结果。实验中发现一致性和幻觉问题，并记录了解决尝试。孩子并不直接接触LLM，而是教师利用其成果。

Conclusion: 所提出的方法能够帮助教师为幼儿快速开发个性化故事，对学前教育具有应用前景。后续计划在真实教育环境中进一步验证。

Abstract: Finding balanced ways to employ Large Language Models (LLMs) in education is
a challenge due to inherent risks of poor understanding of the technology and
of a susceptible audience. This is particularly so with younger children, who
are known to have difficulties with pervasive screen time. Working with a
tangible programming robot called Cubetto, we propose an approach to benefit
from the capabilities of LLMs by employing such models in the preparation of
personalised storytelling, necessary for preschool children to get accustomed
to the practice of commanding the robot. We engage in action research to
develop an early version of a formalised process to rapidly prototype game
stories for Cubetto. Our approach has both reproducible results, because it
employs open weight models, and is model-agnostic, because we test it with 5
different LLMs. We document on one hand the process, the used materials and
prompts, and on the other the learning experience and outcomes. We deem the
generation successful for the intended purposes of using the results as a
teacher aid. Testing the models on 4 different task scenarios, we encounter
issues of consistency and hallucinations and document the corresponding
evaluation process and attempts (some successful and some not) to overcome
these issues. Importantly, the process does not expose children to LLMs
directly. Rather, the technology is used to help teachers easily develop
personalised narratives on children's preferred topics. We believe our method
is adequate for preschool classes and we are planning to further experiment in
real-world educational settings.

</details>
